2026-02-10 02:27:36,649 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/log_20260210_022736.log'에 저장됩니다.
2026-02-10 02:27:36,651 - INFO - ==================================================
2026-02-10 02:27:36,651 - INFO - config.yaml:
2026-02-10 02:27:36,651 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: xie2019
  use_l1_pruning: true
  pruning_params_target: 0.047585

2026-02-10 02:27:36,651 - INFO - ==================================================
2026-02-10 02:27:36,678 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 02:27:36,678 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 02:27:36,678 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 02:27:36,681 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 02:27:36,682 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 02:27:36,682 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 02:27:36,682 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 02:27:36,786 - INFO - ==================================================
2026-02-10 02:27:36,786 - INFO - 모델 파라미터 수:
2026-02-10 02:27:36,786 - INFO -   - 총 파라미터: 9,160,194 개
2026-02-10 02:27:36,786 - INFO -   - 학습 가능한 파라미터: 9,160,194 개
2026-02-10 02:27:36,786 - INFO - ================================================================================
2026-02-10 02:27:36,786 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 02:27:36,786 - INFO - ================================================================================
2026-02-10 02:27:36,786 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 02:27:36,786 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 02:27:36,787 - INFO - ==================================================
2026-02-10 02:27:36,787 - INFO - train 모드를 시작합니다.
2026-02-10 02:27:36,787 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 02:27:36,787 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 02:27:36,787 - INFO - --------------------------------------------------
2026-02-10 02:27:36,787 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 02:27:39,486 - INFO - [Train] [1/10] | Loss: 0.5879 | Train Acc: 74.11%
2026-02-10 02:27:40,316 - INFO - [Valid] [1/10] | Loss: 0.5665 | Val Acc: 75.81%
2026-02-10 02:27:40,322 - INFO - [Metrics for 'abnormal'] | Precision: 0.7451 | Recall: 0.7261 | F1: 0.7355
2026-02-10 02:27:40,323 - INFO - [Metrics for 'normal'] | Precision: 0.7688 | Recall: 0.7857 | F1: 0.7772
2026-02-10 02:27:40,367 - INFO - [Best Model Saved] (val loss: 0.5665) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:27:40,368 - INFO - --------------------------------------------------
2026-02-10 02:27:40,368 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 02:27:42,547 - INFO - [Train] [2/10] | Loss: 0.5329 | Train Acc: 78.79%
2026-02-10 02:27:43,029 - INFO - [Valid] [2/10] | Loss: 0.5851 | Val Acc: 74.04%
2026-02-10 02:27:43,032 - INFO - [Metrics for 'abnormal'] | Precision: 0.6734 | Recall: 0.8535 | F1: 0.7528
2026-02-10 02:27:43,032 - INFO - [Metrics for 'normal'] | Precision: 0.8357 | Recall: 0.6429 | F1: 0.7267
2026-02-10 02:27:43,033 - INFO - --------------------------------------------------
2026-02-10 02:27:43,033 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 02:27:45,316 - INFO - [Train] [3/10] | Loss: 0.5199 | Train Acc: 80.06%
2026-02-10 02:27:45,887 - INFO - [Valid] [3/10] | Loss: 0.5468 | Val Acc: 75.22%
2026-02-10 02:27:45,890 - INFO - [Metrics for 'abnormal'] | Precision: 0.7062 | Recall: 0.7962 | F1: 0.7485
2026-02-10 02:27:45,890 - INFO - [Metrics for 'normal'] | Precision: 0.8025 | Recall: 0.7143 | F1: 0.7558
2026-02-10 02:27:45,933 - INFO - [Best Model Saved] (val loss: 0.5468) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:27:45,933 - INFO - --------------------------------------------------
2026-02-10 02:27:45,933 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 02:27:47,763 - INFO - [Train] [4/10] | Loss: 0.5111 | Train Acc: 80.13%
2026-02-10 02:27:48,492 - INFO - [Valid] [4/10] | Loss: 0.5612 | Val Acc: 76.11%
2026-02-10 02:27:48,497 - INFO - [Metrics for 'abnormal'] | Precision: 0.7836 | Recall: 0.6688 | F1: 0.7216
2026-02-10 02:27:48,497 - INFO - [Metrics for 'normal'] | Precision: 0.7463 | Recall: 0.8407 | F1: 0.7907
2026-02-10 02:27:48,498 - INFO - --------------------------------------------------
2026-02-10 02:27:48,499 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 02:27:50,658 - INFO - [Train] [5/10] | Loss: 0.4935 | Train Acc: 81.18%
2026-02-10 02:27:51,274 - INFO - [Valid] [5/10] | Loss: 0.5270 | Val Acc: 75.81%
2026-02-10 02:27:51,278 - INFO - [Metrics for 'abnormal'] | Precision: 0.7005 | Recall: 0.8344 | F1: 0.7616
2026-02-10 02:27:51,279 - INFO - [Metrics for 'normal'] | Precision: 0.8289 | Recall: 0.6923 | F1: 0.7545
2026-02-10 02:27:51,343 - INFO - [Best Model Saved] (val loss: 0.5270) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:27:51,343 - INFO - --------------------------------------------------
2026-02-10 02:27:51,343 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 02:27:54,620 - INFO - [Train] [6/10] | Loss: 0.4822 | Train Acc: 82.51%
2026-02-10 02:27:55,200 - INFO - [Valid] [6/10] | Loss: 0.5330 | Val Acc: 78.17%
2026-02-10 02:27:55,202 - INFO - [Metrics for 'abnormal'] | Precision: 0.7399 | Recall: 0.8153 | F1: 0.7758
2026-02-10 02:27:55,202 - INFO - [Metrics for 'normal'] | Precision: 0.8253 | Recall: 0.7527 | F1: 0.7874
2026-02-10 02:27:55,203 - INFO - --------------------------------------------------
2026-02-10 02:27:55,203 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 02:27:58,103 - INFO - [Train] [7/10] | Loss: 0.4717 | Train Acc: 83.41%
2026-02-10 02:27:59,020 - INFO - [Valid] [7/10] | Loss: 0.5120 | Val Acc: 79.65%
2026-02-10 02:27:59,025 - INFO - [Metrics for 'abnormal'] | Precision: 0.7716 | Recall: 0.7962 | F1: 0.7837
2026-02-10 02:27:59,025 - INFO - [Metrics for 'normal'] | Precision: 0.8192 | Recall: 0.7967 | F1: 0.8078
2026-02-10 02:27:59,090 - INFO - [Best Model Saved] (val loss: 0.5120) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:27:59,091 - INFO - --------------------------------------------------
2026-02-10 02:27:59,091 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 02:28:01,810 - INFO - [Train] [8/10] | Loss: 0.4531 | Train Acc: 83.78%
2026-02-10 02:28:02,550 - INFO - [Valid] [8/10] | Loss: 0.5259 | Val Acc: 76.70%
2026-02-10 02:28:02,555 - INFO - [Metrics for 'abnormal'] | Precision: 0.7143 | Recall: 0.8280 | F1: 0.7670
2026-02-10 02:28:02,555 - INFO - [Metrics for 'normal'] | Precision: 0.8280 | Recall: 0.7143 | F1: 0.7670
2026-02-10 02:28:02,556 - INFO - --------------------------------------------------
2026-02-10 02:28:02,556 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 02:28:04,972 - INFO - [Train] [9/10] | Loss: 0.4355 | Train Acc: 84.52%
2026-02-10 02:28:05,768 - INFO - [Valid] [9/10] | Loss: 0.5080 | Val Acc: 80.24%
2026-02-10 02:28:05,772 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8025 | F1: 0.7900
2026-02-10 02:28:05,772 - INFO - [Metrics for 'normal'] | Precision: 0.8249 | Recall: 0.8022 | F1: 0.8134
2026-02-10 02:28:05,827 - INFO - [Best Model Saved] (val loss: 0.5080) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:28:05,828 - INFO - --------------------------------------------------
2026-02-10 02:28:05,828 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 02:28:07,951 - INFO - [Train] [10/10] | Loss: 0.4317 | Train Acc: 85.42%
2026-02-10 02:28:08,717 - INFO - [Valid] [10/10] | Loss: 0.5075 | Val Acc: 79.94%
2026-02-10 02:28:08,720 - INFO - [Metrics for 'abnormal'] | Precision: 0.7764 | Recall: 0.7962 | F1: 0.7862
2026-02-10 02:28:08,720 - INFO - [Metrics for 'normal'] | Precision: 0.8202 | Recall: 0.8022 | F1: 0.8111
2026-02-10 02:28:08,789 - INFO - [Best Model Saved] (val loss: 0.5075) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:28:08,789 - INFO - ================================================================================
2026-02-10 02:28:08,789 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 02:28:08,789 - INFO - ================================================================================
2026-02-10 02:28:08,817 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'을(를) 불러왔습니다.
2026-02-10 02:28:08,818 - INFO - ================================================================================
2026-02-10 02:28:08,818 - INFO - 목표 파라미터 수 (0.0476M)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 02:28:08,818 - INFO - 원본 모델 파라미터: 9.1602M
2026-02-10 02:28:08,825 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:08,825 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:09,287 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 02:28:09,287 - INFO - ==================================================
2026-02-10 02:28:09,287 - INFO -   [탐색  1] 희소도: 0.4950 -> 파라미터: 2.3194M (감소율: 74.68%)
2026-02-10 02:28:09,289 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:09,289 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:09,750 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-02-10 02:28:09,750 - INFO - ==================================================
2026-02-10 02:28:09,750 - INFO -   [탐색  2] 희소도: 0.7425 -> 파라미터: 0.5934M (감소율: 93.52%)
2026-02-10 02:28:09,753 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:09,753 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:10,137 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-02-10 02:28:10,138 - INFO - ==================================================
2026-02-10 02:28:10,138 - INFO -   [탐색  3] 희소도: 0.8662 -> 파라미터: 0.1643M (감소율: 98.21%)
2026-02-10 02:28:10,139 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:10,139 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:10,556 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.928125)에 맞춰 변경되었습니다.
2026-02-10 02:28:10,556 - INFO - ==================================================
2026-02-10 02:28:10,556 - INFO -   [탐색  4] 희소도: 0.9281 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:10,557 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:10,557 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:11,049 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8971875)에 맞춰 변경되었습니다.
2026-02-10 02:28:11,050 - INFO - ==================================================
2026-02-10 02:28:11,050 - INFO -   [탐색  5] 희소도: 0.8972 -> 파라미터: 0.0975M (감소율: 98.94%)
2026-02-10 02:28:11,052 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:11,052 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:11,875 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.91265625)에 맞춰 변경되었습니다.
2026-02-10 02:28:11,875 - INFO - ==================================================
2026-02-10 02:28:11,876 - INFO -   [탐색  6] 희소도: 0.9127 -> 파라미터: 0.0702M (감소율: 99.23%)
2026-02-10 02:28:11,878 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:11,878 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:12,237 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.920390625)에 맞춰 변경되었습니다.
2026-02-10 02:28:12,237 - INFO - ==================================================
2026-02-10 02:28:12,237 - INFO -   [탐색  7] 희소도: 0.9204 -> 파라미터: 0.0585M (감소율: 99.36%)
2026-02-10 02:28:12,240 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:12,240 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:12,587 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9242578125)에 맞춰 변경되었습니다.
2026-02-10 02:28:12,588 - INFO - ==================================================
2026-02-10 02:28:12,588 - INFO -   [탐색  8] 희소도: 0.9243 -> 파라미터: 0.0500M (감소율: 99.45%)
2026-02-10 02:28:12,589 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:12,589 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:12,880 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9261914062500001)에 맞춰 변경되었습니다.
2026-02-10 02:28:12,880 - INFO - ==================================================
2026-02-10 02:28:12,880 - INFO -   [탐색  9] 희소도: 0.9262 -> 파라미터: 0.0487M (감소율: 99.47%)
2026-02-10 02:28:12,882 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:12,882 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:13,377 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9271582031250001)에 맞춰 변경되었습니다.
2026-02-10 02:28:13,377 - INFO - ==================================================
2026-02-10 02:28:13,377 - INFO -   [탐색 10] 희소도: 0.9272 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:13,379 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:13,379 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:14,217 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9276416015625)에 맞춰 변경되었습니다.
2026-02-10 02:28:14,218 - INFO - ==================================================
2026-02-10 02:28:14,218 - INFO -   [탐색 11] 희소도: 0.9276 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:14,220 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:14,220 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:14,715 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9278833007812499)에 맞춰 변경되었습니다.
2026-02-10 02:28:14,715 - INFO - ==================================================
2026-02-10 02:28:14,715 - INFO -   [탐색 12] 희소도: 0.9279 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:14,717 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:14,717 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:15,186 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927762451171875)에 맞춰 변경되었습니다.
2026-02-10 02:28:15,186 - INFO - ==================================================
2026-02-10 02:28:15,186 - INFO -   [탐색 13] 희소도: 0.9278 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:15,188 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:15,188 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:15,688 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277020263671875)에 맞춰 변경되었습니다.
2026-02-10 02:28:15,688 - INFO - ==================================================
2026-02-10 02:28:15,688 - INFO -   [탐색 14] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:15,690 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:15,690 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:16,021 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277322387695313)에 맞춰 변경되었습니다.
2026-02-10 02:28:16,022 - INFO - ==================================================
2026-02-10 02:28:16,022 - INFO -   [탐색 15] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:16,023 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:16,023 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:16,493 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277473449707031)에 맞춰 변경되었습니다.
2026-02-10 02:28:16,493 - INFO - ==================================================
2026-02-10 02:28:16,493 - INFO -   [탐색 16] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:16,495 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:16,495 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:17,007 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277397918701171)에 맞춰 변경되었습니다.
2026-02-10 02:28:17,007 - INFO - ==================================================
2026-02-10 02:28:17,007 - INFO -   [탐색 17] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:17,009 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:17,009 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:17,885 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277360153198242)에 맞춰 변경되었습니다.
2026-02-10 02:28:17,885 - INFO - ==================================================
2026-02-10 02:28:17,886 - INFO -   [탐색 18] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:17,887 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:17,887 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:18,336 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277341270446777)에 맞춰 변경되었습니다.
2026-02-10 02:28:18,336 - INFO - ==================================================
2026-02-10 02:28:18,336 - INFO -   [탐색 19] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:18,338 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:18,338 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:18,856 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927735071182251)에 맞춰 변경되었습니다.
2026-02-10 02:28:18,856 - INFO - ==================================================
2026-02-10 02:28:18,856 - INFO -   [탐색 20] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:18,858 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:18,858 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:19,264 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277345991134643)에 맞춰 변경되었습니다.
2026-02-10 02:28:19,264 - INFO - ==================================================
2026-02-10 02:28:19,264 - INFO -   [탐색 21] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:19,266 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:19,266 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:19,710 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343630790711)에 맞춰 변경되었습니다.
2026-02-10 02:28:19,711 - INFO - ==================================================
2026-02-10 02:28:19,711 - INFO -   [탐색 22] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:19,714 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:19,715 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:20,231 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277344810962678)에 맞춰 변경되었습니다.
2026-02-10 02:28:20,231 - INFO - ==================================================
2026-02-10 02:28:20,231 - INFO -   [탐색 23] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:20,233 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:20,233 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:20,681 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277344220876694)에 맞춰 변경되었습니다.
2026-02-10 02:28:20,681 - INFO - ==================================================
2026-02-10 02:28:20,682 - INFO -   [탐색 24] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:20,684 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:20,684 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:21,473 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343925833703)에 맞춰 변경되었습니다.
2026-02-10 02:28:21,473 - INFO - ==================================================
2026-02-10 02:28:21,474 - INFO -   [탐색 25] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:21,475 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:21,475 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:21,985 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343778312207)에 맞춰 변경되었습니다.
2026-02-10 02:28:21,985 - INFO - ==================================================
2026-02-10 02:28:21,986 - INFO -   [탐색 26] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:21,987 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:21,988 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:22,467 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734370455146)에 맞춰 변경되었습니다.
2026-02-10 02:28:22,468 - INFO - ==================================================
2026-02-10 02:28:22,468 - INFO -   [탐색 27] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:22,470 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:22,470 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:22,964 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343741431834)에 맞춰 변경되었습니다.
2026-02-10 02:28:22,964 - INFO - ==================================================
2026-02-10 02:28:22,965 - INFO -   [탐색 28] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:22,967 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:22,967 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:23,478 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343759872021)에 맞춰 변경되었습니다.
2026-02-10 02:28:23,479 - INFO - ==================================================
2026-02-10 02:28:23,479 - INFO -   [탐색 29] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:23,481 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:23,481 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:23,961 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750651927)에 맞춰 변경되었습니다.
2026-02-10 02:28:23,961 - INFO - ==================================================
2026-02-10 02:28:23,961 - INFO -   [탐색 30] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:23,963 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:23,963 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:24,827 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343746041881)에 맞춰 변경되었습니다.
2026-02-10 02:28:24,827 - INFO - ==================================================
2026-02-10 02:28:24,827 - INFO -   [탐색 31] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:24,829 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:24,829 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:25,360 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343748346905)에 맞춰 변경되었습니다.
2026-02-10 02:28:25,361 - INFO - ==================================================
2026-02-10 02:28:25,361 - INFO -   [탐색 32] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:25,364 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:25,364 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:25,904 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343749499416)에 맞춰 변경되었습니다.
2026-02-10 02:28:25,904 - INFO - ==================================================
2026-02-10 02:28:25,904 - INFO -   [탐색 33] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:25,906 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:25,907 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:26,322 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750075672)에 맞춰 변경되었습니다.
2026-02-10 02:28:26,323 - INFO - ==================================================
2026-02-10 02:28:26,323 - INFO -   [탐색 34] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:26,326 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:26,326 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:26,663 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343749787543)에 맞춰 변경되었습니다.
2026-02-10 02:28:26,663 - INFO - ==================================================
2026-02-10 02:28:26,663 - INFO -   [탐색 35] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:26,665 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:26,665 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:27,163 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343749931608)에 맞춰 변경되었습니다.
2026-02-10 02:28:27,163 - INFO - ==================================================
2026-02-10 02:28:27,164 - INFO -   [탐색 36] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:27,165 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:27,166 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:27,619 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750003639)에 맞춰 변경되었습니다.
2026-02-10 02:28:27,620 - INFO - ==================================================
2026-02-10 02:28:27,620 - INFO -   [탐색 37] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:27,622 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:27,622 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:28,428 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343749967624)에 맞춰 변경되었습니다.
2026-02-10 02:28:28,428 - INFO - ==================================================
2026-02-10 02:28:28,428 - INFO -   [탐색 38] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:28,430 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:28,430 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:28,816 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343749985631)에 맞춰 변경되었습니다.
2026-02-10 02:28:28,816 - INFO - ==================================================
2026-02-10 02:28:28,816 - INFO -   [탐색 39] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:28,818 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:28,818 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:29,220 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343749994635)에 맞춰 변경되었습니다.
2026-02-10 02:28:29,220 - INFO - ==================================================
2026-02-10 02:28:29,220 - INFO -   [탐색 40] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:29,222 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:29,222 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:29,608 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343749999137)에 맞춰 변경되었습니다.
2026-02-10 02:28:29,608 - INFO - ==================================================
2026-02-10 02:28:29,608 - INFO -   [탐색 41] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:29,610 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:29,610 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:30,098 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750001388)에 맞춰 변경되었습니다.
2026-02-10 02:28:30,098 - INFO - ==================================================
2026-02-10 02:28:30,098 - INFO -   [탐색 42] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:30,100 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:30,100 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:30,589 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750000262)에 맞춰 변경되었습니다.
2026-02-10 02:28:30,589 - INFO - ==================================================
2026-02-10 02:28:30,590 - INFO -   [탐색 43] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:30,591 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:30,591 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:31,443 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.92773437499997)에 맞춰 변경되었습니다.
2026-02-10 02:28:31,443 - INFO - ==================================================
2026-02-10 02:28:31,443 - INFO -   [탐색 44] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:31,448 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:31,448 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:31,946 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343749999981)에 맞춰 변경되었습니다.
2026-02-10 02:28:31,947 - INFO - ==================================================
2026-02-10 02:28:31,947 - INFO -   [탐색 45] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:31,949 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:31,949 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:32,413 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750000122)에 맞춰 변경되었습니다.
2026-02-10 02:28:32,413 - INFO - ==================================================
2026-02-10 02:28:32,414 - INFO -   [탐색 46] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:32,416 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:32,416 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:32,731 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750000051)에 맞춰 변경되었습니다.
2026-02-10 02:28:32,732 - INFO - ==================================================
2026-02-10 02:28:32,732 - INFO -   [탐색 47] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:32,734 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:32,734 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:33,192 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750000016)에 맞춰 변경되었습니다.
2026-02-10 02:28:33,193 - INFO - ==================================================
2026-02-10 02:28:33,193 - INFO -   [탐색 48] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:33,194 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:33,194 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:33,641 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343749999998)에 맞춰 변경되었습니다.
2026-02-10 02:28:33,642 - INFO - ==================================================
2026-02-10 02:28:33,642 - INFO -   [탐색 49] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:33,644 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:33,644 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:34,152 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750000007)에 맞춰 변경되었습니다.
2026-02-10 02:28:34,152 - INFO - ==================================================
2026-02-10 02:28:34,153 - INFO -   [탐색 50] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:34,154 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:34,155 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:35,004 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750000002)에 맞춰 변경되었습니다.
2026-02-10 02:28:35,004 - INFO - ==================================================
2026-02-10 02:28:35,004 - INFO -   [탐색 51] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:35,006 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:35,006 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:35,513 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:35,513 - INFO - ==================================================
2026-02-10 02:28:35,514 - INFO -   [탐색 52] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:35,516 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:35,516 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:35,904 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9277343750000001)에 맞춰 변경되었습니다.
2026-02-10 02:28:35,905 - INFO - ==================================================
2026-02-10 02:28:35,905 - INFO -   [탐색 53] 희소도: 0.9277 -> 파라미터: 0.0474M (감소율: 99.48%)
2026-02-10 02:28:35,908 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:35,908 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:36,428 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:36,428 - INFO - ==================================================
2026-02-10 02:28:36,428 - INFO -   [탐색 54] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:36,430 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:36,430 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:36,778 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:36,779 - INFO - ==================================================
2026-02-10 02:28:36,779 - INFO -   [탐색 55] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:36,781 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:36,781 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:37,312 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:37,312 - INFO - ==================================================
2026-02-10 02:28:37,313 - INFO -   [탐색 56] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:37,315 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:37,315 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:38,090 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:38,090 - INFO - ==================================================
2026-02-10 02:28:38,090 - INFO -   [탐색 57] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:38,092 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:38,092 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:38,592 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:38,592 - INFO - ==================================================
2026-02-10 02:28:38,592 - INFO -   [탐색 58] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:38,594 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:38,595 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:39,059 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:39,060 - INFO - ==================================================
2026-02-10 02:28:39,060 - INFO -   [탐색 59] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:39,061 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:39,061 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:39,468 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:39,469 - INFO - ==================================================
2026-02-10 02:28:39,469 - INFO -   [탐색 60] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:39,471 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:39,471 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:39,905 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:39,905 - INFO - ==================================================
2026-02-10 02:28:39,905 - INFO -   [탐색 61] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:39,907 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:39,907 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:40,439 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:40,439 - INFO - ==================================================
2026-02-10 02:28:40,439 - INFO -   [탐색 62] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:40,441 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:40,441 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:40,921 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:40,921 - INFO - ==================================================
2026-02-10 02:28:40,921 - INFO -   [탐색 63] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:40,923 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:40,924 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:41,721 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:41,722 - INFO - ==================================================
2026-02-10 02:28:41,722 - INFO -   [탐색 64] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:41,725 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:41,726 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:42,175 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:42,175 - INFO - ==================================================
2026-02-10 02:28:42,176 - INFO -   [탐색 65] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:42,179 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:42,179 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:42,558 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:42,558 - INFO - ==================================================
2026-02-10 02:28:42,559 - INFO -   [탐색 66] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:42,561 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:42,561 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:43,041 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:43,041 - INFO - ==================================================
2026-02-10 02:28:43,042 - INFO -   [탐색 67] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:43,044 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:43,045 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:43,407 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:43,408 - INFO - ==================================================
2026-02-10 02:28:43,408 - INFO -   [탐색 68] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:43,410 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:43,410 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:43,683 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:43,683 - INFO - ==================================================
2026-02-10 02:28:43,683 - INFO -   [탐색 69] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:43,685 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:43,686 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:44,194 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:44,195 - INFO - ==================================================
2026-02-10 02:28:44,195 - INFO -   [탐색 70] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:44,196 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:44,196 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:44,625 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:44,625 - INFO - ==================================================
2026-02-10 02:28:44,626 - INFO -   [탐색 71] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:44,627 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:44,627 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:45,038 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:45,038 - INFO - ==================================================
2026-02-10 02:28:45,039 - INFO -   [탐색 72] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:45,040 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:45,040 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:45,443 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:45,444 - INFO - ==================================================
2026-02-10 02:28:45,444 - INFO -   [탐색 73] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:45,446 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:45,446 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:45,794 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:45,794 - INFO - ==================================================
2026-02-10 02:28:45,795 - INFO -   [탐색 74] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:45,796 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:45,796 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:46,060 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:46,060 - INFO - ==================================================
2026-02-10 02:28:46,060 - INFO -   [탐색 75] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:46,062 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:46,062 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:46,392 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:46,392 - INFO - ==================================================
2026-02-10 02:28:46,392 - INFO -   [탐색 76] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:46,393 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:46,393 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:46,939 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:46,940 - INFO - ==================================================
2026-02-10 02:28:46,940 - INFO -   [탐색 77] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:46,941 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:46,941 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:47,305 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:47,305 - INFO - ==================================================
2026-02-10 02:28:47,305 - INFO -   [탐색 78] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:47,308 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:47,308 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:47,669 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:47,669 - INFO - ==================================================
2026-02-10 02:28:47,669 - INFO -   [탐색 79] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:47,672 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:47,673 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:47,991 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:47,992 - INFO - ==================================================
2026-02-10 02:28:47,992 - INFO -   [탐색 80] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:47,993 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:47,993 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:48,274 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:48,274 - INFO - ==================================================
2026-02-10 02:28:48,274 - INFO -   [탐색 81] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:48,276 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:48,276 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:48,583 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:48,583 - INFO - ==================================================
2026-02-10 02:28:48,583 - INFO -   [탐색 82] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:48,585 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:48,585 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:49,112 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:49,113 - INFO - ==================================================
2026-02-10 02:28:49,113 - INFO -   [탐색 83] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:49,115 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:49,115 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:49,623 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:49,623 - INFO - ==================================================
2026-02-10 02:28:49,623 - INFO -   [탐색 84] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:49,625 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:49,626 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:50,114 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:50,114 - INFO - ==================================================
2026-02-10 02:28:50,114 - INFO -   [탐색 85] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:50,117 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:50,117 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:50,624 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:50,624 - INFO - ==================================================
2026-02-10 02:28:50,625 - INFO -   [탐색 86] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:50,626 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:50,627 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:51,120 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:51,120 - INFO - ==================================================
2026-02-10 02:28:51,120 - INFO -   [탐색 87] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:51,122 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:51,122 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:51,548 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:51,549 - INFO - ==================================================
2026-02-10 02:28:51,549 - INFO -   [탐색 88] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:51,551 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:51,552 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:51,911 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:51,911 - INFO - ==================================================
2026-02-10 02:28:51,911 - INFO -   [탐색 89] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:51,912 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:51,913 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:52,761 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:52,761 - INFO - ==================================================
2026-02-10 02:28:52,761 - INFO -   [탐색 90] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:52,763 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:52,763 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:53,260 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:53,260 - INFO - ==================================================
2026-02-10 02:28:53,261 - INFO -   [탐색 91] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:53,263 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:53,263 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:53,784 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:53,784 - INFO - ==================================================
2026-02-10 02:28:53,784 - INFO -   [탐색 92] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:53,786 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:53,787 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:54,311 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:54,312 - INFO - ==================================================
2026-02-10 02:28:54,312 - INFO -   [탐색 93] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:54,314 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:54,314 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:54,777 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:54,778 - INFO - ==================================================
2026-02-10 02:28:54,778 - INFO -   [탐색 94] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:54,780 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:54,780 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:55,185 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:55,185 - INFO - ==================================================
2026-02-10 02:28:55,186 - INFO -   [탐색 95] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:55,187 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:55,188 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:55,961 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:55,961 - INFO - ==================================================
2026-02-10 02:28:55,961 - INFO -   [탐색 96] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:55,964 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:55,964 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:56,465 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:56,465 - INFO - ==================================================
2026-02-10 02:28:56,465 - INFO -   [탐색 97] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:56,468 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:56,468 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:56,912 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:56,912 - INFO - ==================================================
2026-02-10 02:28:56,912 - INFO -   [탐색 98] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:56,915 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:56,915 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:57,424 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:57,424 - INFO - ==================================================
2026-02-10 02:28:57,424 - INFO -   [탐색 99] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:57,426 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:57,426 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:57,981 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.927734375)에 맞춰 변경되었습니다.
2026-02-10 02:28:57,982 - INFO - ==================================================
2026-02-10 02:28:57,982 - INFO -   [탐색 100] 희소도: 0.9277 -> 파라미터: 0.0481M (감소율: 99.48%)
2026-02-10 02:28:57,982 - INFO - 탐색 완료. 목표 파라미터 수(0.0476M)에 가장 근접한 최적 희소도는 0.9281 입니다.
2026-02-10 02:28:57,982 - INFO - ================================================================================
2026-02-10 02:28:57,984 - INFO - 계산된 Pruning 정보(희소도: 0.9281)를 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/pruning_info.yaml'에 저장했습니다.
2026-02-10 02:28:57,991 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 02:28:57,997 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:28:57,997 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:28:58,538 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.928125)에 맞춰 변경되었습니다.
2026-02-10 02:28:58,538 - INFO - ==================================================
2026-02-10 02:28:58,539 - INFO - ==================================================
2026-02-10 02:28:58,539 - INFO - 모델 파라미터 수:
2026-02-10 02:28:58,539 - INFO -   - 총 파라미터: 47,386 개
2026-02-10 02:28:58,539 - INFO -   - 학습 가능한 파라미터: 47,386 개
2026-02-10 02:28:58,545 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 02:28:58,550 - INFO - FLOPs가 2.8696 GFLOPs에서 0.1481 GFLOPs로 감소했습니다 (감소율: 94.84%).
2026-02-10 02:28:58,551 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 02:28:58,551 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 02:28:58,551 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 02:28:58,551 - INFO - ==================================================
2026-02-10 02:28:58,551 - INFO - train 모드를 시작합니다.
2026-02-10 02:28:58,551 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 02:28:58,551 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 02:28:58,551 - INFO - --------------------------------------------------
2026-02-10 02:28:58,551 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 02:29:02,592 - INFO - [Train] [11/90] | Loss: 0.5731 | Train Acc: 74.33%
2026-02-10 02:29:03,380 - INFO - [Valid] [11/90] | Loss: 0.5699 | Val Acc: 70.50%
2026-02-10 02:29:03,385 - INFO - [Metrics for 'abnormal'] | Precision: 0.6390 | Recall: 0.8344 | F1: 0.7238
2026-02-10 02:29:03,385 - INFO - [Metrics for 'normal'] | Precision: 0.8060 | Recall: 0.5934 | F1: 0.6835
2026-02-10 02:29:03,394 - INFO - [Best Model Saved] (val loss: 0.5699) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:29:03,394 - INFO - --------------------------------------------------
2026-02-10 02:29:03,394 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 02:29:07,256 - INFO - [Train] [12/90] | Loss: 0.5221 | Train Acc: 79.17%
2026-02-10 02:29:07,888 - INFO - [Valid] [12/90] | Loss: 0.5403 | Val Acc: 74.93%
2026-02-10 02:29:07,894 - INFO - [Metrics for 'abnormal'] | Precision: 0.6978 | Recall: 0.8089 | F1: 0.7493
2026-02-10 02:29:07,896 - INFO - [Metrics for 'normal'] | Precision: 0.8089 | Recall: 0.6978 | F1: 0.7493
2026-02-10 02:29:07,902 - INFO - [Best Model Saved] (val loss: 0.5403) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:29:07,903 - INFO - --------------------------------------------------
2026-02-10 02:29:07,903 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 02:29:11,382 - INFO - [Train] [13/90] | Loss: 0.5056 | Train Acc: 80.73%
2026-02-10 02:29:12,260 - INFO - [Valid] [13/90] | Loss: 0.5449 | Val Acc: 75.81%
2026-02-10 02:29:12,262 - INFO - [Metrics for 'abnormal'] | Precision: 0.7193 | Recall: 0.7834 | F1: 0.7500
2026-02-10 02:29:12,262 - INFO - [Metrics for 'normal'] | Precision: 0.7976 | Recall: 0.7363 | F1: 0.7657
2026-02-10 02:29:12,263 - INFO - --------------------------------------------------
2026-02-10 02:29:12,263 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 02:29:15,098 - INFO - [Train] [14/90] | Loss: 0.5155 | Train Acc: 81.32%
2026-02-10 02:29:16,145 - INFO - [Valid] [14/90] | Loss: 0.5285 | Val Acc: 77.29%
2026-02-10 02:29:16,149 - INFO - [Metrics for 'abnormal'] | Precision: 0.7353 | Recall: 0.7962 | F1: 0.7645
2026-02-10 02:29:16,150 - INFO - [Metrics for 'normal'] | Precision: 0.8107 | Recall: 0.7527 | F1: 0.7806
2026-02-10 02:29:16,153 - INFO - [Best Model Saved] (val loss: 0.5285) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:29:16,153 - INFO - --------------------------------------------------
2026-02-10 02:29:16,153 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 02:29:19,033 - INFO - [Train] [15/90] | Loss: 0.4946 | Train Acc: 81.10%
2026-02-10 02:29:20,014 - INFO - [Valid] [15/90] | Loss: 0.5389 | Val Acc: 75.22%
2026-02-10 02:29:20,023 - INFO - [Metrics for 'abnormal'] | Precision: 0.7186 | Recall: 0.7643 | F1: 0.7407
2026-02-10 02:29:20,024 - INFO - [Metrics for 'normal'] | Precision: 0.7849 | Recall: 0.7418 | F1: 0.7627
2026-02-10 02:29:20,026 - INFO - --------------------------------------------------
2026-02-10 02:29:20,026 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 02:29:24,884 - INFO - [Train] [16/90] | Loss: 0.4925 | Train Acc: 82.44%
2026-02-10 02:29:25,958 - INFO - [Valid] [16/90] | Loss: 0.5305 | Val Acc: 75.52%
2026-02-10 02:29:25,967 - INFO - [Metrics for 'abnormal'] | Precision: 0.7126 | Recall: 0.7898 | F1: 0.7492
2026-02-10 02:29:25,967 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.7253 | F1: 0.7608
2026-02-10 02:29:25,969 - INFO - --------------------------------------------------
2026-02-10 02:29:25,969 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 02:29:30,524 - INFO - [Train] [17/90] | Loss: 0.4900 | Train Acc: 81.99%
2026-02-10 02:29:31,794 - INFO - [Valid] [17/90] | Loss: 0.5220 | Val Acc: 76.11%
2026-02-10 02:29:31,799 - INFO - [Metrics for 'abnormal'] | Precision: 0.7262 | Recall: 0.7771 | F1: 0.7508
2026-02-10 02:29:31,799 - INFO - [Metrics for 'normal'] | Precision: 0.7953 | Recall: 0.7473 | F1: 0.7705
2026-02-10 02:29:31,802 - INFO - [Best Model Saved] (val loss: 0.5220) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:29:31,802 - INFO - --------------------------------------------------
2026-02-10 02:29:31,802 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 02:29:36,196 - INFO - [Train] [18/90] | Loss: 0.4878 | Train Acc: 82.37%
2026-02-10 02:29:37,541 - INFO - [Valid] [18/90] | Loss: 0.5125 | Val Acc: 76.70%
2026-02-10 02:29:37,546 - INFO - [Metrics for 'abnormal'] | Precision: 0.7191 | Recall: 0.8153 | F1: 0.7642
2026-02-10 02:29:37,547 - INFO - [Metrics for 'normal'] | Precision: 0.8199 | Recall: 0.7253 | F1: 0.7697
2026-02-10 02:29:37,553 - INFO - [Best Model Saved] (val loss: 0.5125) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:29:37,553 - INFO - --------------------------------------------------
2026-02-10 02:29:37,553 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 02:29:41,558 - INFO - [Train] [19/90] | Loss: 0.4869 | Train Acc: 82.07%
2026-02-10 02:29:42,954 - INFO - [Valid] [19/90] | Loss: 0.5057 | Val Acc: 77.29%
2026-02-10 02:29:42,960 - INFO - [Metrics for 'abnormal'] | Precision: 0.7299 | Recall: 0.8089 | F1: 0.7674
2026-02-10 02:29:42,960 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.7418 | F1: 0.7781
2026-02-10 02:29:42,967 - INFO - [Best Model Saved] (val loss: 0.5057) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:29:42,967 - INFO - --------------------------------------------------
2026-02-10 02:29:42,967 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 02:29:47,317 - INFO - [Train] [20/90] | Loss: 0.4890 | Train Acc: 81.99%
2026-02-10 02:29:48,538 - INFO - [Valid] [20/90] | Loss: 0.5256 | Val Acc: 76.40%
2026-02-10 02:29:48,546 - INFO - [Metrics for 'abnormal'] | Precision: 0.7333 | Recall: 0.7707 | F1: 0.7516
2026-02-10 02:29:48,546 - INFO - [Metrics for 'normal'] | Precision: 0.7931 | Recall: 0.7582 | F1: 0.7753
2026-02-10 02:29:48,548 - INFO - --------------------------------------------------
2026-02-10 02:29:48,548 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 02:29:52,659 - INFO - [Train] [21/90] | Loss: 0.4752 | Train Acc: 82.59%
2026-02-10 02:29:54,000 - INFO - [Valid] [21/90] | Loss: 0.5204 | Val Acc: 78.47%
2026-02-10 02:29:54,010 - INFO - [Metrics for 'abnormal'] | Precision: 0.8043 | Recall: 0.7070 | F1: 0.7525
2026-02-10 02:29:54,010 - INFO - [Metrics for 'normal'] | Precision: 0.7711 | Recall: 0.8516 | F1: 0.8094
2026-02-10 02:29:54,012 - INFO - --------------------------------------------------
2026-02-10 02:29:54,012 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 02:29:58,427 - INFO - [Train] [22/90] | Loss: 0.4782 | Train Acc: 83.63%
2026-02-10 02:30:00,092 - INFO - [Valid] [22/90] | Loss: 0.4982 | Val Acc: 78.17%
2026-02-10 02:30:00,097 - INFO - [Metrics for 'abnormal'] | Precision: 0.7318 | Recall: 0.8344 | F1: 0.7798
2026-02-10 02:30:00,097 - INFO - [Metrics for 'normal'] | Precision: 0.8375 | Recall: 0.7363 | F1: 0.7836
2026-02-10 02:30:00,101 - INFO - [Best Model Saved] (val loss: 0.4982) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:30:00,101 - INFO - --------------------------------------------------
2026-02-10 02:30:00,101 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 02:30:05,309 - INFO - [Train] [23/90] | Loss: 0.4689 | Train Acc: 83.18%
2026-02-10 02:30:06,843 - INFO - [Valid] [23/90] | Loss: 0.5038 | Val Acc: 80.24%
2026-02-10 02:30:06,849 - INFO - [Metrics for 'abnormal'] | Precision: 0.7711 | Recall: 0.8153 | F1: 0.7926
2026-02-10 02:30:06,853 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.7912 | F1: 0.8113
2026-02-10 02:30:06,856 - INFO - --------------------------------------------------
2026-02-10 02:30:06,857 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 02:30:12,695 - INFO - [Train] [24/90] | Loss: 0.4653 | Train Acc: 83.85%
2026-02-10 02:30:14,059 - INFO - [Valid] [24/90] | Loss: 0.4982 | Val Acc: 76.99%
2026-02-10 02:30:14,065 - INFO - [Metrics for 'abnormal'] | Precision: 0.7182 | Recall: 0.8280 | F1: 0.7692
2026-02-10 02:30:14,065 - INFO - [Metrics for 'normal'] | Precision: 0.8291 | Recall: 0.7198 | F1: 0.7706
2026-02-10 02:30:14,070 - INFO - --------------------------------------------------
2026-02-10 02:30:14,070 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 02:30:19,837 - INFO - [Train] [25/90] | Loss: 0.4538 | Train Acc: 84.23%
2026-02-10 02:30:21,081 - INFO - [Valid] [25/90] | Loss: 0.4848 | Val Acc: 82.01%
2026-02-10 02:30:21,091 - INFO - [Metrics for 'abnormal'] | Precision: 0.8117 | Recall: 0.7962 | F1: 0.8039
2026-02-10 02:30:21,091 - INFO - [Metrics for 'normal'] | Precision: 0.8270 | Recall: 0.8407 | F1: 0.8338
2026-02-10 02:30:21,098 - INFO - [Best Model Saved] (val loss: 0.4848) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:30:21,098 - INFO - --------------------------------------------------
2026-02-10 02:30:21,098 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 02:30:26,632 - INFO - [Train] [26/90] | Loss: 0.4573 | Train Acc: 84.23%
2026-02-10 02:30:27,972 - INFO - [Valid] [26/90] | Loss: 0.4996 | Val Acc: 76.11%
2026-02-10 02:30:27,979 - INFO - [Metrics for 'abnormal'] | Precision: 0.7111 | Recall: 0.8153 | F1: 0.7596
2026-02-10 02:30:27,983 - INFO - [Metrics for 'normal'] | Precision: 0.8176 | Recall: 0.7143 | F1: 0.7625
2026-02-10 02:30:27,986 - INFO - --------------------------------------------------
2026-02-10 02:30:27,986 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 02:30:33,627 - INFO - [Train] [27/90] | Loss: 0.4552 | Train Acc: 84.75%
2026-02-10 02:30:35,040 - INFO - [Valid] [27/90] | Loss: 0.5140 | Val Acc: 79.35%
2026-02-10 02:30:35,049 - INFO - [Metrics for 'abnormal'] | Precision: 0.8271 | Recall: 0.7006 | F1: 0.7586
2026-02-10 02:30:35,049 - INFO - [Metrics for 'normal'] | Precision: 0.7718 | Recall: 0.8736 | F1: 0.8196
2026-02-10 02:30:35,051 - INFO - --------------------------------------------------
2026-02-10 02:30:35,051 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 02:30:40,654 - INFO - [Train] [28/90] | Loss: 0.4496 | Train Acc: 84.23%
2026-02-10 02:30:41,600 - INFO - [Valid] [28/90] | Loss: 0.5017 | Val Acc: 77.58%
2026-02-10 02:30:41,609 - INFO - [Metrics for 'abnormal'] | Precision: 0.7098 | Recall: 0.8726 | F1: 0.7829
2026-02-10 02:30:41,610 - INFO - [Metrics for 'normal'] | Precision: 0.8630 | Recall: 0.6923 | F1: 0.7683
2026-02-10 02:30:41,611 - INFO - --------------------------------------------------
2026-02-10 02:30:41,611 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 02:30:47,309 - INFO - [Train] [29/90] | Loss: 0.4440 | Train Acc: 84.75%
2026-02-10 02:30:48,765 - INFO - [Valid] [29/90] | Loss: 0.4877 | Val Acc: 81.42%
2026-02-10 02:30:48,775 - INFO - [Metrics for 'abnormal'] | Precision: 0.8133 | Recall: 0.7771 | F1: 0.7948
2026-02-10 02:30:48,775 - INFO - [Metrics for 'normal'] | Precision: 0.8148 | Recall: 0.8462 | F1: 0.8302
2026-02-10 02:30:48,776 - INFO - --------------------------------------------------
2026-02-10 02:30:48,777 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 02:30:54,029 - INFO - [Train] [30/90] | Loss: 0.4516 | Train Acc: 85.04%
2026-02-10 02:30:55,593 - INFO - [Valid] [30/90] | Loss: 0.4984 | Val Acc: 79.06%
2026-02-10 02:30:55,599 - INFO - [Metrics for 'abnormal'] | Precision: 0.7867 | Recall: 0.7516 | F1: 0.7687
2026-02-10 02:30:55,604 - INFO - [Metrics for 'normal'] | Precision: 0.7937 | Recall: 0.8242 | F1: 0.8086
2026-02-10 02:30:55,607 - INFO - --------------------------------------------------
2026-02-10 02:30:55,607 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 02:31:00,604 - INFO - [Train] [31/90] | Loss: 0.4341 | Train Acc: 85.71%
2026-02-10 02:31:02,087 - INFO - [Valid] [31/90] | Loss: 0.4746 | Val Acc: 80.83%
2026-02-10 02:31:02,095 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-02-10 02:31:02,095 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-02-10 02:31:02,099 - INFO - [Best Model Saved] (val loss: 0.4746) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:31:02,099 - INFO - --------------------------------------------------
2026-02-10 02:31:02,099 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 02:31:07,484 - INFO - [Train] [32/90] | Loss: 0.4307 | Train Acc: 86.09%
2026-02-10 02:31:08,916 - INFO - [Valid] [32/90] | Loss: 0.4801 | Val Acc: 81.71%
2026-02-10 02:31:08,922 - INFO - [Metrics for 'abnormal'] | Precision: 0.8146 | Recall: 0.7834 | F1: 0.7987
2026-02-10 02:31:08,922 - INFO - [Metrics for 'normal'] | Precision: 0.8191 | Recall: 0.8462 | F1: 0.8324
2026-02-10 02:31:08,924 - INFO - --------------------------------------------------
2026-02-10 02:31:08,924 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 02:31:14,483 - INFO - [Train] [33/90] | Loss: 0.4275 | Train Acc: 86.09%
2026-02-10 02:31:15,991 - INFO - [Valid] [33/90] | Loss: 0.4695 | Val Acc: 82.89%
2026-02-10 02:31:16,000 - INFO - [Metrics for 'abnormal'] | Precision: 0.8037 | Recall: 0.8344 | F1: 0.8187
2026-02-10 02:31:16,001 - INFO - [Metrics for 'normal'] | Precision: 0.8523 | Recall: 0.8242 | F1: 0.8380
2026-02-10 02:31:16,005 - INFO - [Best Model Saved] (val loss: 0.4695) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:31:16,005 - INFO - --------------------------------------------------
2026-02-10 02:31:16,005 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 02:31:22,003 - INFO - [Train] [34/90] | Loss: 0.4321 | Train Acc: 86.38%
2026-02-10 02:31:23,374 - INFO - [Valid] [34/90] | Loss: 0.4713 | Val Acc: 81.42%
2026-02-10 02:31:23,383 - INFO - [Metrics for 'abnormal'] | Precision: 0.7798 | Recall: 0.8344 | F1: 0.8062
2026-02-10 02:31:23,383 - INFO - [Metrics for 'normal'] | Precision: 0.8480 | Recall: 0.7967 | F1: 0.8215
2026-02-10 02:31:23,385 - INFO - --------------------------------------------------
2026-02-10 02:31:23,385 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 02:31:28,916 - INFO - [Train] [35/90] | Loss: 0.4168 | Train Acc: 86.24%
2026-02-10 02:31:30,376 - INFO - [Valid] [35/90] | Loss: 0.4733 | Val Acc: 82.01%
2026-02-10 02:31:30,395 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-02-10 02:31:30,397 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-02-10 02:31:30,399 - INFO - --------------------------------------------------
2026-02-10 02:31:30,399 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 02:31:36,090 - INFO - [Train] [36/90] | Loss: 0.4076 | Train Acc: 87.43%
2026-02-10 02:31:37,538 - INFO - [Valid] [36/90] | Loss: 0.4823 | Val Acc: 81.71%
2026-02-10 02:31:37,548 - INFO - [Metrics for 'abnormal'] | Precision: 0.7879 | Recall: 0.8280 | F1: 0.8075
2026-02-10 02:31:37,548 - INFO - [Metrics for 'normal'] | Precision: 0.8448 | Recall: 0.8077 | F1: 0.8258
2026-02-10 02:31:37,550 - INFO - --------------------------------------------------
2026-02-10 02:31:37,551 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 02:31:42,693 - INFO - [Train] [37/90] | Loss: 0.4132 | Train Acc: 86.83%
2026-02-10 02:31:44,359 - INFO - [Valid] [37/90] | Loss: 0.4914 | Val Acc: 80.53%
2026-02-10 02:31:44,368 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-02-10 02:31:44,368 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-02-10 02:31:44,371 - INFO - --------------------------------------------------
2026-02-10 02:31:44,371 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 02:31:49,762 - INFO - [Train] [38/90] | Loss: 0.4123 | Train Acc: 86.90%
2026-02-10 02:31:51,207 - INFO - [Valid] [38/90] | Loss: 0.4750 | Val Acc: 81.12%
2026-02-10 02:31:51,214 - INFO - [Metrics for 'abnormal'] | Precision: 0.7962 | Recall: 0.7962 | F1: 0.7962
2026-02-10 02:31:51,215 - INFO - [Metrics for 'normal'] | Precision: 0.8242 | Recall: 0.8242 | F1: 0.8242
2026-02-10 02:31:51,217 - INFO - --------------------------------------------------
2026-02-10 02:31:51,217 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 02:31:56,725 - INFO - [Train] [39/90] | Loss: 0.4115 | Train Acc: 87.50%
2026-02-10 02:31:57,932 - INFO - [Valid] [39/90] | Loss: 0.4698 | Val Acc: 80.24%
2026-02-10 02:31:57,944 - INFO - [Metrics for 'abnormal'] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-02-10 02:31:57,945 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-02-10 02:31:57,948 - INFO - --------------------------------------------------
2026-02-10 02:31:57,948 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 02:32:03,539 - INFO - [Train] [40/90] | Loss: 0.4043 | Train Acc: 87.87%
2026-02-10 02:32:04,834 - INFO - [Valid] [40/90] | Loss: 0.4715 | Val Acc: 81.12%
2026-02-10 02:32:04,839 - INFO - [Metrics for 'abnormal'] | Precision: 0.8039 | Recall: 0.7834 | F1: 0.7935
2026-02-10 02:32:04,839 - INFO - [Metrics for 'normal'] | Precision: 0.8172 | Recall: 0.8352 | F1: 0.8261
2026-02-10 02:32:04,841 - INFO - --------------------------------------------------
2026-02-10 02:32:04,841 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 02:32:10,640 - INFO - [Train] [41/90] | Loss: 0.3973 | Train Acc: 88.91%
2026-02-10 02:32:11,996 - INFO - [Valid] [41/90] | Loss: 0.4740 | Val Acc: 81.71%
2026-02-10 02:32:12,005 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.8089 | F1: 0.8038
2026-02-10 02:32:12,005 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8242 | F1: 0.8287
2026-02-10 02:32:12,007 - INFO - --------------------------------------------------
2026-02-10 02:32:12,007 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 02:32:17,840 - INFO - [Train] [42/90] | Loss: 0.4054 | Train Acc: 87.80%
2026-02-10 02:32:19,223 - INFO - [Valid] [42/90] | Loss: 0.4746 | Val Acc: 80.83%
2026-02-10 02:32:19,240 - INFO - [Metrics for 'abnormal'] | Precision: 0.7614 | Recall: 0.8535 | F1: 0.8048
2026-02-10 02:32:19,242 - INFO - [Metrics for 'normal'] | Precision: 0.8589 | Recall: 0.7692 | F1: 0.8116
2026-02-10 02:32:19,245 - INFO - --------------------------------------------------
2026-02-10 02:32:19,245 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 02:32:24,352 - INFO - [Train] [43/90] | Loss: 0.4007 | Train Acc: 87.28%
2026-02-10 02:32:25,891 - INFO - [Valid] [43/90] | Loss: 0.5091 | Val Acc: 78.17%
2026-02-10 02:32:25,902 - INFO - [Metrics for 'abnormal'] | Precision: 0.7546 | Recall: 0.7834 | F1: 0.7688
2026-02-10 02:32:25,902 - INFO - [Metrics for 'normal'] | Precision: 0.8068 | Recall: 0.7802 | F1: 0.7933
2026-02-10 02:32:25,904 - INFO - --------------------------------------------------
2026-02-10 02:32:25,904 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 02:32:31,798 - INFO - [Train] [44/90] | Loss: 0.3949 | Train Acc: 88.76%
2026-02-10 02:32:33,076 - INFO - [Valid] [44/90] | Loss: 0.4735 | Val Acc: 80.24%
2026-02-10 02:32:33,082 - INFO - [Metrics for 'abnormal'] | Precision: 0.7557 | Recall: 0.8471 | F1: 0.7988
2026-02-10 02:32:33,082 - INFO - [Metrics for 'normal'] | Precision: 0.8528 | Recall: 0.7637 | F1: 0.8058
2026-02-10 02:32:33,084 - INFO - --------------------------------------------------
2026-02-10 02:32:33,084 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 02:32:39,125 - INFO - [Train] [45/90] | Loss: 0.3918 | Train Acc: 89.43%
2026-02-10 02:32:40,506 - INFO - [Valid] [45/90] | Loss: 0.4581 | Val Acc: 82.30%
2026-02-10 02:32:40,511 - INFO - [Metrics for 'abnormal'] | Precision: 0.8212 | Recall: 0.7898 | F1: 0.8052
2026-02-10 02:32:40,511 - INFO - [Metrics for 'normal'] | Precision: 0.8245 | Recall: 0.8516 | F1: 0.8378
2026-02-10 02:32:40,521 - INFO - [Best Model Saved] (val loss: 0.4581) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:32:40,521 - INFO - --------------------------------------------------
2026-02-10 02:32:40,522 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 02:32:45,872 - INFO - [Train] [46/90] | Loss: 0.3822 | Train Acc: 89.66%
2026-02-10 02:32:46,974 - INFO - [Valid] [46/90] | Loss: 0.4676 | Val Acc: 80.83%
2026-02-10 02:32:46,980 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-02-10 02:32:46,981 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-02-10 02:32:46,985 - INFO - --------------------------------------------------
2026-02-10 02:32:46,986 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 02:32:52,275 - INFO - [Train] [47/90] | Loss: 0.3856 | Train Acc: 89.43%
2026-02-10 02:32:53,664 - INFO - [Valid] [47/90] | Loss: 0.4712 | Val Acc: 80.83%
2026-02-10 02:32:53,671 - INFO - [Metrics for 'abnormal'] | Precision: 0.8026 | Recall: 0.7771 | F1: 0.7896
2026-02-10 02:32:53,671 - INFO - [Metrics for 'normal'] | Precision: 0.8128 | Recall: 0.8352 | F1: 0.8238
2026-02-10 02:32:53,673 - INFO - --------------------------------------------------
2026-02-10 02:32:53,673 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 02:32:58,671 - INFO - [Train] [48/90] | Loss: 0.3768 | Train Acc: 89.36%
2026-02-10 02:33:00,174 - INFO - [Valid] [48/90] | Loss: 0.4774 | Val Acc: 80.24%
2026-02-10 02:33:00,179 - INFO - [Metrics for 'abnormal'] | Precision: 0.7961 | Recall: 0.7707 | F1: 0.7832
2026-02-10 02:33:00,180 - INFO - [Metrics for 'normal'] | Precision: 0.8075 | Recall: 0.8297 | F1: 0.8184
2026-02-10 02:33:00,185 - INFO - --------------------------------------------------
2026-02-10 02:33:00,186 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 02:33:05,680 - INFO - [Train] [49/90] | Loss: 0.3833 | Train Acc: 89.51%
2026-02-10 02:33:07,046 - INFO - [Valid] [49/90] | Loss: 0.4625 | Val Acc: 81.12%
2026-02-10 02:33:07,052 - INFO - [Metrics for 'abnormal'] | Precision: 0.8039 | Recall: 0.7834 | F1: 0.7935
2026-02-10 02:33:07,052 - INFO - [Metrics for 'normal'] | Precision: 0.8172 | Recall: 0.8352 | F1: 0.8261
2026-02-10 02:33:07,054 - INFO - --------------------------------------------------
2026-02-10 02:33:07,054 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 02:33:12,082 - INFO - [Train] [50/90] | Loss: 0.3672 | Train Acc: 90.55%
2026-02-10 02:33:13,489 - INFO - [Valid] [50/90] | Loss: 0.5093 | Val Acc: 80.24%
2026-02-10 02:33:13,494 - INFO - [Metrics for 'abnormal'] | Precision: 0.7679 | Recall: 0.8217 | F1: 0.7938
2026-02-10 02:33:13,494 - INFO - [Metrics for 'normal'] | Precision: 0.8363 | Recall: 0.7857 | F1: 0.8102
2026-02-10 02:33:13,496 - INFO - --------------------------------------------------
2026-02-10 02:33:13,496 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 02:33:19,486 - INFO - [Train] [51/90] | Loss: 0.3736 | Train Acc: 89.43%
2026-02-10 02:33:20,882 - INFO - [Valid] [51/90] | Loss: 0.4669 | Val Acc: 81.12%
2026-02-10 02:33:20,887 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 02:33:20,888 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 02:33:20,890 - INFO - --------------------------------------------------
2026-02-10 02:33:20,890 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 02:33:26,595 - INFO - [Train] [52/90] | Loss: 0.3657 | Train Acc: 91.74%
2026-02-10 02:33:28,290 - INFO - [Valid] [52/90] | Loss: 0.4756 | Val Acc: 81.12%
2026-02-10 02:33:28,299 - INFO - [Metrics for 'abnormal'] | Precision: 0.7751 | Recall: 0.8344 | F1: 0.8037
2026-02-10 02:33:28,299 - INFO - [Metrics for 'normal'] | Precision: 0.8471 | Recall: 0.7912 | F1: 0.8182
2026-02-10 02:33:28,300 - INFO - --------------------------------------------------
2026-02-10 02:33:28,301 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 02:33:33,799 - INFO - [Train] [53/90] | Loss: 0.3626 | Train Acc: 91.00%
2026-02-10 02:33:35,495 - INFO - [Valid] [53/90] | Loss: 0.4605 | Val Acc: 82.60%
2026-02-10 02:33:35,503 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-02-10 02:33:35,503 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-02-10 02:33:35,505 - INFO - --------------------------------------------------
2026-02-10 02:33:35,505 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 02:33:40,832 - INFO - [Train] [54/90] | Loss: 0.3603 | Train Acc: 91.15%
2026-02-10 02:33:42,224 - INFO - [Valid] [54/90] | Loss: 0.4892 | Val Acc: 83.19%
2026-02-10 02:33:42,229 - INFO - [Metrics for 'abnormal'] | Precision: 0.8425 | Recall: 0.7834 | F1: 0.8119
2026-02-10 02:33:42,229 - INFO - [Metrics for 'normal'] | Precision: 0.8238 | Recall: 0.8736 | F1: 0.8480
2026-02-10 02:33:42,231 - INFO - --------------------------------------------------
2026-02-10 02:33:42,231 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 02:33:47,997 - INFO - [Train] [55/90] | Loss: 0.3542 | Train Acc: 92.04%
2026-02-10 02:33:49,538 - INFO - [Valid] [55/90] | Loss: 0.4658 | Val Acc: 82.89%
2026-02-10 02:33:49,550 - INFO - [Metrics for 'abnormal'] | Precision: 0.8235 | Recall: 0.8025 | F1: 0.8129
2026-02-10 02:33:49,551 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8516 | F1: 0.8424
2026-02-10 02:33:49,553 - INFO - --------------------------------------------------
2026-02-10 02:33:49,553 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 02:33:55,293 - INFO - [Train] [56/90] | Loss: 0.3551 | Train Acc: 91.37%
2026-02-10 02:33:56,901 - INFO - [Valid] [56/90] | Loss: 0.4825 | Val Acc: 82.01%
2026-02-10 02:33:56,909 - INFO - [Metrics for 'abnormal'] | Precision: 0.7824 | Recall: 0.8471 | F1: 0.8135
2026-02-10 02:33:56,909 - INFO - [Metrics for 'normal'] | Precision: 0.8580 | Recall: 0.7967 | F1: 0.8262
2026-02-10 02:33:56,911 - INFO - --------------------------------------------------
2026-02-10 02:33:56,911 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 02:34:02,374 - INFO - [Train] [57/90] | Loss: 0.3485 | Train Acc: 92.71%
2026-02-10 02:34:03,994 - INFO - [Valid] [57/90] | Loss: 0.4714 | Val Acc: 83.19%
2026-02-10 02:34:04,008 - INFO - [Metrics for 'abnormal'] | Precision: 0.8247 | Recall: 0.8089 | F1: 0.8167
2026-02-10 02:34:04,012 - INFO - [Metrics for 'normal'] | Precision: 0.8378 | Recall: 0.8516 | F1: 0.8447
2026-02-10 02:34:04,023 - INFO - --------------------------------------------------
2026-02-10 02:34:04,024 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 02:34:09,677 - INFO - [Train] [58/90] | Loss: 0.3532 | Train Acc: 92.11%
2026-02-10 02:34:11,459 - INFO - [Valid] [58/90] | Loss: 0.4614 | Val Acc: 83.48%
2026-02-10 02:34:11,466 - INFO - [Metrics for 'abnormal'] | Precision: 0.8176 | Recall: 0.8280 | F1: 0.8228
2026-02-10 02:34:11,466 - INFO - [Metrics for 'normal'] | Precision: 0.8500 | Recall: 0.8407 | F1: 0.8453
2026-02-10 02:34:11,467 - INFO - --------------------------------------------------
2026-02-10 02:34:11,467 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 02:34:13,743 - INFO - [Train] [59/90] | Loss: 0.3512 | Train Acc: 91.74%
2026-02-10 02:34:14,291 - INFO - [Valid] [59/90] | Loss: 0.4779 | Val Acc: 82.01%
2026-02-10 02:34:14,296 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 02:34:14,297 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 02:34:14,298 - INFO - --------------------------------------------------
2026-02-10 02:34:14,298 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 02:34:16,752 - INFO - [Train] [60/90] | Loss: 0.3518 | Train Acc: 92.41%
2026-02-10 02:34:17,271 - INFO - [Valid] [60/90] | Loss: 0.4601 | Val Acc: 83.19%
2026-02-10 02:34:17,273 - INFO - [Metrics for 'abnormal'] | Precision: 0.8165 | Recall: 0.8217 | F1: 0.8190
2026-02-10 02:34:17,273 - INFO - [Metrics for 'normal'] | Precision: 0.8453 | Recall: 0.8407 | F1: 0.8430
2026-02-10 02:34:17,274 - INFO - --------------------------------------------------
2026-02-10 02:34:17,274 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 02:34:19,498 - INFO - [Train] [61/90] | Loss: 0.3383 | Train Acc: 93.53%
2026-02-10 02:34:20,015 - INFO - [Valid] [61/90] | Loss: 0.4617 | Val Acc: 83.19%
2026-02-10 02:34:20,017 - INFO - [Metrics for 'abnormal'] | Precision: 0.8205 | Recall: 0.8153 | F1: 0.8179
2026-02-10 02:34:20,017 - INFO - [Metrics for 'normal'] | Precision: 0.8415 | Recall: 0.8462 | F1: 0.8438
2026-02-10 02:34:20,018 - INFO - --------------------------------------------------
2026-02-10 02:34:20,018 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 02:34:22,180 - INFO - [Train] [62/90] | Loss: 0.3429 | Train Acc: 92.49%
2026-02-10 02:34:22,849 - INFO - [Valid] [62/90] | Loss: 0.4678 | Val Acc: 84.07%
2026-02-10 02:34:22,853 - INFO - [Metrics for 'abnormal'] | Precision: 0.8160 | Recall: 0.8471 | F1: 0.8313
2026-02-10 02:34:22,853 - INFO - [Metrics for 'normal'] | Precision: 0.8636 | Recall: 0.8352 | F1: 0.8492
2026-02-10 02:34:22,854 - INFO - --------------------------------------------------
2026-02-10 02:34:22,854 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 02:34:24,778 - INFO - [Train] [63/90] | Loss: 0.3451 | Train Acc: 92.41%
2026-02-10 02:34:25,390 - INFO - [Valid] [63/90] | Loss: 0.4802 | Val Acc: 81.42%
2026-02-10 02:34:25,394 - INFO - [Metrics for 'abnormal'] | Precision: 0.7831 | Recall: 0.8280 | F1: 0.8050
2026-02-10 02:34:25,394 - INFO - [Metrics for 'normal'] | Precision: 0.8439 | Recall: 0.8022 | F1: 0.8225
2026-02-10 02:34:25,396 - INFO - --------------------------------------------------
2026-02-10 02:34:25,396 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 02:34:27,382 - INFO - [Train] [64/90] | Loss: 0.3424 | Train Acc: 92.56%
2026-02-10 02:34:27,988 - INFO - [Valid] [64/90] | Loss: 0.4786 | Val Acc: 83.19%
2026-02-10 02:34:27,992 - INFO - [Metrics for 'abnormal'] | Precision: 0.8165 | Recall: 0.8217 | F1: 0.8190
2026-02-10 02:34:27,992 - INFO - [Metrics for 'normal'] | Precision: 0.8453 | Recall: 0.8407 | F1: 0.8430
2026-02-10 02:34:27,993 - INFO - --------------------------------------------------
2026-02-10 02:34:27,994 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 02:34:30,033 - INFO - [Train] [65/90] | Loss: 0.3360 | Train Acc: 93.38%
2026-02-10 02:34:30,508 - INFO - [Valid] [65/90] | Loss: 0.4799 | Val Acc: 84.66%
2026-02-10 02:34:30,510 - INFO - [Metrics for 'abnormal'] | Precision: 0.8431 | Recall: 0.8217 | F1: 0.8323
2026-02-10 02:34:30,511 - INFO - [Metrics for 'normal'] | Precision: 0.8495 | Recall: 0.8681 | F1: 0.8587
2026-02-10 02:34:30,511 - INFO - --------------------------------------------------
2026-02-10 02:34:30,512 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 02:34:32,716 - INFO - [Train] [66/90] | Loss: 0.3338 | Train Acc: 93.15%
2026-02-10 02:34:33,293 - INFO - [Valid] [66/90] | Loss: 0.4769 | Val Acc: 81.71%
2026-02-10 02:34:33,296 - INFO - [Metrics for 'abnormal'] | Precision: 0.7879 | Recall: 0.8280 | F1: 0.8075
2026-02-10 02:34:33,296 - INFO - [Metrics for 'normal'] | Precision: 0.8448 | Recall: 0.8077 | F1: 0.8258
2026-02-10 02:34:33,296 - INFO - --------------------------------------------------
2026-02-10 02:34:33,297 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 02:34:35,247 - INFO - [Train] [67/90] | Loss: 0.3342 | Train Acc: 92.93%
2026-02-10 02:34:35,882 - INFO - [Valid] [67/90] | Loss: 0.4807 | Val Acc: 84.96%
2026-02-10 02:34:35,886 - INFO - [Metrics for 'abnormal'] | Precision: 0.8397 | Recall: 0.8344 | F1: 0.8371
2026-02-10 02:34:35,886 - INFO - [Metrics for 'normal'] | Precision: 0.8579 | Recall: 0.8626 | F1: 0.8603
2026-02-10 02:34:35,887 - INFO - --------------------------------------------------
2026-02-10 02:34:35,887 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 02:34:37,747 - INFO - [Train] [68/90] | Loss: 0.3272 | Train Acc: 93.97%
2026-02-10 02:34:38,326 - INFO - [Valid] [68/90] | Loss: 0.4737 | Val Acc: 83.78%
2026-02-10 02:34:38,330 - INFO - [Metrics for 'abnormal'] | Precision: 0.8187 | Recall: 0.8344 | F1: 0.8265
2026-02-10 02:34:38,330 - INFO - [Metrics for 'normal'] | Precision: 0.8547 | Recall: 0.8407 | F1: 0.8476
2026-02-10 02:34:38,331 - INFO - --------------------------------------------------
2026-02-10 02:34:38,332 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 02:34:40,498 - INFO - [Train] [69/90] | Loss: 0.3246 | Train Acc: 94.42%
2026-02-10 02:34:41,076 - INFO - [Valid] [69/90] | Loss: 0.4731 | Val Acc: 83.48%
2026-02-10 02:34:41,079 - INFO - [Metrics for 'abnormal'] | Precision: 0.8061 | Recall: 0.8471 | F1: 0.8261
2026-02-10 02:34:41,079 - INFO - [Metrics for 'normal'] | Precision: 0.8621 | Recall: 0.8242 | F1: 0.8427
2026-02-10 02:34:41,080 - INFO - --------------------------------------------------
2026-02-10 02:34:41,080 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 02:34:43,382 - INFO - [Train] [70/90] | Loss: 0.3289 | Train Acc: 93.68%
2026-02-10 02:34:43,829 - INFO - [Valid] [70/90] | Loss: 0.4778 | Val Acc: 82.60%
2026-02-10 02:34:43,833 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8280 | F1: 0.8150
2026-02-10 02:34:43,833 - INFO - [Metrics for 'normal'] | Precision: 0.8475 | Recall: 0.8242 | F1: 0.8357
2026-02-10 02:34:43,834 - INFO - --------------------------------------------------
2026-02-10 02:34:43,834 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 02:34:46,142 - INFO - [Train] [71/90] | Loss: 0.3177 | Train Acc: 93.75%
2026-02-10 02:34:46,737 - INFO - [Valid] [71/90] | Loss: 0.4800 | Val Acc: 82.30%
2026-02-10 02:34:46,740 - INFO - [Metrics for 'abnormal'] | Precision: 0.7939 | Recall: 0.8344 | F1: 0.8137
2026-02-10 02:34:46,740 - INFO - [Metrics for 'normal'] | Precision: 0.8506 | Recall: 0.8132 | F1: 0.8315
2026-02-10 02:34:46,741 - INFO - --------------------------------------------------
2026-02-10 02:34:46,741 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 02:34:48,851 - INFO - [Train] [72/90] | Loss: 0.3283 | Train Acc: 93.68%
2026-02-10 02:34:49,491 - INFO - [Valid] [72/90] | Loss: 0.4816 | Val Acc: 83.78%
2026-02-10 02:34:49,494 - INFO - [Metrics for 'abnormal'] | Precision: 0.8269 | Recall: 0.8217 | F1: 0.8243
2026-02-10 02:34:49,494 - INFO - [Metrics for 'normal'] | Precision: 0.8470 | Recall: 0.8516 | F1: 0.8493
2026-02-10 02:34:49,495 - INFO - --------------------------------------------------
2026-02-10 02:34:49,495 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 02:34:51,396 - INFO - [Train] [73/90] | Loss: 0.3218 | Train Acc: 94.12%
2026-02-10 02:34:52,116 - INFO - [Valid] [73/90] | Loss: 0.4761 | Val Acc: 84.07%
2026-02-10 02:34:52,120 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.8535 | F1: 0.8323
2026-02-10 02:34:52,120 - INFO - [Metrics for 'normal'] | Precision: 0.8678 | Recall: 0.8297 | F1: 0.8483
2026-02-10 02:34:52,121 - INFO - --------------------------------------------------
2026-02-10 02:34:52,122 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 02:34:54,077 - INFO - [Train] [74/90] | Loss: 0.3217 | Train Acc: 93.97%
2026-02-10 02:34:54,639 - INFO - [Valid] [74/90] | Loss: 0.4793 | Val Acc: 83.19%
2026-02-10 02:34:54,644 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.7962 | F1: 0.8143
2026-02-10 02:34:54,644 - INFO - [Metrics for 'normal'] | Precision: 0.8307 | Recall: 0.8626 | F1: 0.8464
2026-02-10 02:34:54,645 - INFO - --------------------------------------------------
2026-02-10 02:34:54,646 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 02:34:56,897 - INFO - [Train] [75/90] | Loss: 0.3206 | Train Acc: 93.90%
2026-02-10 02:34:57,496 - INFO - [Valid] [75/90] | Loss: 0.4733 | Val Acc: 84.07%
2026-02-10 02:34:57,501 - INFO - [Metrics for 'abnormal'] | Precision: 0.8160 | Recall: 0.8471 | F1: 0.8313
2026-02-10 02:34:57,501 - INFO - [Metrics for 'normal'] | Precision: 0.8636 | Recall: 0.8352 | F1: 0.8492
2026-02-10 02:34:57,502 - INFO - --------------------------------------------------
2026-02-10 02:34:57,503 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 02:34:59,835 - INFO - [Train] [76/90] | Loss: 0.3197 | Train Acc: 94.57%
2026-02-10 02:35:00,294 - INFO - [Valid] [76/90] | Loss: 0.4728 | Val Acc: 83.78%
2026-02-10 02:35:00,298 - INFO - [Metrics for 'abnormal'] | Precision: 0.8110 | Recall: 0.8471 | F1: 0.8287
2026-02-10 02:35:00,298 - INFO - [Metrics for 'normal'] | Precision: 0.8629 | Recall: 0.8297 | F1: 0.8459
2026-02-10 02:35:00,300 - INFO - --------------------------------------------------
2026-02-10 02:35:00,300 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 02:35:02,606 - INFO - [Train] [77/90] | Loss: 0.3240 | Train Acc: 93.53%
2026-02-10 02:35:03,173 - INFO - [Valid] [77/90] | Loss: 0.4834 | Val Acc: 83.48%
2026-02-10 02:35:03,177 - INFO - [Metrics for 'abnormal'] | Precision: 0.8098 | Recall: 0.8408 | F1: 0.8250
2026-02-10 02:35:03,177 - INFO - [Metrics for 'normal'] | Precision: 0.8580 | Recall: 0.8297 | F1: 0.8436
2026-02-10 02:35:03,178 - INFO - --------------------------------------------------
2026-02-10 02:35:03,178 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 02:35:05,513 - INFO - [Train] [78/90] | Loss: 0.3158 | Train Acc: 94.94%
2026-02-10 02:35:06,103 - INFO - [Valid] [78/90] | Loss: 0.4737 | Val Acc: 84.37%
2026-02-10 02:35:06,106 - INFO - [Metrics for 'abnormal'] | Precision: 0.8133 | Recall: 0.8599 | F1: 0.8359
2026-02-10 02:35:06,106 - INFO - [Metrics for 'normal'] | Precision: 0.8728 | Recall: 0.8297 | F1: 0.8507
2026-02-10 02:35:06,107 - INFO - --------------------------------------------------
2026-02-10 02:35:06,107 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 02:35:08,035 - INFO - [Train] [79/90] | Loss: 0.3160 | Train Acc: 94.57%
2026-02-10 02:35:08,670 - INFO - [Valid] [79/90] | Loss: 0.4831 | Val Acc: 83.78%
2026-02-10 02:35:08,674 - INFO - [Metrics for 'abnormal'] | Precision: 0.8148 | Recall: 0.8408 | F1: 0.8276
2026-02-10 02:35:08,674 - INFO - [Metrics for 'normal'] | Precision: 0.8588 | Recall: 0.8352 | F1: 0.8468
2026-02-10 02:35:08,675 - INFO - --------------------------------------------------
2026-02-10 02:35:08,675 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 02:35:10,585 - INFO - [Train] [80/90] | Loss: 0.3136 | Train Acc: 95.16%
2026-02-10 02:35:11,170 - INFO - [Valid] [80/90] | Loss: 0.4782 | Val Acc: 84.66%
2026-02-10 02:35:11,174 - INFO - [Metrics for 'abnormal'] | Precision: 0.8261 | Recall: 0.8471 | F1: 0.8365
2026-02-10 02:35:11,174 - INFO - [Metrics for 'normal'] | Precision: 0.8652 | Recall: 0.8462 | F1: 0.8556
2026-02-10 02:35:11,175 - INFO - --------------------------------------------------
2026-02-10 02:35:11,175 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 02:35:13,344 - INFO - [Train] [81/90] | Loss: 0.3168 | Train Acc: 94.57%
2026-02-10 02:35:13,905 - INFO - [Valid] [81/90] | Loss: 0.4816 | Val Acc: 84.37%
2026-02-10 02:35:13,908 - INFO - [Metrics for 'abnormal'] | Precision: 0.8210 | Recall: 0.8471 | F1: 0.8339
2026-02-10 02:35:13,908 - INFO - [Metrics for 'normal'] | Precision: 0.8644 | Recall: 0.8407 | F1: 0.8524
2026-02-10 02:35:13,909 - INFO - --------------------------------------------------
2026-02-10 02:35:13,909 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 02:35:16,150 - INFO - [Train] [82/90] | Loss: 0.3188 | Train Acc: 94.42%
2026-02-10 02:35:16,661 - INFO - [Valid] [82/90] | Loss: 0.4851 | Val Acc: 83.78%
2026-02-10 02:35:16,664 - INFO - [Metrics for 'abnormal'] | Precision: 0.8148 | Recall: 0.8408 | F1: 0.8276
2026-02-10 02:35:16,664 - INFO - [Metrics for 'normal'] | Precision: 0.8588 | Recall: 0.8352 | F1: 0.8468
2026-02-10 02:35:16,665 - INFO - --------------------------------------------------
2026-02-10 02:35:16,665 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 02:35:18,958 - INFO - [Train] [83/90] | Loss: 0.3120 | Train Acc: 95.01%
2026-02-10 02:35:19,627 - INFO - [Valid] [83/90] | Loss: 0.4879 | Val Acc: 84.07%
2026-02-10 02:35:19,629 - INFO - [Metrics for 'abnormal'] | Precision: 0.8239 | Recall: 0.8344 | F1: 0.8291
2026-02-10 02:35:19,629 - INFO - [Metrics for 'normal'] | Precision: 0.8556 | Recall: 0.8462 | F1: 0.8508
2026-02-10 02:35:19,630 - INFO - --------------------------------------------------
2026-02-10 02:35:19,630 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 02:35:21,756 - INFO - [Train] [84/90] | Loss: 0.3146 | Train Acc: 94.87%
2026-02-10 02:35:22,385 - INFO - [Valid] [84/90] | Loss: 0.4793 | Val Acc: 83.78%
2026-02-10 02:35:22,388 - INFO - [Metrics for 'abnormal'] | Precision: 0.8187 | Recall: 0.8344 | F1: 0.8265
2026-02-10 02:35:22,388 - INFO - [Metrics for 'normal'] | Precision: 0.8547 | Recall: 0.8407 | F1: 0.8476
2026-02-10 02:35:22,389 - INFO - --------------------------------------------------
2026-02-10 02:35:22,389 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 02:35:24,326 - INFO - [Train] [85/90] | Loss: 0.3130 | Train Acc: 94.72%
2026-02-10 02:35:25,061 - INFO - [Valid] [85/90] | Loss: 0.4756 | Val Acc: 84.66%
2026-02-10 02:35:25,065 - INFO - [Metrics for 'abnormal'] | Precision: 0.8302 | Recall: 0.8408 | F1: 0.8354
2026-02-10 02:35:25,065 - INFO - [Metrics for 'normal'] | Precision: 0.8611 | Recall: 0.8516 | F1: 0.8564
2026-02-10 02:35:25,067 - INFO - --------------------------------------------------
2026-02-10 02:35:25,067 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 02:35:26,995 - INFO - [Train] [86/90] | Loss: 0.3136 | Train Acc: 95.09%
2026-02-10 02:35:27,569 - INFO - [Valid] [86/90] | Loss: 0.4788 | Val Acc: 83.78%
2026-02-10 02:35:27,572 - INFO - [Metrics for 'abnormal'] | Precision: 0.8228 | Recall: 0.8280 | F1: 0.8254
2026-02-10 02:35:27,572 - INFO - [Metrics for 'normal'] | Precision: 0.8508 | Recall: 0.8462 | F1: 0.8485
2026-02-10 02:35:27,573 - INFO - --------------------------------------------------
2026-02-10 02:35:27,573 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 02:35:29,670 - INFO - [Train] [87/90] | Loss: 0.3070 | Train Acc: 95.31%
2026-02-10 02:35:30,140 - INFO - [Valid] [87/90] | Loss: 0.4892 | Val Acc: 84.07%
2026-02-10 02:35:30,145 - INFO - [Metrics for 'abnormal'] | Precision: 0.8323 | Recall: 0.8217 | F1: 0.8269
2026-02-10 02:35:30,145 - INFO - [Metrics for 'normal'] | Precision: 0.8478 | Recall: 0.8571 | F1: 0.8525
2026-02-10 02:35:30,147 - INFO - --------------------------------------------------
2026-02-10 02:35:30,147 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 02:35:32,321 - INFO - [Train] [88/90] | Loss: 0.3137 | Train Acc: 94.64%
2026-02-10 02:35:32,890 - INFO - [Valid] [88/90] | Loss: 0.4836 | Val Acc: 83.19%
2026-02-10 02:35:32,893 - INFO - [Metrics for 'abnormal'] | Precision: 0.8049 | Recall: 0.8408 | F1: 0.8224
2026-02-10 02:35:32,893 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.8242 | F1: 0.8403
2026-02-10 02:35:32,894 - INFO - --------------------------------------------------
2026-02-10 02:35:32,894 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 02:35:34,852 - INFO - [Train] [89/90] | Loss: 0.3102 | Train Acc: 94.64%
2026-02-10 02:35:35,488 - INFO - [Valid] [89/90] | Loss: 0.4798 | Val Acc: 84.07%
2026-02-10 02:35:35,492 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.8535 | F1: 0.8323
2026-02-10 02:35:35,492 - INFO - [Metrics for 'normal'] | Precision: 0.8678 | Recall: 0.8297 | F1: 0.8483
2026-02-10 02:35:35,494 - INFO - --------------------------------------------------
2026-02-10 02:35:35,494 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 02:35:37,445 - INFO - [Train] [90/90] | Loss: 0.3106 | Train Acc: 94.72%
2026-02-10 02:35:38,211 - INFO - [Valid] [90/90] | Loss: 0.4731 | Val Acc: 84.37%
2026-02-10 02:35:38,215 - INFO - [Metrics for 'abnormal'] | Precision: 0.8291 | Recall: 0.8344 | F1: 0.8317
2026-02-10 02:35:38,215 - INFO - [Metrics for 'normal'] | Precision: 0.8564 | Recall: 0.8516 | F1: 0.8540
2026-02-10 02:35:38,218 - INFO - ==================================================
2026-02-10 02:35:38,219 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 02:35:38,219 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 02:35:38,219 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 02:35:38,273 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 02:35:38,275 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:35:38,275 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:35:39,043 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.928125)에 맞춰 변경되었습니다.
2026-02-10 02:35:39,043 - INFO - ==================================================
2026-02-10 02:35:39,047 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/best_model.pth'
2026-02-10 02:35:39,047 - INFO - ==================================================
2026-02-10 02:35:39,047 - INFO - Test 모드를 시작합니다.
2026-02-10 02:35:39,080 - INFO - 연산량 (MACs): 0.0741 GMACs per sample
2026-02-10 02:35:39,080 - INFO - 연산량 (FLOPs): 0.1481 GFLOPs per sample
2026-02-10 02:35:39,080 - INFO - ==================================================
2026-02-10 02:35:39,080 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 02:35:39,488 - INFO - 샘플 당 평균 Forward Pass 시간: 0.12ms (std: 0.04ms), FPS: 8655.26 (std: 957.88) (1개 샘플 x 100회 반복)
2026-02-10 02:35:39,488 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 159.82 MB
2026-02-10 02:35:39,488 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 02:35:40,616 - INFO - [Test] Loss: 0.3875 | Test Acc: 82.30%
2026-02-10 02:35:40,620 - INFO - [Metrics for 'abnormal'] | Precision: 0.8212 | Recall: 0.7898 | F1: 0.8052
2026-02-10 02:35:40,620 - INFO - [Metrics for 'normal'] | Precision: 0.8245 | Recall: 0.8516 | F1: 0.8378
2026-02-10 02:35:40,910 - INFO - ==================================================
2026-02-10 02:35:40,910 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/confusion_matrix_20260210_022736.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/confusion_matrix_20260210_022736.pdf'
2026-02-10 02:35:40,910 - INFO - ==================================================
2026-02-10 02:35:40,911 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 02:35:41,077 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/model_fp32_20260210_022736.onnx'에 저장되었습니다. (크기: 0.18 MB)
2026-02-10 02:35:41,296 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 13.50 MB
2026-02-10 02:35:41,296 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 02:35:41,764 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 2.56ms (std: 3.19ms)
2026-02-10 02:35:41,764 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 1164.88 FPS (std: 733.18) (1개 샘플 x 100회 반복)
2026-02-10 02:35:41,764 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 5.44 MB
2026-02-10 02:35:41,764 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 20.15 MB
2026-02-10 02:35:42,661 - INFO - [Test (ONNX)] | Test Acc (ONNX): 82.30%
2026-02-10 02:35:42,668 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8212 | Recall: 0.7898 | F1: 0.8052
2026-02-10 02:35:42,668 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8245 | Recall: 0.8516 | F1: 0.8378
2026-02-10 02:35:42,854 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/val_acc.pdf'
2026-02-10 02:35:43,059 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/train_val_acc.pdf'
2026-02-10 02:35:43,222 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/F1_normal.pdf'
2026-02-10 02:35:43,409 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/val_loss.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/val_loss.pdf'
2026-02-10 02:35:43,554 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/learning_rate.pdf'
2026-02-10 02:35:44,935 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/compile.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_022736/graph_20260210_022736/compile.pdf'
