2026-02-10 08:52:44,936 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/log_20260210_085244.log'에 저장됩니다.
2026-02-10 08:52:44,939 - INFO - ==================================================
2026-02-10 08:52:44,939 - INFO - config.yaml:
2026-02-10 08:52:44,939 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: mobilenet_v4_s
  use_wanda_pruning: true
  num_wanda_calib_samples: 1353
  pruning_params_target: 0.047585

2026-02-10 08:52:44,939 - INFO - ==================================================
2026-02-10 08:52:44,969 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 08:52:44,969 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 08:52:44,969 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 08:52:44,975 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 08:52:44,976 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 08:52:44,976 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 08:52:44,976 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 08:52:45,116 - INFO - ==================================================
2026-02-10 08:52:45,116 - INFO - 모델 파라미터 수:
2026-02-10 08:52:45,116 - INFO -   - 총 파라미터: 2,495,586 개
2026-02-10 08:52:45,116 - INFO -   - 학습 가능한 파라미터: 2,495,586 개
2026-02-10 08:52:45,116 - INFO - ================================================================================
2026-02-10 08:52:45,116 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 08:52:45,116 - INFO - ================================================================================
2026-02-10 08:52:45,116 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 08:52:45,116 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 08:52:45,116 - INFO - ==================================================
2026-02-10 08:52:45,116 - INFO - train 모드를 시작합니다.
2026-02-10 08:52:45,116 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 08:52:45,116 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 08:52:45,116 - INFO - --------------------------------------------------
2026-02-10 08:52:45,117 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 08:52:46,946 - INFO - [Train] [1/10] | Loss: 2.7770 | Train Acc: 65.70%
2026-02-10 08:52:47,558 - INFO - [Valid] [1/10] | Loss: 0.8568 | Val Acc: 70.21%
2026-02-10 08:52:47,561 - INFO - [Metrics for 'abnormal'] | Precision: 0.6667 | Recall: 0.7134 | F1: 0.6892
2026-02-10 08:52:47,561 - INFO - [Metrics for 'normal'] | Precision: 0.7368 | Recall: 0.6923 | F1: 0.7139
2026-02-10 08:52:47,575 - INFO - [Best Model Saved] (val loss: 0.8568) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 08:52:47,575 - INFO - --------------------------------------------------
2026-02-10 08:52:47,575 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 08:52:49,107 - INFO - [Train] [2/10] | Loss: 0.7867 | Train Acc: 70.83%
2026-02-10 08:52:49,506 - INFO - [Valid] [2/10] | Loss: 1.0260 | Val Acc: 68.44%
2026-02-10 08:52:49,509 - INFO - [Metrics for 'abnormal'] | Precision: 0.8378 | Recall: 0.3949 | F1: 0.5368
2026-02-10 08:52:49,509 - INFO - [Metrics for 'normal'] | Precision: 0.6415 | Recall: 0.9341 | F1: 0.7606
2026-02-10 08:52:49,509 - INFO - --------------------------------------------------
2026-02-10 08:52:49,510 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 08:52:51,027 - INFO - [Train] [3/10] | Loss: 0.6779 | Train Acc: 73.14%
2026-02-10 08:52:51,427 - INFO - [Valid] [3/10] | Loss: 0.9420 | Val Acc: 70.50%
2026-02-10 08:52:51,430 - INFO - [Metrics for 'abnormal'] | Precision: 0.7714 | Recall: 0.5159 | F1: 0.6183
2026-02-10 08:52:51,430 - INFO - [Metrics for 'normal'] | Precision: 0.6752 | Recall: 0.8681 | F1: 0.7596
2026-02-10 08:52:51,431 - INFO - --------------------------------------------------
2026-02-10 08:52:51,431 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 08:52:52,951 - INFO - [Train] [4/10] | Loss: 0.5971 | Train Acc: 76.19%
2026-02-10 08:52:53,353 - INFO - [Valid] [4/10] | Loss: 0.6316 | Val Acc: 74.34%
2026-02-10 08:52:53,356 - INFO - [Metrics for 'abnormal'] | Precision: 0.6862 | Recall: 0.8217 | F1: 0.7478
2026-02-10 08:52:53,356 - INFO - [Metrics for 'normal'] | Precision: 0.8146 | Recall: 0.6758 | F1: 0.7387
2026-02-10 08:52:53,382 - INFO - [Best Model Saved] (val loss: 0.6316) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 08:52:53,382 - INFO - --------------------------------------------------
2026-02-10 08:52:53,383 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 08:52:54,882 - INFO - [Train] [5/10] | Loss: 0.5715 | Train Acc: 77.31%
2026-02-10 08:52:55,287 - INFO - [Valid] [5/10] | Loss: 1.0225 | Val Acc: 66.37%
2026-02-10 08:52:55,290 - INFO - [Metrics for 'abnormal'] | Precision: 0.6272 | Recall: 0.6752 | F1: 0.6503
2026-02-10 08:52:55,290 - INFO - [Metrics for 'normal'] | Precision: 0.7000 | Recall: 0.6538 | F1: 0.6761
2026-02-10 08:52:55,291 - INFO - --------------------------------------------------
2026-02-10 08:52:55,291 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 08:52:57,249 - INFO - [Train] [6/10] | Loss: 0.5393 | Train Acc: 79.46%
2026-02-10 08:52:57,984 - INFO - [Valid] [6/10] | Loss: 0.8052 | Val Acc: 73.75%
2026-02-10 08:52:57,987 - INFO - [Metrics for 'abnormal'] | Precision: 0.7329 | Recall: 0.6815 | F1: 0.7063
2026-02-10 08:52:57,987 - INFO - [Metrics for 'normal'] | Precision: 0.7409 | Recall: 0.7857 | F1: 0.7627
2026-02-10 08:52:57,988 - INFO - --------------------------------------------------
2026-02-10 08:52:57,988 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 08:53:00,113 - INFO - [Train] [7/10] | Loss: 0.5310 | Train Acc: 79.99%
2026-02-10 08:53:00,789 - INFO - [Valid] [7/10] | Loss: 0.6420 | Val Acc: 78.47%
2026-02-10 08:53:00,793 - INFO - [Metrics for 'abnormal'] | Precision: 0.7283 | Recall: 0.8535 | F1: 0.7859
2026-02-10 08:53:00,793 - INFO - [Metrics for 'normal'] | Precision: 0.8516 | Recall: 0.7253 | F1: 0.7834
2026-02-10 08:53:00,794 - INFO - --------------------------------------------------
2026-02-10 08:53:00,795 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 08:53:02,955 - INFO - [Train] [8/10] | Loss: 0.4954 | Train Acc: 80.51%
2026-02-10 08:53:03,649 - INFO - [Valid] [8/10] | Loss: 0.5733 | Val Acc: 79.06%
2026-02-10 08:53:03,653 - INFO - [Metrics for 'abnormal'] | Precision: 0.7287 | Recall: 0.8726 | F1: 0.7942
2026-02-10 08:53:03,653 - INFO - [Metrics for 'normal'] | Precision: 0.8675 | Recall: 0.7198 | F1: 0.7868
2026-02-10 08:53:03,685 - INFO - [Best Model Saved] (val loss: 0.5733) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 08:53:03,685 - INFO - --------------------------------------------------
2026-02-10 08:53:03,686 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 08:53:05,888 - INFO - [Train] [9/10] | Loss: 0.4568 | Train Acc: 83.78%
2026-02-10 08:53:06,548 - INFO - [Valid] [9/10] | Loss: 0.5529 | Val Acc: 76.70%
2026-02-10 08:53:06,553 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.6879 | F1: 0.7322
2026-02-10 08:53:06,553 - INFO - [Metrics for 'normal'] | Precision: 0.7562 | Recall: 0.8352 | F1: 0.7937
2026-02-10 08:53:06,580 - INFO - [Best Model Saved] (val loss: 0.5529) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 08:53:06,580 - INFO - --------------------------------------------------
2026-02-10 08:53:06,581 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 08:53:08,749 - INFO - [Train] [10/10] | Loss: 0.4386 | Train Acc: 84.67%
2026-02-10 08:53:09,387 - INFO - [Valid] [10/10] | Loss: 0.5495 | Val Acc: 79.65%
2026-02-10 08:53:09,392 - INFO - [Metrics for 'abnormal'] | Precision: 0.7933 | Recall: 0.7580 | F1: 0.7752
2026-02-10 08:53:09,399 - INFO - [Metrics for 'normal'] | Precision: 0.7989 | Recall: 0.8297 | F1: 0.8140
2026-02-10 08:53:09,427 - INFO - [Best Model Saved] (val loss: 0.5495) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 08:53:09,428 - INFO - ================================================================================
2026-02-10 08:53:09,428 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 08:53:09,428 - INFO - ================================================================================
2026-02-10 08:53:09,468 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'을(를) 불러왔습니다.
2026-02-10 08:53:09,469 - INFO - ================================================================================
2026-02-10 08:53:09,469 - INFO - 목표 파라미터 수 (0.0476M)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 08:53:09,470 - INFO - 원본 모델 파라미터: 2.4956M
2026-02-10 08:53:09,507 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:09,508 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:09,509 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:11,655 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:11,766 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 08:53:11,766 - INFO - ==================================================
2026-02-10 08:53:11,767 - INFO -   [탐색  1] 희소도: 0.4950 -> 파라미터: 0.6554M (감소율: 73.74%)
2026-02-10 08:53:11,781 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:11,781 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:11,782 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:13,921 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:13,972 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-02-10 08:53:13,972 - INFO - ==================================================
2026-02-10 08:53:13,973 - INFO -   [탐색  2] 희소도: 0.7425 -> 파라미터: 0.1807M (감소율: 92.76%)
2026-02-10 08:53:13,992 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:13,992 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:13,993 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:16,365 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:16,422 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-02-10 08:53:16,422 - INFO - ==================================================
2026-02-10 08:53:16,425 - INFO -   [탐색  3] 희소도: 0.8662 -> 파라미터: 0.0548M (감소율: 97.80%)
2026-02-10 08:53:16,444 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:16,444 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:16,445 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:18,635 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:18,696 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.928125)에 맞춰 변경되었습니다.
2026-02-10 08:53:18,697 - INFO - ==================================================
2026-02-10 08:53:18,698 - INFO -   [탐색  4] 희소도: 0.9281 -> 파라미터: 0.0186M (감소율: 99.25%)
2026-02-10 08:53:18,939 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:18,939 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:18,940 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:21,050 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:21,129 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8971875)에 맞춰 변경되었습니다.
2026-02-10 08:53:21,129 - INFO - ==================================================
2026-02-10 08:53:21,130 - INFO -   [탐색  5] 희소도: 0.8972 -> 파라미터: 0.0343M (감소율: 98.63%)
2026-02-10 08:53:21,151 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:21,151 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:21,152 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:23,254 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:23,337 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.88171875)에 맞춰 변경되었습니다.
2026-02-10 08:53:23,338 - INFO - ==================================================
2026-02-10 08:53:23,339 - INFO -   [탐색  6] 희소도: 0.8817 -> 파라미터: 0.0441M (감소율: 98.23%)
2026-02-10 08:53:23,359 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:23,359 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:23,360 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:25,659 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:25,692 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.873984375)에 맞춰 변경되었습니다.
2026-02-10 08:53:25,692 - INFO - ==================================================
2026-02-10 08:53:25,693 - INFO -   [탐색  7] 희소도: 0.8740 -> 파라미터: 0.0497M (감소율: 98.01%)
2026-02-10 08:53:25,704 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:25,704 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:25,705 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:28,043 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:28,121 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8778515625000001)에 맞춰 변경되었습니다.
2026-02-10 08:53:28,121 - INFO - ==================================================
2026-02-10 08:53:28,122 - INFO -   [탐색  8] 희소도: 0.8779 -> 파라미터: 0.0461M (감소율: 98.15%)
2026-02-10 08:53:28,133 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:28,133 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:28,134 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:30,360 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:30,426 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.87591796875)에 맞춰 변경되었습니다.
2026-02-10 08:53:30,426 - INFO - ==================================================
2026-02-10 08:53:30,427 - INFO -   [탐색  9] 희소도: 0.8759 -> 파라미터: 0.0470M (감소율: 98.12%)
2026-02-10 08:53:30,442 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:30,442 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:30,444 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:32,653 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:32,718 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.874951171875)에 맞춰 변경되었습니다.
2026-02-10 08:53:32,718 - INFO - ==================================================
2026-02-10 08:53:32,720 - INFO -   [탐색 10] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:53:32,743 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:32,743 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:32,745 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:35,433 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:35,499 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8754345703125)에 맞춰 변경되었습니다.
2026-02-10 08:53:35,500 - INFO - ==================================================
2026-02-10 08:53:35,501 - INFO -   [탐색 11] 희소도: 0.8754 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:53:35,533 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:35,534 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:35,535 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:38,845 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:38,958 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.87519287109375)에 맞춰 변경되었습니다.
2026-02-10 08:53:38,959 - INFO - ==================================================
2026-02-10 08:53:38,960 - INFO -   [탐색 12] 희소도: 0.8752 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:53:38,983 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:38,983 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:38,984 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:41,977 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:42,072 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875072021484375)에 맞춰 변경되었습니다.
2026-02-10 08:53:42,072 - INFO - ==================================================
2026-02-10 08:53:42,074 - INFO -   [탐색 13] 희소도: 0.8751 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:53:42,098 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:42,098 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:42,100 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:44,927 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:44,999 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750115966796875)에 맞춰 변경되었습니다.
2026-02-10 08:53:44,999 - INFO - ==================================================
2026-02-10 08:53:45,001 - INFO -   [탐색 14] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:53:45,026 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:45,027 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:45,028 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:48,424 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:48,513 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749813842773437)에 맞춰 변경되었습니다.
2026-02-10 08:53:48,513 - INFO - ==================================================
2026-02-10 08:53:48,514 - INFO -   [탐색 15] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:53:48,540 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:48,540 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:48,542 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:51,875 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:51,945 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749964904785157)에 맞춰 변경되었습니다.
2026-02-10 08:53:51,945 - INFO - ==================================================
2026-02-10 08:53:51,947 - INFO -   [탐색 16] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:53:52,478 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:52,479 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:52,482 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:53:56,669 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:53:56,766 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750040435791016)에 맞춰 변경되었습니다.
2026-02-10 08:53:56,766 - INFO - ==================================================
2026-02-10 08:53:56,769 - INFO -   [탐색 17] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:53:56,794 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:53:56,794 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:53:56,796 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:00,825 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:00,913 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750002670288086)에 맞춰 변경되었습니다.
2026-02-10 08:54:00,913 - INFO - ==================================================
2026-02-10 08:54:00,915 - INFO -   [탐색 18] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:54:00,941 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:00,941 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:00,943 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:05,565 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:05,655 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749983787536622)에 맞춰 변경되었습니다.
2026-02-10 08:54:05,656 - INFO - ==================================================
2026-02-10 08:54:05,658 - INFO -   [탐색 19] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:54:05,682 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:05,682 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:05,684 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:10,471 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:10,586 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749993228912354)에 맞춰 변경되었습니다.
2026-02-10 08:54:10,586 - INFO - ==================================================
2026-02-10 08:54:10,588 - INFO -   [탐색 20] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:54:10,611 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:10,611 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:10,614 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:16,267 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:16,352 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.874999794960022)에 맞춰 변경되었습니다.
2026-02-10 08:54:16,352 - INFO - ==================================================
2026-02-10 08:54:16,355 - INFO -   [탐색 21] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:54:16,383 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:16,383 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:16,385 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:21,707 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:21,830 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000309944153)에 맞춰 변경되었습니다.
2026-02-10 08:54:21,831 - INFO - ==================================================
2026-02-10 08:54:21,833 - INFO -   [탐색 22] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:54:21,858 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:21,858 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:21,859 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:27,266 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:27,357 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999129772186)에 맞춰 변경되었습니다.
2026-02-10 08:54:27,358 - INFO - ==================================================
2026-02-10 08:54:27,360 - INFO -   [탐색 23] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:54:27,384 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:27,385 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:27,387 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:32,150 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:32,229 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999719858169)에 맞춰 변경되었습니다.
2026-02-10 08:54:32,229 - INFO - ==================================================
2026-02-10 08:54:32,231 - INFO -   [탐색 24] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:54:32,254 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:32,254 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:32,256 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:38,076 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:38,233 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875000001490116)에 맞춰 변경되었습니다.
2026-02-10 08:54:38,234 - INFO - ==================================================
2026-02-10 08:54:38,236 - INFO -   [탐색 25] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:54:38,259 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:38,260 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:38,262 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:43,408 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:43,493 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999867379665)에 맞춰 변경되었습니다.
2026-02-10 08:54:43,493 - INFO - ==================================================
2026-02-10 08:54:43,495 - INFO -   [탐색 26] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:54:43,519 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:43,519 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:43,521 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:49,177 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:49,329 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999941140413)에 맞춰 변경되었습니다.
2026-02-10 08:54:49,329 - INFO - ==================================================
2026-02-10 08:54:49,331 - INFO -   [탐색 27] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:54:49,355 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:49,355 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:49,357 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:54:55,039 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:54:55,189 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999978020786)에 맞춰 변경되었습니다.
2026-02-10 08:54:55,189 - INFO - ==================================================
2026-02-10 08:54:55,190 - INFO -   [탐색 28] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:54:55,218 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:54:55,218 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:54:55,220 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:00,142 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:00,236 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999996460973)에 맞춰 변경되었습니다.
2026-02-10 08:55:00,236 - INFO - ==================================================
2026-02-10 08:55:00,238 - INFO -   [탐색 29] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:55:00,795 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:00,795 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:00,796 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:06,105 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:06,286 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000005681067)에 맞춰 변경되었습니다.
2026-02-10 08:55:06,287 - INFO - ==================================================
2026-02-10 08:55:06,290 - INFO -   [탐색 30] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:55:06,315 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:06,316 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:06,317 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:12,488 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:12,611 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875000000107102)에 맞춰 변경되었습니다.
2026-02-10 08:55:12,611 - INFO - ==================================================
2026-02-10 08:55:12,613 - INFO -   [탐색 31] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:55:12,640 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:12,640 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:12,642 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:17,011 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:17,093 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999998765996)에 맞춰 변경되었습니다.
2026-02-10 08:55:17,093 - INFO - ==================================================
2026-02-10 08:55:17,095 - INFO -   [탐색 32] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:55:17,119 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:17,119 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:17,121 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:21,133 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:21,246 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999999918507)에 맞춰 변경되었습니다.
2026-02-10 08:55:21,247 - INFO - ==================================================
2026-02-10 08:55:21,249 - INFO -   [탐색 33] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:55:21,272 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:21,272 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:21,275 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:26,148 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:26,246 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000494764)에 맞춰 변경되었습니다.
2026-02-10 08:55:26,247 - INFO - ==================================================
2026-02-10 08:55:26,249 - INFO -   [탐색 34] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:55:26,272 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:26,272 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:26,274 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:30,805 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:30,899 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000206636)에 맞춰 변경되었습니다.
2026-02-10 08:55:30,899 - INFO - ==================================================
2026-02-10 08:55:30,901 - INFO -   [탐색 35] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:55:30,923 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:30,923 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:30,926 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:34,285 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:34,372 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000062572)에 맞춰 변경되었습니다.
2026-02-10 08:55:34,372 - INFO - ==================================================
2026-02-10 08:55:34,374 - INFO -   [탐색 36] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:55:34,395 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:34,396 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:34,397 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:37,629 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:37,699 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.874999999999054)에 맞춰 변경되었습니다.
2026-02-10 08:55:37,699 - INFO - ==================================================
2026-02-10 08:55:37,701 - INFO -   [탐색 37] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:55:37,715 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:37,715 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:37,716 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:41,502 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:41,628 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000026557)에 맞춰 변경되었습니다.
2026-02-10 08:55:41,628 - INFO - ==================================================
2026-02-10 08:55:41,629 - INFO -   [탐색 38] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:55:41,640 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:41,640 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:41,641 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:45,017 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:45,153 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000008549)에 맞춰 변경되었습니다.
2026-02-10 08:55:45,153 - INFO - ==================================================
2026-02-10 08:55:45,155 - INFO -   [탐색 39] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:55:45,178 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:45,178 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:45,179 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:48,461 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:48,595 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999999999545)에 맞춰 변경되었습니다.
2026-02-10 08:55:48,595 - INFO - ==================================================
2026-02-10 08:55:48,597 - INFO -   [탐색 40] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:55:48,620 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:48,620 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:48,621 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:52,230 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:52,840 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000004047)에 맞춰 변경되었습니다.
2026-02-10 08:55:52,840 - INFO - ==================================================
2026-02-10 08:55:52,854 - INFO -   [탐색 41] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:55:52,881 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:52,881 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:52,883 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:55:56,182 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:55:56,279 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000001796)에 맞춰 변경되었습니다.
2026-02-10 08:55:56,279 - INFO - ==================================================
2026-02-10 08:55:56,281 - INFO -   [탐색 42] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:55:56,306 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:55:56,306 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:55:56,307 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:00,157 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:00,231 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000000671)에 맞춰 변경되었습니다.
2026-02-10 08:56:00,231 - INFO - ==================================================
2026-02-10 08:56:00,232 - INFO -   [탐색 43] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:56:00,264 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:00,264 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:00,266 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:03,602 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:03,715 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000000108)에 맞춰 변경되었습니다.
2026-02-10 08:56:03,715 - INFO - ==================================================
2026-02-10 08:56:03,716 - INFO -   [탐색 44] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:56:03,739 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:03,739 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:03,740 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:06,979 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:07,118 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999999999827)에 맞춰 변경되었습니다.
2026-02-10 08:56:07,118 - INFO - ==================================================
2026-02-10 08:56:07,122 - INFO -   [탐색 45] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:56:07,149 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:07,150 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:07,151 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:11,881 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:11,973 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999999999967)에 맞춰 변경되었습니다.
2026-02-10 08:56:11,974 - INFO - ==================================================
2026-02-10 08:56:11,976 - INFO -   [탐색 46] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:56:12,001 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:12,002 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:12,003 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:16,537 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:16,641 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000000038)에 맞춰 변경되었습니다.
2026-02-10 08:56:16,642 - INFO - ==================================================
2026-02-10 08:56:16,643 - INFO -   [탐색 47] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:56:16,669 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:16,669 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:16,670 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:21,042 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:21,171 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000000002)에 맞춰 변경되었습니다.
2026-02-10 08:56:21,171 - INFO - ==================================================
2026-02-10 08:56:21,173 - INFO -   [탐색 48] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:56:21,199 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:21,199 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:21,201 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:26,014 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:26,116 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999999999984)에 맞춰 변경되었습니다.
2026-02-10 08:56:26,117 - INFO - ==================================================
2026-02-10 08:56:26,119 - INFO -   [탐색 49] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:56:26,146 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:26,147 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:26,149 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:31,714 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:31,879 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999999999993)에 맞춰 변경되었습니다.
2026-02-10 08:56:31,879 - INFO - ==================================================
2026-02-10 08:56:31,881 - INFO -   [탐색 50] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:56:31,908 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:31,909 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:31,910 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:37,964 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:38,062 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8749999999999998)에 맞춰 변경되었습니다.
2026-02-10 08:56:38,062 - INFO - ==================================================
2026-02-10 08:56:38,064 - INFO -   [탐색 51] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:56:38,617 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:38,617 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:38,620 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:44,394 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:44,471 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:56:44,471 - INFO - ==================================================
2026-02-10 08:56:44,475 - INFO -   [탐색 52] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:56:44,501 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:44,502 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:44,504 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:50,172 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:50,275 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8750000000000001)에 맞춰 변경되었습니다.
2026-02-10 08:56:50,275 - INFO - ==================================================
2026-02-10 08:56:50,277 - INFO -   [탐색 53] 희소도: 0.8750 -> 파라미터: 0.0472M (감소율: 98.11%)
2026-02-10 08:56:50,305 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:50,306 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:50,307 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:56:55,876 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:56:55,963 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:56:55,963 - INFO - ==================================================
2026-02-10 08:56:55,966 - INFO -   [탐색 54] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:56:55,991 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:56:55,992 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:56:55,994 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:01,494 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:01,591 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:01,592 - INFO - ==================================================
2026-02-10 08:57:01,594 - INFO -   [탐색 55] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:01,622 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:01,622 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:01,624 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:06,947 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:07,084 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:07,084 - INFO - ==================================================
2026-02-10 08:57:07,085 - INFO -   [탐색 56] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:07,109 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:07,110 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:07,111 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:12,996 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:13,087 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:13,087 - INFO - ==================================================
2026-02-10 08:57:13,089 - INFO -   [탐색 57] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:13,115 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:13,115 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:13,117 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:18,449 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:18,533 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:18,533 - INFO - ==================================================
2026-02-10 08:57:18,535 - INFO -   [탐색 58] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:18,564 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:18,564 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:18,566 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:24,236 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:24,380 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:24,381 - INFO - ==================================================
2026-02-10 08:57:24,382 - INFO -   [탐색 59] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:24,407 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:24,407 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:24,409 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:29,451 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:29,600 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:29,600 - INFO - ==================================================
2026-02-10 08:57:29,603 - INFO -   [탐색 60] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:29,625 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:29,625 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:29,627 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:34,848 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:34,927 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:34,927 - INFO - ==================================================
2026-02-10 08:57:34,929 - INFO -   [탐색 61] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:34,955 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:34,955 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:34,957 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:40,389 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:40,563 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:40,563 - INFO - ==================================================
2026-02-10 08:57:40,566 - INFO -   [탐색 62] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:40,590 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:40,590 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:40,592 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:45,885 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:46,000 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:46,000 - INFO - ==================================================
2026-02-10 08:57:46,003 - INFO -   [탐색 63] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:46,561 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:46,561 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:46,563 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:51,723 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:51,793 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:51,794 - INFO - ==================================================
2026-02-10 08:57:51,796 - INFO -   [탐색 64] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:51,822 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:51,822 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:51,824 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:57:57,371 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:57:57,488 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:57:57,488 - INFO - ==================================================
2026-02-10 08:57:57,491 - INFO -   [탐색 65] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:57:57,516 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:57:57,517 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:57:57,518 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:02,815 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:02,919 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:02,920 - INFO - ==================================================
2026-02-10 08:58:02,921 - INFO -   [탐색 66] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:02,947 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:02,947 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:02,949 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:08,528 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:08,624 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:08,624 - INFO - ==================================================
2026-02-10 08:58:08,626 - INFO -   [탐색 67] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:08,651 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:08,652 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:08,655 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:14,462 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:14,590 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:14,590 - INFO - ==================================================
2026-02-10 08:58:14,592 - INFO -   [탐색 68] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:14,618 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:14,618 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:14,619 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:19,902 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:19,982 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:19,982 - INFO - ==================================================
2026-02-10 08:58:19,984 - INFO -   [탐색 69] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:20,013 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:20,013 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:20,015 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:25,012 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:25,128 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:25,128 - INFO - ==================================================
2026-02-10 08:58:25,130 - INFO -   [탐색 70] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:25,157 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:25,157 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:25,159 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:30,902 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:30,987 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:30,987 - INFO - ==================================================
2026-02-10 08:58:30,989 - INFO -   [탐색 71] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:31,020 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:31,020 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:31,024 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:36,513 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:36,615 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:36,615 - INFO - ==================================================
2026-02-10 08:58:36,617 - INFO -   [탐색 72] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:36,641 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:36,641 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:36,643 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:42,162 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:42,276 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:42,276 - INFO - ==================================================
2026-02-10 08:58:42,277 - INFO -   [탐색 73] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:42,302 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:42,303 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:42,304 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:48,336 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:48,438 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:48,438 - INFO - ==================================================
2026-02-10 08:58:48,440 - INFO -   [탐색 74] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:48,466 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:48,467 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:48,468 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:53,775 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:58:53,867 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:58:53,868 - INFO - ==================================================
2026-02-10 08:58:53,870 - INFO -   [탐색 75] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:58:53,893 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:58:53,893 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:58:53,895 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:58:59,486 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:00,065 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:00,065 - INFO - ==================================================
2026-02-10 08:59:00,068 - INFO -   [탐색 76] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:00,092 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:00,092 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:00,093 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:05,882 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:06,005 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:06,006 - INFO - ==================================================
2026-02-10 08:59:06,008 - INFO -   [탐색 77] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:06,031 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:06,031 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:06,033 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:11,606 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:11,709 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:11,710 - INFO - ==================================================
2026-02-10 08:59:11,712 - INFO -   [탐색 78] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:11,737 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:11,737 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:11,739 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:17,399 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:17,485 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:17,485 - INFO - ==================================================
2026-02-10 08:59:17,487 - INFO -   [탐색 79] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:17,512 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:17,512 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:17,514 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:23,484 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:23,678 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:23,679 - INFO - ==================================================
2026-02-10 08:59:23,681 - INFO -   [탐색 80] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:23,706 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:23,706 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:23,708 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:28,930 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:29,024 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:29,024 - INFO - ==================================================
2026-02-10 08:59:29,027 - INFO -   [탐색 81] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:29,056 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:29,056 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:29,058 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:33,878 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:33,969 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:33,969 - INFO - ==================================================
2026-02-10 08:59:33,971 - INFO -   [탐색 82] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:33,995 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:33,995 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:33,997 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:40,025 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:40,183 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:40,184 - INFO - ==================================================
2026-02-10 08:59:40,186 - INFO -   [탐색 83] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:40,215 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:40,215 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:40,217 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:45,210 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:45,332 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:45,332 - INFO - ==================================================
2026-02-10 08:59:45,334 - INFO -   [탐색 84] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:45,356 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:45,356 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:45,358 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:50,858 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:50,993 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:50,994 - INFO - ==================================================
2026-02-10 08:59:50,996 - INFO -   [탐색 85] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:51,020 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:51,020 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:51,023 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 08:59:56,557 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 08:59:56,659 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 08:59:56,660 - INFO - ==================================================
2026-02-10 08:59:56,662 - INFO -   [탐색 86] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 08:59:56,686 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 08:59:56,686 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 08:59:56,688 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:02,085 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:02,178 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:02,178 - INFO - ==================================================
2026-02-10 09:00:02,181 - INFO -   [탐색 87] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:02,207 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:02,207 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:02,209 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:07,821 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:08,473 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:08,473 - INFO - ==================================================
2026-02-10 09:00:08,477 - INFO -   [탐색 88] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:08,501 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:08,501 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:08,502 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:14,716 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:14,896 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:14,896 - INFO - ==================================================
2026-02-10 09:00:14,898 - INFO -   [탐색 89] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:14,923 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:14,924 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:14,926 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:20,147 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:20,241 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:20,241 - INFO - ==================================================
2026-02-10 09:00:20,243 - INFO -   [탐색 90] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:20,271 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:20,272 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:20,274 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:25,253 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:25,337 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:25,338 - INFO - ==================================================
2026-02-10 09:00:25,340 - INFO -   [탐색 91] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:25,367 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:25,368 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:25,369 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:31,181 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:31,350 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:31,350 - INFO - ==================================================
2026-02-10 09:00:31,352 - INFO -   [탐색 92] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:31,377 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:31,378 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:31,379 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:37,060 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:37,186 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:37,186 - INFO - ==================================================
2026-02-10 09:00:37,188 - INFO -   [탐색 93] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:37,215 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:37,215 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:37,218 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:42,432 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:42,525 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:42,525 - INFO - ==================================================
2026-02-10 09:00:42,528 - INFO -   [탐색 94] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:42,554 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:42,554 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:42,556 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:48,296 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:48,418 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:48,418 - INFO - ==================================================
2026-02-10 09:00:48,420 - INFO -   [탐색 95] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:48,445 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:48,445 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:48,447 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:53,431 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:53,531 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:53,532 - INFO - ==================================================
2026-02-10 09:00:53,534 - INFO -   [탐색 96] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:53,559 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:53,559 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:53,562 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:00:57,952 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:00:57,997 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:00:57,997 - INFO - ==================================================
2026-02-10 09:00:57,998 - INFO -   [탐색 97] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:00:58,017 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:00:58,018 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:00:58,019 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:01:03,114 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:01:03,193 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:01:03,193 - INFO - ==================================================
2026-02-10 09:01:03,195 - INFO -   [탐색 98] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:01:03,219 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:01:03,220 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:01:03,221 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:01:06,579 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:01:07,089 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:01:07,089 - INFO - ==================================================
2026-02-10 09:01:07,092 - INFO -   [탐색 99] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:01:07,116 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:01:07,117 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:01:07,118 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:01:11,785 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:01:12,133 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.875)에 맞춰 변경되었습니다.
2026-02-10 09:01:12,133 - INFO - ==================================================
2026-02-10 09:01:12,135 - INFO -   [탐색 100] 희소도: 0.8750 -> 파라미터: 0.0495M (감소율: 98.02%)
2026-02-10 09:01:12,135 - INFO - 탐색 완료. 목표 파라미터 수(0.0476M)에 가장 근접한 최적 희소도는 0.8754 입니다.
2026-02-10 09:01:12,135 - INFO - ================================================================================
2026-02-10 09:01:12,136 - INFO - 계산된 Pruning 정보(희소도: 0.8754)를 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/pruning_info.yaml'에 저장했습니다.
2026-02-10 09:01:12,163 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 09:01:12,203 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:01:12,203 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:01:12,204 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:01:16,388 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:01:16,517 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8754345703125)에 맞춰 변경되었습니다.
2026-02-10 09:01:16,517 - INFO - ==================================================
2026-02-10 09:01:16,519 - INFO - ==================================================
2026-02-10 09:01:16,519 - INFO - 모델 파라미터 수:
2026-02-10 09:01:16,519 - INFO -   - 총 파라미터: 47,152 개
2026-02-10 09:01:16,519 - INFO -   - 학습 가능한 파라미터: 47,152 개
2026-02-10 09:01:16,564 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 09:01:16,624 - INFO - FLOPs가 0.3853 GFLOPs에서 0.0095 GFLOPs로 감소했습니다 (감소율: 97.52%).
2026-02-10 09:01:16,624 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 09:01:16,624 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 09:01:16,625 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 09:01:16,625 - INFO - ==================================================
2026-02-10 09:01:16,625 - INFO - train 모드를 시작합니다.
2026-02-10 09:01:16,625 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 09:01:16,625 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 09:01:16,625 - INFO - --------------------------------------------------
2026-02-10 09:01:16,626 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 09:01:19,907 - INFO - [Train] [11/90] | Loss: 0.8547 | Train Acc: 62.72%
2026-02-10 09:01:21,046 - INFO - [Valid] [11/90] | Loss: 0.7978 | Val Acc: 63.72%
2026-02-10 09:01:21,051 - INFO - [Metrics for 'abnormal'] | Precision: 0.8036 | Recall: 0.2866 | F1: 0.4225
2026-02-10 09:01:21,051 - INFO - [Metrics for 'normal'] | Precision: 0.6042 | Recall: 0.9396 | F1: 0.7355
2026-02-10 09:01:21,077 - INFO - [Best Model Saved] (val loss: 0.7978) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 09:01:21,078 - INFO - --------------------------------------------------
2026-02-10 09:01:21,079 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 09:01:25,996 - INFO - [Train] [12/90] | Loss: 0.6349 | Train Acc: 66.89%
2026-02-10 09:01:26,508 - INFO - [Valid] [12/90] | Loss: 0.7143 | Val Acc: 65.78%
2026-02-10 09:01:26,513 - INFO - [Metrics for 'abnormal'] | Precision: 0.7733 | Recall: 0.3694 | F1: 0.5000
2026-02-10 09:01:26,513 - INFO - [Metrics for 'normal'] | Precision: 0.6250 | Recall: 0.9066 | F1: 0.7399
2026-02-10 09:01:26,529 - INFO - [Best Model Saved] (val loss: 0.7143) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 09:01:26,529 - INFO - --------------------------------------------------
2026-02-10 09:01:26,531 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 09:01:31,327 - INFO - [Train] [13/90] | Loss: 0.6019 | Train Acc: 71.88%
2026-02-10 09:01:32,377 - INFO - [Valid] [13/90] | Loss: 0.6221 | Val Acc: 70.50%
2026-02-10 09:01:32,382 - INFO - [Metrics for 'abnormal'] | Precision: 0.7615 | Recall: 0.5287 | F1: 0.6241
2026-02-10 09:01:32,386 - INFO - [Metrics for 'normal'] | Precision: 0.6783 | Recall: 0.8571 | F1: 0.7573
2026-02-10 09:01:32,405 - INFO - [Best Model Saved] (val loss: 0.6221) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 09:01:32,405 - INFO - --------------------------------------------------
2026-02-10 09:01:32,407 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 09:01:36,733 - INFO - [Train] [14/90] | Loss: 0.5811 | Train Acc: 71.95%
2026-02-10 09:01:38,025 - INFO - [Valid] [14/90] | Loss: 0.6423 | Val Acc: 69.62%
2026-02-10 09:01:38,030 - INFO - [Metrics for 'abnormal'] | Precision: 0.6517 | Recall: 0.7389 | F1: 0.6925
2026-02-10 09:01:38,030 - INFO - [Metrics for 'normal'] | Precision: 0.7453 | Recall: 0.6593 | F1: 0.6997
2026-02-10 09:01:38,031 - INFO - --------------------------------------------------
2026-02-10 09:01:38,033 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 09:01:41,380 - INFO - [Train] [15/90] | Loss: 0.5734 | Train Acc: 73.36%
2026-02-10 09:01:42,614 - INFO - [Valid] [15/90] | Loss: 0.6503 | Val Acc: 68.73%
2026-02-10 09:01:42,627 - INFO - [Metrics for 'abnormal'] | Precision: 0.6835 | Recall: 0.6051 | F1: 0.6419
2026-02-10 09:01:42,627 - INFO - [Metrics for 'normal'] | Precision: 0.6900 | Recall: 0.7582 | F1: 0.7225
2026-02-10 09:01:42,630 - INFO - --------------------------------------------------
2026-02-10 09:01:42,631 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 09:01:46,871 - INFO - [Train] [16/90] | Loss: 0.5819 | Train Acc: 72.62%
2026-02-10 09:01:47,849 - INFO - [Valid] [16/90] | Loss: 0.6163 | Val Acc: 68.44%
2026-02-10 09:01:47,855 - INFO - [Metrics for 'abnormal'] | Precision: 0.6238 | Recall: 0.8025 | F1: 0.7019
2026-02-10 09:01:47,855 - INFO - [Metrics for 'normal'] | Precision: 0.7737 | Recall: 0.5824 | F1: 0.6646
2026-02-10 09:01:47,872 - INFO - [Best Model Saved] (val loss: 0.6163) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 09:01:47,873 - INFO - --------------------------------------------------
2026-02-10 09:01:47,874 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 09:01:52,477 - INFO - [Train] [17/90] | Loss: 0.5590 | Train Acc: 75.00%
2026-02-10 09:01:53,410 - INFO - [Valid] [17/90] | Loss: 0.6397 | Val Acc: 71.98%
2026-02-10 09:01:53,415 - INFO - [Metrics for 'abnormal'] | Precision: 0.7460 | Recall: 0.5987 | F1: 0.6643
2026-02-10 09:01:53,415 - INFO - [Metrics for 'normal'] | Precision: 0.7042 | Recall: 0.8242 | F1: 0.7595
2026-02-10 09:01:53,417 - INFO - --------------------------------------------------
2026-02-10 09:01:53,419 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 09:01:57,566 - INFO - [Train] [18/90] | Loss: 0.5439 | Train Acc: 76.64%
2026-02-10 09:01:58,669 - INFO - [Valid] [18/90] | Loss: 0.6406 | Val Acc: 71.68%
2026-02-10 09:01:58,673 - INFO - [Metrics for 'abnormal'] | Precision: 0.7402 | Recall: 0.5987 | F1: 0.6620
2026-02-10 09:01:58,674 - INFO - [Metrics for 'normal'] | Precision: 0.7028 | Recall: 0.8187 | F1: 0.7563
2026-02-10 09:01:58,675 - INFO - --------------------------------------------------
2026-02-10 09:01:58,677 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 09:02:02,712 - INFO - [Train] [19/90] | Loss: 0.5382 | Train Acc: 76.71%
2026-02-10 09:02:03,702 - INFO - [Valid] [19/90] | Loss: 0.5694 | Val Acc: 74.34%
2026-02-10 09:02:03,709 - INFO - [Metrics for 'abnormal'] | Precision: 0.7465 | Recall: 0.6752 | F1: 0.7090
2026-02-10 09:02:03,709 - INFO - [Metrics for 'normal'] | Precision: 0.7411 | Recall: 0.8022 | F1: 0.7704
2026-02-10 09:02:03,726 - INFO - [Best Model Saved] (val loss: 0.5694) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 09:02:03,726 - INFO - --------------------------------------------------
2026-02-10 09:02:03,727 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 09:02:08,063 - INFO - [Train] [20/90] | Loss: 0.5231 | Train Acc: 78.12%
2026-02-10 09:02:09,393 - INFO - [Valid] [20/90] | Loss: 0.5775 | Val Acc: 74.04%
2026-02-10 09:02:09,398 - INFO - [Metrics for 'abnormal'] | Precision: 0.7518 | Recall: 0.6561 | F1: 0.7007
2026-02-10 09:02:09,401 - INFO - [Metrics for 'normal'] | Precision: 0.7327 | Recall: 0.8132 | F1: 0.7708
2026-02-10 09:02:09,405 - INFO - --------------------------------------------------
2026-02-10 09:02:09,406 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 09:02:13,818 - INFO - [Train] [21/90] | Loss: 0.5034 | Train Acc: 79.61%
2026-02-10 09:02:14,843 - INFO - [Valid] [21/90] | Loss: 0.5370 | Val Acc: 74.93%
2026-02-10 09:02:14,850 - INFO - [Metrics for 'abnormal'] | Precision: 0.7466 | Recall: 0.6943 | F1: 0.7195
2026-02-10 09:02:14,851 - INFO - [Metrics for 'normal'] | Precision: 0.7513 | Recall: 0.7967 | F1: 0.7733
2026-02-10 09:02:14,870 - INFO - [Best Model Saved] (val loss: 0.5370) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 09:02:14,870 - INFO - --------------------------------------------------
2026-02-10 09:02:14,872 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 09:02:19,289 - INFO - [Train] [22/90] | Loss: 0.5115 | Train Acc: 79.02%
2026-02-10 09:02:20,379 - INFO - [Valid] [22/90] | Loss: 0.5508 | Val Acc: 76.11%
2026-02-10 09:02:20,384 - INFO - [Metrics for 'abnormal'] | Precision: 0.7317 | Recall: 0.7643 | F1: 0.7477
2026-02-10 09:02:20,384 - INFO - [Metrics for 'normal'] | Precision: 0.7886 | Recall: 0.7582 | F1: 0.7731
2026-02-10 09:02:20,387 - INFO - --------------------------------------------------
2026-02-10 09:02:20,388 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 09:02:24,741 - INFO - [Train] [23/90] | Loss: 0.4899 | Train Acc: 80.58%
2026-02-10 09:02:25,870 - INFO - [Valid] [23/90] | Loss: 0.5912 | Val Acc: 78.47%
2026-02-10 09:02:25,874 - INFO - [Metrics for 'abnormal'] | Precision: 0.8182 | Recall: 0.6879 | F1: 0.7474
2026-02-10 09:02:25,874 - INFO - [Metrics for 'normal'] | Precision: 0.7633 | Recall: 0.8681 | F1: 0.8123
2026-02-10 09:02:25,876 - INFO - --------------------------------------------------
2026-02-10 09:02:25,877 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 09:02:29,288 - INFO - [Train] [24/90] | Loss: 0.4980 | Train Acc: 80.51%
2026-02-10 09:02:30,574 - INFO - [Valid] [24/90] | Loss: 0.5685 | Val Acc: 76.70%
2026-02-10 09:02:30,579 - INFO - [Metrics for 'abnormal'] | Precision: 0.7468 | Recall: 0.7516 | F1: 0.7492
2026-02-10 09:02:30,579 - INFO - [Metrics for 'normal'] | Precision: 0.7845 | Recall: 0.7802 | F1: 0.7824
2026-02-10 09:02:30,581 - INFO - --------------------------------------------------
2026-02-10 09:02:30,583 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 09:02:34,962 - INFO - [Train] [25/90] | Loss: 0.4910 | Train Acc: 81.40%
2026-02-10 09:02:35,870 - INFO - [Valid] [25/90] | Loss: 0.5546 | Val Acc: 77.58%
2026-02-10 09:02:35,875 - INFO - [Metrics for 'abnormal'] | Precision: 0.7793 | Recall: 0.7197 | F1: 0.7483
2026-02-10 09:02:35,876 - INFO - [Metrics for 'normal'] | Precision: 0.7732 | Recall: 0.8242 | F1: 0.7979
2026-02-10 09:02:35,877 - INFO - --------------------------------------------------
2026-02-10 09:02:35,879 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 09:02:40,488 - INFO - [Train] [26/90] | Loss: 0.4874 | Train Acc: 80.36%
2026-02-10 09:02:41,322 - INFO - [Valid] [26/90] | Loss: 0.5622 | Val Acc: 77.88%
2026-02-10 09:02:41,327 - INFO - [Metrics for 'abnormal'] | Precision: 0.7662 | Recall: 0.7516 | F1: 0.7588
2026-02-10 09:02:41,327 - INFO - [Metrics for 'normal'] | Precision: 0.7892 | Recall: 0.8022 | F1: 0.7956
2026-02-10 09:02:41,329 - INFO - --------------------------------------------------
2026-02-10 09:02:41,330 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 09:02:45,779 - INFO - [Train] [27/90] | Loss: 0.4632 | Train Acc: 82.51%
2026-02-10 09:02:46,654 - INFO - [Valid] [27/90] | Loss: 0.6611 | Val Acc: 69.32%
2026-02-10 09:02:46,658 - INFO - [Metrics for 'abnormal'] | Precision: 0.6178 | Recall: 0.8854 | F1: 0.7277
2026-02-10 09:02:46,658 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.5275 | F1: 0.6486
2026-02-10 09:02:46,659 - INFO - --------------------------------------------------
2026-02-10 09:02:46,660 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 09:02:51,085 - INFO - [Train] [28/90] | Loss: 0.4756 | Train Acc: 80.88%
2026-02-10 09:02:52,318 - INFO - [Valid] [28/90] | Loss: 0.5845 | Val Acc: 77.29%
2026-02-10 09:02:52,323 - INFO - [Metrics for 'abnormal'] | Precision: 0.8125 | Recall: 0.6624 | F1: 0.7298
2026-02-10 09:02:52,323 - INFO - [Metrics for 'normal'] | Precision: 0.7488 | Recall: 0.8681 | F1: 0.8041
2026-02-10 09:02:52,325 - INFO - --------------------------------------------------
2026-02-10 09:02:52,327 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 09:02:57,965 - INFO - [Train] [29/90] | Loss: 0.4639 | Train Acc: 83.11%
2026-02-10 09:02:59,533 - INFO - [Valid] [29/90] | Loss: 0.5955 | Val Acc: 74.04%
2026-02-10 09:02:59,538 - INFO - [Metrics for 'abnormal'] | Precision: 0.6885 | Recall: 0.8025 | F1: 0.7412
2026-02-10 09:02:59,538 - INFO - [Metrics for 'normal'] | Precision: 0.8013 | Recall: 0.6868 | F1: 0.7396
2026-02-10 09:02:59,540 - INFO - --------------------------------------------------
2026-02-10 09:02:59,541 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 09:03:04,089 - INFO - [Train] [30/90] | Loss: 0.4620 | Train Acc: 83.78%
2026-02-10 09:03:05,741 - INFO - [Valid] [30/90] | Loss: 0.6195 | Val Acc: 75.52%
2026-02-10 09:03:05,747 - INFO - [Metrics for 'abnormal'] | Precision: 0.7937 | Recall: 0.6369 | F1: 0.7067
2026-02-10 09:03:05,748 - INFO - [Metrics for 'normal'] | Precision: 0.7324 | Recall: 0.8571 | F1: 0.7899
2026-02-10 09:03:05,749 - INFO - --------------------------------------------------
2026-02-10 09:03:05,750 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 09:03:10,880 - INFO - [Train] [31/90] | Loss: 0.4449 | Train Acc: 83.93%
2026-02-10 09:03:12,122 - INFO - [Valid] [31/90] | Loss: 0.5641 | Val Acc: 78.17%
2026-02-10 09:03:12,128 - INFO - [Metrics for 'abnormal'] | Precision: 0.7902 | Recall: 0.7197 | F1: 0.7533
2026-02-10 09:03:12,128 - INFO - [Metrics for 'normal'] | Precision: 0.7755 | Recall: 0.8352 | F1: 0.8042
2026-02-10 09:03:12,130 - INFO - --------------------------------------------------
2026-02-10 09:03:12,131 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 09:03:17,957 - INFO - [Train] [32/90] | Loss: 0.4435 | Train Acc: 84.60%
2026-02-10 09:03:18,491 - INFO - [Valid] [32/90] | Loss: 0.6444 | Val Acc: 73.75%
2026-02-10 09:03:18,495 - INFO - [Metrics for 'abnormal'] | Precision: 0.8469 | Recall: 0.5287 | F1: 0.6510
2026-02-10 09:03:18,495 - INFO - [Metrics for 'normal'] | Precision: 0.6929 | Recall: 0.9176 | F1: 0.7896
2026-02-10 09:03:18,496 - INFO - --------------------------------------------------
2026-02-10 09:03:18,497 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 09:03:24,293 - INFO - [Train] [33/90] | Loss: 0.4424 | Train Acc: 84.45%
2026-02-10 09:03:25,568 - INFO - [Valid] [33/90] | Loss: 0.6276 | Val Acc: 69.03%
2026-02-10 09:03:25,573 - INFO - [Metrics for 'abnormal'] | Precision: 0.6548 | Recall: 0.7006 | F1: 0.6769
2026-02-10 09:03:25,573 - INFO - [Metrics for 'normal'] | Precision: 0.7251 | Recall: 0.6813 | F1: 0.7025
2026-02-10 09:03:25,575 - INFO - --------------------------------------------------
2026-02-10 09:03:25,577 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 09:03:30,361 - INFO - [Train] [34/90] | Loss: 0.4356 | Train Acc: 84.45%
2026-02-10 09:03:31,819 - INFO - [Valid] [34/90] | Loss: 0.5872 | Val Acc: 73.75%
2026-02-10 09:03:31,824 - INFO - [Metrics for 'abnormal'] | Precision: 0.6667 | Recall: 0.8662 | F1: 0.7535
2026-02-10 09:03:31,824 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.6264 | F1: 0.7192
2026-02-10 09:03:31,825 - INFO - --------------------------------------------------
2026-02-10 09:03:31,827 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 09:03:36,942 - INFO - [Train] [35/90] | Loss: 0.4157 | Train Acc: 86.46%
2026-02-10 09:03:38,654 - INFO - [Valid] [35/90] | Loss: 0.5867 | Val Acc: 77.58%
2026-02-10 09:03:38,659 - INFO - [Metrics for 'abnormal'] | Precision: 0.7755 | Recall: 0.7261 | F1: 0.7500
2026-02-10 09:03:38,659 - INFO - [Metrics for 'normal'] | Precision: 0.7760 | Recall: 0.8187 | F1: 0.7968
2026-02-10 09:03:38,660 - INFO - --------------------------------------------------
2026-02-10 09:03:38,662 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 09:03:43,866 - INFO - [Train] [36/90] | Loss: 0.4263 | Train Acc: 85.71%
2026-02-10 09:03:45,199 - INFO - [Valid] [36/90] | Loss: 0.5626 | Val Acc: 77.29%
2026-02-10 09:03:45,204 - INFO - [Metrics for 'abnormal'] | Precision: 0.7326 | Recall: 0.8025 | F1: 0.7660
2026-02-10 09:03:45,204 - INFO - [Metrics for 'normal'] | Precision: 0.8144 | Recall: 0.7473 | F1: 0.7794
2026-02-10 09:03:45,206 - INFO - --------------------------------------------------
2026-02-10 09:03:45,212 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 09:03:51,143 - INFO - [Train] [37/90] | Loss: 0.4298 | Train Acc: 86.46%
2026-02-10 09:03:52,280 - INFO - [Valid] [37/90] | Loss: 0.5259 | Val Acc: 76.99%
2026-02-10 09:03:52,285 - INFO - [Metrics for 'abnormal'] | Precision: 0.7801 | Recall: 0.7006 | F1: 0.7383
2026-02-10 09:03:52,286 - INFO - [Metrics for 'normal'] | Precision: 0.7626 | Recall: 0.8297 | F1: 0.7947
2026-02-10 09:03:52,314 - INFO - [Best Model Saved] (val loss: 0.5259) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 09:03:52,315 - INFO - --------------------------------------------------
2026-02-10 09:03:52,317 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 09:03:58,051 - INFO - [Train] [38/90] | Loss: 0.3857 | Train Acc: 88.24%
2026-02-10 09:03:59,397 - INFO - [Valid] [38/90] | Loss: 0.6291 | Val Acc: 74.63%
2026-02-10 09:03:59,405 - INFO - [Metrics for 'abnormal'] | Precision: 0.7669 | Recall: 0.6497 | F1: 0.7034
2026-02-10 09:03:59,406 - INFO - [Metrics for 'normal'] | Precision: 0.7330 | Recall: 0.8297 | F1: 0.7784
2026-02-10 09:03:59,407 - INFO - --------------------------------------------------
2026-02-10 09:03:59,409 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 09:04:03,953 - INFO - [Train] [39/90] | Loss: 0.4256 | Train Acc: 85.79%
2026-02-10 09:04:05,483 - INFO - [Valid] [39/90] | Loss: 0.5929 | Val Acc: 73.75%
2026-02-10 09:04:05,492 - INFO - [Metrics for 'abnormal'] | Precision: 0.7361 | Recall: 0.6752 | F1: 0.7043
2026-02-10 09:04:05,492 - INFO - [Metrics for 'normal'] | Precision: 0.7385 | Recall: 0.7912 | F1: 0.7639
2026-02-10 09:04:05,494 - INFO - --------------------------------------------------
2026-02-10 09:04:05,495 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 09:04:10,906 - INFO - [Train] [40/90] | Loss: 0.4121 | Train Acc: 87.13%
2026-02-10 09:04:12,441 - INFO - [Valid] [40/90] | Loss: 0.5662 | Val Acc: 76.40%
2026-02-10 09:04:12,451 - INFO - [Metrics for 'abnormal'] | Precision: 0.7305 | Recall: 0.7771 | F1: 0.7531
2026-02-10 09:04:12,451 - INFO - [Metrics for 'normal'] | Precision: 0.7965 | Recall: 0.7527 | F1: 0.7740
2026-02-10 09:04:12,452 - INFO - --------------------------------------------------
2026-02-10 09:04:12,454 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 09:04:18,182 - INFO - [Train] [41/90] | Loss: 0.3982 | Train Acc: 87.50%
2026-02-10 09:04:19,562 - INFO - [Valid] [41/90] | Loss: 0.5579 | Val Acc: 76.99%
2026-02-10 09:04:19,568 - INFO - [Metrics for 'abnormal'] | Precision: 0.7516 | Recall: 0.7516 | F1: 0.7516
2026-02-10 09:04:19,568 - INFO - [Metrics for 'normal'] | Precision: 0.7857 | Recall: 0.7857 | F1: 0.7857
2026-02-10 09:04:19,569 - INFO - --------------------------------------------------
2026-02-10 09:04:19,571 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 09:04:25,453 - INFO - [Train] [42/90] | Loss: 0.3893 | Train Acc: 88.47%
2026-02-10 09:04:26,793 - INFO - [Valid] [42/90] | Loss: 0.5539 | Val Acc: 81.12%
2026-02-10 09:04:26,799 - INFO - [Metrics for 'abnormal'] | Precision: 0.7818 | Recall: 0.8217 | F1: 0.8012
2026-02-10 09:04:26,799 - INFO - [Metrics for 'normal'] | Precision: 0.8391 | Recall: 0.8022 | F1: 0.8202
2026-02-10 09:04:26,801 - INFO - --------------------------------------------------
2026-02-10 09:04:26,802 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 09:04:33,092 - INFO - [Train] [43/90] | Loss: 0.3859 | Train Acc: 88.39%
2026-02-10 09:04:34,633 - INFO - [Valid] [43/90] | Loss: 0.5362 | Val Acc: 78.76%
2026-02-10 09:04:34,637 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.7580 | F1: 0.7677
2026-02-10 09:04:34,637 - INFO - [Metrics for 'normal'] | Precision: 0.7957 | Recall: 0.8132 | F1: 0.8043
2026-02-10 09:04:34,639 - INFO - --------------------------------------------------
2026-02-10 09:04:34,643 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 09:04:40,092 - INFO - [Train] [44/90] | Loss: 0.3851 | Train Acc: 88.76%
2026-02-10 09:04:41,381 - INFO - [Valid] [44/90] | Loss: 0.5234 | Val Acc: 81.42%
2026-02-10 09:04:41,387 - INFO - [Metrics for 'abnormal'] | Precision: 0.7975 | Recall: 0.8025 | F1: 0.8000
2026-02-10 09:04:41,387 - INFO - [Metrics for 'normal'] | Precision: 0.8287 | Recall: 0.8242 | F1: 0.8264
2026-02-10 09:04:41,405 - INFO - [Best Model Saved] (val loss: 0.5234) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 09:04:41,405 - INFO - --------------------------------------------------
2026-02-10 09:04:41,407 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 09:04:47,299 - INFO - [Train] [45/90] | Loss: 0.3794 | Train Acc: 89.58%
2026-02-10 09:04:48,653 - INFO - [Valid] [45/90] | Loss: 0.5510 | Val Acc: 82.89%
2026-02-10 09:04:48,663 - INFO - [Metrics for 'abnormal'] | Precision: 0.8367 | Recall: 0.7834 | F1: 0.8092
2026-02-10 09:04:48,663 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.8681 | F1: 0.8449
2026-02-10 09:04:48,665 - INFO - --------------------------------------------------
2026-02-10 09:04:48,666 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 09:04:54,032 - INFO - [Train] [46/90] | Loss: 0.3767 | Train Acc: 89.43%
2026-02-10 09:04:55,541 - INFO - [Valid] [46/90] | Loss: 0.5695 | Val Acc: 77.88%
2026-02-10 09:04:55,546 - INFO - [Metrics for 'abnormal'] | Precision: 0.7356 | Recall: 0.8153 | F1: 0.7734
2026-02-10 09:04:55,547 - INFO - [Metrics for 'normal'] | Precision: 0.8242 | Recall: 0.7473 | F1: 0.7839
2026-02-10 09:04:55,548 - INFO - --------------------------------------------------
2026-02-10 09:04:55,550 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 09:05:01,037 - INFO - [Train] [47/90] | Loss: 0.3724 | Train Acc: 89.14%
2026-02-10 09:05:02,424 - INFO - [Valid] [47/90] | Loss: 0.5633 | Val Acc: 79.35%
2026-02-10 09:05:02,430 - INFO - [Metrics for 'abnormal'] | Precision: 0.7605 | Recall: 0.8089 | F1: 0.7840
2026-02-10 09:05:02,430 - INFO - [Metrics for 'normal'] | Precision: 0.8256 | Recall: 0.7802 | F1: 0.8023
2026-02-10 09:05:02,432 - INFO - --------------------------------------------------
2026-02-10 09:05:02,433 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 09:05:08,114 - INFO - [Train] [48/90] | Loss: 0.3750 | Train Acc: 89.51%
2026-02-10 09:05:09,347 - INFO - [Valid] [48/90] | Loss: 0.5647 | Val Acc: 82.01%
2026-02-10 09:05:09,352 - INFO - [Metrics for 'abnormal'] | Precision: 0.7759 | Recall: 0.8599 | F1: 0.8157
2026-02-10 09:05:09,353 - INFO - [Metrics for 'normal'] | Precision: 0.8667 | Recall: 0.7857 | F1: 0.8242
2026-02-10 09:05:09,354 - INFO - --------------------------------------------------
2026-02-10 09:05:09,355 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 09:05:15,463 - INFO - [Train] [49/90] | Loss: 0.3553 | Train Acc: 90.55%
2026-02-10 09:05:16,412 - INFO - [Valid] [49/90] | Loss: 0.5680 | Val Acc: 79.06%
2026-02-10 09:05:16,419 - INFO - [Metrics for 'abnormal'] | Precision: 0.7471 | Recall: 0.8280 | F1: 0.7855
2026-02-10 09:05:16,419 - INFO - [Metrics for 'normal'] | Precision: 0.8364 | Recall: 0.7582 | F1: 0.7954
2026-02-10 09:05:16,421 - INFO - --------------------------------------------------
2026-02-10 09:05:16,423 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 09:05:22,200 - INFO - [Train] [50/90] | Loss: 0.3524 | Train Acc: 90.85%
2026-02-10 09:05:23,454 - INFO - [Valid] [50/90] | Loss: 0.5652 | Val Acc: 77.88%
2026-02-10 09:05:23,459 - INFO - [Metrics for 'abnormal'] | Precision: 0.8154 | Recall: 0.6752 | F1: 0.7387
2026-02-10 09:05:23,459 - INFO - [Metrics for 'normal'] | Precision: 0.7560 | Recall: 0.8681 | F1: 0.8082
2026-02-10 09:05:23,460 - INFO - --------------------------------------------------
2026-02-10 09:05:23,462 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 09:05:28,935 - INFO - [Train] [51/90] | Loss: 0.3599 | Train Acc: 91.00%
2026-02-10 09:05:30,227 - INFO - [Valid] [51/90] | Loss: 0.5739 | Val Acc: 80.24%
2026-02-10 09:05:30,233 - INFO - [Metrics for 'abnormal'] | Precision: 0.8358 | Recall: 0.7134 | F1: 0.7698
2026-02-10 09:05:30,233 - INFO - [Metrics for 'normal'] | Precision: 0.7805 | Recall: 0.8791 | F1: 0.8269
2026-02-10 09:05:30,234 - INFO - --------------------------------------------------
2026-02-10 09:05:30,236 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 09:05:35,785 - INFO - [Train] [52/90] | Loss: 0.3674 | Train Acc: 90.77%
2026-02-10 09:05:37,437 - INFO - [Valid] [52/90] | Loss: 0.5428 | Val Acc: 79.65%
2026-02-10 09:05:37,447 - INFO - [Metrics for 'abnormal'] | Precision: 0.7683 | Recall: 0.8025 | F1: 0.7850
2026-02-10 09:05:37,447 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.7912 | F1: 0.8067
2026-02-10 09:05:37,449 - INFO - --------------------------------------------------
2026-02-10 09:05:37,450 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 09:05:42,915 - INFO - [Train] [53/90] | Loss: 0.3330 | Train Acc: 93.01%
2026-02-10 09:05:44,427 - INFO - [Valid] [53/90] | Loss: 0.5820 | Val Acc: 79.65%
2026-02-10 09:05:44,432 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.7707 | F1: 0.7781
2026-02-10 09:05:44,432 - INFO - [Metrics for 'normal'] | Precision: 0.8054 | Recall: 0.8187 | F1: 0.8120
2026-02-10 09:05:44,434 - INFO - --------------------------------------------------
2026-02-10 09:05:44,436 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 09:05:50,024 - INFO - [Train] [54/90] | Loss: 0.3311 | Train Acc: 92.41%
2026-02-10 09:05:51,743 - INFO - [Valid] [54/90] | Loss: 0.6063 | Val Acc: 80.53%
2026-02-10 09:05:51,748 - INFO - [Metrics for 'abnormal'] | Precision: 0.8054 | Recall: 0.7643 | F1: 0.7843
2026-02-10 09:05:51,748 - INFO - [Metrics for 'normal'] | Precision: 0.8053 | Recall: 0.8407 | F1: 0.8226
2026-02-10 09:05:51,750 - INFO - --------------------------------------------------
2026-02-10 09:05:51,751 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 09:05:57,194 - INFO - [Train] [55/90] | Loss: 0.3287 | Train Acc: 92.26%
2026-02-10 09:05:58,676 - INFO - [Valid] [55/90] | Loss: 0.5659 | Val Acc: 79.06%
2026-02-10 09:05:58,681 - INFO - [Metrics for 'abnormal'] | Precision: 0.7756 | Recall: 0.7707 | F1: 0.7732
2026-02-10 09:05:58,689 - INFO - [Metrics for 'normal'] | Precision: 0.8033 | Recall: 0.8077 | F1: 0.8055
2026-02-10 09:05:58,692 - INFO - --------------------------------------------------
2026-02-10 09:05:58,693 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 09:06:04,325 - INFO - [Train] [56/90] | Loss: 0.3403 | Train Acc: 92.04%
2026-02-10 09:06:05,721 - INFO - [Valid] [56/90] | Loss: 0.5588 | Val Acc: 79.94%
2026-02-10 09:06:05,726 - INFO - [Metrics for 'abnormal'] | Precision: 0.7730 | Recall: 0.8025 | F1: 0.7875
2026-02-10 09:06:05,726 - INFO - [Metrics for 'normal'] | Precision: 0.8239 | Recall: 0.7967 | F1: 0.8101
2026-02-10 09:06:05,728 - INFO - --------------------------------------------------
2026-02-10 09:06:05,729 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 09:06:11,637 - INFO - [Train] [57/90] | Loss: 0.3165 | Train Acc: 93.60%
2026-02-10 09:06:12,739 - INFO - [Valid] [57/90] | Loss: 0.5913 | Val Acc: 77.88%
2026-02-10 09:06:12,744 - INFO - [Metrics for 'abnormal'] | Precision: 0.7733 | Recall: 0.7389 | F1: 0.7557
2026-02-10 09:06:12,745 - INFO - [Metrics for 'normal'] | Precision: 0.7831 | Recall: 0.8132 | F1: 0.7978
2026-02-10 09:06:12,746 - INFO - --------------------------------------------------
2026-02-10 09:06:12,747 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 09:06:17,851 - INFO - [Train] [58/90] | Loss: 0.3259 | Train Acc: 92.49%
2026-02-10 09:06:19,529 - INFO - [Valid] [58/90] | Loss: 0.6046 | Val Acc: 79.06%
2026-02-10 09:06:19,533 - INFO - [Metrics for 'abnormal'] | Precision: 0.7622 | Recall: 0.7962 | F1: 0.7788
2026-02-10 09:06:19,534 - INFO - [Metrics for 'normal'] | Precision: 0.8171 | Recall: 0.7857 | F1: 0.8011
2026-02-10 09:06:19,536 - INFO - --------------------------------------------------
2026-02-10 09:06:19,541 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 09:06:24,713 - INFO - [Train] [59/90] | Loss: 0.3374 | Train Acc: 92.56%
2026-02-10 09:06:26,362 - INFO - [Valid] [59/90] | Loss: 0.5807 | Val Acc: 79.06%
2026-02-10 09:06:26,367 - INFO - [Metrics for 'abnormal'] | Precision: 0.7722 | Recall: 0.7771 | F1: 0.7746
2026-02-10 09:06:26,367 - INFO - [Metrics for 'normal'] | Precision: 0.8066 | Recall: 0.8022 | F1: 0.8044
2026-02-10 09:06:26,369 - INFO - --------------------------------------------------
2026-02-10 09:06:26,371 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 09:06:31,315 - INFO - [Train] [60/90] | Loss: 0.3170 | Train Acc: 93.97%
2026-02-10 09:06:32,811 - INFO - [Valid] [60/90] | Loss: 0.5833 | Val Acc: 79.35%
2026-02-10 09:06:32,816 - INFO - [Metrics for 'abnormal'] | Precision: 0.7669 | Recall: 0.7962 | F1: 0.7812
2026-02-10 09:06:32,817 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.7912 | F1: 0.8045
2026-02-10 09:06:32,818 - INFO - --------------------------------------------------
2026-02-10 09:06:32,820 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 09:06:38,479 - INFO - [Train] [61/90] | Loss: 0.3206 | Train Acc: 93.38%
2026-02-10 09:06:39,958 - INFO - [Valid] [61/90] | Loss: 0.5737 | Val Acc: 77.58%
2026-02-10 09:06:39,964 - INFO - [Metrics for 'abnormal'] | Precision: 0.7580 | Recall: 0.7580 | F1: 0.7580
2026-02-10 09:06:39,964 - INFO - [Metrics for 'normal'] | Precision: 0.7912 | Recall: 0.7912 | F1: 0.7912
2026-02-10 09:06:39,965 - INFO - --------------------------------------------------
2026-02-10 09:06:39,966 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 09:06:46,165 - INFO - [Train] [62/90] | Loss: 0.3053 | Train Acc: 94.12%
2026-02-10 09:06:47,276 - INFO - [Valid] [62/90] | Loss: 0.6409 | Val Acc: 77.88%
2026-02-10 09:06:47,282 - INFO - [Metrics for 'abnormal'] | Precision: 0.7440 | Recall: 0.7962 | F1: 0.7692
2026-02-10 09:06:47,282 - INFO - [Metrics for 'normal'] | Precision: 0.8129 | Recall: 0.7637 | F1: 0.7875
2026-02-10 09:06:47,283 - INFO - --------------------------------------------------
2026-02-10 09:06:47,284 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 09:06:52,627 - INFO - [Train] [63/90] | Loss: 0.2857 | Train Acc: 95.39%
2026-02-10 09:06:53,655 - INFO - [Valid] [63/90] | Loss: 0.6033 | Val Acc: 77.88%
2026-02-10 09:06:53,659 - INFO - [Metrics for 'abnormal'] | Precision: 0.7662 | Recall: 0.7516 | F1: 0.7588
2026-02-10 09:06:53,660 - INFO - [Metrics for 'normal'] | Precision: 0.7892 | Recall: 0.8022 | F1: 0.7956
2026-02-10 09:06:53,662 - INFO - --------------------------------------------------
2026-02-10 09:06:53,666 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 09:06:58,655 - INFO - [Train] [64/90] | Loss: 0.2995 | Train Acc: 94.42%
2026-02-10 09:06:59,591 - INFO - [Valid] [64/90] | Loss: 0.6265 | Val Acc: 76.40%
2026-02-10 09:06:59,601 - INFO - [Metrics for 'abnormal'] | Precision: 0.7391 | Recall: 0.7580 | F1: 0.7484
2026-02-10 09:06:59,601 - INFO - [Metrics for 'normal'] | Precision: 0.7865 | Recall: 0.7692 | F1: 0.7778
2026-02-10 09:06:59,602 - INFO - --------------------------------------------------
2026-02-10 09:06:59,604 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 09:07:05,193 - INFO - [Train] [65/90] | Loss: 0.2930 | Train Acc: 95.16%
2026-02-10 09:07:06,686 - INFO - [Valid] [65/90] | Loss: 0.6133 | Val Acc: 79.94%
2026-02-10 09:07:06,691 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8089 | F1: 0.7888
2026-02-10 09:07:06,695 - INFO - [Metrics for 'normal'] | Precision: 0.8276 | Recall: 0.7912 | F1: 0.8090
2026-02-10 09:07:06,697 - INFO - --------------------------------------------------
2026-02-10 09:07:06,699 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 09:07:12,431 - INFO - [Train] [66/90] | Loss: 0.2991 | Train Acc: 94.94%
2026-02-10 09:07:13,468 - INFO - [Valid] [66/90] | Loss: 0.6226 | Val Acc: 76.70%
2026-02-10 09:07:13,475 - INFO - [Metrics for 'abnormal'] | Precision: 0.7566 | Recall: 0.7325 | F1: 0.7443
2026-02-10 09:07:13,477 - INFO - [Metrics for 'normal'] | Precision: 0.7754 | Recall: 0.7967 | F1: 0.7859
2026-02-10 09:07:13,479 - INFO - --------------------------------------------------
2026-02-10 09:07:13,481 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 09:07:17,989 - INFO - [Train] [67/90] | Loss: 0.3103 | Train Acc: 94.20%
2026-02-10 09:07:18,888 - INFO - [Valid] [67/90] | Loss: 0.5902 | Val Acc: 77.29%
2026-02-10 09:07:18,893 - INFO - [Metrics for 'abnormal'] | Precision: 0.7667 | Recall: 0.7325 | F1: 0.7492
2026-02-10 09:07:18,893 - INFO - [Metrics for 'normal'] | Precision: 0.7778 | Recall: 0.8077 | F1: 0.7925
2026-02-10 09:07:18,898 - INFO - --------------------------------------------------
2026-02-10 09:07:18,900 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 09:07:23,760 - INFO - [Train] [68/90] | Loss: 0.2854 | Train Acc: 95.09%
2026-02-10 09:07:24,849 - INFO - [Valid] [68/90] | Loss: 0.6353 | Val Acc: 76.70%
2026-02-10 09:07:24,855 - INFO - [Metrics for 'abnormal'] | Precision: 0.7532 | Recall: 0.7389 | F1: 0.7460
2026-02-10 09:07:24,856 - INFO - [Metrics for 'normal'] | Precision: 0.7784 | Recall: 0.7912 | F1: 0.7847
2026-02-10 09:07:24,857 - INFO - --------------------------------------------------
2026-02-10 09:07:24,858 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 09:07:29,240 - INFO - [Train] [69/90] | Loss: 0.2866 | Train Acc: 96.06%
2026-02-10 09:07:30,256 - INFO - [Valid] [69/90] | Loss: 0.6368 | Val Acc: 76.70%
2026-02-10 09:07:30,262 - INFO - [Metrics for 'abnormal'] | Precision: 0.7566 | Recall: 0.7325 | F1: 0.7443
2026-02-10 09:07:30,263 - INFO - [Metrics for 'normal'] | Precision: 0.7754 | Recall: 0.7967 | F1: 0.7859
2026-02-10 09:07:30,265 - INFO - --------------------------------------------------
2026-02-10 09:07:30,266 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 09:07:34,032 - INFO - [Train] [70/90] | Loss: 0.2924 | Train Acc: 95.01%
2026-02-10 09:07:34,844 - INFO - [Valid] [70/90] | Loss: 0.6247 | Val Acc: 76.99%
2026-02-10 09:07:34,849 - INFO - [Metrics for 'abnormal'] | Precision: 0.7232 | Recall: 0.8153 | F1: 0.7665
2026-02-10 09:07:34,849 - INFO - [Metrics for 'normal'] | Precision: 0.8210 | Recall: 0.7308 | F1: 0.7733
2026-02-10 09:07:34,851 - INFO - --------------------------------------------------
2026-02-10 09:07:34,852 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 09:07:38,345 - INFO - [Train] [71/90] | Loss: 0.2870 | Train Acc: 95.46%
2026-02-10 09:07:39,287 - INFO - [Valid] [71/90] | Loss: 0.5946 | Val Acc: 79.65%
2026-02-10 09:07:39,292 - INFO - [Metrics for 'abnormal'] | Precision: 0.7895 | Recall: 0.7643 | F1: 0.7767
2026-02-10 09:07:39,292 - INFO - [Metrics for 'normal'] | Precision: 0.8021 | Recall: 0.8242 | F1: 0.8130
2026-02-10 09:07:39,293 - INFO - --------------------------------------------------
2026-02-10 09:07:39,294 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 09:07:42,436 - INFO - [Train] [72/90] | Loss: 0.2651 | Train Acc: 96.95%
2026-02-10 09:07:43,443 - INFO - [Valid] [72/90] | Loss: 0.6109 | Val Acc: 78.47%
2026-02-10 09:07:43,446 - INFO - [Metrics for 'abnormal'] | Precision: 0.7625 | Recall: 0.7771 | F1: 0.7697
2026-02-10 09:07:43,454 - INFO - [Metrics for 'normal'] | Precision: 0.8045 | Recall: 0.7912 | F1: 0.7978
2026-02-10 09:07:43,457 - INFO - --------------------------------------------------
2026-02-10 09:07:43,458 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 09:07:46,419 - INFO - [Train] [73/90] | Loss: 0.2768 | Train Acc: 96.13%
2026-02-10 09:07:47,413 - INFO - [Valid] [73/90] | Loss: 0.6226 | Val Acc: 78.17%
2026-02-10 09:07:47,416 - INFO - [Metrics for 'abnormal'] | Precision: 0.7546 | Recall: 0.7834 | F1: 0.7688
2026-02-10 09:07:47,416 - INFO - [Metrics for 'normal'] | Precision: 0.8068 | Recall: 0.7802 | F1: 0.7933
2026-02-10 09:07:47,417 - INFO - --------------------------------------------------
2026-02-10 09:07:47,417 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 09:07:50,593 - INFO - [Train] [74/90] | Loss: 0.2742 | Train Acc: 96.43%
2026-02-10 09:07:51,518 - INFO - [Valid] [74/90] | Loss: 0.5747 | Val Acc: 79.35%
2026-02-10 09:07:51,523 - INFO - [Metrics for 'abnormal'] | Precision: 0.7574 | Recall: 0.8153 | F1: 0.7853
2026-02-10 09:07:51,523 - INFO - [Metrics for 'normal'] | Precision: 0.8294 | Recall: 0.7747 | F1: 0.8011
2026-02-10 09:07:51,525 - INFO - --------------------------------------------------
2026-02-10 09:07:51,526 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 09:07:54,872 - INFO - [Train] [75/90] | Loss: 0.2747 | Train Acc: 96.35%
2026-02-10 09:07:55,796 - INFO - [Valid] [75/90] | Loss: 0.5852 | Val Acc: 80.53%
2026-02-10 09:07:55,801 - INFO - [Metrics for 'abnormal'] | Precision: 0.8054 | Recall: 0.7643 | F1: 0.7843
2026-02-10 09:07:55,801 - INFO - [Metrics for 'normal'] | Precision: 0.8053 | Recall: 0.8407 | F1: 0.8226
2026-02-10 09:07:55,803 - INFO - --------------------------------------------------
2026-02-10 09:07:55,804 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 09:07:59,032 - INFO - [Train] [76/90] | Loss: 0.2741 | Train Acc: 95.68%
2026-02-10 09:08:00,052 - INFO - [Valid] [76/90] | Loss: 0.6049 | Val Acc: 80.24%
2026-02-10 09:08:00,062 - INFO - [Metrics for 'abnormal'] | Precision: 0.8125 | Recall: 0.7452 | F1: 0.7774
2026-02-10 09:08:00,062 - INFO - [Metrics for 'normal'] | Precision: 0.7949 | Recall: 0.8516 | F1: 0.8223
2026-02-10 09:08:00,064 - INFO - --------------------------------------------------
2026-02-10 09:08:00,065 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 09:08:03,198 - INFO - [Train] [77/90] | Loss: 0.2570 | Train Acc: 97.77%
2026-02-10 09:08:04,190 - INFO - [Valid] [77/90] | Loss: 0.6161 | Val Acc: 79.06%
2026-02-10 09:08:04,195 - INFO - [Metrics for 'abnormal'] | Precision: 0.7471 | Recall: 0.8280 | F1: 0.7855
2026-02-10 09:08:04,196 - INFO - [Metrics for 'normal'] | Precision: 0.8364 | Recall: 0.7582 | F1: 0.7954
2026-02-10 09:08:04,198 - INFO - --------------------------------------------------
2026-02-10 09:08:04,199 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 09:08:07,731 - INFO - [Train] [78/90] | Loss: 0.2590 | Train Acc: 97.40%
2026-02-10 09:08:08,523 - INFO - [Valid] [78/90] | Loss: 0.6115 | Val Acc: 80.53%
2026-02-10 09:08:08,528 - INFO - [Metrics for 'abnormal'] | Precision: 0.7974 | Recall: 0.7771 | F1: 0.7871
2026-02-10 09:08:08,528 - INFO - [Metrics for 'normal'] | Precision: 0.8118 | Recall: 0.8297 | F1: 0.8207
2026-02-10 09:08:08,530 - INFO - --------------------------------------------------
2026-02-10 09:08:08,531 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 09:08:12,022 - INFO - [Train] [79/90] | Loss: 0.2620 | Train Acc: 97.32%
2026-02-10 09:08:12,678 - INFO - [Valid] [79/90] | Loss: 0.6134 | Val Acc: 78.17%
2026-02-10 09:08:12,681 - INFO - [Metrics for 'abnormal'] | Precision: 0.8168 | Recall: 0.6815 | F1: 0.7431
2026-02-10 09:08:12,681 - INFO - [Metrics for 'normal'] | Precision: 0.7596 | Recall: 0.8681 | F1: 0.8103
2026-02-10 09:08:12,682 - INFO - --------------------------------------------------
2026-02-10 09:08:12,682 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 09:08:16,205 - INFO - [Train] [80/90] | Loss: 0.2539 | Train Acc: 97.77%
2026-02-10 09:08:16,846 - INFO - [Valid] [80/90] | Loss: 0.6120 | Val Acc: 78.47%
2026-02-10 09:08:16,851 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7134 | F1: 0.7542
2026-02-10 09:08:16,851 - INFO - [Metrics for 'normal'] | Precision: 0.7739 | Recall: 0.8462 | F1: 0.8084
2026-02-10 09:08:16,852 - INFO - --------------------------------------------------
2026-02-10 09:08:16,856 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 09:08:20,143 - INFO - [Train] [81/90] | Loss: 0.2598 | Train Acc: 97.02%
2026-02-10 09:08:20,843 - INFO - [Valid] [81/90] | Loss: 0.6262 | Val Acc: 81.12%
2026-02-10 09:08:20,848 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 09:08:20,848 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 09:08:20,849 - INFO - --------------------------------------------------
2026-02-10 09:08:20,850 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 09:08:24,022 - INFO - [Train] [82/90] | Loss: 0.2673 | Train Acc: 97.02%
2026-02-10 09:08:24,742 - INFO - [Valid] [82/90] | Loss: 0.6169 | Val Acc: 81.42%
2026-02-10 09:08:24,746 - INFO - [Metrics for 'abnormal'] | Precision: 0.7701 | Recall: 0.8535 | F1: 0.8097
2026-02-10 09:08:24,746 - INFO - [Metrics for 'normal'] | Precision: 0.8606 | Recall: 0.7802 | F1: 0.8184
2026-02-10 09:08:24,747 - INFO - --------------------------------------------------
2026-02-10 09:08:24,748 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 09:08:27,902 - INFO - [Train] [83/90] | Loss: 0.2636 | Train Acc: 96.65%
2026-02-10 09:08:28,533 - INFO - [Valid] [83/90] | Loss: 0.5949 | Val Acc: 78.76%
2026-02-10 09:08:28,538 - INFO - [Metrics for 'abnormal'] | Precision: 0.7576 | Recall: 0.7962 | F1: 0.7764
2026-02-10 09:08:28,538 - INFO - [Metrics for 'normal'] | Precision: 0.8161 | Recall: 0.7802 | F1: 0.7978
2026-02-10 09:08:28,539 - INFO - --------------------------------------------------
2026-02-10 09:08:28,540 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 09:08:31,985 - INFO - [Train] [84/90] | Loss: 0.2573 | Train Acc: 97.32%
2026-02-10 09:08:32,590 - INFO - [Valid] [84/90] | Loss: 0.5789 | Val Acc: 82.60%
2026-02-10 09:08:32,595 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-02-10 09:08:32,596 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-02-10 09:08:32,598 - INFO - --------------------------------------------------
2026-02-10 09:08:32,599 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 09:08:36,205 - INFO - [Train] [85/90] | Loss: 0.2507 | Train Acc: 97.69%
2026-02-10 09:08:36,873 - INFO - [Valid] [85/90] | Loss: 0.5903 | Val Acc: 79.94%
2026-02-10 09:08:36,878 - INFO - [Metrics for 'abnormal'] | Precision: 0.7799 | Recall: 0.7898 | F1: 0.7848
2026-02-10 09:08:36,878 - INFO - [Metrics for 'normal'] | Precision: 0.8167 | Recall: 0.8077 | F1: 0.8122
2026-02-10 09:08:36,880 - INFO - --------------------------------------------------
2026-02-10 09:08:36,881 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 09:08:40,348 - INFO - [Train] [86/90] | Loss: 0.2524 | Train Acc: 97.47%
2026-02-10 09:08:40,849 - INFO - [Valid] [86/90] | Loss: 0.6011 | Val Acc: 81.42%
2026-02-10 09:08:40,853 - INFO - [Metrics for 'abnormal'] | Precision: 0.8176 | Recall: 0.7707 | F1: 0.7934
2026-02-10 09:08:40,853 - INFO - [Metrics for 'normal'] | Precision: 0.8115 | Recall: 0.8516 | F1: 0.8311
2026-02-10 09:08:40,855 - INFO - --------------------------------------------------
2026-02-10 09:08:40,857 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 09:08:44,494 - INFO - [Train] [87/90] | Loss: 0.2559 | Train Acc: 97.47%
2026-02-10 09:08:45,240 - INFO - [Valid] [87/90] | Loss: 0.6018 | Val Acc: 81.42%
2026-02-10 09:08:45,245 - INFO - [Metrics for 'abnormal'] | Precision: 0.7798 | Recall: 0.8344 | F1: 0.8062
2026-02-10 09:08:45,245 - INFO - [Metrics for 'normal'] | Precision: 0.8480 | Recall: 0.7967 | F1: 0.8215
2026-02-10 09:08:45,246 - INFO - --------------------------------------------------
2026-02-10 09:08:45,247 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 09:08:48,844 - INFO - [Train] [88/90] | Loss: 0.2446 | Train Acc: 98.21%
2026-02-10 09:08:49,801 - INFO - [Valid] [88/90] | Loss: 0.5963 | Val Acc: 79.94%
2026-02-10 09:08:49,806 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.7580 | F1: 0.7778
2026-02-10 09:08:49,806 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.8352 | F1: 0.8172
2026-02-10 09:08:49,807 - INFO - --------------------------------------------------
2026-02-10 09:08:49,808 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 09:08:53,263 - INFO - [Train] [89/90] | Loss: 0.2503 | Train Acc: 97.77%
2026-02-10 09:08:54,301 - INFO - [Valid] [89/90] | Loss: 0.5960 | Val Acc: 79.94%
2026-02-10 09:08:54,306 - INFO - [Metrics for 'abnormal'] | Precision: 0.8248 | Recall: 0.7197 | F1: 0.7687
2026-02-10 09:08:54,307 - INFO - [Metrics for 'normal'] | Precision: 0.7822 | Recall: 0.8681 | F1: 0.8229
2026-02-10 09:08:54,308 - INFO - --------------------------------------------------
2026-02-10 09:08:54,309 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 09:08:57,338 - INFO - [Train] [90/90] | Loss: 0.2584 | Train Acc: 97.10%
2026-02-10 09:08:58,322 - INFO - [Valid] [90/90] | Loss: 0.6194 | Val Acc: 79.06%
2026-02-10 09:08:58,326 - INFO - [Metrics for 'abnormal'] | Precision: 0.7590 | Recall: 0.8025 | F1: 0.7802
2026-02-10 09:08:58,326 - INFO - [Metrics for 'normal'] | Precision: 0.8208 | Recall: 0.7802 | F1: 0.8000
2026-02-10 09:08:58,328 - INFO - ==================================================
2026-02-10 09:08:58,328 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 09:08:58,328 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 09:08:58,328 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 09:08:58,397 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 09:08:58,398 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 09:08:58,398 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 09:08:58,400 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 09:09:00,891 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 09:09:00,981 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8754345703125)에 맞춰 변경되었습니다.
2026-02-10 09:09:00,981 - INFO - ==================================================
2026-02-10 09:09:01,036 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/best_model.pth'
2026-02-10 09:09:01,037 - INFO - ==================================================
2026-02-10 09:09:01,037 - INFO - Test 모드를 시작합니다.
2026-02-10 09:09:01,142 - INFO - 연산량 (MACs): 0.0048 GMACs per sample
2026-02-10 09:09:01,143 - INFO - 연산량 (FLOPs): 0.0095 GFLOPs per sample
2026-02-10 09:09:01,143 - INFO - ==================================================
2026-02-10 09:09:01,143 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 09:09:02,234 - INFO - 샘플 당 평균 Forward Pass 시간: 1.64ms (std: 0.84ms), FPS: 822.41 (std: 516.56) (1개 샘플 x 100회 반복)
2026-02-10 09:09:02,234 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 58.12 MB
2026-02-10 09:09:02,234 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 09:09:03,658 - INFO - [Test] Loss: 0.4526 | Test Acc: 81.42%
2026-02-10 09:09:03,665 - INFO - [Metrics for 'abnormal'] | Precision: 0.7975 | Recall: 0.8025 | F1: 0.8000
2026-02-10 09:09:03,665 - INFO - [Metrics for 'normal'] | Precision: 0.8287 | Recall: 0.8242 | F1: 0.8264
2026-02-10 09:09:03,955 - INFO - ==================================================
2026-02-10 09:09:03,955 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/confusion_matrix_20260210_085244.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/confusion_matrix_20260210_085244.pdf'
2026-02-10 09:09:03,955 - INFO - ==================================================
2026-02-10 09:09:03,955 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 09:09:05,184 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/model_fp32_20260210_085244.onnx'에 저장되었습니다. (크기: 0.20 MB)
2026-02-10 09:09:05,546 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 8.06 MB
2026-02-10 09:09:05,546 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 09:09:06,296 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 2.91ms (std: 3.35ms)
2026-02-10 09:09:06,296 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 832.69 FPS (std: 524.36) (1개 샘플 x 100회 반복)
2026-02-10 09:09:06,296 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 2.81 MB
2026-02-10 09:09:06,297 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 11.44 MB
2026-02-10 09:09:07,776 - INFO - [Test (ONNX)] | Test Acc (ONNX): 81.42%
2026-02-10 09:09:07,783 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.7975 | Recall: 0.8025 | F1: 0.8000
2026-02-10 09:09:07,787 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8287 | Recall: 0.8242 | F1: 0.8264
2026-02-10 09:09:07,921 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/val_acc.pdf'
2026-02-10 09:09:08,115 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/train_val_acc.pdf'
2026-02-10 09:09:08,280 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/F1_normal.pdf'
2026-02-10 09:09:08,436 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/val_loss.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/val_loss.pdf'
2026-02-10 09:09:08,625 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/learning_rate.pdf'
2026-02-10 09:09:10,618 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/compile.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_085244/graph_20260210_085244/compile.pdf'
