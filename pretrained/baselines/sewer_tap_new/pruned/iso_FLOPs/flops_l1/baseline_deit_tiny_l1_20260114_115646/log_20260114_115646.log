2026-01-14 11:56:46,461 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/log_20260114_115646.log'에 저장됩니다.
2026-01-14 11:56:46,469 - INFO - ==================================================
2026-01-14 11:56:46,469 - INFO - config.yaml:
2026-01-14 11:56:46,469 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_inference_dir: ./pretrained
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      train_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/cau/workspace/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  patch_size: 56
  stride: 56
  cnn_feature_extractor:
    name: efficientnet_b0_feat2
  featured_patch_dim: 24
  emb_dim: 24
  num_heads: 2
  num_decoder_layers: 2
  num_decoder_patches: 1
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: true
  save_attention: true
  num_plot_attention: 5
baseline:
  model_name: deit_tiny
  use_l1_pruning: true
  pruning_flops_target: 0.1829

2026-01-14 11:56:46,471 - INFO - ==================================================
2026-01-14 11:56:46,534 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA RTX PRO 6000 Blackwell Server Edition)
2026-01-14 11:56:46,534 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-01-14 11:56:46,534 - INFO - '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-01-14 11:56:46,548 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-01-14 11:56:46,549 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-01-14 11:56:46,549 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-01-14 11:56:46,549 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 11:56:51,343 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-01-14 11:56:51,347 - INFO - ==================================================
2026-01-14 11:56:51,347 - INFO - 모델 파라미터 수:
2026-01-14 11:56:51,348 - INFO -   - 총 파라미터: 5,524,802 개
2026-01-14 11:56:51,348 - INFO -   - 학습 가능한 파라미터: 5,524,802 개
2026-01-14 11:56:51,348 - INFO - ================================================================================
2026-01-14 11:56:51,349 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-01-14 11:56:51,349 - INFO - ================================================================================
2026-01-14 11:56:51,350 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 11:56:51,352 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-01-14 11:56:51,353 - INFO - ==================================================
2026-01-14 11:56:51,354 - INFO - train 모드를 시작합니다.
2026-01-14 11:56:51,354 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 11:56:51,355 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 11:56:51,356 - INFO - --------------------------------------------------
2026-01-14 11:56:51,358 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-01-14 11:56:59,325 - INFO - [Train] [1/10] | Loss: 0.7051 | Train Acc: 60.49%
2026-01-14 11:57:01,727 - INFO - [Valid] [1/10] | Loss: 0.6723 | Val Acc: 59.88%
2026-01-14 11:57:01,764 - INFO - [Metrics for 'abnormal'] | Precision: 0.5484 | Recall: 0.7580 | F1: 0.6364
2026-01-14 11:57:01,764 - INFO - [Metrics for 'normal'] | Precision: 0.6885 | Recall: 0.4615 | F1: 0.5526
2026-01-14 11:57:01,845 - INFO - [Best Model Saved] (val loss: 0.6723) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 11:57:01,846 - INFO - --------------------------------------------------
2026-01-14 11:57:01,848 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-01-14 11:57:07,595 - INFO - [Train] [2/10] | Loss: 0.6453 | Train Acc: 65.48%
2026-01-14 11:57:09,104 - INFO - [Valid] [2/10] | Loss: 0.6491 | Val Acc: 63.42%
2026-01-14 11:57:09,118 - INFO - [Metrics for 'abnormal'] | Precision: 0.7260 | Recall: 0.3376 | F1: 0.4609
2026-01-14 11:57:09,118 - INFO - [Metrics for 'normal'] | Precision: 0.6090 | Recall: 0.8901 | F1: 0.7232
2026-01-14 11:57:09,181 - INFO - [Best Model Saved] (val loss: 0.6491) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 11:57:09,182 - INFO - --------------------------------------------------
2026-01-14 11:57:09,184 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-01-14 11:57:15,239 - INFO - [Train] [3/10] | Loss: 0.5895 | Train Acc: 71.50%
2026-01-14 11:57:16,779 - INFO - [Valid] [3/10] | Loss: 0.5874 | Val Acc: 73.16%
2026-01-14 11:57:16,792 - INFO - [Metrics for 'abnormal'] | Precision: 0.7115 | Recall: 0.7070 | F1: 0.7093
2026-01-14 11:57:16,793 - INFO - [Metrics for 'normal'] | Precision: 0.7486 | Recall: 0.7527 | F1: 0.7507
2026-01-14 11:57:16,854 - INFO - [Best Model Saved] (val loss: 0.5874) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 11:57:16,854 - INFO - --------------------------------------------------
2026-01-14 11:57:16,856 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-01-14 11:57:22,824 - INFO - [Train] [4/10] | Loss: 0.5698 | Train Acc: 75.07%
2026-01-14 11:57:24,561 - INFO - [Valid] [4/10] | Loss: 0.6173 | Val Acc: 69.03%
2026-01-14 11:57:24,574 - INFO - [Metrics for 'abnormal'] | Precision: 0.6182 | Recall: 0.8662 | F1: 0.7215
2026-01-14 11:57:24,574 - INFO - [Metrics for 'normal'] | Precision: 0.8235 | Recall: 0.5385 | F1: 0.6512
2026-01-14 11:57:24,578 - INFO - --------------------------------------------------
2026-01-14 11:57:24,581 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-01-14 11:57:30,835 - INFO - [Train] [5/10] | Loss: 0.5272 | Train Acc: 78.05%
2026-01-14 11:57:32,471 - INFO - [Valid] [5/10] | Loss: 0.5682 | Val Acc: 74.34%
2026-01-14 11:57:32,485 - INFO - [Metrics for 'abnormal'] | Precision: 0.6804 | Recall: 0.8408 | F1: 0.7521
2026-01-14 11:57:32,486 - INFO - [Metrics for 'normal'] | Precision: 0.8276 | Recall: 0.6593 | F1: 0.7339
2026-01-14 11:57:32,563 - INFO - [Best Model Saved] (val loss: 0.5682) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 11:57:32,564 - INFO - --------------------------------------------------
2026-01-14 11:57:32,566 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-01-14 11:57:37,528 - INFO - [Train] [6/10] | Loss: 0.5211 | Train Acc: 79.32%
2026-01-14 11:57:38,922 - INFO - [Valid] [6/10] | Loss: 0.5826 | Val Acc: 75.22%
2026-01-14 11:57:38,934 - INFO - [Metrics for 'abnormal'] | Precision: 0.8288 | Recall: 0.5860 | F1: 0.6866
2026-01-14 11:57:38,935 - INFO - [Metrics for 'normal'] | Precision: 0.7149 | Recall: 0.8956 | F1: 0.7951
2026-01-14 11:57:38,939 - INFO - --------------------------------------------------
2026-01-14 11:57:38,941 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-01-14 11:57:44,044 - INFO - [Train] [7/10] | Loss: 0.5065 | Train Acc: 79.17%
2026-01-14 11:57:45,701 - INFO - [Valid] [7/10] | Loss: 0.5709 | Val Acc: 76.40%
2026-01-14 11:57:45,712 - INFO - [Metrics for 'abnormal'] | Precision: 0.8468 | Recall: 0.5987 | F1: 0.7015
2026-01-14 11:57:45,712 - INFO - [Metrics for 'normal'] | Precision: 0.7237 | Recall: 0.9066 | F1: 0.8049
2026-01-14 11:57:45,716 - INFO - --------------------------------------------------
2026-01-14 11:57:45,718 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-01-14 11:57:54,323 - INFO - [Train] [8/10] | Loss: 0.4964 | Train Acc: 79.99%
2026-01-14 11:57:56,626 - INFO - [Valid] [8/10] | Loss: 0.5420 | Val Acc: 76.70%
2026-01-14 11:57:56,636 - INFO - [Metrics for 'abnormal'] | Precision: 0.8197 | Recall: 0.6369 | F1: 0.7168
2026-01-14 11:57:56,637 - INFO - [Metrics for 'normal'] | Precision: 0.7373 | Recall: 0.8791 | F1: 0.8020
2026-01-14 11:57:56,690 - INFO - [Best Model Saved] (val loss: 0.5420) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 11:57:56,691 - INFO - --------------------------------------------------
2026-01-14 11:57:56,693 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-01-14 11:58:05,188 - INFO - [Train] [9/10] | Loss: 0.4954 | Train Acc: 79.46%
2026-01-14 11:58:07,356 - INFO - [Valid] [9/10] | Loss: 0.5336 | Val Acc: 75.81%
2026-01-14 11:58:07,371 - INFO - [Metrics for 'abnormal'] | Precision: 0.7660 | Recall: 0.6879 | F1: 0.7248
2026-01-14 11:58:07,375 - INFO - [Metrics for 'normal'] | Precision: 0.7525 | Recall: 0.8187 | F1: 0.7842
2026-01-14 11:58:07,437 - INFO - [Best Model Saved] (val loss: 0.5336) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 11:58:07,437 - INFO - --------------------------------------------------
2026-01-14 11:58:07,439 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-01-14 11:58:15,761 - INFO - [Train] [10/10] | Loss: 0.4774 | Train Acc: 81.40%
2026-01-14 11:58:17,725 - INFO - [Valid] [10/10] | Loss: 0.5236 | Val Acc: 77.58%
2026-01-14 11:58:17,737 - INFO - [Metrics for 'abnormal'] | Precision: 0.7613 | Recall: 0.7516 | F1: 0.7564
2026-01-14 11:58:17,738 - INFO - [Metrics for 'normal'] | Precision: 0.7880 | Recall: 0.7967 | F1: 0.7923
2026-01-14 11:58:17,809 - INFO - [Best Model Saved] (val loss: 0.5236) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 11:58:17,810 - INFO - ================================================================================
2026-01-14 11:58:17,811 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-01-14 11:58:17,811 - INFO - ================================================================================
2026-01-14 11:58:17,887 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'을(를) 불러왔습니다.
2026-01-14 11:58:17,888 - INFO - ================================================================================
2026-01-14 11:58:17,888 - INFO - 목표 FLOPs (0.1829 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-01-14 11:58:17,945 - INFO - 원본 모델 FLOPs: 2.1493 GFLOPs
2026-01-14 11:58:18,058 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:18,059 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:18,060 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:18,758 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-01-14 11:58:18,758 - INFO - ==================================================
2026-01-14 11:58:18,818 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.7288 GFLOPs (감소율: 66.09%)
2026-01-14 11:58:18,893 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:18,893 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:18,894 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:19,488 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-01-14 11:58:19,488 - INFO - ==================================================
2026-01-14 11:58:19,530 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.2840 GFLOPs (감소율: 86.79%)
2026-01-14 11:58:19,608 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:19,608 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:19,609 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:20,657 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-01-14 11:58:20,657 - INFO - ==================================================
2026-01-14 11:58:20,699 - INFO -   [탐색  3] 희소도: 0.8662 -> FLOPs: 0.1224 GFLOPs (감소율: 94.30%)
2026-01-14 11:58:20,755 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:20,756 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:20,758 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:21,402 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.804375)에 맞춰 변경되었습니다.
2026-01-14 11:58:21,403 - INFO - ==================================================
2026-01-14 11:58:21,447 - INFO -   [탐색  4] 희소도: 0.8044 -> FLOPs: 0.1980 GFLOPs (감소율: 90.79%)
2026-01-14 11:58:21,505 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:21,506 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:21,507 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:22,071 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8353124999999999)에 맞춰 변경되었습니다.
2026-01-14 11:58:22,072 - INFO - ==================================================
2026-01-14 11:58:22,123 - INFO -   [탐색  5] 희소도: 0.8353 -> FLOPs: 0.1588 GFLOPs (감소율: 92.61%)
2026-01-14 11:58:22,191 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:22,192 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:22,193 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:22,772 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81984375)에 맞춰 변경되었습니다.
2026-01-14 11:58:22,773 - INFO - ==================================================
2026-01-14 11:58:22,905 - INFO -   [탐색  6] 희소도: 0.8198 -> FLOPs: 0.1781 GFLOPs (감소율: 91.72%)
2026-01-14 11:58:22,966 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:22,967 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:22,968 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:23,565 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8121093749999999)에 맞춰 변경되었습니다.
2026-01-14 11:58:23,565 - INFO - ==================================================
2026-01-14 11:58:23,603 - INFO -   [탐색  7] 희소도: 0.8121 -> FLOPs: 0.1906 GFLOPs (감소율: 91.13%)
2026-01-14 11:58:23,647 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:23,648 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:23,649 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:24,479 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8159765625)에 맞춰 변경되었습니다.
2026-01-14 11:58:24,480 - INFO - ==================================================
2026-01-14 11:58:24,512 - INFO -   [탐색  8] 희소도: 0.8160 -> FLOPs: 0.1843 GFLOPs (감소율: 91.43%)
2026-01-14 11:58:24,562 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:24,563 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:24,564 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:25,245 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81791015625)에 맞춰 변경되었습니다.
2026-01-14 11:58:25,248 - INFO - ==================================================
2026-01-14 11:58:25,329 - INFO -   [탐색  9] 희소도: 0.8179 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:25,453 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:25,459 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:25,460 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:26,372 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-01-14 11:58:26,373 - INFO - ==================================================
2026-01-14 11:58:26,412 - INFO -   [탐색 10] 희소도: 0.8169 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:26,471 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:26,471 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:26,472 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:27,310 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8174267578125)에 맞춰 변경되었습니다.
2026-01-14 11:58:27,312 - INFO - ==================================================
2026-01-14 11:58:27,375 - INFO -   [탐색 11] 희소도: 0.8174 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:27,431 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:27,432 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:27,433 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:28,028 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81766845703125)에 맞춰 변경되었습니다.
2026-01-14 11:58:28,032 - INFO - ==================================================
2026-01-14 11:58:28,112 - INFO -   [탐색 12] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:28,190 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:28,194 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:28,195 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:29,445 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.817789306640625)에 맞춰 변경되었습니다.
2026-01-14 11:58:29,446 - INFO - ==================================================
2026-01-14 11:58:29,485 - INFO -   [탐색 13] 희소도: 0.8178 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:29,544 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:29,545 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:29,546 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:30,145 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177288818359375)에 맞춰 변경되었습니다.
2026-01-14 11:58:30,145 - INFO - ==================================================
2026-01-14 11:58:30,187 - INFO -   [탐색 14] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:30,243 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:30,243 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:30,244 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:30,853 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8176986694335937)에 맞춰 변경되었습니다.
2026-01-14 11:58:30,854 - INFO - ==================================================
2026-01-14 11:58:30,897 - INFO -   [탐색 15] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:30,947 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:30,947 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:30,948 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:31,732 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177137756347657)에 맞춰 변경되었습니다.
2026-01-14 11:58:31,733 - INFO - ==================================================
2026-01-14 11:58:31,773 - INFO -   [탐색 16] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:31,837 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:31,838 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:31,839 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:32,420 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177062225341797)에 맞춰 변경되었습니다.
2026-01-14 11:58:32,420 - INFO - ==================================================
2026-01-14 11:58:32,459 - INFO -   [탐색 17] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:32,514 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:32,515 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:32,516 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:33,462 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177099990844727)에 맞춰 변경되었습니다.
2026-01-14 11:58:33,462 - INFO - ==================================================
2026-01-14 11:58:33,501 - INFO -   [탐색 18] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:33,557 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:33,558 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:33,559 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:34,167 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177081108093263)에 맞춰 변경되었습니다.
2026-01-14 11:58:34,168 - INFO - ==================================================
2026-01-14 11:58:34,207 - INFO -   [탐색 19] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:34,280 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:34,281 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:34,281 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:34,928 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177090549468995)에 맞춰 변경되었습니다.
2026-01-14 11:58:34,929 - INFO - ==================================================
2026-01-14 11:58:34,967 - INFO -   [탐색 20] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:35,022 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:35,022 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:35,023 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:35,663 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177085828781129)에 맞춰 변경되었습니다.
2026-01-14 11:58:35,664 - INFO - ==================================================
2026-01-14 11:58:35,703 - INFO -   [탐색 21] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:35,757 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:35,758 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:35,759 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:36,353 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083468437196)에 맞춰 변경되었습니다.
2026-01-14 11:58:36,354 - INFO - ==================================================
2026-01-14 11:58:36,393 - INFO -   [탐색 22] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:36,448 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:36,448 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:36,449 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:37,281 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082288265229)에 맞춰 변경되었습니다.
2026-01-14 11:58:37,282 - INFO - ==================================================
2026-01-14 11:58:37,321 - INFO -   [탐색 23] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:37,376 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:37,377 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:37,377 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:37,990 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082878351213)에 맞춰 변경되었습니다.
2026-01-14 11:58:37,992 - INFO - ==================================================
2026-01-14 11:58:38,031 - INFO -   [탐색 24] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:38,071 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:38,072 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:38,072 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:38,576 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083173394204)에 맞춰 변경되었습니다.
2026-01-14 11:58:38,577 - INFO - ==================================================
2026-01-14 11:58:38,615 - INFO -   [탐색 25] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:38,672 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:38,673 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:38,674 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:39,198 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833209157)에 맞춰 변경되었습니다.
2026-01-14 11:58:39,198 - INFO - ==================================================
2026-01-14 11:58:39,224 - INFO -   [탐색 26] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:39,267 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:39,267 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:39,268 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:40,035 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083394676448)에 맞춰 변경되었습니다.
2026-01-14 11:58:40,035 - INFO - ==================================================
2026-01-14 11:58:40,074 - INFO -   [탐색 27] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:40,128 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:40,128 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:40,129 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:40,594 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083357796073)에 맞춰 변경되었습니다.
2026-01-14 11:58:40,594 - INFO - ==================================================
2026-01-14 11:58:40,633 - INFO -   [탐색 28] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:40,688 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:40,689 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:40,690 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:41,244 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083339355886)에 맞춰 변경되었습니다.
2026-01-14 11:58:41,244 - INFO - ==================================================
2026-01-14 11:58:41,275 - INFO -   [탐색 29] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:41,327 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:41,328 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:41,329 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:41,931 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083330135793)에 맞춰 변경되었습니다.
2026-01-14 11:58:41,931 - INFO - ==================================================
2026-01-14 11:58:41,963 - INFO -   [탐색 30] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:42,007 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:42,008 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:42,009 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:42,466 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083334745839)에 맞춰 변경되었습니다.
2026-01-14 11:58:42,466 - INFO - ==================================================
2026-01-14 11:58:42,510 - INFO -   [탐색 31] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:42,571 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:42,572 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:42,573 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:43,425 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083332440815)에 맞춰 변경되었습니다.
2026-01-14 11:58:43,426 - INFO - ==================================================
2026-01-14 11:58:43,465 - INFO -   [탐색 32] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:43,522 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:43,523 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:43,524 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:44,069 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333593327)에 맞춰 변경되었습니다.
2026-01-14 11:58:44,070 - INFO - ==================================================
2026-01-14 11:58:44,104 - INFO -   [탐색 33] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:44,159 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:44,159 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:44,161 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:44,707 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333017071)에 맞춰 변경되었습니다.
2026-01-14 11:58:44,707 - INFO - ==================================================
2026-01-14 11:58:44,741 - INFO -   [탐색 34] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:44,786 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:44,787 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:44,788 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:45,259 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833333052)에 맞춰 변경되었습니다.
2026-01-14 11:58:45,260 - INFO - ==================================================
2026-01-14 11:58:45,285 - INFO -   [탐색 35] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:45,320 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:45,320 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:45,320 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:45,785 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333449263)에 맞춰 변경되었습니다.
2026-01-14 11:58:45,785 - INFO - ==================================================
2026-01-14 11:58:45,823 - INFO -   [탐색 36] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:45,877 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:45,878 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:45,879 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:46,699 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333377231)에 맞춰 변경되었습니다.
2026-01-14 11:58:46,700 - INFO - ==================================================
2026-01-14 11:58:46,738 - INFO -   [탐색 37] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:46,792 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:46,793 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:46,794 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:47,338 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333341215)에 맞춰 변경되었습니다.
2026-01-14 11:58:47,339 - INFO - ==================================================
2026-01-14 11:58:47,377 - INFO -   [탐색 38] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:47,431 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:47,431 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:47,432 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:48,149 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333323207)에 맞춰 변경되었습니다.
2026-01-14 11:58:48,150 - INFO - ==================================================
2026-01-14 11:58:48,191 - INFO -   [탐색 39] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:48,250 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:48,250 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:48,251 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:48,775 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332211)에 맞춰 변경되었습니다.
2026-01-14 11:58:48,776 - INFO - ==================================================
2026-01-14 11:58:48,814 - INFO -   [탐색 40] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:48,869 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:48,870 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:48,871 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:49,518 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333336713)에 맞춰 변경되었습니다.
2026-01-14 11:58:49,519 - INFO - ==================================================
2026-01-14 11:58:49,557 - INFO -   [탐색 41] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:49,613 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:49,614 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:49,614 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:50,433 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333334463)에 맞춰 변경되었습니다.
2026-01-14 11:58:50,433 - INFO - ==================================================
2026-01-14 11:58:50,478 - INFO -   [탐색 42] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:50,533 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:50,533 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:50,534 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:51,244 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333337)에 맞춰 변경되었습니다.
2026-01-14 11:58:51,245 - INFO - ==================================================
2026-01-14 11:58:51,285 - INFO -   [탐색 43] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:51,345 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:51,346 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:51,347 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:51,898 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332774)에 맞춰 변경되었습니다.
2026-01-14 11:58:51,899 - INFO - ==================================================
2026-01-14 11:58:51,939 - INFO -   [탐색 44] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:51,997 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:51,998 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:51,999 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:52,556 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333055)에 맞춰 변경되었습니다.
2026-01-14 11:58:52,557 - INFO - ==================================================
2026-01-14 11:58:52,596 - INFO -   [탐색 45] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:52,650 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:52,651 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:52,652 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:53,183 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333196)에 맞춰 변경되었습니다.
2026-01-14 11:58:53,183 - INFO - ==================================================
2026-01-14 11:58:53,218 - INFO -   [탐색 46] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:53,269 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:53,269 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:53,270 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:54,046 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333266)에 맞춰 변경되었습니다.
2026-01-14 11:58:54,047 - INFO - ==================================================
2026-01-14 11:58:54,083 - INFO -   [탐색 47] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:54,133 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:54,133 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:54,134 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:54,673 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333302)에 맞춰 변경되었습니다.
2026-01-14 11:58:54,673 - INFO - ==================================================
2026-01-14 11:58:54,703 - INFO -   [탐색 48] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:54,747 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:54,748 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:54,749 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:55,211 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333319)에 맞춰 변경되었습니다.
2026-01-14 11:58:55,212 - INFO - ==================================================
2026-01-14 11:58:55,250 - INFO -   [탐색 49] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:55,305 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:55,306 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:55,307 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:55,832 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333328)에 맞춰 변경되었습니다.
2026-01-14 11:58:55,833 - INFO - ==================================================
2026-01-14 11:58:55,872 - INFO -   [탐색 50] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:55,930 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:55,930 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:55,932 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:56,488 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:58:56,489 - INFO - ==================================================
2026-01-14 11:58:56,530 - INFO -   [탐색 51] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:56,585 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:56,586 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:56,587 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:57,651 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333335)에 맞춰 변경되었습니다.
2026-01-14 11:58:57,652 - INFO - ==================================================
2026-01-14 11:58:57,726 - INFO -   [탐색 52] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:57,847 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:57,847 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:57,848 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:58,778 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333334)에 맞춰 변경되었습니다.
2026-01-14 11:58:58,778 - INFO - ==================================================
2026-01-14 11:58:58,824 - INFO -   [탐색 53] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 11:58:58,878 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:58,879 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:58,880 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:59,489 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:58:59,490 - INFO - ==================================================
2026-01-14 11:58:59,586 - INFO -   [탐색 54] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:58:59,712 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:59,712 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:59,713 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:00,519 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:00,519 - INFO - ==================================================
2026-01-14 11:59:00,567 - INFO -   [탐색 55] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:00,630 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:00,631 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:00,632 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:01,567 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:01,568 - INFO - ==================================================
2026-01-14 11:59:01,610 - INFO -   [탐색 56] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:01,666 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:01,667 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:01,668 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:02,233 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:02,234 - INFO - ==================================================
2026-01-14 11:59:02,274 - INFO -   [탐색 57] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:02,330 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:02,331 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:02,332 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:02,887 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:02,888 - INFO - ==================================================
2026-01-14 11:59:02,926 - INFO -   [탐색 58] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:02,980 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:02,980 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:02,981 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:03,536 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:03,537 - INFO - ==================================================
2026-01-14 11:59:03,569 - INFO -   [탐색 59] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:03,608 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:03,608 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:03,609 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:04,117 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:04,117 - INFO - ==================================================
2026-01-14 11:59:04,159 - INFO -   [탐색 60] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:04,215 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:04,216 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:04,217 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:05,263 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:05,263 - INFO - ==================================================
2026-01-14 11:59:05,303 - INFO -   [탐색 61] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:05,358 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:05,358 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:05,359 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:05,932 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:05,932 - INFO - ==================================================
2026-01-14 11:59:05,990 - INFO -   [탐색 62] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:06,055 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:06,055 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:06,056 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:06,626 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:06,627 - INFO - ==================================================
2026-01-14 11:59:06,668 - INFO -   [탐색 63] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:06,724 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:06,724 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:06,725 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:07,522 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:07,523 - INFO - ==================================================
2026-01-14 11:59:07,567 - INFO -   [탐색 64] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:07,606 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:07,606 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:07,607 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:08,423 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:08,423 - INFO - ==================================================
2026-01-14 11:59:08,462 - INFO -   [탐색 65] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:08,511 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:08,512 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:08,513 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:09,371 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:09,372 - INFO - ==================================================
2026-01-14 11:59:09,410 - INFO -   [탐색 66] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:09,465 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:09,465 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:09,466 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:10,614 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:10,618 - INFO - ==================================================
2026-01-14 11:59:10,704 - INFO -   [탐색 67] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:10,820 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:10,821 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:10,822 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:11,371 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:11,372 - INFO - ==================================================
2026-01-14 11:59:11,410 - INFO -   [탐색 68] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:11,452 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:11,452 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:11,453 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:11,957 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:11,958 - INFO - ==================================================
2026-01-14 11:59:11,995 - INFO -   [탐색 69] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:12,050 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:12,051 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:12,052 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:12,686 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:12,687 - INFO - ==================================================
2026-01-14 11:59:12,727 - INFO -   [탐색 70] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:12,787 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:12,788 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:12,789 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:13,684 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:13,685 - INFO - ==================================================
2026-01-14 11:59:13,726 - INFO -   [탐색 71] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:13,797 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:13,798 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:13,799 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:14,346 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:14,350 - INFO - ==================================================
2026-01-14 11:59:14,434 - INFO -   [탐색 72] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:14,555 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:14,555 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:14,559 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:15,089 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:15,090 - INFO - ==================================================
2026-01-14 11:59:15,132 - INFO -   [탐색 73] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:15,230 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:15,231 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:15,232 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:16,049 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:16,049 - INFO - ==================================================
2026-01-14 11:59:16,109 - INFO -   [탐색 74] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:16,161 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:16,161 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:16,162 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:17,045 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:17,047 - INFO - ==================================================
2026-01-14 11:59:17,111 - INFO -   [탐색 75] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:17,191 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:17,191 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:17,192 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:18,043 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:18,044 - INFO - ==================================================
2026-01-14 11:59:18,083 - INFO -   [탐색 76] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:18,139 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:18,139 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:18,140 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:18,727 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:18,727 - INFO - ==================================================
2026-01-14 11:59:18,765 - INFO -   [탐색 77] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:18,820 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:18,821 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:18,821 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:19,344 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:19,345 - INFO - ==================================================
2026-01-14 11:59:19,373 - INFO -   [탐색 78] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:19,415 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:19,415 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:19,416 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:19,946 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:19,946 - INFO - ==================================================
2026-01-14 11:59:19,985 - INFO -   [탐색 79] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:20,042 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:20,042 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:20,043 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:20,981 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:20,982 - INFO - ==================================================
2026-01-14 11:59:21,029 - INFO -   [탐색 80] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:21,287 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:21,287 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:21,288 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:21,925 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:21,926 - INFO - ==================================================
2026-01-14 11:59:22,057 - INFO -   [탐색 81] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:22,113 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:22,113 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:22,115 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:22,734 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:22,735 - INFO - ==================================================
2026-01-14 11:59:22,855 - INFO -   [탐색 82] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:22,974 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:22,974 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:22,975 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:24,068 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:24,068 - INFO - ==================================================
2026-01-14 11:59:24,121 - INFO -   [탐색 83] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:24,178 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:24,179 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:24,180 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:24,748 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:24,749 - INFO - ==================================================
2026-01-14 11:59:24,818 - INFO -   [탐색 84] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:24,876 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:24,876 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:24,877 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:25,740 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:25,741 - INFO - ==================================================
2026-01-14 11:59:25,780 - INFO -   [탐색 85] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:25,836 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:25,837 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:25,838 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:26,379 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:26,379 - INFO - ==================================================
2026-01-14 11:59:26,417 - INFO -   [탐색 86] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:26,471 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:26,472 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:26,473 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:27,029 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:27,029 - INFO - ==================================================
2026-01-14 11:59:27,068 - INFO -   [탐색 87] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:27,125 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:27,125 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:27,126 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:27,639 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:27,640 - INFO - ==================================================
2026-01-14 11:59:27,682 - INFO -   [탐색 88] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:27,743 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:27,744 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:27,745 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:28,326 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:28,327 - INFO - ==================================================
2026-01-14 11:59:28,366 - INFO -   [탐색 89] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:28,447 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:28,447 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:28,448 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:29,686 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:29,687 - INFO - ==================================================
2026-01-14 11:59:29,763 - INFO -   [탐색 90] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:29,817 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:29,818 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:29,819 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:30,428 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:30,428 - INFO - ==================================================
2026-01-14 11:59:30,467 - INFO -   [탐색 91] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:30,519 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:30,520 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:30,521 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:31,103 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:31,104 - INFO - ==================================================
2026-01-14 11:59:31,143 - INFO -   [탐색 92] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:31,199 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:31,200 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:31,200 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:31,731 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:31,732 - INFO - ==================================================
2026-01-14 11:59:31,770 - INFO -   [탐색 93] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:31,827 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:31,828 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:31,829 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:32,375 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:32,376 - INFO - ==================================================
2026-01-14 11:59:32,415 - INFO -   [탐색 94] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:32,470 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:32,470 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:32,471 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:33,325 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:33,325 - INFO - ==================================================
2026-01-14 11:59:33,364 - INFO -   [탐색 95] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:33,421 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:33,421 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:33,422 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:33,954 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:33,955 - INFO - ==================================================
2026-01-14 11:59:34,006 - INFO -   [탐색 96] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:34,062 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:34,062 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:34,063 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:34,595 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:34,596 - INFO - ==================================================
2026-01-14 11:59:34,629 - INFO -   [탐색 97] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:34,683 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:34,684 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:34,685 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:35,187 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:35,188 - INFO - ==================================================
2026-01-14 11:59:35,219 - INFO -   [탐색 98] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:35,262 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:35,263 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:35,263 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:35,792 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:35,792 - INFO - ==================================================
2026-01-14 11:59:35,830 - INFO -   [탐색 99] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:35,885 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:35,886 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:35,887 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:36,693 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 11:59:36,694 - INFO - ==================================================
2026-01-14 11:59:36,732 - INFO -   [탐색 100] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 11:59:36,733 - INFO - 탐색 완료. 목표 FLOPs(0.1829)에 가장 근접한 최적 희소도는 0.8169 입니다.
2026-01-14 11:59:36,733 - INFO - ================================================================================
2026-01-14 11:59:36,738 - INFO - 계산된 Pruning 정보(희소도: 0.8169)를 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/pruning_info.yaml'에 저장했습니다.
2026-01-14 11:59:36,821 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-01-14 11:59:36,957 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:36,961 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:36,962 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:37,754 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-01-14 11:59:37,754 - INFO - ==================================================
2026-01-14 11:59:37,761 - INFO - ==================================================
2026-01-14 11:59:37,761 - INFO - 모델 파라미터 수:
2026-01-14 11:59:37,761 - INFO -   - 총 파라미터: 485,259 개
2026-01-14 11:59:37,761 - INFO -   - 학습 가능한 파라미터: 485,259 개
2026-01-14 11:59:37,826 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-01-14 11:59:37,942 - INFO - FLOPs가 2.1493 GFLOPs에서 0.1840 GFLOPs로 감소했습니다 (감소율: 91.44%).
2026-01-14 11:59:37,946 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-01-14 11:59:37,946 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 11:59:37,947 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-01-14 11:59:37,947 - INFO - ==================================================
2026-01-14 11:59:37,948 - INFO - train 모드를 시작합니다.
2026-01-14 11:59:37,948 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 11:59:37,948 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 11:59:37,948 - INFO - --------------------------------------------------
2026-01-14 11:59:37,954 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-01-14 11:59:45,696 - INFO - [Train] [11/90] | Loss: 0.5596 | Train Acc: 75.37%
2026-01-14 11:59:47,792 - INFO - [Valid] [11/90] | Loss: 0.5639 | Val Acc: 74.63%
2026-01-14 11:59:47,827 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.6242 | F1: 0.6950
2026-01-14 11:59:47,827 - INFO - [Metrics for 'normal'] | Precision: 0.7243 | Recall: 0.8516 | F1: 0.7828
2026-01-14 11:59:47,894 - INFO - [Best Model Saved] (val loss: 0.5639) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 11:59:47,894 - INFO - --------------------------------------------------
2026-01-14 11:59:47,896 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-01-14 11:59:55,993 - INFO - [Train] [12/90] | Loss: 0.5113 | Train Acc: 79.99%
2026-01-14 11:59:57,869 - INFO - [Valid] [12/90] | Loss: 0.5603 | Val Acc: 76.11%
2026-01-14 11:59:57,881 - INFO - [Metrics for 'abnormal'] | Precision: 0.7676 | Recall: 0.6943 | F1: 0.7291
2026-01-14 11:59:57,882 - INFO - [Metrics for 'normal'] | Precision: 0.7563 | Recall: 0.8187 | F1: 0.7863
2026-01-14 11:59:57,934 - INFO - [Best Model Saved] (val loss: 0.5603) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 11:59:57,935 - INFO - --------------------------------------------------
2026-01-14 11:59:57,940 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-01-14 12:00:06,317 - INFO - [Train] [13/90] | Loss: 0.5192 | Train Acc: 78.94%
2026-01-14 12:00:08,610 - INFO - [Valid] [13/90] | Loss: 0.5472 | Val Acc: 76.11%
2026-01-14 12:00:08,635 - INFO - [Metrics for 'abnormal'] | Precision: 0.7794 | Recall: 0.6752 | F1: 0.7235
2026-01-14 12:00:08,635 - INFO - [Metrics for 'normal'] | Precision: 0.7488 | Recall: 0.8352 | F1: 0.7896
2026-01-14 12:00:08,703 - INFO - [Best Model Saved] (val loss: 0.5472) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:00:08,703 - INFO - --------------------------------------------------
2026-01-14 12:00:08,705 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-01-14 12:00:16,268 - INFO - [Train] [14/90] | Loss: 0.4930 | Train Acc: 80.88%
2026-01-14 12:00:18,647 - INFO - [Valid] [14/90] | Loss: 0.5367 | Val Acc: 76.70%
2026-01-14 12:00:18,660 - INFO - [Metrics for 'abnormal'] | Precision: 0.7216 | Recall: 0.8089 | F1: 0.7628
2026-01-14 12:00:18,661 - INFO - [Metrics for 'normal'] | Precision: 0.8160 | Recall: 0.7308 | F1: 0.7710
2026-01-14 12:00:18,697 - INFO - [Best Model Saved] (val loss: 0.5367) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:00:18,698 - INFO - --------------------------------------------------
2026-01-14 12:00:18,701 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-01-14 12:00:26,540 - INFO - [Train] [15/90] | Loss: 0.4941 | Train Acc: 80.88%
2026-01-14 12:00:29,201 - INFO - [Valid] [15/90] | Loss: 0.5416 | Val Acc: 76.70%
2026-01-14 12:00:29,215 - INFO - [Metrics for 'abnormal'] | Precision: 0.7532 | Recall: 0.7389 | F1: 0.7460
2026-01-14 12:00:29,216 - INFO - [Metrics for 'normal'] | Precision: 0.7784 | Recall: 0.7912 | F1: 0.7847
2026-01-14 12:00:29,222 - INFO - --------------------------------------------------
2026-01-14 12:00:29,225 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-01-14 12:00:38,095 - INFO - [Train] [16/90] | Loss: 0.4850 | Train Acc: 81.32%
2026-01-14 12:00:41,479 - INFO - [Valid] [16/90] | Loss: 0.5233 | Val Acc: 77.29%
2026-01-14 12:00:41,491 - INFO - [Metrics for 'abnormal'] | Precision: 0.7247 | Recall: 0.8217 | F1: 0.7701
2026-01-14 12:00:41,492 - INFO - [Metrics for 'normal'] | Precision: 0.8261 | Recall: 0.7308 | F1: 0.7755
2026-01-14 12:00:41,526 - INFO - [Best Model Saved] (val loss: 0.5233) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:00:41,527 - INFO - --------------------------------------------------
2026-01-14 12:00:41,529 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-01-14 12:00:50,272 - INFO - [Train] [17/90] | Loss: 0.4779 | Train Acc: 82.22%
2026-01-14 12:00:53,562 - INFO - [Valid] [17/90] | Loss: 0.5526 | Val Acc: 76.11%
2026-01-14 12:00:53,574 - INFO - [Metrics for 'abnormal'] | Precision: 0.7065 | Recall: 0.8280 | F1: 0.7625
2026-01-14 12:00:53,575 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.7033 | F1: 0.7596
2026-01-14 12:00:53,579 - INFO - --------------------------------------------------
2026-01-14 12:00:53,581 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-01-14 12:01:01,965 - INFO - [Train] [18/90] | Loss: 0.4723 | Train Acc: 82.37%
2026-01-14 12:01:05,474 - INFO - [Valid] [18/90] | Loss: 0.5110 | Val Acc: 78.47%
2026-01-14 12:01:05,495 - INFO - [Metrics for 'abnormal'] | Precision: 0.7727 | Recall: 0.7580 | F1: 0.7653
2026-01-14 12:01:05,496 - INFO - [Metrics for 'normal'] | Precision: 0.7946 | Recall: 0.8077 | F1: 0.8011
2026-01-14 12:01:05,532 - INFO - [Best Model Saved] (val loss: 0.5110) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:01:05,533 - INFO - --------------------------------------------------
2026-01-14 12:01:05,535 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-01-14 12:01:15,116 - INFO - [Train] [19/90] | Loss: 0.4609 | Train Acc: 83.33%
2026-01-14 12:01:17,262 - INFO - [Valid] [19/90] | Loss: 0.5242 | Val Acc: 77.88%
2026-01-14 12:01:17,273 - INFO - [Metrics for 'abnormal'] | Precision: 0.7356 | Recall: 0.8153 | F1: 0.7734
2026-01-14 12:01:17,274 - INFO - [Metrics for 'normal'] | Precision: 0.8242 | Recall: 0.7473 | F1: 0.7839
2026-01-14 12:01:17,278 - INFO - --------------------------------------------------
2026-01-14 12:01:17,280 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-01-14 12:01:26,349 - INFO - [Train] [20/90] | Loss: 0.4703 | Train Acc: 82.66%
2026-01-14 12:01:29,566 - INFO - [Valid] [20/90] | Loss: 0.5120 | Val Acc: 76.99%
2026-01-14 12:01:29,580 - INFO - [Metrics for 'abnormal'] | Precision: 0.7453 | Recall: 0.7643 | F1: 0.7547
2026-01-14 12:01:29,580 - INFO - [Metrics for 'normal'] | Precision: 0.7921 | Recall: 0.7747 | F1: 0.7833
2026-01-14 12:01:29,584 - INFO - --------------------------------------------------
2026-01-14 12:01:29,587 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-01-14 12:01:37,577 - INFO - [Train] [21/90] | Loss: 0.4592 | Train Acc: 82.81%
2026-01-14 12:01:40,569 - INFO - [Valid] [21/90] | Loss: 0.5212 | Val Acc: 78.76%
2026-01-14 12:01:40,582 - INFO - [Metrics for 'abnormal'] | Precision: 0.7348 | Recall: 0.8471 | F1: 0.7870
2026-01-14 12:01:40,583 - INFO - [Metrics for 'normal'] | Precision: 0.8481 | Recall: 0.7363 | F1: 0.7882
2026-01-14 12:01:40,590 - INFO - --------------------------------------------------
2026-01-14 12:01:40,593 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-01-14 12:01:49,654 - INFO - [Train] [22/90] | Loss: 0.4547 | Train Acc: 82.81%
2026-01-14 12:01:53,003 - INFO - [Valid] [22/90] | Loss: 0.5010 | Val Acc: 79.35%
2026-01-14 12:01:53,016 - INFO - [Metrics for 'abnormal'] | Precision: 0.7881 | Recall: 0.7580 | F1: 0.7727
2026-01-14 12:01:53,016 - INFO - [Metrics for 'normal'] | Precision: 0.7979 | Recall: 0.8242 | F1: 0.8108
2026-01-14 12:01:53,057 - INFO - [Best Model Saved] (val loss: 0.5010) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:01:53,057 - INFO - --------------------------------------------------
2026-01-14 12:01:53,059 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-01-14 12:02:03,427 - INFO - [Train] [23/90] | Loss: 0.4614 | Train Acc: 82.96%
2026-01-14 12:02:05,965 - INFO - [Valid] [23/90] | Loss: 0.5168 | Val Acc: 77.29%
2026-01-14 12:02:05,978 - INFO - [Metrics for 'abnormal'] | Precision: 0.7817 | Recall: 0.7070 | F1: 0.7425
2026-01-14 12:02:05,978 - INFO - [Metrics for 'normal'] | Precision: 0.7665 | Recall: 0.8297 | F1: 0.7968
2026-01-14 12:02:05,985 - INFO - --------------------------------------------------
2026-01-14 12:02:05,988 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-01-14 12:02:14,907 - INFO - [Train] [24/90] | Loss: 0.4458 | Train Acc: 84.15%
2026-01-14 12:02:18,216 - INFO - [Valid] [24/90] | Loss: 0.5014 | Val Acc: 79.35%
2026-01-14 12:02:18,226 - INFO - [Metrics for 'abnormal'] | Precision: 0.8271 | Recall: 0.7006 | F1: 0.7586
2026-01-14 12:02:18,227 - INFO - [Metrics for 'normal'] | Precision: 0.7718 | Recall: 0.8736 | F1: 0.8196
2026-01-14 12:02:18,232 - INFO - --------------------------------------------------
2026-01-14 12:02:18,234 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-01-14 12:02:28,603 - INFO - [Train] [25/90] | Loss: 0.4447 | Train Acc: 84.00%
2026-01-14 12:02:30,986 - INFO - [Valid] [25/90] | Loss: 0.4869 | Val Acc: 79.65%
2026-01-14 12:02:30,995 - INFO - [Metrics for 'abnormal'] | Precision: 0.8143 | Recall: 0.7261 | F1: 0.7677
2026-01-14 12:02:30,996 - INFO - [Metrics for 'normal'] | Precision: 0.7839 | Recall: 0.8571 | F1: 0.8189
2026-01-14 12:02:31,022 - INFO - [Best Model Saved] (val loss: 0.4869) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:02:31,023 - INFO - --------------------------------------------------
2026-01-14 12:02:31,024 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-01-14 12:02:41,585 - INFO - [Train] [26/90] | Loss: 0.4481 | Train Acc: 83.93%
2026-01-14 12:02:43,340 - INFO - [Valid] [26/90] | Loss: 0.5247 | Val Acc: 78.17%
2026-01-14 12:02:43,354 - INFO - [Metrics for 'abnormal'] | Precision: 0.7150 | Recall: 0.8790 | F1: 0.7886
2026-01-14 12:02:43,354 - INFO - [Metrics for 'normal'] | Precision: 0.8699 | Recall: 0.6978 | F1: 0.7744
2026-01-14 12:02:43,358 - INFO - --------------------------------------------------
2026-01-14 12:02:43,360 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-01-14 12:02:53,110 - INFO - [Train] [27/90] | Loss: 0.4369 | Train Acc: 84.45%
2026-01-14 12:02:55,896 - INFO - [Valid] [27/90] | Loss: 0.5023 | Val Acc: 78.47%
2026-01-14 12:02:55,906 - INFO - [Metrics for 'abnormal'] | Precision: 0.7283 | Recall: 0.8535 | F1: 0.7859
2026-01-14 12:02:55,907 - INFO - [Metrics for 'normal'] | Precision: 0.8516 | Recall: 0.7253 | F1: 0.7834
2026-01-14 12:02:55,910 - INFO - --------------------------------------------------
2026-01-14 12:02:55,912 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-01-14 12:03:05,609 - INFO - [Train] [28/90] | Loss: 0.4404 | Train Acc: 84.23%
2026-01-14 12:03:07,941 - INFO - [Valid] [28/90] | Loss: 0.5236 | Val Acc: 78.76%
2026-01-14 12:03:07,953 - INFO - [Metrics for 'abnormal'] | Precision: 0.7322 | Recall: 0.8535 | F1: 0.7882
2026-01-14 12:03:07,954 - INFO - [Metrics for 'normal'] | Precision: 0.8526 | Recall: 0.7308 | F1: 0.7870
2026-01-14 12:03:07,959 - INFO - --------------------------------------------------
2026-01-14 12:03:07,962 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-01-14 12:03:17,607 - INFO - [Train] [29/90] | Loss: 0.4530 | Train Acc: 83.04%
2026-01-14 12:03:20,467 - INFO - [Valid] [29/90] | Loss: 0.5021 | Val Acc: 78.17%
2026-01-14 12:03:20,477 - INFO - [Metrics for 'abnormal'] | Precision: 0.8120 | Recall: 0.6879 | F1: 0.7448
2026-01-14 12:03:20,478 - INFO - [Metrics for 'normal'] | Precision: 0.7621 | Recall: 0.8626 | F1: 0.8093
2026-01-14 12:03:20,481 - INFO - --------------------------------------------------
2026-01-14 12:03:20,484 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-01-14 12:03:29,472 - INFO - [Train] [30/90] | Loss: 0.4353 | Train Acc: 85.42%
2026-01-14 12:03:32,146 - INFO - [Valid] [30/90] | Loss: 0.4877 | Val Acc: 79.35%
2026-01-14 12:03:32,158 - INFO - [Metrics for 'abnormal'] | Precision: 0.7574 | Recall: 0.8153 | F1: 0.7853
2026-01-14 12:03:32,158 - INFO - [Metrics for 'normal'] | Precision: 0.8294 | Recall: 0.7747 | F1: 0.8011
2026-01-14 12:03:32,162 - INFO - --------------------------------------------------
2026-01-14 12:03:32,164 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-01-14 12:03:42,063 - INFO - [Train] [31/90] | Loss: 0.4202 | Train Acc: 85.42%
2026-01-14 12:03:44,851 - INFO - [Valid] [31/90] | Loss: 0.4841 | Val Acc: 79.94%
2026-01-14 12:03:44,862 - INFO - [Metrics for 'abnormal'] | Precision: 0.7908 | Recall: 0.7707 | F1: 0.7806
2026-01-14 12:03:44,863 - INFO - [Metrics for 'normal'] | Precision: 0.8065 | Recall: 0.8242 | F1: 0.8152
2026-01-14 12:03:44,895 - INFO - [Best Model Saved] (val loss: 0.4841) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:03:44,895 - INFO - --------------------------------------------------
2026-01-14 12:03:44,898 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-01-14 12:03:54,834 - INFO - [Train] [32/90] | Loss: 0.4220 | Train Acc: 85.04%
2026-01-14 12:03:58,760 - INFO - [Valid] [32/90] | Loss: 0.5021 | Val Acc: 79.94%
2026-01-14 12:03:58,787 - INFO - [Metrics for 'abnormal'] | Precision: 0.7354 | Recall: 0.8854 | F1: 0.8035
2026-01-14 12:03:58,788 - INFO - [Metrics for 'normal'] | Precision: 0.8800 | Recall: 0.7253 | F1: 0.7952
2026-01-14 12:03:58,797 - INFO - --------------------------------------------------
2026-01-14 12:03:58,802 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-01-14 12:04:07,981 - INFO - [Train] [33/90] | Loss: 0.4198 | Train Acc: 85.12%
2026-01-14 12:04:11,576 - INFO - [Valid] [33/90] | Loss: 0.4840 | Val Acc: 79.06%
2026-01-14 12:04:11,600 - INFO - [Metrics for 'abnormal'] | Precision: 0.7722 | Recall: 0.7771 | F1: 0.7746
2026-01-14 12:04:11,601 - INFO - [Metrics for 'normal'] | Precision: 0.8066 | Recall: 0.8022 | F1: 0.8044
2026-01-14 12:04:11,673 - INFO - [Best Model Saved] (val loss: 0.4840) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:04:11,673 - INFO - --------------------------------------------------
2026-01-14 12:04:11,675 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-01-14 12:04:20,954 - INFO - [Train] [34/90] | Loss: 0.4211 | Train Acc: 85.42%
2026-01-14 12:04:24,299 - INFO - [Valid] [34/90] | Loss: 0.4988 | Val Acc: 79.65%
2026-01-14 12:04:24,326 - INFO - [Metrics for 'abnormal'] | Precision: 0.7750 | Recall: 0.7898 | F1: 0.7823
2026-01-14 12:04:24,329 - INFO - [Metrics for 'normal'] | Precision: 0.8156 | Recall: 0.8022 | F1: 0.8089
2026-01-14 12:04:24,336 - INFO - --------------------------------------------------
2026-01-14 12:04:24,341 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-01-14 12:04:33,949 - INFO - [Train] [35/90] | Loss: 0.4146 | Train Acc: 86.31%
2026-01-14 12:04:37,176 - INFO - [Valid] [35/90] | Loss: 0.4786 | Val Acc: 80.24%
2026-01-14 12:04:37,212 - INFO - [Metrics for 'abnormal'] | Precision: 0.7557 | Recall: 0.8471 | F1: 0.7988
2026-01-14 12:04:37,212 - INFO - [Metrics for 'normal'] | Precision: 0.8528 | Recall: 0.7637 | F1: 0.8058
2026-01-14 12:04:37,246 - INFO - [Best Model Saved] (val loss: 0.4786) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:04:37,246 - INFO - --------------------------------------------------
2026-01-14 12:04:37,248 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-01-14 12:04:46,840 - INFO - [Train] [36/90] | Loss: 0.4135 | Train Acc: 85.79%
2026-01-14 12:04:49,804 - INFO - [Valid] [36/90] | Loss: 0.4892 | Val Acc: 80.83%
2026-01-14 12:04:49,815 - INFO - [Metrics for 'abnormal'] | Precision: 0.7949 | Recall: 0.7898 | F1: 0.7923
2026-01-14 12:04:49,816 - INFO - [Metrics for 'normal'] | Precision: 0.8197 | Recall: 0.8242 | F1: 0.8219
2026-01-14 12:04:49,821 - INFO - --------------------------------------------------
2026-01-14 12:04:49,823 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-01-14 12:04:58,545 - INFO - [Train] [37/90] | Loss: 0.4097 | Train Acc: 86.61%
2026-01-14 12:05:01,034 - INFO - [Valid] [37/90] | Loss: 0.4673 | Val Acc: 81.42%
2026-01-14 12:05:01,044 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-01-14 12:05:01,045 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-01-14 12:05:01,076 - INFO - [Best Model Saved] (val loss: 0.4673) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:05:01,077 - INFO - --------------------------------------------------
2026-01-14 12:05:01,079 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-01-14 12:05:10,117 - INFO - [Train] [38/90] | Loss: 0.4002 | Train Acc: 86.68%
2026-01-14 12:05:13,026 - INFO - [Valid] [38/90] | Loss: 0.5004 | Val Acc: 81.12%
2026-01-14 12:05:13,051 - INFO - [Metrics for 'abnormal'] | Precision: 0.7598 | Recall: 0.8662 | F1: 0.8095
2026-01-14 12:05:13,052 - INFO - [Metrics for 'normal'] | Precision: 0.8688 | Recall: 0.7637 | F1: 0.8129
2026-01-14 12:05:13,056 - INFO - --------------------------------------------------
2026-01-14 12:05:13,066 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-01-14 12:05:22,043 - INFO - [Train] [39/90] | Loss: 0.3932 | Train Acc: 88.17%
2026-01-14 12:05:24,485 - INFO - [Valid] [39/90] | Loss: 0.4975 | Val Acc: 82.01%
2026-01-14 12:05:24,496 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-01-14 12:05:24,497 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-01-14 12:05:24,501 - INFO - --------------------------------------------------
2026-01-14 12:05:24,503 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-01-14 12:05:34,063 - INFO - [Train] [40/90] | Loss: 0.3910 | Train Acc: 87.43%
2026-01-14 12:05:36,463 - INFO - [Valid] [40/90] | Loss: 0.4886 | Val Acc: 79.06%
2026-01-14 12:05:36,473 - INFO - [Metrics for 'abnormal'] | Precision: 0.7654 | Recall: 0.7898 | F1: 0.7774
2026-01-14 12:05:36,473 - INFO - [Metrics for 'normal'] | Precision: 0.8136 | Recall: 0.7912 | F1: 0.8022
2026-01-14 12:05:36,477 - INFO - --------------------------------------------------
2026-01-14 12:05:36,479 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-01-14 12:05:44,613 - INFO - [Train] [41/90] | Loss: 0.4062 | Train Acc: 86.38%
2026-01-14 12:05:47,547 - INFO - [Valid] [41/90] | Loss: 0.4837 | Val Acc: 79.94%
2026-01-14 12:05:47,574 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8089 | F1: 0.7888
2026-01-14 12:05:47,578 - INFO - [Metrics for 'normal'] | Precision: 0.8276 | Recall: 0.7912 | F1: 0.8090
2026-01-14 12:05:47,586 - INFO - --------------------------------------------------
2026-01-14 12:05:47,592 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-01-14 12:05:56,787 - INFO - [Train] [42/90] | Loss: 0.3918 | Train Acc: 87.35%
2026-01-14 12:06:00,042 - INFO - [Valid] [42/90] | Loss: 0.4931 | Val Acc: 81.71%
2026-01-14 12:06:00,056 - INFO - [Metrics for 'abnormal'] | Precision: 0.8467 | Recall: 0.7389 | F1: 0.7891
2026-01-14 12:06:00,056 - INFO - [Metrics for 'normal'] | Precision: 0.7970 | Recall: 0.8846 | F1: 0.8385
2026-01-14 12:06:00,062 - INFO - --------------------------------------------------
2026-01-14 12:06:00,064 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-01-14 12:06:09,197 - INFO - [Train] [43/90] | Loss: 0.3905 | Train Acc: 88.10%
2026-01-14 12:06:12,249 - INFO - [Valid] [43/90] | Loss: 0.4901 | Val Acc: 79.94%
2026-01-14 12:06:12,274 - INFO - [Metrics for 'abnormal'] | Precision: 0.7834 | Recall: 0.7834 | F1: 0.7834
2026-01-14 12:06:12,275 - INFO - [Metrics for 'normal'] | Precision: 0.8132 | Recall: 0.8132 | F1: 0.8132
2026-01-14 12:06:12,286 - INFO - --------------------------------------------------
2026-01-14 12:06:12,288 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-01-14 12:06:22,773 - INFO - [Train] [44/90] | Loss: 0.3921 | Train Acc: 87.28%
2026-01-14 12:06:25,813 - INFO - [Valid] [44/90] | Loss: 0.5331 | Val Acc: 77.88%
2026-01-14 12:06:25,824 - INFO - [Metrics for 'abnormal'] | Precision: 0.7071 | Recall: 0.8917 | F1: 0.7887
2026-01-14 12:06:25,824 - INFO - [Metrics for 'normal'] | Precision: 0.8794 | Recall: 0.6813 | F1: 0.7678
2026-01-14 12:06:25,828 - INFO - --------------------------------------------------
2026-01-14 12:06:25,830 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-01-14 12:06:35,084 - INFO - [Train] [45/90] | Loss: 0.3825 | Train Acc: 87.80%
2026-01-14 12:06:37,773 - INFO - [Valid] [45/90] | Loss: 0.5081 | Val Acc: 79.06%
2026-01-14 12:06:37,785 - INFO - [Metrics for 'abnormal'] | Precision: 0.7363 | Recall: 0.8535 | F1: 0.7906
2026-01-14 12:06:37,786 - INFO - [Metrics for 'normal'] | Precision: 0.8535 | Recall: 0.7363 | F1: 0.7906
2026-01-14 12:06:37,790 - INFO - --------------------------------------------------
2026-01-14 12:06:37,792 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-01-14 12:06:47,751 - INFO - [Train] [46/90] | Loss: 0.3762 | Train Acc: 88.39%
2026-01-14 12:06:50,418 - INFO - [Valid] [46/90] | Loss: 0.4923 | Val Acc: 79.94%
2026-01-14 12:06:50,430 - INFO - [Metrics for 'abnormal'] | Precision: 0.7543 | Recall: 0.8408 | F1: 0.7952
2026-01-14 12:06:50,430 - INFO - [Metrics for 'normal'] | Precision: 0.8476 | Recall: 0.7637 | F1: 0.8035
2026-01-14 12:06:50,435 - INFO - --------------------------------------------------
2026-01-14 12:06:50,437 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-01-14 12:06:59,947 - INFO - [Train] [47/90] | Loss: 0.3675 | Train Acc: 89.14%
2026-01-14 12:07:02,567 - INFO - [Valid] [47/90] | Loss: 0.4962 | Val Acc: 80.24%
2026-01-14 12:07:02,599 - INFO - [Metrics for 'abnormal'] | Precision: 0.7711 | Recall: 0.8153 | F1: 0.7926
2026-01-14 12:07:02,600 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.7912 | F1: 0.8113
2026-01-14 12:07:02,604 - INFO - --------------------------------------------------
2026-01-14 12:07:02,606 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-01-14 12:07:11,205 - INFO - [Train] [48/90] | Loss: 0.3776 | Train Acc: 89.36%
2026-01-14 12:07:14,364 - INFO - [Valid] [48/90] | Loss: 0.5145 | Val Acc: 78.76%
2026-01-14 12:07:14,384 - INFO - [Metrics for 'abnormal'] | Precision: 0.7485 | Recall: 0.8153 | F1: 0.7805
2026-01-14 12:07:14,388 - INFO - [Metrics for 'normal'] | Precision: 0.8274 | Recall: 0.7637 | F1: 0.7943
2026-01-14 12:07:14,396 - INFO - --------------------------------------------------
2026-01-14 12:07:14,399 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-01-14 12:07:24,605 - INFO - [Train] [49/90] | Loss: 0.3889 | Train Acc: 87.95%
2026-01-14 12:07:26,923 - INFO - [Valid] [49/90] | Loss: 0.5259 | Val Acc: 80.53%
2026-01-14 12:07:26,934 - INFO - [Metrics for 'abnormal'] | Precision: 0.7661 | Recall: 0.8344 | F1: 0.7988
2026-01-14 12:07:26,934 - INFO - [Metrics for 'normal'] | Precision: 0.8452 | Recall: 0.7802 | F1: 0.8114
2026-01-14 12:07:26,939 - INFO - --------------------------------------------------
2026-01-14 12:07:26,942 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-01-14 12:07:36,056 - INFO - [Train] [50/90] | Loss: 0.3692 | Train Acc: 88.84%
2026-01-14 12:07:38,395 - INFO - [Valid] [50/90] | Loss: 0.5111 | Val Acc: 79.06%
2026-01-14 12:07:38,407 - INFO - [Metrics for 'abnormal'] | Precision: 0.7622 | Recall: 0.7962 | F1: 0.7788
2026-01-14 12:07:38,408 - INFO - [Metrics for 'normal'] | Precision: 0.8171 | Recall: 0.7857 | F1: 0.8011
2026-01-14 12:07:38,413 - INFO - --------------------------------------------------
2026-01-14 12:07:38,415 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-01-14 12:07:48,121 - INFO - [Train] [51/90] | Loss: 0.3746 | Train Acc: 88.99%
2026-01-14 12:07:50,216 - INFO - [Valid] [51/90] | Loss: 0.5002 | Val Acc: 80.83%
2026-01-14 12:07:50,228 - INFO - [Metrics for 'abnormal'] | Precision: 0.7738 | Recall: 0.8280 | F1: 0.8000
2026-01-14 12:07:50,231 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.7912 | F1: 0.8159
2026-01-14 12:07:50,236 - INFO - --------------------------------------------------
2026-01-14 12:07:50,239 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-01-14 12:08:00,670 - INFO - [Train] [52/90] | Loss: 0.3568 | Train Acc: 89.88%
2026-01-14 12:08:02,909 - INFO - [Valid] [52/90] | Loss: 0.4840 | Val Acc: 81.42%
2026-01-14 12:08:02,921 - INFO - [Metrics for 'abnormal'] | Precision: 0.7866 | Recall: 0.8217 | F1: 0.8037
2026-01-14 12:08:02,922 - INFO - [Metrics for 'normal'] | Precision: 0.8400 | Recall: 0.8077 | F1: 0.8235
2026-01-14 12:08:02,927 - INFO - --------------------------------------------------
2026-01-14 12:08:02,929 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-01-14 12:08:12,203 - INFO - [Train] [53/90] | Loss: 0.3661 | Train Acc: 88.91%
2026-01-14 12:08:15,049 - INFO - [Valid] [53/90] | Loss: 0.4757 | Val Acc: 81.71%
2026-01-14 12:08:15,060 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8025 | F1: 0.8025
2026-01-14 12:08:15,060 - INFO - [Metrics for 'normal'] | Precision: 0.8297 | Recall: 0.8297 | F1: 0.8297
2026-01-14 12:08:15,064 - INFO - --------------------------------------------------
2026-01-14 12:08:15,066 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-01-14 12:08:24,050 - INFO - [Train] [54/90] | Loss: 0.3500 | Train Acc: 90.40%
2026-01-14 12:08:26,184 - INFO - [Valid] [54/90] | Loss: 0.4999 | Val Acc: 79.35%
2026-01-14 12:08:26,246 - INFO - [Metrics for 'abnormal'] | Precision: 0.7605 | Recall: 0.8089 | F1: 0.7840
2026-01-14 12:08:26,246 - INFO - [Metrics for 'normal'] | Precision: 0.8256 | Recall: 0.7802 | F1: 0.8023
2026-01-14 12:08:26,251 - INFO - --------------------------------------------------
2026-01-14 12:08:26,255 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-01-14 12:08:35,532 - INFO - [Train] [55/90] | Loss: 0.3580 | Train Acc: 89.14%
2026-01-14 12:08:37,866 - INFO - [Valid] [55/90] | Loss: 0.4680 | Val Acc: 82.60%
2026-01-14 12:08:37,891 - INFO - [Metrics for 'abnormal'] | Precision: 0.8224 | Recall: 0.7962 | F1: 0.8091
2026-01-14 12:08:37,891 - INFO - [Metrics for 'normal'] | Precision: 0.8289 | Recall: 0.8516 | F1: 0.8401
2026-01-14 12:08:37,898 - INFO - --------------------------------------------------
2026-01-14 12:08:37,903 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-01-14 12:08:46,631 - INFO - [Train] [56/90] | Loss: 0.3594 | Train Acc: 88.99%
2026-01-14 12:08:49,116 - INFO - [Valid] [56/90] | Loss: 0.4849 | Val Acc: 80.24%
2026-01-14 12:08:49,129 - INFO - [Metrics for 'abnormal'] | Precision: 0.7679 | Recall: 0.8217 | F1: 0.7938
2026-01-14 12:08:49,130 - INFO - [Metrics for 'normal'] | Precision: 0.8363 | Recall: 0.7857 | F1: 0.8102
2026-01-14 12:08:49,135 - INFO - --------------------------------------------------
2026-01-14 12:08:49,137 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-01-14 12:09:00,058 - INFO - [Train] [57/90] | Loss: 0.3446 | Train Acc: 90.77%
2026-01-14 12:09:02,468 - INFO - [Valid] [57/90] | Loss: 0.5122 | Val Acc: 79.94%
2026-01-14 12:09:02,478 - INFO - [Metrics for 'abnormal'] | Precision: 0.7405 | Recall: 0.8726 | F1: 0.8012
2026-01-14 12:09:02,479 - INFO - [Metrics for 'normal'] | Precision: 0.8701 | Recall: 0.7363 | F1: 0.7976
2026-01-14 12:09:02,482 - INFO - --------------------------------------------------
2026-01-14 12:09:02,484 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-01-14 12:09:12,516 - INFO - [Train] [58/90] | Loss: 0.3475 | Train Acc: 90.25%
2026-01-14 12:09:15,738 - INFO - [Valid] [58/90] | Loss: 0.4859 | Val Acc: 80.53%
2026-01-14 12:09:15,752 - INFO - [Metrics for 'abnormal'] | Precision: 0.7862 | Recall: 0.7962 | F1: 0.7911
2026-01-14 12:09:15,752 - INFO - [Metrics for 'normal'] | Precision: 0.8222 | Recall: 0.8132 | F1: 0.8177
2026-01-14 12:09:15,757 - INFO - --------------------------------------------------
2026-01-14 12:09:15,760 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-01-14 12:09:25,828 - INFO - [Train] [59/90] | Loss: 0.3297 | Train Acc: 91.82%
2026-01-14 12:09:28,537 - INFO - [Valid] [59/90] | Loss: 0.5383 | Val Acc: 80.83%
2026-01-14 12:09:28,561 - INFO - [Metrics for 'abnormal'] | Precision: 0.7614 | Recall: 0.8535 | F1: 0.8048
2026-01-14 12:09:28,561 - INFO - [Metrics for 'normal'] | Precision: 0.8589 | Recall: 0.7692 | F1: 0.8116
2026-01-14 12:09:28,570 - INFO - --------------------------------------------------
2026-01-14 12:09:28,575 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-01-14 12:09:38,092 - INFO - [Train] [60/90] | Loss: 0.3407 | Train Acc: 91.37%
2026-01-14 12:09:40,587 - INFO - [Valid] [60/90] | Loss: 0.5462 | Val Acc: 77.88%
2026-01-14 12:09:40,598 - INFO - [Metrics for 'abnormal'] | Precision: 0.7181 | Recall: 0.8599 | F1: 0.7826
2026-01-14 12:09:40,599 - INFO - [Metrics for 'normal'] | Precision: 0.8543 | Recall: 0.7088 | F1: 0.7748
2026-01-14 12:09:40,603 - INFO - --------------------------------------------------
2026-01-14 12:09:40,605 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-01-14 12:09:50,299 - INFO - [Train] [61/90] | Loss: 0.3371 | Train Acc: 91.00%
2026-01-14 12:09:53,244 - INFO - [Valid] [61/90] | Loss: 0.5017 | Val Acc: 80.24%
2026-01-14 12:09:53,269 - INFO - [Metrics for 'abnormal'] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-01-14 12:09:53,269 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-01-14 12:09:53,277 - INFO - --------------------------------------------------
2026-01-14 12:09:53,283 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-01-14 12:10:03,578 - INFO - [Train] [62/90] | Loss: 0.3366 | Train Acc: 91.44%
2026-01-14 12:10:06,007 - INFO - [Valid] [62/90] | Loss: 0.5447 | Val Acc: 79.65%
2026-01-14 12:10:06,020 - INFO - [Metrics for 'abnormal'] | Precision: 0.7619 | Recall: 0.8153 | F1: 0.7877
2026-01-14 12:10:06,021 - INFO - [Metrics for 'normal'] | Precision: 0.8304 | Recall: 0.7802 | F1: 0.8045
2026-01-14 12:10:06,025 - INFO - --------------------------------------------------
2026-01-14 12:10:06,029 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-01-14 12:10:14,874 - INFO - [Train] [63/90] | Loss: 0.3189 | Train Acc: 92.49%
2026-01-14 12:10:18,106 - INFO - [Valid] [63/90] | Loss: 0.5152 | Val Acc: 79.94%
2026-01-14 12:10:18,119 - INFO - [Metrics for 'abnormal'] | Precision: 0.7602 | Recall: 0.8280 | F1: 0.7927
2026-01-14 12:10:18,119 - INFO - [Metrics for 'normal'] | Precision: 0.8393 | Recall: 0.7747 | F1: 0.8057
2026-01-14 12:10:18,127 - INFO - --------------------------------------------------
2026-01-14 12:10:18,131 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-01-14 12:10:28,372 - INFO - [Train] [64/90] | Loss: 0.3332 | Train Acc: 91.59%
2026-01-14 12:10:32,896 - INFO - [Valid] [64/90] | Loss: 0.5260 | Val Acc: 80.53%
2026-01-14 12:10:32,923 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-01-14 12:10:32,927 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-01-14 12:10:32,935 - INFO - --------------------------------------------------
2026-01-14 12:10:32,941 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-01-14 12:10:43,012 - INFO - [Train] [65/90] | Loss: 0.3264 | Train Acc: 91.82%
2026-01-14 12:10:46,495 - INFO - [Valid] [65/90] | Loss: 0.5269 | Val Acc: 79.94%
2026-01-14 12:10:46,519 - INFO - [Metrics for 'abnormal'] | Precision: 0.7633 | Recall: 0.8217 | F1: 0.7914
2026-01-14 12:10:46,524 - INFO - [Metrics for 'normal'] | Precision: 0.8353 | Recall: 0.7802 | F1: 0.8068
2026-01-14 12:10:46,528 - INFO - --------------------------------------------------
2026-01-14 12:10:46,535 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-01-14 12:10:55,885 - INFO - [Train] [66/90] | Loss: 0.3224 | Train Acc: 92.11%
2026-01-14 12:10:58,776 - INFO - [Valid] [66/90] | Loss: 0.5045 | Val Acc: 81.71%
2026-01-14 12:10:58,786 - INFO - [Metrics for 'abnormal'] | Precision: 0.8146 | Recall: 0.7834 | F1: 0.7987
2026-01-14 12:10:58,788 - INFO - [Metrics for 'normal'] | Precision: 0.8191 | Recall: 0.8462 | F1: 0.8324
2026-01-14 12:10:58,791 - INFO - --------------------------------------------------
2026-01-14 12:10:58,793 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-01-14 12:11:07,259 - INFO - [Train] [67/90] | Loss: 0.3203 | Train Acc: 92.49%
2026-01-14 12:11:10,194 - INFO - [Valid] [67/90] | Loss: 0.5106 | Val Acc: 82.01%
2026-01-14 12:11:10,204 - INFO - [Metrics for 'abnormal'] | Precision: 0.7963 | Recall: 0.8217 | F1: 0.8088
2026-01-14 12:11:10,205 - INFO - [Metrics for 'normal'] | Precision: 0.8418 | Recall: 0.8187 | F1: 0.8301
2026-01-14 12:11:10,209 - INFO - --------------------------------------------------
2026-01-14 12:11:10,211 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-01-14 12:11:18,722 - INFO - [Train] [68/90] | Loss: 0.3159 | Train Acc: 92.78%
2026-01-14 12:11:21,230 - INFO - [Valid] [68/90] | Loss: 0.5211 | Val Acc: 78.76%
2026-01-14 12:11:21,243 - INFO - [Metrics for 'abnormal'] | Precision: 0.7545 | Recall: 0.8025 | F1: 0.7778
2026-01-14 12:11:21,244 - INFO - [Metrics for 'normal'] | Precision: 0.8198 | Recall: 0.7747 | F1: 0.7966
2026-01-14 12:11:21,249 - INFO - --------------------------------------------------
2026-01-14 12:11:21,252 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-01-14 12:11:30,449 - INFO - [Train] [69/90] | Loss: 0.3138 | Train Acc: 93.01%
2026-01-14 12:11:33,488 - INFO - [Valid] [69/90] | Loss: 0.5055 | Val Acc: 80.83%
2026-01-14 12:11:33,510 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.7834 | F1: 0.7910
2026-01-14 12:11:33,510 - INFO - [Metrics for 'normal'] | Precision: 0.8162 | Recall: 0.8297 | F1: 0.8229
2026-01-14 12:11:33,521 - INFO - --------------------------------------------------
2026-01-14 12:11:33,523 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-01-14 12:11:42,247 - INFO - [Train] [70/90] | Loss: 0.3017 | Train Acc: 93.68%
2026-01-14 12:11:45,599 - INFO - [Valid] [70/90] | Loss: 0.5166 | Val Acc: 79.94%
2026-01-14 12:11:45,626 - INFO - [Metrics for 'abnormal'] | Precision: 0.7730 | Recall: 0.8025 | F1: 0.7875
2026-01-14 12:11:45,630 - INFO - [Metrics for 'normal'] | Precision: 0.8239 | Recall: 0.7967 | F1: 0.8101
2026-01-14 12:11:45,633 - INFO - --------------------------------------------------
2026-01-14 12:11:45,639 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-01-14 12:11:55,800 - INFO - [Train] [71/90] | Loss: 0.3077 | Train Acc: 93.68%
2026-01-14 12:11:58,083 - INFO - [Valid] [71/90] | Loss: 0.5324 | Val Acc: 79.94%
2026-01-14 12:11:58,094 - INFO - [Metrics for 'abnormal'] | Precision: 0.7572 | Recall: 0.8344 | F1: 0.7939
2026-01-14 12:11:58,094 - INFO - [Metrics for 'normal'] | Precision: 0.8434 | Recall: 0.7692 | F1: 0.8046
2026-01-14 12:11:58,099 - INFO - --------------------------------------------------
2026-01-14 12:11:58,101 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-01-14 12:12:07,982 - INFO - [Train] [72/90] | Loss: 0.3034 | Train Acc: 94.05%
2026-01-14 12:12:10,589 - INFO - [Valid] [72/90] | Loss: 0.5611 | Val Acc: 78.17%
2026-01-14 12:12:10,602 - INFO - [Metrics for 'abnormal'] | Precision: 0.7318 | Recall: 0.8344 | F1: 0.7798
2026-01-14 12:12:10,602 - INFO - [Metrics for 'normal'] | Precision: 0.8375 | Recall: 0.7363 | F1: 0.7836
2026-01-14 12:12:10,606 - INFO - --------------------------------------------------
2026-01-14 12:12:10,608 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-01-14 12:12:19,700 - INFO - [Train] [73/90] | Loss: 0.2962 | Train Acc: 93.90%
2026-01-14 12:12:22,527 - INFO - [Valid] [73/90] | Loss: 0.5571 | Val Acc: 80.53%
2026-01-14 12:12:22,542 - INFO - [Metrics for 'abnormal'] | Precision: 0.7725 | Recall: 0.8217 | F1: 0.7963
2026-01-14 12:12:22,543 - INFO - [Metrics for 'normal'] | Precision: 0.8372 | Recall: 0.7912 | F1: 0.8136
2026-01-14 12:12:22,548 - INFO - --------------------------------------------------
2026-01-14 12:12:22,551 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-01-14 12:12:31,230 - INFO - [Train] [74/90] | Loss: 0.2962 | Train Acc: 94.20%
2026-01-14 12:12:34,392 - INFO - [Valid] [74/90] | Loss: 0.5529 | Val Acc: 79.06%
2026-01-14 12:12:34,405 - INFO - [Metrics for 'abnormal'] | Precision: 0.7500 | Recall: 0.8217 | F1: 0.7842
2026-01-14 12:12:34,406 - INFO - [Metrics for 'normal'] | Precision: 0.8323 | Recall: 0.7637 | F1: 0.7966
2026-01-14 12:12:34,410 - INFO - --------------------------------------------------
2026-01-14 12:12:34,413 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-01-14 12:12:43,667 - INFO - [Train] [75/90] | Loss: 0.2916 | Train Acc: 94.35%
2026-01-14 12:12:47,250 - INFO - [Valid] [75/90] | Loss: 0.5577 | Val Acc: 80.53%
2026-01-14 12:12:47,263 - INFO - [Metrics for 'abnormal'] | Precision: 0.7935 | Recall: 0.7834 | F1: 0.7885
2026-01-14 12:12:47,264 - INFO - [Metrics for 'normal'] | Precision: 0.8152 | Recall: 0.8242 | F1: 0.8197
2026-01-14 12:12:47,268 - INFO - --------------------------------------------------
2026-01-14 12:12:47,270 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-01-14 12:12:56,411 - INFO - [Train] [76/90] | Loss: 0.2901 | Train Acc: 94.49%
2026-01-14 12:12:58,628 - INFO - [Valid] [76/90] | Loss: 0.5742 | Val Acc: 77.58%
2026-01-14 12:12:58,639 - INFO - [Metrics for 'abnormal'] | Precision: 0.7341 | Recall: 0.8089 | F1: 0.7697
2026-01-14 12:12:58,639 - INFO - [Metrics for 'normal'] | Precision: 0.8193 | Recall: 0.7473 | F1: 0.7816
2026-01-14 12:12:58,642 - INFO - --------------------------------------------------
2026-01-14 12:12:58,644 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-01-14 12:13:07,196 - INFO - [Train] [77/90] | Loss: 0.3029 | Train Acc: 93.82%
2026-01-14 12:13:10,205 - INFO - [Valid] [77/90] | Loss: 0.5453 | Val Acc: 79.65%
2026-01-14 12:13:10,218 - INFO - [Metrics for 'abnormal'] | Precision: 0.7529 | Recall: 0.8344 | F1: 0.7915
2026-01-14 12:13:10,219 - INFO - [Metrics for 'normal'] | Precision: 0.8424 | Recall: 0.7637 | F1: 0.8012
2026-01-14 12:13:10,223 - INFO - --------------------------------------------------
2026-01-14 12:13:10,226 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-01-14 12:13:21,145 - INFO - [Train] [78/90] | Loss: 0.2867 | Train Acc: 94.94%
2026-01-14 12:13:24,735 - INFO - [Valid] [78/90] | Loss: 0.5540 | Val Acc: 80.24%
2026-01-14 12:13:24,748 - INFO - [Metrics for 'abnormal'] | Precision: 0.7557 | Recall: 0.8471 | F1: 0.7988
2026-01-14 12:13:24,748 - INFO - [Metrics for 'normal'] | Precision: 0.8528 | Recall: 0.7637 | F1: 0.8058
2026-01-14 12:13:24,753 - INFO - --------------------------------------------------
2026-01-14 12:13:24,756 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-01-14 12:13:30,747 - INFO - [Train] [79/90] | Loss: 0.2789 | Train Acc: 95.39%
2026-01-14 12:13:32,717 - INFO - [Valid] [79/90] | Loss: 0.5541 | Val Acc: 79.65%
2026-01-14 12:13:32,730 - INFO - [Metrics for 'abnormal'] | Precision: 0.7558 | Recall: 0.8280 | F1: 0.7903
2026-01-14 12:13:32,730 - INFO - [Metrics for 'normal'] | Precision: 0.8383 | Recall: 0.7692 | F1: 0.8023
2026-01-14 12:13:32,734 - INFO - --------------------------------------------------
2026-01-14 12:13:32,736 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-01-14 12:13:40,198 - INFO - [Train] [80/90] | Loss: 0.2808 | Train Acc: 95.91%
2026-01-14 12:13:42,234 - INFO - [Valid] [80/90] | Loss: 0.5818 | Val Acc: 79.94%
2026-01-14 12:13:42,245 - INFO - [Metrics for 'abnormal'] | Precision: 0.7633 | Recall: 0.8217 | F1: 0.7914
2026-01-14 12:13:42,246 - INFO - [Metrics for 'normal'] | Precision: 0.8353 | Recall: 0.7802 | F1: 0.8068
2026-01-14 12:13:42,250 - INFO - --------------------------------------------------
2026-01-14 12:13:42,252 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-01-14 12:13:51,801 - INFO - [Train] [81/90] | Loss: 0.2873 | Train Acc: 94.79%
2026-01-14 12:13:55,177 - INFO - [Valid] [81/90] | Loss: 0.5551 | Val Acc: 79.94%
2026-01-14 12:13:55,188 - INFO - [Metrics for 'abnormal'] | Precision: 0.7764 | Recall: 0.7962 | F1: 0.7862
2026-01-14 12:13:55,189 - INFO - [Metrics for 'normal'] | Precision: 0.8202 | Recall: 0.8022 | F1: 0.8111
2026-01-14 12:13:55,192 - INFO - --------------------------------------------------
2026-01-14 12:13:55,195 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-01-14 12:14:02,163 - INFO - [Train] [82/90] | Loss: 0.2688 | Train Acc: 95.91%
2026-01-14 12:14:03,971 - INFO - [Valid] [82/90] | Loss: 0.5703 | Val Acc: 80.83%
2026-01-14 12:14:03,983 - INFO - [Metrics for 'abnormal'] | Precision: 0.7706 | Recall: 0.8344 | F1: 0.8012
2026-01-14 12:14:03,984 - INFO - [Metrics for 'normal'] | Precision: 0.8462 | Recall: 0.7857 | F1: 0.8148
2026-01-14 12:14:03,989 - INFO - --------------------------------------------------
2026-01-14 12:14:03,992 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-01-14 12:14:09,885 - INFO - [Train] [83/90] | Loss: 0.2675 | Train Acc: 95.83%
2026-01-14 12:14:11,276 - INFO - [Valid] [83/90] | Loss: 0.5866 | Val Acc: 80.24%
2026-01-14 12:14:11,284 - INFO - [Metrics for 'abnormal'] | Precision: 0.7616 | Recall: 0.8344 | F1: 0.7964
2026-01-14 12:14:11,284 - INFO - [Metrics for 'normal'] | Precision: 0.8443 | Recall: 0.7747 | F1: 0.8080
2026-01-14 12:14:11,286 - INFO - --------------------------------------------------
2026-01-14 12:14:11,288 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-01-14 12:14:16,115 - INFO - [Train] [84/90] | Loss: 0.2741 | Train Acc: 95.46%
2026-01-14 12:14:17,859 - INFO - [Valid] [84/90] | Loss: 0.5849 | Val Acc: 80.24%
2026-01-14 12:14:17,869 - INFO - [Metrics for 'abnormal'] | Precision: 0.7616 | Recall: 0.8344 | F1: 0.7964
2026-01-14 12:14:17,869 - INFO - [Metrics for 'normal'] | Precision: 0.8443 | Recall: 0.7747 | F1: 0.8080
2026-01-14 12:14:17,872 - INFO - --------------------------------------------------
2026-01-14 12:14:17,874 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-01-14 12:14:23,323 - INFO - [Train] [85/90] | Loss: 0.2670 | Train Acc: 95.83%
2026-01-14 12:14:24,886 - INFO - [Valid] [85/90] | Loss: 0.5849 | Val Acc: 80.24%
2026-01-14 12:14:24,896 - INFO - [Metrics for 'abnormal'] | Precision: 0.8082 | Recall: 0.7516 | F1: 0.7789
2026-01-14 12:14:24,896 - INFO - [Metrics for 'normal'] | Precision: 0.7979 | Recall: 0.8462 | F1: 0.8213
2026-01-14 12:14:24,899 - INFO - --------------------------------------------------
2026-01-14 12:14:24,900 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-01-14 12:14:30,044 - INFO - [Train] [86/90] | Loss: 0.2757 | Train Acc: 95.68%
2026-01-14 12:14:31,517 - INFO - [Valid] [86/90] | Loss: 0.5782 | Val Acc: 80.53%
2026-01-14 12:14:31,529 - INFO - [Metrics for 'abnormal'] | Precision: 0.7661 | Recall: 0.8344 | F1: 0.7988
2026-01-14 12:14:31,530 - INFO - [Metrics for 'normal'] | Precision: 0.8452 | Recall: 0.7802 | F1: 0.8114
2026-01-14 12:14:31,534 - INFO - --------------------------------------------------
2026-01-14 12:14:31,536 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-01-14 12:14:36,503 - INFO - [Train] [87/90] | Loss: 0.2644 | Train Acc: 96.80%
2026-01-14 12:14:38,084 - INFO - [Valid] [87/90] | Loss: 0.5856 | Val Acc: 78.76%
2026-01-14 12:14:38,093 - INFO - [Metrics for 'abnormal'] | Precision: 0.7515 | Recall: 0.8089 | F1: 0.7791
2026-01-14 12:14:38,094 - INFO - [Metrics for 'normal'] | Precision: 0.8235 | Recall: 0.7692 | F1: 0.7955
2026-01-14 12:14:38,097 - INFO - --------------------------------------------------
2026-01-14 12:14:38,098 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-01-14 12:14:43,291 - INFO - [Train] [88/90] | Loss: 0.2643 | Train Acc: 96.35%
2026-01-14 12:14:44,746 - INFO - [Valid] [88/90] | Loss: 0.5728 | Val Acc: 79.35%
2026-01-14 12:14:44,759 - INFO - [Metrics for 'abnormal'] | Precision: 0.7702 | Recall: 0.7898 | F1: 0.7799
2026-01-14 12:14:44,760 - INFO - [Metrics for 'normal'] | Precision: 0.8146 | Recall: 0.7967 | F1: 0.8056
2026-01-14 12:14:44,765 - INFO - --------------------------------------------------
2026-01-14 12:14:44,767 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-01-14 12:14:49,784 - INFO - [Train] [89/90] | Loss: 0.2658 | Train Acc: 95.54%
2026-01-14 12:14:51,195 - INFO - [Valid] [89/90] | Loss: 0.6010 | Val Acc: 81.42%
2026-01-14 12:14:51,207 - INFO - [Metrics for 'abnormal'] | Precision: 0.7765 | Recall: 0.8408 | F1: 0.8073
2026-01-14 12:14:51,207 - INFO - [Metrics for 'normal'] | Precision: 0.8521 | Recall: 0.7912 | F1: 0.8205
2026-01-14 12:14:51,210 - INFO - --------------------------------------------------
2026-01-14 12:14:51,212 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-01-14 12:14:56,181 - INFO - [Train] [90/90] | Loss: 0.2691 | Train Acc: 95.98%
2026-01-14 12:14:57,512 - INFO - [Valid] [90/90] | Loss: 0.5849 | Val Acc: 81.12%
2026-01-14 12:14:57,524 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-01-14 12:14:57,525 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-01-14 12:14:57,530 - INFO - ==================================================
2026-01-14 12:14:57,531 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-01-14 12:14:57,532 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-01-14 12:14:57,532 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:14:59,866 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-01-14 12:14:59,868 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-01-14 12:14:59,869 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:14:59,870 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:14:59,870 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:15:00,316 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-01-14 12:15:00,317 - INFO - ==================================================
2026-01-14 12:15:00,363 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/best_model.pth'
2026-01-14 12:15:00,364 - INFO - ==================================================
2026-01-14 12:15:00,364 - INFO - Test 모드를 시작합니다.
2026-01-14 12:15:00,481 - INFO - 연산량 (MACs): 0.0920 GMACs per sample
2026-01-14 12:15:00,482 - INFO - 연산량 (FLOPs): 0.1840 GFLOPs per sample
2026-01-14 12:15:00,482 - INFO - ==================================================
2026-01-14 12:15:00,482 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-01-14 12:15:01,527 - INFO - 샘플 당 평균 Forward Pass 시간: 5.28ms (std: 0.55ms), FPS: 191.72 (std: 21.35) (1개 샘플 x 100회 반복)
2026-01-14 12:15:01,527 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 112.93 MB
2026-01-14 12:15:01,527 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-01-14 12:15:03,866 - INFO - [Test] Loss: 0.4048 | Test Acc: 81.42%
2026-01-14 12:15:03,878 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-01-14 12:15:03,879 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-01-14 12:15:04,370 - INFO - ==================================================
2026-01-14 12:15:04,370 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/confusion_matrix_20260114_115646.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/confusion_matrix_20260114_115646.pdf'
2026-01-14 12:15:04,371 - INFO - ==================================================
2026-01-14 12:15:04,371 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-01-14 12:15:06,974 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/model_fp32_20260114_115646.onnx'에 저장되었습니다. (크기: 1.99 MB)
2026-01-14 12:15:07,355 - INFO - [Model Load] ONNX 모델(FP32) 로드 메모리: 2293.43 MB (증가량: 1.64 MB)
2026-01-14 12:15:07,355 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-01-14 12:15:09,297 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 16.25ms (std: 25.13ms)
2026-01-14 12:15:09,298 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 85.43 FPS (std: 28.60) (1개 샘플 x 100회 반복)
2026-01-14 12:15:09,298 - INFO - [Inference Only] ONNX 런타임 추론 중 최대 CPU 메모리: 2301.87 MB (순수 증가량: 3.05 MB)
2026-01-14 12:15:09,299 - INFO - [Total Process] ONNX 모델(FP32) 전체 메모리 사용량: 2301.87 MB (전체 증가량: 10.07 MB)
2026-01-14 12:15:12,412 - INFO - [Test (ONNX)] | Test Acc (ONNX): 81.42%
2026-01-14 12:15:12,424 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-01-14 12:15:12,424 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-01-14 12:15:12,951 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/val_acc.pdf'
2026-01-14 12:15:13,393 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/train_val_acc.pdf'
2026-01-14 12:15:13,757 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/F1_normal.pdf'
2026-01-14 12:15:14,093 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/val_loss.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/val_loss.pdf'
2026-01-14 12:15:14,458 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/learning_rate.pdf'
2026-01-14 12:15:18,939 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/compile.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260114_115646/graph_20260114_115646/compile.pdf'
