2026-01-14 11:55:24,866 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/log_20260114_115524.log'에 저장됩니다.
2026-01-14 11:55:24,872 - INFO - ==================================================
2026-01-14 11:55:24,872 - INFO - config.yaml:
2026-01-14 11:55:24,872 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_inference_dir: ./pretrained
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      train_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/cau/workspace/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  patch_size: 56
  stride: 56
  cnn_feature_extractor:
    name: efficientnet_b0_feat2
  featured_patch_dim: 24
  emb_dim: 24
  num_heads: 2
  num_decoder_layers: 2
  num_decoder_patches: 1
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: true
  save_attention: true
  num_plot_attention: 5
baseline:
  model_name: efficientnet_b0
  use_l1_pruning: true
  pruning_flops_target: 0.1829

2026-01-14 11:55:24,873 - INFO - ==================================================
2026-01-14 11:55:25,068 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA RTX PRO 6000 Blackwell Server Edition)
2026-01-14 11:55:25,069 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-01-14 11:55:25,069 - INFO - '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-01-14 11:55:25,077 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-01-14 11:55:25,077 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-01-14 11:55:25,077 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-01-14 11:55:25,077 - INFO - Baseline 모델 'efficientnet_b0'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 11:55:25,359 - INFO - ==================================================
2026-01-14 11:55:25,359 - INFO - 모델 파라미터 수:
2026-01-14 11:55:25,359 - INFO -   - 총 파라미터: 4,010,110 개
2026-01-14 11:55:25,360 - INFO -   - 학습 가능한 파라미터: 4,010,110 개
2026-01-14 11:55:25,360 - INFO - ================================================================================
2026-01-14 11:55:25,360 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-01-14 11:55:25,360 - INFO - ================================================================================
2026-01-14 11:55:25,360 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 11:55:25,362 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-01-14 11:55:25,362 - INFO - ==================================================
2026-01-14 11:55:25,362 - INFO - train 모드를 시작합니다.
2026-01-14 11:55:25,362 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 11:55:25,362 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 11:55:25,362 - INFO - --------------------------------------------------
2026-01-14 11:55:25,364 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-01-14 11:55:31,346 - INFO - [Train] [1/10] | Loss: 0.5876 | Train Acc: 73.14%
2026-01-14 11:55:33,667 - INFO - [Valid] [1/10] | Loss: 0.6117 | Val Acc: 68.44%
2026-01-14 11:55:33,680 - INFO - [Metrics for 'abnormal'] | Precision: 0.9310 | Recall: 0.3439 | F1: 0.5023
2026-01-14 11:55:33,680 - INFO - [Metrics for 'normal'] | Precision: 0.6335 | Recall: 0.9780 | F1: 0.7689
2026-01-14 11:55:33,731 - INFO - [Best Model Saved] (val loss: 0.6117) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'
2026-01-14 11:55:33,732 - INFO - --------------------------------------------------
2026-01-14 11:55:33,733 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-01-14 11:55:38,386 - INFO - [Train] [2/10] | Loss: 0.5461 | Train Acc: 77.75%
2026-01-14 11:55:39,691 - INFO - [Valid] [2/10] | Loss: 0.6134 | Val Acc: 71.39%
2026-01-14 11:55:39,700 - INFO - [Metrics for 'abnormal'] | Precision: 0.8947 | Recall: 0.4331 | F1: 0.5837
2026-01-14 11:55:39,700 - INFO - [Metrics for 'normal'] | Precision: 0.6616 | Recall: 0.9560 | F1: 0.7820
2026-01-14 11:55:39,703 - INFO - --------------------------------------------------
2026-01-14 11:55:39,705 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-01-14 11:55:44,451 - INFO - [Train] [3/10] | Loss: 0.5132 | Train Acc: 81.10%
2026-01-14 11:55:45,809 - INFO - [Valid] [3/10] | Loss: 0.4806 | Val Acc: 82.60%
2026-01-14 11:55:45,817 - INFO - [Metrics for 'abnormal'] | Precision: 0.8500 | Recall: 0.7580 | F1: 0.8013
2026-01-14 11:55:45,817 - INFO - [Metrics for 'normal'] | Precision: 0.8090 | Recall: 0.8846 | F1: 0.8451
2026-01-14 11:55:45,866 - INFO - [Best Model Saved] (val loss: 0.4806) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'
2026-01-14 11:55:45,867 - INFO - --------------------------------------------------
2026-01-14 11:55:45,868 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-01-14 11:55:51,104 - INFO - [Train] [4/10] | Loss: 0.4694 | Train Acc: 84.00%
2026-01-14 11:55:52,602 - INFO - [Valid] [4/10] | Loss: 0.5283 | Val Acc: 75.52%
2026-01-14 11:55:52,610 - INFO - [Metrics for 'abnormal'] | Precision: 0.6968 | Recall: 0.8344 | F1: 0.7594
2026-01-14 11:55:52,611 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.6868 | F1: 0.7508
2026-01-14 11:55:52,613 - INFO - --------------------------------------------------
2026-01-14 11:55:52,615 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-01-14 11:55:57,724 - INFO - [Train] [5/10] | Loss: 0.4661 | Train Acc: 82.74%
2026-01-14 11:55:59,271 - INFO - [Valid] [5/10] | Loss: 0.5244 | Val Acc: 82.60%
2026-01-14 11:55:59,280 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-01-14 11:55:59,280 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-01-14 11:55:59,283 - INFO - --------------------------------------------------
2026-01-14 11:55:59,285 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-01-14 11:56:04,382 - INFO - [Train] [6/10] | Loss: 0.4403 | Train Acc: 85.04%
2026-01-14 11:56:05,925 - INFO - [Valid] [6/10] | Loss: 0.5151 | Val Acc: 82.89%
2026-01-14 11:56:05,937 - INFO - [Metrics for 'abnormal'] | Precision: 0.8194 | Recall: 0.8089 | F1: 0.8141
2026-01-14 11:56:05,938 - INFO - [Metrics for 'normal'] | Precision: 0.8370 | Recall: 0.8462 | F1: 0.8415
2026-01-14 11:56:05,941 - INFO - --------------------------------------------------
2026-01-14 11:56:05,943 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-01-14 11:56:11,133 - INFO - [Train] [7/10] | Loss: 0.4037 | Train Acc: 88.32%
2026-01-14 11:56:12,652 - INFO - [Valid] [7/10] | Loss: 0.4901 | Val Acc: 83.78%
2026-01-14 11:56:12,664 - INFO - [Metrics for 'abnormal'] | Precision: 0.8312 | Recall: 0.8153 | F1: 0.8232
2026-01-14 11:56:12,664 - INFO - [Metrics for 'normal'] | Precision: 0.8432 | Recall: 0.8571 | F1: 0.8501
2026-01-14 11:56:12,671 - INFO - --------------------------------------------------
2026-01-14 11:56:12,674 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-01-14 11:56:18,382 - INFO - [Train] [8/10] | Loss: 0.3841 | Train Acc: 88.84%
2026-01-14 11:56:20,787 - INFO - [Valid] [8/10] | Loss: 0.4742 | Val Acc: 84.37%
2026-01-14 11:56:20,800 - INFO - [Metrics for 'abnormal'] | Precision: 0.8210 | Recall: 0.8471 | F1: 0.8339
2026-01-14 11:56:20,800 - INFO - [Metrics for 'normal'] | Precision: 0.8644 | Recall: 0.8407 | F1: 0.8524
2026-01-14 11:56:20,873 - INFO - [Best Model Saved] (val loss: 0.4742) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'
2026-01-14 11:56:20,873 - INFO - --------------------------------------------------
2026-01-14 11:56:20,876 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-01-14 11:56:26,551 - INFO - [Train] [9/10] | Loss: 0.3485 | Train Acc: 91.15%
2026-01-14 11:56:28,634 - INFO - [Valid] [9/10] | Loss: 0.4911 | Val Acc: 84.07%
2026-01-14 11:56:28,645 - INFO - [Metrics for 'abnormal'] | Precision: 0.8199 | Recall: 0.8408 | F1: 0.8302
2026-01-14 11:56:28,645 - INFO - [Metrics for 'normal'] | Precision: 0.8596 | Recall: 0.8407 | F1: 0.8500
2026-01-14 11:56:28,648 - INFO - --------------------------------------------------
2026-01-14 11:56:28,651 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-01-14 11:56:35,096 - INFO - [Train] [10/10] | Loss: 0.3259 | Train Acc: 93.15%
2026-01-14 11:56:36,782 - INFO - [Valid] [10/10] | Loss: 0.4835 | Val Acc: 84.07%
2026-01-14 11:56:36,794 - INFO - [Metrics for 'abnormal'] | Precision: 0.8601 | Recall: 0.7834 | F1: 0.8200
2026-01-14 11:56:36,795 - INFO - [Metrics for 'normal'] | Precision: 0.8265 | Recall: 0.8901 | F1: 0.8571
2026-01-14 11:56:36,800 - INFO - ================================================================================
2026-01-14 11:56:36,801 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-01-14 11:56:36,801 - INFO - ================================================================================
2026-01-14 11:56:36,964 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'을(를) 불러왔습니다.
2026-01-14 11:56:36,964 - INFO - ================================================================================
2026-01-14 11:56:36,964 - INFO - 목표 FLOPs (0.1829 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-01-14 11:56:37,096 - INFO - 원본 모델 FLOPs: 0.8277 GFLOPs
2026-01-14 11:56:37,217 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:37,218 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:37,642 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-01-14 11:56:37,642 - INFO - ==================================================
2026-01-14 11:56:37,726 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.2459 GFLOPs (감소율: 70.30%)
2026-01-14 11:56:37,784 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:37,785 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:38,091 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-01-14 11:56:38,094 - INFO - ==================================================
2026-01-14 11:56:38,210 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.0817 GFLOPs (감소율: 90.13%)
2026-01-14 11:56:38,276 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:38,276 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:38,571 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.6187499999999999)에 맞춰 변경되었습니다.
2026-01-14 11:56:38,571 - INFO - ==================================================
2026-01-14 11:56:38,673 - INFO -   [탐색  3] 희소도: 0.6187 -> FLOPs: 0.1535 GFLOPs (감소율: 81.46%)
2026-01-14 11:56:38,746 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:38,746 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:39,018 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.556875)에 맞춰 변경되었습니다.
2026-01-14 11:56:39,018 - INFO - ==================================================
2026-01-14 11:56:39,105 - INFO -   [탐색  4] 희소도: 0.5569 -> FLOPs: 0.1960 GFLOPs (감소율: 76.32%)
2026-01-14 11:56:39,559 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:39,559 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:40,015 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5878125)에 맞춰 변경되었습니다.
2026-01-14 11:56:40,016 - INFO - ==================================================
2026-01-14 11:56:40,129 - INFO -   [탐색  5] 희소도: 0.5878 -> FLOPs: 0.1727 GFLOPs (감소율: 79.13%)
2026-01-14 11:56:40,212 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:40,213 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:40,601 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5723437499999999)에 맞춰 변경되었습니다.
2026-01-14 11:56:40,601 - INFO - ==================================================
2026-01-14 11:56:40,695 - INFO -   [탐색  6] 희소도: 0.5723 -> FLOPs: 0.1841 GFLOPs (감소율: 77.76%)
2026-01-14 11:56:40,750 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:40,750 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:41,117 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.580078125)에 맞춰 변경되었습니다.
2026-01-14 11:56:41,118 - INFO - ==================================================
2026-01-14 11:56:41,221 - INFO -   [탐색  7] 희소도: 0.5801 -> FLOPs: 0.1790 GFLOPs (감소율: 78.37%)
2026-01-14 11:56:41,296 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:41,297 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:41,669 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5762109375)에 맞춰 변경되었습니다.
2026-01-14 11:56:41,669 - INFO - ==================================================
2026-01-14 11:56:41,800 - INFO -   [탐색  8] 희소도: 0.5762 -> FLOPs: 0.1810 GFLOPs (감소율: 78.13%)
2026-01-14 11:56:41,873 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:41,874 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:42,293 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.57427734375)에 맞춰 변경되었습니다.
2026-01-14 11:56:42,294 - INFO - ==================================================
2026-01-14 11:56:42,379 - INFO -   [탐색  9] 희소도: 0.5743 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:42,445 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:42,445 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:42,711 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5733105468749999)에 맞춰 변경되었습니다.
2026-01-14 11:56:42,711 - INFO - ==================================================
2026-01-14 11:56:42,778 - INFO -   [탐색 10] 희소도: 0.5733 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:42,848 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:42,848 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:43,295 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737939453124999)에 맞춰 변경되었습니다.
2026-01-14 11:56:43,295 - INFO - ==================================================
2026-01-14 11:56:43,375 - INFO -   [탐색 11] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:44,018 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:44,024 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:44,320 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5735522460937499)에 맞춰 변경되었습니다.
2026-01-14 11:56:44,320 - INFO - ==================================================
2026-01-14 11:56:44,375 - INFO -   [탐색 12] 희소도: 0.5736 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:44,433 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:44,433 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:44,853 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5736730957031249)에 맞춰 변경되었습니다.
2026-01-14 11:56:44,854 - INFO - ==================================================
2026-01-14 11:56:44,911 - INFO -   [탐색 13] 희소도: 0.5737 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:44,970 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:44,970 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:45,226 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737335205078125)에 맞춰 변경되었습니다.
2026-01-14 11:56:45,227 - INFO - ==================================================
2026-01-14 11:56:45,290 - INFO -   [탐색 14] 희소도: 0.5737 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:45,361 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:45,362 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:45,649 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737637329101561)에 맞춰 변경되었습니다.
2026-01-14 11:56:45,649 - INFO - ==================================================
2026-01-14 11:56:45,713 - INFO -   [탐색 15] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:45,776 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:45,776 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:46,037 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737788391113281)에 맞춰 변경되었습니다.
2026-01-14 11:56:46,037 - INFO - ==================================================
2026-01-14 11:56:46,105 - INFO -   [탐색 16] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:46,164 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:46,165 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:46,432 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.573786392211914)에 맞춰 변경되었습니다.
2026-01-14 11:56:46,433 - INFO - ==================================================
2026-01-14 11:56:46,517 - INFO -   [탐색 17] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:46,599 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:46,599 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:46,887 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.573782615661621)에 맞춰 변경되었습니다.
2026-01-14 11:56:46,887 - INFO - ==================================================
2026-01-14 11:56:46,948 - INFO -   [탐색 18] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:47,571 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:47,572 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:48,226 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737845039367675)에 맞춰 변경되었습니다.
2026-01-14 11:56:48,227 - INFO - ==================================================
2026-01-14 11:56:48,339 - INFO -   [탐색 19] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:48,507 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:48,508 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:49,017 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737854480743407)에 맞춰 변경되었습니다.
2026-01-14 11:56:49,017 - INFO - ==================================================
2026-01-14 11:56:49,089 - INFO -   [탐색 20] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:49,166 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:49,166 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:49,633 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737849760055541)에 맞춰 변경되었습니다.
2026-01-14 11:56:49,633 - INFO - ==================================================
2026-01-14 11:56:49,717 - INFO -   [탐색 21] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:49,791 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:49,791 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:51,128 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847399711609)에 맞춰 변경되었습니다.
2026-01-14 11:56:51,128 - INFO - ==================================================
2026-01-14 11:56:51,190 - INFO -   [탐색 22] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:51,258 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:51,258 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:51,535 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737846219539642)에 맞춰 변경되었습니다.
2026-01-14 11:56:51,535 - INFO - ==================================================
2026-01-14 11:56:51,600 - INFO -   [탐색 23] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:51,660 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:51,661 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:52,006 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737846809625625)에 맞춰 변경되었습니다.
2026-01-14 11:56:52,007 - INFO - ==================================================
2026-01-14 11:56:52,067 - INFO -   [탐색 24] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:52,264 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:52,265 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:52,722 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847104668616)에 맞춰 변경되었습니다.
2026-01-14 11:56:52,723 - INFO - ==================================================
2026-01-14 11:56:52,772 - INFO -   [탐색 25] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:52,867 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:52,868 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:53,842 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847252190112)에 맞춰 변경되었습니다.
2026-01-14 11:56:53,842 - INFO - ==================================================
2026-01-14 11:56:53,909 - INFO -   [탐색 26] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:53,974 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:53,974 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:54,306 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847178429365)에 맞춰 변경되었습니다.
2026-01-14 11:56:54,306 - INFO - ==================================================
2026-01-14 11:56:54,389 - INFO -   [탐색 27] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:54,459 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:54,459 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:54,810 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847215309739)에 맞춰 변경되었습니다.
2026-01-14 11:56:54,810 - INFO - ==================================================
2026-01-14 11:56:54,881 - INFO -   [탐색 28] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:54,933 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:54,933 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:55,265 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847233749926)에 맞춰 변경되었습니다.
2026-01-14 11:56:55,265 - INFO - ==================================================
2026-01-14 11:56:55,330 - INFO -   [탐색 29] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:55,454 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:55,455 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:55,830 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847224529833)에 맞춰 변경되었습니다.
2026-01-14 11:56:55,831 - INFO - ==================================================
2026-01-14 11:56:55,896 - INFO -   [탐색 30] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:55,958 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:55,959 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:56,404 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847219919786)에 맞춰 변경되었습니다.
2026-01-14 11:56:56,404 - INFO - ==================================================
2026-01-14 11:56:56,482 - INFO -   [탐색 31] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:56,568 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:56,568 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:57,274 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.573784722222481)에 맞춰 변경되었습니다.
2026-01-14 11:56:57,275 - INFO - ==================================================
2026-01-14 11:56:57,478 - INFO -   [탐색 32] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:56:57,757 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:57,758 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:59,067 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847221072299)에 맞춰 변경되었습니다.
2026-01-14 11:56:59,067 - INFO - ==================================================
2026-01-14 11:56:59,152 - INFO -   [탐색 33] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:59,238 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:59,239 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:56:59,644 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847221648554)에 맞춰 변경되었습니다.
2026-01-14 11:56:59,645 - INFO - ==================================================
2026-01-14 11:56:59,725 - INFO -   [탐색 34] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:56:59,797 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:56:59,797 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:00,235 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847221936683)에 맞춰 변경되었습니다.
2026-01-14 11:57:00,236 - INFO - ==================================================
2026-01-14 11:57:00,312 - INFO -   [탐색 35] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:00,387 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:00,387 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:00,805 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222080746)에 맞춰 변경되었습니다.
2026-01-14 11:57:00,806 - INFO - ==================================================
2026-01-14 11:57:00,925 - INFO -   [탐색 36] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:01,013 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:01,014 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:01,335 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222152779)에 맞춰 변경되었습니다.
2026-01-14 11:57:01,336 - INFO - ==================================================
2026-01-14 11:57:01,407 - INFO -   [탐색 37] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:01,484 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:01,485 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:01,938 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222188794)에 맞춰 변경되었습니다.
2026-01-14 11:57:01,938 - INFO - ==================================================
2026-01-14 11:57:02,038 - INFO -   [탐색 38] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:02,117 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:02,117 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:02,537 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222206802)에 맞춰 변경되었습니다.
2026-01-14 11:57:02,538 - INFO - ==================================================
2026-01-14 11:57:02,611 - INFO -   [탐색 39] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:03,309 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:03,309 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:03,762 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222215806)에 맞춰 변경되었습니다.
2026-01-14 11:57:03,762 - INFO - ==================================================
2026-01-14 11:57:03,838 - INFO -   [탐색 40] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:03,917 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:03,917 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:04,319 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222220308)에 맞춰 변경되었습니다.
2026-01-14 11:57:04,319 - INFO - ==================================================
2026-01-14 11:57:04,393 - INFO -   [탐색 41] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:04,475 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:04,475 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:04,901 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222558)에 맞춰 변경되었습니다.
2026-01-14 11:57:04,902 - INFO - ==================================================
2026-01-14 11:57:04,975 - INFO -   [탐색 42] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:05,056 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:05,057 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:05,680 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222221433)에 맞춰 변경되었습니다.
2026-01-14 11:57:05,680 - INFO - ==================================================
2026-01-14 11:57:05,771 - INFO -   [탐색 43] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:05,864 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:05,864 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:06,446 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222221996)에 맞춰 변경되었습니다.
2026-01-14 11:57:06,449 - INFO - ==================================================
2026-01-14 11:57:06,553 - INFO -   [탐색 44] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:06,619 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:06,620 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:07,075 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222276)에 맞춰 변경되었습니다.
2026-01-14 11:57:07,076 - INFO - ==================================================
2026-01-14 11:57:07,136 - INFO -   [탐색 45] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:07,195 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:07,195 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:07,534 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222137)에 맞춰 변경되었습니다.
2026-01-14 11:57:07,535 - INFO - ==================================================
2026-01-14 11:57:07,597 - INFO -   [탐색 46] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:08,044 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:08,044 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:08,443 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222207)에 맞춰 변경되었습니다.
2026-01-14 11:57:08,443 - INFO - ==================================================
2026-01-14 11:57:08,501 - INFO -   [탐색 47] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:08,563 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:08,564 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:08,903 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222241)에 맞춰 변경되었습니다.
2026-01-14 11:57:08,904 - INFO - ==================================================
2026-01-14 11:57:08,980 - INFO -   [탐색 48] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:09,058 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:09,058 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:09,431 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:09,431 - INFO - ==================================================
2026-01-14 11:57:09,508 - INFO -   [탐색 49] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:09,583 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:09,583 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:09,946 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222214)에 맞춰 변경되었습니다.
2026-01-14 11:57:09,947 - INFO - ==================================================
2026-01-14 11:57:10,020 - INFO -   [탐색 50] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:10,101 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:10,101 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:10,581 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222219)에 맞춰 변경되었습니다.
2026-01-14 11:57:10,582 - INFO - ==================================================
2026-01-14 11:57:10,657 - INFO -   [탐색 51] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:10,737 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:10,738 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:11,161 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222221)에 맞춰 변경되었습니다.
2026-01-14 11:57:11,162 - INFO - ==================================================
2026-01-14 11:57:11,237 - INFO -   [탐색 52] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:11,316 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:11,316 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:11,714 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222222)에 맞춰 변경되었습니다.
2026-01-14 11:57:11,715 - INFO - ==================================================
2026-01-14 11:57:11,788 - INFO -   [탐색 53] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 11:57:11,867 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:11,868 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:12,605 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:12,606 - INFO - ==================================================
2026-01-14 11:57:12,664 - INFO -   [탐색 54] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:12,725 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:12,725 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:13,232 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:13,232 - INFO - ==================================================
2026-01-14 11:57:13,354 - INFO -   [탐색 55] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:13,448 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:13,450 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:14,134 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:14,134 - INFO - ==================================================
2026-01-14 11:57:14,208 - INFO -   [탐색 56] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:14,279 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:14,279 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:14,867 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:14,871 - INFO - ==================================================
2026-01-14 11:57:14,932 - INFO -   [탐색 57] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:14,992 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:14,992 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:15,373 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:15,373 - INFO - ==================================================
2026-01-14 11:57:15,429 - INFO -   [탐색 58] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:15,488 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:15,489 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:15,875 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:15,875 - INFO - ==================================================
2026-01-14 11:57:15,955 - INFO -   [탐색 59] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:16,037 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:16,038 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:16,508 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:16,508 - INFO - ==================================================
2026-01-14 11:57:16,584 - INFO -   [탐색 60] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:16,662 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:16,663 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:17,479 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:17,479 - INFO - ==================================================
2026-01-14 11:57:17,560 - INFO -   [탐색 61] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:17,626 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:17,627 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:18,323 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:18,324 - INFO - ==================================================
2026-01-14 11:57:18,399 - INFO -   [탐색 62] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:18,476 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:18,477 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:19,207 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:19,208 - INFO - ==================================================
2026-01-14 11:57:19,292 - INFO -   [탐색 63] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:19,379 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:19,379 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:19,976 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:19,977 - INFO - ==================================================
2026-01-14 11:57:20,042 - INFO -   [탐색 64] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:20,100 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:20,100 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:20,698 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:20,699 - INFO - ==================================================
2026-01-14 11:57:20,778 - INFO -   [탐색 65] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:20,860 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:20,860 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:21,678 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:21,679 - INFO - ==================================================
2026-01-14 11:57:21,856 - INFO -   [탐색 66] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:21,968 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:21,969 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:22,579 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:22,580 - INFO - ==================================================
2026-01-14 11:57:22,638 - INFO -   [탐색 67] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:22,686 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:22,686 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:23,084 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:23,085 - INFO - ==================================================
2026-01-14 11:57:23,143 - INFO -   [탐색 68] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:23,620 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:23,622 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:24,162 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:24,162 - INFO - ==================================================
2026-01-14 11:57:24,354 - INFO -   [탐색 69] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:24,461 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:24,462 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:24,911 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:24,911 - INFO - ==================================================
2026-01-14 11:57:24,985 - INFO -   [탐색 70] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:25,059 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:25,059 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:25,480 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:25,481 - INFO - ==================================================
2026-01-14 11:57:25,562 - INFO -   [탐색 71] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:25,695 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:25,695 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:26,191 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:26,191 - INFO - ==================================================
2026-01-14 11:57:26,272 - INFO -   [탐색 72] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:26,356 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:26,357 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:27,124 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:27,124 - INFO - ==================================================
2026-01-14 11:57:27,200 - INFO -   [탐색 73] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:27,328 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:27,329 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:28,185 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:28,185 - INFO - ==================================================
2026-01-14 11:57:28,260 - INFO -   [탐색 74] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:28,384 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:28,385 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:29,223 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:29,224 - INFO - ==================================================
2026-01-14 11:57:29,287 - INFO -   [탐색 75] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:29,355 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:29,356 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:29,841 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:29,841 - INFO - ==================================================
2026-01-14 11:57:29,906 - INFO -   [탐색 76] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:29,972 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:29,973 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:30,519 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:30,519 - INFO - ==================================================
2026-01-14 11:57:30,579 - INFO -   [탐색 77] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:30,663 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:30,663 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:31,073 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:31,074 - INFO - ==================================================
2026-01-14 11:57:31,150 - INFO -   [탐색 78] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:31,238 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:31,239 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:31,652 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:31,653 - INFO - ==================================================
2026-01-14 11:57:31,723 - INFO -   [탐색 79] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:31,796 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:31,796 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:32,271 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:32,271 - INFO - ==================================================
2026-01-14 11:57:32,337 - INFO -   [탐색 80] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:32,404 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:32,405 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:32,836 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:32,836 - INFO - ==================================================
2026-01-14 11:57:32,921 - INFO -   [탐색 81] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:32,997 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:32,998 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:33,820 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:33,820 - INFO - ==================================================
2026-01-14 11:57:33,892 - INFO -   [탐색 82] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:33,968 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:33,969 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:34,403 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:34,403 - INFO - ==================================================
2026-01-14 11:57:34,475 - INFO -   [탐색 83] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:34,554 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:34,554 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:35,125 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:35,125 - INFO - ==================================================
2026-01-14 11:57:35,207 - INFO -   [탐색 84] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:35,289 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:35,290 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:35,849 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:35,849 - INFO - ==================================================
2026-01-14 11:57:35,909 - INFO -   [탐색 85] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:35,968 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:35,969 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:36,478 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:36,478 - INFO - ==================================================
2026-01-14 11:57:36,535 - INFO -   [탐색 86] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:36,595 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:36,595 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:37,056 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:37,057 - INFO - ==================================================
2026-01-14 11:57:37,132 - INFO -   [탐색 87] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:37,210 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:37,211 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:37,643 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:37,644 - INFO - ==================================================
2026-01-14 11:57:37,703 - INFO -   [탐색 88] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:37,762 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:37,762 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:38,463 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:38,464 - INFO - ==================================================
2026-01-14 11:57:38,523 - INFO -   [탐색 89] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:38,582 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:38,583 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:38,853 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:38,853 - INFO - ==================================================
2026-01-14 11:57:38,917 - INFO -   [탐색 90] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:38,977 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:38,978 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:39,459 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:39,459 - INFO - ==================================================
2026-01-14 11:57:39,529 - INFO -   [탐색 91] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:39,639 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:39,641 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:40,352 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:40,352 - INFO - ==================================================
2026-01-14 11:57:40,479 - INFO -   [탐색 92] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:40,563 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:40,564 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:41,259 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:41,260 - INFO - ==================================================
2026-01-14 11:57:41,362 - INFO -   [탐색 93] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:41,414 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:41,414 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:42,003 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:42,007 - INFO - ==================================================
2026-01-14 11:57:42,074 - INFO -   [탐색 94] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:42,187 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:42,188 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:42,728 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:42,728 - INFO - ==================================================
2026-01-14 11:57:42,801 - INFO -   [탐색 95] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:42,887 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:42,887 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:43,830 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:43,831 - INFO - ==================================================
2026-01-14 11:57:43,905 - INFO -   [탐색 96] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:43,981 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:43,982 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:44,400 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:44,400 - INFO - ==================================================
2026-01-14 11:57:44,488 - INFO -   [탐색 97] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:44,571 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:44,571 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:44,953 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:44,953 - INFO - ==================================================
2026-01-14 11:57:45,025 - INFO -   [탐색 98] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:45,083 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:45,083 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:45,424 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:45,425 - INFO - ==================================================
2026-01-14 11:57:45,499 - INFO -   [탐색 99] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:45,575 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:45,575 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:45,924 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 11:57:45,924 - INFO - ==================================================
2026-01-14 11:57:46,008 - INFO -   [탐색 100] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 11:57:46,008 - INFO - 탐색 완료. 목표 FLOPs(0.1829)에 가장 근접한 최적 희소도는 0.5738 입니다.
2026-01-14 11:57:46,009 - INFO - ================================================================================
2026-01-14 11:57:46,014 - INFO - 계산된 Pruning 정보(희소도: 0.5738)를 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/pruning_info.yaml'에 저장했습니다.
2026-01-14 11:57:46,118 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-01-14 11:57:46,281 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:57:46,281 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:57:46,770 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737939453124999)에 맞춰 변경되었습니다.
2026-01-14 11:57:46,770 - INFO - ==================================================
2026-01-14 11:57:46,773 - INFO - ==================================================
2026-01-14 11:57:46,773 - INFO - 모델 파라미터 수:
2026-01-14 11:57:46,773 - INFO -   - 총 파라미터: 775,493 개
2026-01-14 11:57:46,773 - INFO -   - 학습 가능한 파라미터: 775,493 개
2026-01-14 11:57:46,835 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-01-14 11:57:47,016 - INFO - FLOPs가 0.8277 GFLOPs에서 0.1829 GFLOPs로 감소했습니다 (감소율: 77.91%).
2026-01-14 11:57:47,017 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-01-14 11:57:47,017 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 11:57:47,018 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-01-14 11:57:47,019 - INFO - ==================================================
2026-01-14 11:57:47,019 - INFO - train 모드를 시작합니다.
2026-01-14 11:57:47,019 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 11:57:47,019 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 11:57:47,019 - INFO - --------------------------------------------------
2026-01-14 11:57:47,020 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-01-14 11:57:55,791 - INFO - [Train] [11/90] | Loss: 0.5468 | Train Acc: 75.89%
2026-01-14 11:57:58,141 - INFO - [Valid] [11/90] | Loss: 0.5454 | Val Acc: 75.22%
2026-01-14 11:57:58,164 - INFO - [Metrics for 'abnormal'] | Precision: 0.7704 | Recall: 0.6624 | F1: 0.7123
2026-01-14 11:57:58,167 - INFO - [Metrics for 'normal'] | Precision: 0.7402 | Recall: 0.8297 | F1: 0.7824
2026-01-14 11:57:58,277 - INFO - [Best Model Saved] (val loss: 0.5454) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'
2026-01-14 11:57:58,278 - INFO - --------------------------------------------------
2026-01-14 11:57:58,280 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-01-14 11:58:06,658 - INFO - [Train] [12/90] | Loss: 0.5009 | Train Acc: 80.73%
2026-01-14 11:58:09,326 - INFO - [Valid] [12/90] | Loss: 0.5432 | Val Acc: 81.12%
2026-01-14 11:58:09,352 - INFO - [Metrics for 'abnormal'] | Precision: 0.8252 | Recall: 0.7516 | F1: 0.7867
2026-01-14 11:58:09,352 - INFO - [Metrics for 'normal'] | Precision: 0.8010 | Recall: 0.8626 | F1: 0.8307
2026-01-14 11:58:09,427 - INFO - [Best Model Saved] (val loss: 0.5432) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'
2026-01-14 11:58:09,427 - INFO - --------------------------------------------------
2026-01-14 11:58:09,429 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-01-14 11:58:16,625 - INFO - [Train] [13/90] | Loss: 0.4707 | Train Acc: 84.15%
2026-01-14 11:58:18,610 - INFO - [Valid] [13/90] | Loss: 0.5135 | Val Acc: 78.76%
2026-01-14 11:58:18,620 - INFO - [Metrics for 'abnormal'] | Precision: 0.7931 | Recall: 0.7325 | F1: 0.7616
2026-01-14 11:58:18,621 - INFO - [Metrics for 'normal'] | Precision: 0.7835 | Recall: 0.8352 | F1: 0.8085
2026-01-14 11:58:18,674 - INFO - [Best Model Saved] (val loss: 0.5135) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'
2026-01-14 11:58:18,675 - INFO - --------------------------------------------------
2026-01-14 11:58:18,677 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-01-14 11:58:24,499 - INFO - [Train] [14/90] | Loss: 0.4752 | Train Acc: 82.89%
2026-01-14 11:58:26,589 - INFO - [Valid] [14/90] | Loss: 0.4805 | Val Acc: 81.12%
2026-01-14 11:58:26,610 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-01-14 11:58:26,611 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-01-14 11:58:26,655 - INFO - [Best Model Saved] (val loss: 0.4805) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'
2026-01-14 11:58:26,656 - INFO - --------------------------------------------------
2026-01-14 11:58:26,657 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-01-14 11:58:32,287 - INFO - [Train] [15/90] | Loss: 0.4517 | Train Acc: 83.93%
2026-01-14 11:58:34,052 - INFO - [Valid] [15/90] | Loss: 0.5193 | Val Acc: 80.24%
2026-01-14 11:58:34,074 - INFO - [Metrics for 'abnormal'] | Precision: 0.8261 | Recall: 0.7261 | F1: 0.7729
2026-01-14 11:58:34,074 - INFO - [Metrics for 'normal'] | Precision: 0.7861 | Recall: 0.8681 | F1: 0.8251
2026-01-14 11:58:34,085 - INFO - --------------------------------------------------
2026-01-14 11:58:34,087 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-01-14 11:58:39,821 - INFO - [Train] [16/90] | Loss: 0.4684 | Train Acc: 82.96%
2026-01-14 11:58:41,267 - INFO - [Valid] [16/90] | Loss: 0.5166 | Val Acc: 77.58%
2026-01-14 11:58:41,281 - INFO - [Metrics for 'abnormal'] | Precision: 0.8857 | Recall: 0.5924 | F1: 0.7099
2026-01-14 11:58:41,281 - INFO - [Metrics for 'normal'] | Precision: 0.7265 | Recall: 0.9341 | F1: 0.8173
2026-01-14 11:58:41,285 - INFO - --------------------------------------------------
2026-01-14 11:58:41,288 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-01-14 11:58:46,724 - INFO - [Train] [17/90] | Loss: 0.4318 | Train Acc: 85.04%
2026-01-14 11:58:48,628 - INFO - [Valid] [17/90] | Loss: 0.4922 | Val Acc: 79.65%
2026-01-14 11:58:48,641 - INFO - [Metrics for 'abnormal'] | Precision: 0.7750 | Recall: 0.7898 | F1: 0.7823
2026-01-14 11:58:48,642 - INFO - [Metrics for 'normal'] | Precision: 0.8156 | Recall: 0.8022 | F1: 0.8089
2026-01-14 11:58:48,645 - INFO - --------------------------------------------------
2026-01-14 11:58:48,648 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-01-14 11:58:54,911 - INFO - [Train] [18/90] | Loss: 0.4124 | Train Acc: 87.28%
2026-01-14 11:58:56,893 - INFO - [Valid] [18/90] | Loss: 0.5004 | Val Acc: 79.94%
2026-01-14 11:58:56,905 - INFO - [Metrics for 'abnormal'] | Precision: 0.7764 | Recall: 0.7962 | F1: 0.7862
2026-01-14 11:58:56,906 - INFO - [Metrics for 'normal'] | Precision: 0.8202 | Recall: 0.8022 | F1: 0.8111
2026-01-14 11:58:56,910 - INFO - --------------------------------------------------
2026-01-14 11:58:56,912 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-01-14 11:59:03,788 - INFO - [Train] [19/90] | Loss: 0.4106 | Train Acc: 87.65%
2026-01-14 11:59:05,795 - INFO - [Valid] [19/90] | Loss: 0.4853 | Val Acc: 81.42%
2026-01-14 11:59:05,885 - INFO - [Metrics for 'abnormal'] | Precision: 0.7975 | Recall: 0.8025 | F1: 0.8000
2026-01-14 11:59:05,885 - INFO - [Metrics for 'normal'] | Precision: 0.8287 | Recall: 0.8242 | F1: 0.8264
2026-01-14 11:59:05,896 - INFO - --------------------------------------------------
2026-01-14 11:59:05,903 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-01-14 11:59:12,250 - INFO - [Train] [20/90] | Loss: 0.4058 | Train Acc: 86.90%
2026-01-14 11:59:14,289 - INFO - [Valid] [20/90] | Loss: 0.5135 | Val Acc: 80.24%
2026-01-14 11:59:14,303 - INFO - [Metrics for 'abnormal'] | Precision: 0.7711 | Recall: 0.8153 | F1: 0.7926
2026-01-14 11:59:14,304 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.7912 | F1: 0.8113
2026-01-14 11:59:14,307 - INFO - --------------------------------------------------
2026-01-14 11:59:14,309 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-01-14 11:59:20,588 - INFO - [Train] [21/90] | Loss: 0.3958 | Train Acc: 88.24%
2026-01-14 11:59:22,586 - INFO - [Valid] [21/90] | Loss: 0.5724 | Val Acc: 81.42%
2026-01-14 11:59:22,597 - INFO - [Metrics for 'abnormal'] | Precision: 0.7554 | Recall: 0.8854 | F1: 0.8152
2026-01-14 11:59:22,597 - INFO - [Metrics for 'normal'] | Precision: 0.8839 | Recall: 0.7527 | F1: 0.8131
2026-01-14 11:59:22,601 - INFO - --------------------------------------------------
2026-01-14 11:59:22,603 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-01-14 11:59:28,774 - INFO - [Train] [22/90] | Loss: 0.3872 | Train Acc: 87.87%
2026-01-14 11:59:31,076 - INFO - [Valid] [22/90] | Loss: 0.4595 | Val Acc: 82.01%
2026-01-14 11:59:31,087 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 11:59:31,087 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 11:59:31,134 - INFO - [Best Model Saved] (val loss: 0.4595) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'
2026-01-14 11:59:31,135 - INFO - --------------------------------------------------
2026-01-14 11:59:31,136 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-01-14 11:59:37,136 - INFO - [Train] [23/90] | Loss: 0.3749 | Train Acc: 89.29%
2026-01-14 11:59:39,849 - INFO - [Valid] [23/90] | Loss: 0.4992 | Val Acc: 80.24%
2026-01-14 11:59:39,861 - INFO - [Metrics for 'abnormal'] | Precision: 0.7679 | Recall: 0.8217 | F1: 0.7938
2026-01-14 11:59:39,862 - INFO - [Metrics for 'normal'] | Precision: 0.8363 | Recall: 0.7857 | F1: 0.8102
2026-01-14 11:59:39,866 - INFO - --------------------------------------------------
2026-01-14 11:59:39,869 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-01-14 11:59:47,408 - INFO - [Train] [24/90] | Loss: 0.3587 | Train Acc: 90.62%
2026-01-14 11:59:49,932 - INFO - [Valid] [24/90] | Loss: 0.4925 | Val Acc: 81.42%
2026-01-14 11:59:49,942 - INFO - [Metrics for 'abnormal'] | Precision: 0.7831 | Recall: 0.8280 | F1: 0.8050
2026-01-14 11:59:49,943 - INFO - [Metrics for 'normal'] | Precision: 0.8439 | Recall: 0.8022 | F1: 0.8225
2026-01-14 11:59:49,946 - INFO - --------------------------------------------------
2026-01-14 11:59:49,948 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-01-14 11:59:57,884 - INFO - [Train] [25/90] | Loss: 0.3545 | Train Acc: 90.48%
2026-01-14 12:00:00,724 - INFO - [Valid] [25/90] | Loss: 0.4936 | Val Acc: 81.71%
2026-01-14 12:00:00,735 - INFO - [Metrics for 'abnormal'] | Precision: 0.8065 | Recall: 0.7962 | F1: 0.8013
2026-01-14 12:00:00,736 - INFO - [Metrics for 'normal'] | Precision: 0.8261 | Recall: 0.8352 | F1: 0.8306
2026-01-14 12:00:00,740 - INFO - --------------------------------------------------
2026-01-14 12:00:00,745 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-01-14 12:00:08,248 - INFO - [Train] [26/90] | Loss: 0.3470 | Train Acc: 91.07%
2026-01-14 12:00:11,476 - INFO - [Valid] [26/90] | Loss: 0.4760 | Val Acc: 82.60%
2026-01-14 12:00:11,488 - INFO - [Metrics for 'abnormal'] | Precision: 0.8451 | Recall: 0.7643 | F1: 0.8027
2026-01-14 12:00:11,489 - INFO - [Metrics for 'normal'] | Precision: 0.8122 | Recall: 0.8791 | F1: 0.8443
2026-01-14 12:00:11,493 - INFO - --------------------------------------------------
2026-01-14 12:00:11,495 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-01-14 12:00:19,072 - INFO - [Train] [27/90] | Loss: 0.3473 | Train Acc: 91.15%
2026-01-14 12:00:21,874 - INFO - [Valid] [27/90] | Loss: 0.5079 | Val Acc: 82.01%
2026-01-14 12:00:21,886 - INFO - [Metrics for 'abnormal'] | Precision: 0.8200 | Recall: 0.7834 | F1: 0.8013
2026-01-14 12:00:21,886 - INFO - [Metrics for 'normal'] | Precision: 0.8201 | Recall: 0.8516 | F1: 0.8356
2026-01-14 12:00:21,890 - INFO - --------------------------------------------------
2026-01-14 12:00:21,893 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-01-14 12:00:29,409 - INFO - [Train] [28/90] | Loss: 0.3436 | Train Acc: 91.82%
2026-01-14 12:00:31,940 - INFO - [Valid] [28/90] | Loss: 0.5229 | Val Acc: 80.53%
2026-01-14 12:00:31,950 - INFO - [Metrics for 'abnormal'] | Precision: 0.7433 | Recall: 0.8854 | F1: 0.8081
2026-01-14 12:00:31,950 - INFO - [Metrics for 'normal'] | Precision: 0.8816 | Recall: 0.7363 | F1: 0.8024
2026-01-14 12:00:31,953 - INFO - --------------------------------------------------
2026-01-14 12:00:31,955 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-01-14 12:00:41,389 - INFO - [Train] [29/90] | Loss: 0.3403 | Train Acc: 92.71%
2026-01-14 12:00:43,762 - INFO - [Valid] [29/90] | Loss: 0.5164 | Val Acc: 83.19%
2026-01-14 12:00:43,780 - INFO - [Metrics for 'abnormal'] | Precision: 0.7809 | Recall: 0.8854 | F1: 0.8299
2026-01-14 12:00:43,780 - INFO - [Metrics for 'normal'] | Precision: 0.8882 | Recall: 0.7857 | F1: 0.8338
2026-01-14 12:00:43,783 - INFO - --------------------------------------------------
2026-01-14 12:00:43,785 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-01-14 12:00:52,421 - INFO - [Train] [30/90] | Loss: 0.3214 | Train Acc: 93.01%
2026-01-14 12:00:55,330 - INFO - [Valid] [30/90] | Loss: 0.4851 | Val Acc: 82.60%
2026-01-14 12:00:55,352 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.8153 | F1: 0.8127
2026-01-14 12:00:55,353 - INFO - [Metrics for 'normal'] | Precision: 0.8398 | Recall: 0.8352 | F1: 0.8375
2026-01-14 12:00:55,357 - INFO - --------------------------------------------------
2026-01-14 12:00:55,360 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-01-14 12:01:05,025 - INFO - [Train] [31/90] | Loss: 0.3125 | Train Acc: 94.20%
2026-01-14 12:01:07,076 - INFO - [Valid] [31/90] | Loss: 0.4800 | Val Acc: 82.60%
2026-01-14 12:01:07,096 - INFO - [Metrics for 'abnormal'] | Precision: 0.7917 | Recall: 0.8471 | F1: 0.8185
2026-01-14 12:01:07,099 - INFO - [Metrics for 'normal'] | Precision: 0.8596 | Recall: 0.8077 | F1: 0.8329
2026-01-14 12:01:07,106 - INFO - --------------------------------------------------
2026-01-14 12:01:07,138 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-01-14 12:01:18,128 - INFO - [Train] [32/90] | Loss: 0.3024 | Train Acc: 94.49%
2026-01-14 12:01:20,641 - INFO - [Valid] [32/90] | Loss: 0.4710 | Val Acc: 83.78%
2026-01-14 12:01:20,653 - INFO - [Metrics for 'abnormal'] | Precision: 0.8592 | Recall: 0.7771 | F1: 0.8161
2026-01-14 12:01:20,654 - INFO - [Metrics for 'normal'] | Precision: 0.8223 | Recall: 0.8901 | F1: 0.8549
2026-01-14 12:01:20,658 - INFO - --------------------------------------------------
2026-01-14 12:01:20,660 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-01-14 12:01:31,137 - INFO - [Train] [33/90] | Loss: 0.3179 | Train Acc: 93.45%
2026-01-14 12:01:34,135 - INFO - [Valid] [33/90] | Loss: 0.4917 | Val Acc: 82.60%
2026-01-14 12:01:34,154 - INFO - [Metrics for 'abnormal'] | Precision: 0.7917 | Recall: 0.8471 | F1: 0.8185
2026-01-14 12:01:34,155 - INFO - [Metrics for 'normal'] | Precision: 0.8596 | Recall: 0.8077 | F1: 0.8329
2026-01-14 12:01:34,160 - INFO - --------------------------------------------------
2026-01-14 12:01:34,204 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-01-14 12:01:43,448 - INFO - [Train] [34/90] | Loss: 0.2902 | Train Acc: 95.16%
2026-01-14 12:01:46,239 - INFO - [Valid] [34/90] | Loss: 0.5221 | Val Acc: 82.89%
2026-01-14 12:01:46,264 - INFO - [Metrics for 'abnormal'] | Precision: 0.8511 | Recall: 0.7643 | F1: 0.8054
2026-01-14 12:01:46,274 - INFO - [Metrics for 'normal'] | Precision: 0.8131 | Recall: 0.8846 | F1: 0.8474
2026-01-14 12:01:46,282 - INFO - --------------------------------------------------
2026-01-14 12:01:46,288 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-01-14 12:01:56,280 - INFO - [Train] [35/90] | Loss: 0.2994 | Train Acc: 94.12%
2026-01-14 12:01:59,350 - INFO - [Valid] [35/90] | Loss: 0.4967 | Val Acc: 80.53%
2026-01-14 12:01:59,361 - INFO - [Metrics for 'abnormal'] | Precision: 0.8054 | Recall: 0.7643 | F1: 0.7843
2026-01-14 12:01:59,362 - INFO - [Metrics for 'normal'] | Precision: 0.8053 | Recall: 0.8407 | F1: 0.8226
2026-01-14 12:01:59,366 - INFO - --------------------------------------------------
2026-01-14 12:01:59,368 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-01-14 12:02:07,408 - INFO - [Train] [36/90] | Loss: 0.2827 | Train Acc: 95.68%
2026-01-14 12:02:10,093 - INFO - [Valid] [36/90] | Loss: 0.5065 | Val Acc: 83.78%
2026-01-14 12:02:10,120 - INFO - [Metrics for 'abnormal'] | Precision: 0.8036 | Recall: 0.8599 | F1: 0.8308
2026-01-14 12:02:10,120 - INFO - [Metrics for 'normal'] | Precision: 0.8713 | Recall: 0.8187 | F1: 0.8442
2026-01-14 12:02:10,124 - INFO - --------------------------------------------------
2026-01-14 12:02:10,128 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-01-14 12:02:19,415 - INFO - [Train] [37/90] | Loss: 0.2860 | Train Acc: 94.64%
2026-01-14 12:02:22,845 - INFO - [Valid] [37/90] | Loss: 0.5527 | Val Acc: 79.35%
2026-01-14 12:02:22,877 - INFO - [Metrics for 'abnormal'] | Precision: 0.7430 | Recall: 0.8471 | F1: 0.7917
2026-01-14 12:02:22,877 - INFO - [Metrics for 'normal'] | Precision: 0.8500 | Recall: 0.7473 | F1: 0.7953
2026-01-14 12:02:22,886 - INFO - --------------------------------------------------
2026-01-14 12:02:22,892 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-01-14 12:02:31,488 - INFO - [Train] [38/90] | Loss: 0.2956 | Train Acc: 94.42%
2026-01-14 12:02:34,628 - INFO - [Valid] [38/90] | Loss: 0.4672 | Val Acc: 83.19%
2026-01-14 12:02:34,641 - INFO - [Metrics for 'abnormal'] | Precision: 0.7976 | Recall: 0.8535 | F1: 0.8246
2026-01-14 12:02:34,641 - INFO - [Metrics for 'normal'] | Precision: 0.8655 | Recall: 0.8132 | F1: 0.8385
2026-01-14 12:02:34,645 - INFO - --------------------------------------------------
2026-01-14 12:02:34,648 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-01-14 12:02:43,668 - INFO - [Train] [39/90] | Loss: 0.2904 | Train Acc: 95.16%
2026-01-14 12:02:46,332 - INFO - [Valid] [39/90] | Loss: 0.5457 | Val Acc: 82.01%
2026-01-14 12:02:46,345 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-01-14 12:02:46,346 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-01-14 12:02:46,353 - INFO - --------------------------------------------------
2026-01-14 12:02:46,356 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-01-14 12:02:56,127 - INFO - [Train] [40/90] | Loss: 0.2770 | Train Acc: 95.91%
2026-01-14 12:02:59,239 - INFO - [Valid] [40/90] | Loss: 0.5069 | Val Acc: 81.12%
2026-01-14 12:02:59,248 - INFO - [Metrics for 'abnormal'] | Precision: 0.8207 | Recall: 0.7580 | F1: 0.7881
2026-01-14 12:02:59,249 - INFO - [Metrics for 'normal'] | Precision: 0.8041 | Recall: 0.8571 | F1: 0.8298
2026-01-14 12:02:59,253 - INFO - --------------------------------------------------
2026-01-14 12:02:59,255 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-01-14 12:03:08,937 - INFO - [Train] [41/90] | Loss: 0.2617 | Train Acc: 96.65%
2026-01-14 12:03:12,032 - INFO - [Valid] [41/90] | Loss: 0.5238 | Val Acc: 81.42%
2026-01-14 12:03:12,080 - INFO - [Metrics for 'abnormal'] | Precision: 0.8176 | Recall: 0.7707 | F1: 0.7934
2026-01-14 12:03:12,082 - INFO - [Metrics for 'normal'] | Precision: 0.8115 | Recall: 0.8516 | F1: 0.8311
2026-01-14 12:03:12,087 - INFO - --------------------------------------------------
2026-01-14 12:03:12,091 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-01-14 12:03:22,394 - INFO - [Train] [42/90] | Loss: 0.2670 | Train Acc: 96.43%
2026-01-14 12:03:26,479 - INFO - [Valid] [42/90] | Loss: 0.5379 | Val Acc: 81.42%
2026-01-14 12:03:26,492 - INFO - [Metrics for 'abnormal'] | Precision: 0.8264 | Recall: 0.7580 | F1: 0.7907
2026-01-14 12:03:26,492 - INFO - [Metrics for 'normal'] | Precision: 0.8051 | Recall: 0.8626 | F1: 0.8329
2026-01-14 12:03:26,496 - INFO - --------------------------------------------------
2026-01-14 12:03:26,499 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-01-14 12:03:36,571 - INFO - [Train] [43/90] | Loss: 0.2543 | Train Acc: 97.32%
2026-01-14 12:03:39,199 - INFO - [Valid] [43/90] | Loss: 0.5408 | Val Acc: 83.19%
2026-01-14 12:03:39,216 - INFO - [Metrics for 'abnormal'] | Precision: 0.8623 | Recall: 0.7580 | F1: 0.8068
2026-01-14 12:03:39,216 - INFO - [Metrics for 'normal'] | Precision: 0.8109 | Recall: 0.8956 | F1: 0.8512
2026-01-14 12:03:39,222 - INFO - --------------------------------------------------
2026-01-14 12:03:39,226 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-01-14 12:03:49,730 - INFO - [Train] [44/90] | Loss: 0.2589 | Train Acc: 96.95%
2026-01-14 12:03:53,064 - INFO - [Valid] [44/90] | Loss: 0.4981 | Val Acc: 83.78%
2026-01-14 12:03:53,076 - INFO - [Metrics for 'abnormal'] | Precision: 0.8446 | Recall: 0.7962 | F1: 0.8197
2026-01-14 12:03:53,076 - INFO - [Metrics for 'normal'] | Precision: 0.8325 | Recall: 0.8736 | F1: 0.8525
2026-01-14 12:03:53,080 - INFO - --------------------------------------------------
2026-01-14 12:03:53,083 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-01-14 12:04:02,503 - INFO - [Train] [45/90] | Loss: 0.2594 | Train Acc: 96.73%
2026-01-14 12:04:06,482 - INFO - [Valid] [45/90] | Loss: 0.5280 | Val Acc: 80.83%
2026-01-14 12:04:06,502 - INFO - [Metrics for 'abnormal'] | Precision: 0.7949 | Recall: 0.7898 | F1: 0.7923
2026-01-14 12:04:06,502 - INFO - [Metrics for 'normal'] | Precision: 0.8197 | Recall: 0.8242 | F1: 0.8219
2026-01-14 12:04:06,513 - INFO - --------------------------------------------------
2026-01-14 12:04:06,516 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-01-14 12:04:15,437 - INFO - [Train] [46/90] | Loss: 0.2405 | Train Acc: 97.99%
2026-01-14 12:04:18,756 - INFO - [Valid] [46/90] | Loss: 0.5314 | Val Acc: 83.78%
2026-01-14 12:04:18,789 - INFO - [Metrics for 'abnormal'] | Precision: 0.8355 | Recall: 0.8089 | F1: 0.8220
2026-01-14 12:04:18,790 - INFO - [Metrics for 'normal'] | Precision: 0.8396 | Recall: 0.8626 | F1: 0.8509
2026-01-14 12:04:18,793 - INFO - --------------------------------------------------
2026-01-14 12:04:18,797 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-01-14 12:04:27,992 - INFO - [Train] [47/90] | Loss: 0.2525 | Train Acc: 96.80%
2026-01-14 12:04:31,077 - INFO - [Valid] [47/90] | Loss: 0.5566 | Val Acc: 80.24%
2026-01-14 12:04:31,094 - INFO - [Metrics for 'abnormal'] | Precision: 0.7812 | Recall: 0.7962 | F1: 0.7886
2026-01-14 12:04:31,094 - INFO - [Metrics for 'normal'] | Precision: 0.8212 | Recall: 0.8077 | F1: 0.8144
2026-01-14 12:04:31,101 - INFO - --------------------------------------------------
2026-01-14 12:04:31,104 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-01-14 12:04:39,772 - INFO - [Train] [48/90] | Loss: 0.2581 | Train Acc: 96.73%
2026-01-14 12:04:42,938 - INFO - [Valid] [48/90] | Loss: 0.5620 | Val Acc: 81.42%
2026-01-14 12:04:42,961 - INFO - [Metrics for 'abnormal'] | Precision: 0.8013 | Recall: 0.7962 | F1: 0.7987
2026-01-14 12:04:42,975 - INFO - [Metrics for 'normal'] | Precision: 0.8251 | Recall: 0.8297 | F1: 0.8274
2026-01-14 12:04:42,984 - INFO - --------------------------------------------------
2026-01-14 12:04:42,992 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-01-14 12:04:51,633 - INFO - [Train] [49/90] | Loss: 0.2481 | Train Acc: 97.32%
2026-01-14 12:04:54,794 - INFO - [Valid] [49/90] | Loss: 0.5462 | Val Acc: 79.65%
2026-01-14 12:04:54,803 - INFO - [Metrics for 'abnormal'] | Precision: 0.7750 | Recall: 0.7898 | F1: 0.7823
2026-01-14 12:04:54,804 - INFO - [Metrics for 'normal'] | Precision: 0.8156 | Recall: 0.8022 | F1: 0.8089
2026-01-14 12:04:54,807 - INFO - --------------------------------------------------
2026-01-14 12:04:54,809 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-01-14 12:05:04,430 - INFO - [Train] [50/90] | Loss: 0.2381 | Train Acc: 98.21%
2026-01-14 12:05:06,829 - INFO - [Valid] [50/90] | Loss: 0.5753 | Val Acc: 82.30%
2026-01-14 12:05:06,842 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-01-14 12:05:06,842 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-01-14 12:05:06,847 - INFO - --------------------------------------------------
2026-01-14 12:05:06,850 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-01-14 12:05:15,017 - INFO - [Train] [51/90] | Loss: 0.2315 | Train Acc: 98.66%
2026-01-14 12:05:18,000 - INFO - [Valid] [51/90] | Loss: 0.5624 | Val Acc: 79.94%
2026-01-14 12:05:18,010 - INFO - [Metrics for 'abnormal'] | Precision: 0.7871 | Recall: 0.7771 | F1: 0.7821
2026-01-14 12:05:18,011 - INFO - [Metrics for 'normal'] | Precision: 0.8098 | Recall: 0.8187 | F1: 0.8142
2026-01-14 12:05:18,014 - INFO - --------------------------------------------------
2026-01-14 12:05:18,017 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-01-14 12:05:26,922 - INFO - [Train] [52/90] | Loss: 0.2552 | Train Acc: 96.88%
2026-01-14 12:05:30,287 - INFO - [Valid] [52/90] | Loss: 0.5125 | Val Acc: 82.89%
2026-01-14 12:05:30,305 - INFO - [Metrics for 'abnormal'] | Precision: 0.8037 | Recall: 0.8344 | F1: 0.8187
2026-01-14 12:05:30,306 - INFO - [Metrics for 'normal'] | Precision: 0.8523 | Recall: 0.8242 | F1: 0.8380
2026-01-14 12:05:30,310 - INFO - --------------------------------------------------
2026-01-14 12:05:30,313 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-01-14 12:05:40,226 - INFO - [Train] [53/90] | Loss: 0.2352 | Train Acc: 97.92%
2026-01-14 12:05:44,197 - INFO - [Valid] [53/90] | Loss: 0.5537 | Val Acc: 81.42%
2026-01-14 12:05:44,212 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-01-14 12:05:44,212 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-01-14 12:05:44,216 - INFO - --------------------------------------------------
2026-01-14 12:05:44,219 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-01-14 12:05:53,173 - INFO - [Train] [54/90] | Loss: 0.2373 | Train Acc: 98.29%
2026-01-14 12:05:56,811 - INFO - [Valid] [54/90] | Loss: 0.5261 | Val Acc: 82.01%
2026-01-14 12:05:56,824 - INFO - [Metrics for 'abnormal'] | Precision: 0.8380 | Recall: 0.7580 | F1: 0.7960
2026-01-14 12:05:56,825 - INFO - [Metrics for 'normal'] | Precision: 0.8071 | Recall: 0.8736 | F1: 0.8391
2026-01-14 12:05:56,829 - INFO - --------------------------------------------------
2026-01-14 12:05:56,832 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-01-14 12:06:06,554 - INFO - [Train] [55/90] | Loss: 0.2278 | Train Acc: 98.81%
2026-01-14 12:06:09,320 - INFO - [Valid] [55/90] | Loss: 0.5362 | Val Acc: 83.19%
2026-01-14 12:06:09,330 - INFO - [Metrics for 'abnormal'] | Precision: 0.8165 | Recall: 0.8217 | F1: 0.8190
2026-01-14 12:06:09,330 - INFO - [Metrics for 'normal'] | Precision: 0.8453 | Recall: 0.8407 | F1: 0.8430
2026-01-14 12:06:09,334 - INFO - --------------------------------------------------
2026-01-14 12:06:09,337 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-01-14 12:06:19,406 - INFO - [Train] [56/90] | Loss: 0.2232 | Train Acc: 98.88%
2026-01-14 12:06:22,920 - INFO - [Valid] [56/90] | Loss: 0.5076 | Val Acc: 83.19%
2026-01-14 12:06:22,933 - INFO - [Metrics for 'abnormal'] | Precision: 0.8086 | Recall: 0.8344 | F1: 0.8213
2026-01-14 12:06:22,933 - INFO - [Metrics for 'normal'] | Precision: 0.8531 | Recall: 0.8297 | F1: 0.8412
2026-01-14 12:06:22,938 - INFO - --------------------------------------------------
2026-01-14 12:06:22,941 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-01-14 12:06:32,416 - INFO - [Train] [57/90] | Loss: 0.2167 | Train Acc: 99.26%
2026-01-14 12:06:35,211 - INFO - [Valid] [57/90] | Loss: 0.5046 | Val Acc: 84.96%
2026-01-14 12:06:35,232 - INFO - [Metrics for 'abnormal'] | Precision: 0.8533 | Recall: 0.8153 | F1: 0.8339
2026-01-14 12:06:35,233 - INFO - [Metrics for 'normal'] | Precision: 0.8466 | Recall: 0.8791 | F1: 0.8625
2026-01-14 12:06:35,240 - INFO - --------------------------------------------------
2026-01-14 12:06:35,244 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-01-14 12:06:45,282 - INFO - [Train] [58/90] | Loss: 0.2153 | Train Acc: 99.26%
2026-01-14 12:06:48,190 - INFO - [Valid] [58/90] | Loss: 0.5245 | Val Acc: 84.37%
2026-01-14 12:06:48,201 - INFO - [Metrics for 'abnormal'] | Precision: 0.8467 | Recall: 0.8089 | F1: 0.8274
2026-01-14 12:06:48,201 - INFO - [Metrics for 'normal'] | Precision: 0.8413 | Recall: 0.8736 | F1: 0.8571
2026-01-14 12:06:48,206 - INFO - --------------------------------------------------
2026-01-14 12:06:48,209 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-01-14 12:06:57,745 - INFO - [Train] [59/90] | Loss: 0.2138 | Train Acc: 99.26%
2026-01-14 12:07:00,380 - INFO - [Valid] [59/90] | Loss: 0.5239 | Val Acc: 83.78%
2026-01-14 12:07:00,389 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8662 | F1: 0.8318
2026-01-14 12:07:00,390 - INFO - [Metrics for 'normal'] | Precision: 0.8757 | Recall: 0.8132 | F1: 0.8433
2026-01-14 12:07:00,393 - INFO - --------------------------------------------------
2026-01-14 12:07:00,395 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-01-14 12:07:09,038 - INFO - [Train] [60/90] | Loss: 0.2115 | Train Acc: 99.48%
2026-01-14 12:07:11,677 - INFO - [Valid] [60/90] | Loss: 0.5271 | Val Acc: 82.60%
2026-01-14 12:07:11,689 - INFO - [Metrics for 'abnormal'] | Precision: 0.7988 | Recall: 0.8344 | F1: 0.8162
2026-01-14 12:07:11,690 - INFO - [Metrics for 'normal'] | Precision: 0.8514 | Recall: 0.8187 | F1: 0.8347
2026-01-14 12:07:11,694 - INFO - --------------------------------------------------
2026-01-14 12:07:11,696 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-01-14 12:07:22,192 - INFO - [Train] [61/90] | Loss: 0.2157 | Train Acc: 99.18%
2026-01-14 12:07:24,919 - INFO - [Valid] [61/90] | Loss: 0.5052 | Val Acc: 84.66%
2026-01-14 12:07:24,970 - INFO - [Metrics for 'abnormal'] | Precision: 0.8344 | Recall: 0.8344 | F1: 0.8344
2026-01-14 12:07:24,970 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.8571 | F1: 0.8571
2026-01-14 12:07:24,975 - INFO - --------------------------------------------------
2026-01-14 12:07:24,977 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-01-14 12:07:34,254 - INFO - [Train] [62/90] | Loss: 0.2169 | Train Acc: 99.11%
2026-01-14 12:07:36,871 - INFO - [Valid] [62/90] | Loss: 0.5196 | Val Acc: 83.48%
2026-01-14 12:07:36,892 - INFO - [Metrics for 'abnormal'] | Precision: 0.8344 | Recall: 0.8025 | F1: 0.8182
2026-01-14 12:07:36,892 - INFO - [Metrics for 'normal'] | Precision: 0.8351 | Recall: 0.8626 | F1: 0.8486
2026-01-14 12:07:36,899 - INFO - --------------------------------------------------
2026-01-14 12:07:36,904 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-01-14 12:07:46,633 - INFO - [Train] [63/90] | Loss: 0.2135 | Train Acc: 99.55%
2026-01-14 12:07:49,075 - INFO - [Valid] [63/90] | Loss: 0.5076 | Val Acc: 84.66%
2026-01-14 12:07:49,089 - INFO - [Metrics for 'abnormal'] | Precision: 0.8523 | Recall: 0.8089 | F1: 0.8301
2026-01-14 12:07:49,090 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.8791 | F1: 0.8602
2026-01-14 12:07:49,094 - INFO - --------------------------------------------------
2026-01-14 12:07:49,097 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-01-14 12:07:59,220 - INFO - [Train] [64/90] | Loss: 0.2192 | Train Acc: 98.88%
2026-01-14 12:08:02,178 - INFO - [Valid] [64/90] | Loss: 0.5028 | Val Acc: 84.07%
2026-01-14 12:08:02,191 - INFO - [Metrics for 'abnormal'] | Precision: 0.8323 | Recall: 0.8217 | F1: 0.8269
2026-01-14 12:08:02,192 - INFO - [Metrics for 'normal'] | Precision: 0.8478 | Recall: 0.8571 | F1: 0.8525
2026-01-14 12:08:02,196 - INFO - --------------------------------------------------
2026-01-14 12:08:02,200 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-01-14 12:08:11,799 - INFO - [Train] [65/90] | Loss: 0.2157 | Train Acc: 99.11%
2026-01-14 12:08:13,784 - INFO - [Valid] [65/90] | Loss: 0.5201 | Val Acc: 84.07%
2026-01-14 12:08:13,795 - INFO - [Metrics for 'abnormal'] | Precision: 0.8323 | Recall: 0.8217 | F1: 0.8269
2026-01-14 12:08:13,795 - INFO - [Metrics for 'normal'] | Precision: 0.8478 | Recall: 0.8571 | F1: 0.8525
2026-01-14 12:08:13,798 - INFO - --------------------------------------------------
2026-01-14 12:08:13,800 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-01-14 12:08:23,507 - INFO - [Train] [66/90] | Loss: 0.2128 | Train Acc: 99.40%
2026-01-14 12:08:25,543 - INFO - [Valid] [66/90] | Loss: 0.5232 | Val Acc: 84.07%
2026-01-14 12:08:25,559 - INFO - [Metrics for 'abnormal'] | Precision: 0.8456 | Recall: 0.8025 | F1: 0.8235
2026-01-14 12:08:25,559 - INFO - [Metrics for 'normal'] | Precision: 0.8368 | Recall: 0.8736 | F1: 0.8548
2026-01-14 12:08:25,562 - INFO - --------------------------------------------------
2026-01-14 12:08:25,564 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-01-14 12:08:35,241 - INFO - [Train] [67/90] | Loss: 0.2130 | Train Acc: 99.40%
2026-01-14 12:08:37,401 - INFO - [Valid] [67/90] | Loss: 0.5267 | Val Acc: 82.30%
2026-01-14 12:08:37,418 - INFO - [Metrics for 'abnormal'] | Precision: 0.8255 | Recall: 0.7834 | F1: 0.8039
2026-01-14 12:08:37,418 - INFO - [Metrics for 'normal'] | Precision: 0.8211 | Recall: 0.8571 | F1: 0.8387
2026-01-14 12:08:37,427 - INFO - --------------------------------------------------
2026-01-14 12:08:37,432 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-01-14 12:08:46,761 - INFO - [Train] [68/90] | Loss: 0.2164 | Train Acc: 99.26%
2026-01-14 12:08:49,173 - INFO - [Valid] [68/90] | Loss: 0.5326 | Val Acc: 84.07%
2026-01-14 12:08:49,184 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.8535 | F1: 0.8323
2026-01-14 12:08:49,185 - INFO - [Metrics for 'normal'] | Precision: 0.8678 | Recall: 0.8297 | F1: 0.8483
2026-01-14 12:08:49,189 - INFO - --------------------------------------------------
2026-01-14 12:08:49,192 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-01-14 12:08:59,839 - INFO - [Train] [69/90] | Loss: 0.2066 | Train Acc: 99.70%
2026-01-14 12:09:02,291 - INFO - [Valid] [69/90] | Loss: 0.5395 | Val Acc: 83.19%
2026-01-14 12:09:02,301 - INFO - [Metrics for 'abnormal'] | Precision: 0.8289 | Recall: 0.8025 | F1: 0.8155
2026-01-14 12:09:02,301 - INFO - [Metrics for 'normal'] | Precision: 0.8342 | Recall: 0.8571 | F1: 0.8455
2026-01-14 12:09:02,305 - INFO - --------------------------------------------------
2026-01-14 12:09:02,307 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-01-14 12:09:11,992 - INFO - [Train] [70/90] | Loss: 0.2098 | Train Acc: 99.40%
2026-01-14 12:09:14,036 - INFO - [Valid] [70/90] | Loss: 0.5457 | Val Acc: 83.78%
2026-01-14 12:09:14,046 - INFO - [Metrics for 'abnormal'] | Precision: 0.8269 | Recall: 0.8217 | F1: 0.8243
2026-01-14 12:09:14,046 - INFO - [Metrics for 'normal'] | Precision: 0.8470 | Recall: 0.8516 | F1: 0.8493
2026-01-14 12:09:14,049 - INFO - --------------------------------------------------
2026-01-14 12:09:14,051 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-01-14 12:09:25,243 - INFO - [Train] [71/90] | Loss: 0.2150 | Train Acc: 99.26%
2026-01-14 12:09:27,643 - INFO - [Valid] [71/90] | Loss: 0.5278 | Val Acc: 83.48%
2026-01-14 12:09:27,661 - INFO - [Metrics for 'abnormal'] | Precision: 0.8258 | Recall: 0.8153 | F1: 0.8205
2026-01-14 12:09:27,663 - INFO - [Metrics for 'normal'] | Precision: 0.8424 | Recall: 0.8516 | F1: 0.8470
2026-01-14 12:09:27,669 - INFO - --------------------------------------------------
2026-01-14 12:09:27,673 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-01-14 12:09:37,570 - INFO - [Train] [72/90] | Loss: 0.2088 | Train Acc: 99.63%
2026-01-14 12:09:40,054 - INFO - [Valid] [72/90] | Loss: 0.5358 | Val Acc: 83.19%
2026-01-14 12:09:40,123 - INFO - [Metrics for 'abnormal'] | Precision: 0.8086 | Recall: 0.8344 | F1: 0.8213
2026-01-14 12:09:40,123 - INFO - [Metrics for 'normal'] | Precision: 0.8531 | Recall: 0.8297 | F1: 0.8412
2026-01-14 12:09:40,128 - INFO - --------------------------------------------------
2026-01-14 12:09:40,131 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-01-14 12:09:50,137 - INFO - [Train] [73/90] | Loss: 0.2104 | Train Acc: 99.55%
2026-01-14 12:09:53,098 - INFO - [Valid] [73/90] | Loss: 0.5232 | Val Acc: 82.60%
2026-01-14 12:09:53,110 - INFO - [Metrics for 'abnormal'] | Precision: 0.8267 | Recall: 0.7898 | F1: 0.8078
2026-01-14 12:09:53,110 - INFO - [Metrics for 'normal'] | Precision: 0.8254 | Recall: 0.8571 | F1: 0.8410
2026-01-14 12:09:53,114 - INFO - --------------------------------------------------
2026-01-14 12:09:53,117 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-01-14 12:10:02,396 - INFO - [Train] [74/90] | Loss: 0.2136 | Train Acc: 99.11%
2026-01-14 12:10:05,527 - INFO - [Valid] [74/90] | Loss: 0.5491 | Val Acc: 82.30%
2026-01-14 12:10:05,551 - INFO - [Metrics for 'abnormal'] | Precision: 0.7836 | Recall: 0.8535 | F1: 0.8171
2026-01-14 12:10:05,551 - INFO - [Metrics for 'normal'] | Precision: 0.8631 | Recall: 0.7967 | F1: 0.8286
2026-01-14 12:10:05,556 - INFO - --------------------------------------------------
2026-01-14 12:10:05,559 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-01-14 12:10:15,211 - INFO - [Train] [75/90] | Loss: 0.2144 | Train Acc: 99.03%
2026-01-14 12:10:18,620 - INFO - [Valid] [75/90] | Loss: 0.5584 | Val Acc: 81.71%
2026-01-14 12:10:18,632 - INFO - [Metrics for 'abnormal'] | Precision: 0.7844 | Recall: 0.8344 | F1: 0.8086
2026-01-14 12:10:18,633 - INFO - [Metrics for 'normal'] | Precision: 0.8488 | Recall: 0.8022 | F1: 0.8249
2026-01-14 12:10:18,637 - INFO - --------------------------------------------------
2026-01-14 12:10:18,640 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-01-14 12:10:27,773 - INFO - [Train] [76/90] | Loss: 0.2110 | Train Acc: 99.48%
2026-01-14 12:10:31,422 - INFO - [Valid] [76/90] | Loss: 0.5285 | Val Acc: 83.78%
2026-01-14 12:10:31,441 - INFO - [Metrics for 'abnormal'] | Precision: 0.8269 | Recall: 0.8217 | F1: 0.8243
2026-01-14 12:10:31,441 - INFO - [Metrics for 'normal'] | Precision: 0.8470 | Recall: 0.8516 | F1: 0.8493
2026-01-14 12:10:31,445 - INFO - --------------------------------------------------
2026-01-14 12:10:31,448 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-01-14 12:10:40,847 - INFO - [Train] [77/90] | Loss: 0.2047 | Train Acc: 99.78%
2026-01-14 12:10:43,606 - INFO - [Valid] [77/90] | Loss: 0.5430 | Val Acc: 83.78%
2026-01-14 12:10:43,635 - INFO - [Metrics for 'abnormal'] | Precision: 0.8312 | Recall: 0.8153 | F1: 0.8232
2026-01-14 12:10:43,636 - INFO - [Metrics for 'normal'] | Precision: 0.8432 | Recall: 0.8571 | F1: 0.8501
2026-01-14 12:10:43,644 - INFO - --------------------------------------------------
2026-01-14 12:10:43,650 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-01-14 12:10:52,170 - INFO - [Train] [78/90] | Loss: 0.2072 | Train Acc: 99.55%
2026-01-14 12:10:55,425 - INFO - [Valid] [78/90] | Loss: 0.5313 | Val Acc: 84.07%
2026-01-14 12:10:55,436 - INFO - [Metrics for 'abnormal'] | Precision: 0.8411 | Recall: 0.8089 | F1: 0.8247
2026-01-14 12:10:55,436 - INFO - [Metrics for 'normal'] | Precision: 0.8404 | Recall: 0.8681 | F1: 0.8541
2026-01-14 12:10:55,440 - INFO - --------------------------------------------------
2026-01-14 12:10:55,443 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-01-14 12:11:05,016 - INFO - [Train] [79/90] | Loss: 0.2042 | Train Acc: 99.85%
2026-01-14 12:11:07,878 - INFO - [Valid] [79/90] | Loss: 0.5270 | Val Acc: 83.48%
2026-01-14 12:11:07,886 - INFO - [Metrics for 'abnormal'] | Precision: 0.8301 | Recall: 0.8089 | F1: 0.8194
2026-01-14 12:11:07,887 - INFO - [Metrics for 'normal'] | Precision: 0.8387 | Recall: 0.8571 | F1: 0.8478
2026-01-14 12:11:07,890 - INFO - --------------------------------------------------
2026-01-14 12:11:07,891 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-01-14 12:11:16,732 - INFO - [Train] [80/90] | Loss: 0.2064 | Train Acc: 99.63%
2026-01-14 12:11:19,672 - INFO - [Valid] [80/90] | Loss: 0.5258 | Val Acc: 84.66%
2026-01-14 12:11:19,687 - INFO - [Metrics for 'abnormal'] | Precision: 0.8431 | Recall: 0.8217 | F1: 0.8323
2026-01-14 12:11:19,687 - INFO - [Metrics for 'normal'] | Precision: 0.8495 | Recall: 0.8681 | F1: 0.8587
2026-01-14 12:11:19,691 - INFO - --------------------------------------------------
2026-01-14 12:11:19,693 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-01-14 12:11:28,855 - INFO - [Train] [81/90] | Loss: 0.2051 | Train Acc: 99.78%
2026-01-14 12:11:32,385 - INFO - [Valid] [81/90] | Loss: 0.5329 | Val Acc: 83.48%
2026-01-14 12:11:32,416 - INFO - [Metrics for 'abnormal'] | Precision: 0.8061 | Recall: 0.8471 | F1: 0.8261
2026-01-14 12:11:32,417 - INFO - [Metrics for 'normal'] | Precision: 0.8621 | Recall: 0.8242 | F1: 0.8427
2026-01-14 12:11:32,436 - INFO - --------------------------------------------------
2026-01-14 12:11:32,441 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-01-14 12:11:41,879 - INFO - [Train] [82/90] | Loss: 0.2093 | Train Acc: 99.48%
2026-01-14 12:11:44,312 - INFO - [Valid] [82/90] | Loss: 0.5198 | Val Acc: 84.66%
2026-01-14 12:11:44,321 - INFO - [Metrics for 'abnormal'] | Precision: 0.8344 | Recall: 0.8344 | F1: 0.8344
2026-01-14 12:11:44,321 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.8571 | F1: 0.8571
2026-01-14 12:11:44,324 - INFO - --------------------------------------------------
2026-01-14 12:11:44,326 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-01-14 12:11:54,617 - INFO - [Train] [83/90] | Loss: 0.2064 | Train Acc: 99.70%
2026-01-14 12:11:57,114 - INFO - [Valid] [83/90] | Loss: 0.5181 | Val Acc: 84.96%
2026-01-14 12:11:57,125 - INFO - [Metrics for 'abnormal'] | Precision: 0.8354 | Recall: 0.8408 | F1: 0.8381
2026-01-14 12:11:57,126 - INFO - [Metrics for 'normal'] | Precision: 0.8619 | Recall: 0.8571 | F1: 0.8595
2026-01-14 12:11:57,130 - INFO - --------------------------------------------------
2026-01-14 12:11:57,243 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-01-14 12:12:07,154 - INFO - [Train] [84/90] | Loss: 0.2039 | Train Acc: 99.78%
2026-01-14 12:12:09,882 - INFO - [Valid] [84/90] | Loss: 0.5271 | Val Acc: 84.37%
2026-01-14 12:12:09,910 - INFO - [Metrics for 'abnormal'] | Precision: 0.8291 | Recall: 0.8344 | F1: 0.8317
2026-01-14 12:12:09,910 - INFO - [Metrics for 'normal'] | Precision: 0.8564 | Recall: 0.8516 | F1: 0.8540
2026-01-14 12:12:09,919 - INFO - --------------------------------------------------
2026-01-14 12:12:09,924 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-01-14 12:12:19,490 - INFO - [Train] [85/90] | Loss: 0.2045 | Train Acc: 99.85%
2026-01-14 12:12:21,830 - INFO - [Valid] [85/90] | Loss: 0.5272 | Val Acc: 84.07%
2026-01-14 12:12:21,841 - INFO - [Metrics for 'abnormal'] | Precision: 0.8199 | Recall: 0.8408 | F1: 0.8302
2026-01-14 12:12:21,842 - INFO - [Metrics for 'normal'] | Precision: 0.8596 | Recall: 0.8407 | F1: 0.8500
2026-01-14 12:12:21,845 - INFO - --------------------------------------------------
2026-01-14 12:12:21,849 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-01-14 12:12:31,891 - INFO - [Train] [86/90] | Loss: 0.2231 | Train Acc: 98.81%
2026-01-14 12:12:34,726 - INFO - [Valid] [86/90] | Loss: 0.5255 | Val Acc: 85.25%
2026-01-14 12:12:34,738 - INFO - [Metrics for 'abnormal'] | Precision: 0.8497 | Recall: 0.8280 | F1: 0.8387
2026-01-14 12:12:34,738 - INFO - [Metrics for 'normal'] | Precision: 0.8548 | Recall: 0.8736 | F1: 0.8641
2026-01-14 12:12:34,752 - INFO - --------------------------------------------------
2026-01-14 12:12:34,757 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-01-14 12:12:43,815 - INFO - [Train] [87/90] | Loss: 0.2064 | Train Acc: 99.70%
2026-01-14 12:12:47,026 - INFO - [Valid] [87/90] | Loss: 0.5395 | Val Acc: 83.78%
2026-01-14 12:12:47,131 - INFO - [Metrics for 'abnormal'] | Precision: 0.8148 | Recall: 0.8408 | F1: 0.8276
2026-01-14 12:12:47,134 - INFO - [Metrics for 'normal'] | Precision: 0.8588 | Recall: 0.8352 | F1: 0.8468
2026-01-14 12:12:47,141 - INFO - --------------------------------------------------
2026-01-14 12:12:47,147 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-01-14 12:12:56,663 - INFO - [Train] [88/90] | Loss: 0.2058 | Train Acc: 99.78%
2026-01-14 12:12:58,812 - INFO - [Valid] [88/90] | Loss: 0.5393 | Val Acc: 83.78%
2026-01-14 12:12:58,824 - INFO - [Metrics for 'abnormal'] | Precision: 0.8355 | Recall: 0.8089 | F1: 0.8220
2026-01-14 12:12:58,824 - INFO - [Metrics for 'normal'] | Precision: 0.8396 | Recall: 0.8626 | F1: 0.8509
2026-01-14 12:12:58,829 - INFO - --------------------------------------------------
2026-01-14 12:12:58,831 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-01-14 12:13:07,982 - INFO - [Train] [89/90] | Loss: 0.2064 | Train Acc: 99.55%
2026-01-14 12:13:11,081 - INFO - [Valid] [89/90] | Loss: 0.5155 | Val Acc: 84.37%
2026-01-14 12:13:11,159 - INFO - [Metrics for 'abnormal'] | Precision: 0.8250 | Recall: 0.8408 | F1: 0.8328
2026-01-14 12:13:11,160 - INFO - [Metrics for 'normal'] | Precision: 0.8603 | Recall: 0.8462 | F1: 0.8532
2026-01-14 12:13:11,164 - INFO - --------------------------------------------------
2026-01-14 12:13:11,167 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-01-14 12:13:21,823 - INFO - [Train] [90/90] | Loss: 0.2084 | Train Acc: 99.48%
2026-01-14 12:13:25,513 - INFO - [Valid] [90/90] | Loss: 0.5223 | Val Acc: 83.19%
2026-01-14 12:13:25,526 - INFO - [Metrics for 'abnormal'] | Precision: 0.8289 | Recall: 0.8025 | F1: 0.8155
2026-01-14 12:13:25,530 - INFO - [Metrics for 'normal'] | Precision: 0.8342 | Recall: 0.8571 | F1: 0.8455
2026-01-14 12:13:25,542 - INFO - ==================================================
2026-01-14 12:13:25,543 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-01-14 12:13:25,545 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-01-14 12:13:25,545 - INFO - Baseline 모델 'efficientnet_b0'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:13:25,754 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-01-14 12:13:25,757 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:13:25,757 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:13:26,531 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737939453124999)에 맞춰 변경되었습니다.
2026-01-14 12:13:26,531 - INFO - ==================================================
2026-01-14 12:13:26,680 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/best_model.pth'
2026-01-14 12:13:26,681 - INFO - ==================================================
2026-01-14 12:13:26,681 - INFO - Test 모드를 시작합니다.
2026-01-14 12:13:27,001 - INFO - 연산량 (MACs): 0.0914 GMACs per sample
2026-01-14 12:13:27,001 - INFO - 연산량 (FLOPs): 0.1829 GFLOPs per sample
2026-01-14 12:13:27,002 - INFO - ==================================================
2026-01-14 12:13:27,002 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-01-14 12:13:28,822 - INFO - 샘플 당 평균 Forward Pass 시간: 9.84ms (std: 4.25ms), FPS: 114.39 (std: 32.91) (1개 샘플 x 100회 반복)
2026-01-14 12:13:28,822 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 98.43 MB
2026-01-14 12:13:28,822 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-01-14 12:13:32,443 - INFO - [Test] Loss: 0.3790 | Test Acc: 82.01%
2026-01-14 12:13:32,454 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 12:13:32,455 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 12:13:32,989 - INFO - ==================================================
2026-01-14 12:13:32,990 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/confusion_matrix_20260114_115524.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/confusion_matrix_20260114_115524.pdf'
2026-01-14 12:13:32,990 - INFO - ==================================================
2026-01-14 12:13:32,990 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-01-14 12:13:42,751 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/model_fp32_20260114_115524.onnx'에 저장되었습니다. (크기: 2.99 MB)
2026-01-14 12:13:43,233 - INFO - [Model Load] ONNX 모델(FP32) 로드 메모리: 2487.90 MB (증가량: 7.52 MB)
2026-01-14 12:13:43,233 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-01-14 12:13:48,539 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 43.24ms (std: 39.11ms)
2026-01-14 12:13:48,539 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 35.55 FPS (std: 21.65) (1개 샘플 x 100회 반복)
2026-01-14 12:13:48,539 - INFO - [Inference Only] ONNX 런타임 추론 중 최대 CPU 메모리: 2489.40 MB (순수 증가량: 1.50 MB)
2026-01-14 12:13:48,540 - INFO - [Total Process] ONNX 모델(FP32) 전체 메모리 사용량: 2489.40 MB (전체 증가량: 9.02 MB)
2026-01-14 12:13:58,451 - INFO - [Test (ONNX)] | Test Acc (ONNX): 82.01%
2026-01-14 12:13:58,483 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 12:13:58,486 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 12:13:59,385 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/val_acc.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/val_acc.pdf'
2026-01-14 12:13:59,934 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/train_val_acc.pdf'
2026-01-14 12:14:00,376 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/F1_normal.pdf'
2026-01-14 12:14:01,146 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/val_loss.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/val_loss.pdf'
2026-01-14 12:14:01,635 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/learning_rate.pdf'
2026-01-14 12:14:08,506 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/compile.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_l1_20260114_115524/graph_20260114_115524/compile.pdf'
