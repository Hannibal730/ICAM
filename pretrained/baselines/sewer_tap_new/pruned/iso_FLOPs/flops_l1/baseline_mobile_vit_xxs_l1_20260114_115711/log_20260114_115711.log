2026-01-14 11:57:11,689 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/log_20260114_115711.log'에 저장됩니다.
2026-01-14 11:57:11,695 - INFO - ==================================================
2026-01-14 11:57:11,696 - INFO - config.yaml:
2026-01-14 11:57:11,696 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_inference_dir: ./pretrained
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      train_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/cau/workspace/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  patch_size: 56
  stride: 56
  cnn_feature_extractor:
    name: efficientnet_b0_feat2
  featured_patch_dim: 24
  emb_dim: 24
  num_heads: 2
  num_decoder_layers: 2
  num_decoder_patches: 1
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: true
  save_attention: true
  num_plot_attention: 5
baseline:
  model_name: mobile_vit_xxs
  use_l1_pruning: true
  pruning_flops_target: 0.1829

2026-01-14 11:57:11,696 - INFO - ==================================================
2026-01-14 11:57:11,757 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA RTX PRO 6000 Blackwell Server Edition)
2026-01-14 11:57:11,758 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-01-14 11:57:11,758 - INFO - '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-01-14 11:57:11,769 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-01-14 11:57:11,770 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-01-14 11:57:11,771 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-01-14 11:57:11,771 - INFO - Baseline 모델 'mobile_vit_xxs'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 11:57:12,109 - INFO - timm 모델(mobile_vit_xxs)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-01-14 11:57:12,165 - INFO - ==================================================
2026-01-14 11:57:12,165 - INFO - 모델 파라미터 수:
2026-01-14 11:57:12,165 - INFO -   - 총 파라미터: 951,666 개
2026-01-14 11:57:12,165 - INFO -   - 학습 가능한 파라미터: 951,666 개
2026-01-14 11:57:12,165 - INFO - ================================================================================
2026-01-14 11:57:12,165 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-01-14 11:57:12,165 - INFO - ================================================================================
2026-01-14 11:57:12,165 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 11:57:12,167 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-01-14 11:57:12,167 - INFO - ==================================================
2026-01-14 11:57:12,167 - INFO - train 모드를 시작합니다.
2026-01-14 11:57:12,167 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 11:57:12,167 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 11:57:12,167 - INFO - --------------------------------------------------
2026-01-14 11:57:12,169 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-01-14 11:57:19,934 - INFO - [Train] [1/10] | Loss: 0.5204 | Train Acc: 78.50%
2026-01-14 11:57:22,731 - INFO - [Valid] [1/10] | Loss: 0.5378 | Val Acc: 81.42%
2026-01-14 11:57:22,744 - INFO - [Metrics for 'abnormal'] | Precision: 0.8013 | Recall: 0.7962 | F1: 0.7987
2026-01-14 11:57:22,745 - INFO - [Metrics for 'normal'] | Precision: 0.8251 | Recall: 0.8297 | F1: 0.8274
2026-01-14 11:57:22,788 - INFO - [Best Model Saved] (val loss: 0.5378) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 11:57:22,788 - INFO - --------------------------------------------------
2026-01-14 11:57:22,791 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-01-14 11:57:28,911 - INFO - [Train] [2/10] | Loss: 0.4607 | Train Acc: 83.18%
2026-01-14 11:57:30,702 - INFO - [Valid] [2/10] | Loss: 0.5210 | Val Acc: 82.89%
2026-01-14 11:57:30,714 - INFO - [Metrics for 'abnormal'] | Precision: 0.8235 | Recall: 0.8025 | F1: 0.8129
2026-01-14 11:57:30,715 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8516 | F1: 0.8424
2026-01-14 11:57:30,767 - INFO - [Best Model Saved] (val loss: 0.5210) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 11:57:30,767 - INFO - --------------------------------------------------
2026-01-14 11:57:30,770 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-01-14 11:57:36,586 - INFO - [Train] [3/10] | Loss: 0.4284 | Train Acc: 86.16%
2026-01-14 11:57:38,095 - INFO - [Valid] [3/10] | Loss: 0.5150 | Val Acc: 81.12%
2026-01-14 11:57:38,107 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-01-14 11:57:38,108 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-01-14 11:57:38,153 - INFO - [Best Model Saved] (val loss: 0.5150) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 11:57:38,153 - INFO - --------------------------------------------------
2026-01-14 11:57:38,156 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-01-14 11:57:43,757 - INFO - [Train] [4/10] | Loss: 0.3988 | Train Acc: 87.35%
2026-01-14 11:57:45,144 - INFO - [Valid] [4/10] | Loss: 0.5051 | Val Acc: 80.83%
2026-01-14 11:57:45,153 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-01-14 11:57:45,153 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-01-14 11:57:45,198 - INFO - [Best Model Saved] (val loss: 0.5051) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 11:57:45,198 - INFO - --------------------------------------------------
2026-01-14 11:57:45,200 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-01-14 11:57:52,264 - INFO - [Train] [5/10] | Loss: 0.3765 | Train Acc: 89.43%
2026-01-14 11:57:54,107 - INFO - [Valid] [5/10] | Loss: 0.5375 | Val Acc: 81.42%
2026-01-14 11:57:54,119 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-01-14 11:57:54,120 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-01-14 11:57:54,124 - INFO - --------------------------------------------------
2026-01-14 11:57:54,128 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-01-14 11:58:01,251 - INFO - [Train] [6/10] | Loss: 0.3491 | Train Acc: 91.37%
2026-01-14 11:58:03,291 - INFO - [Valid] [6/10] | Loss: 0.5140 | Val Acc: 79.35%
2026-01-14 11:58:03,301 - INFO - [Metrics for 'abnormal'] | Precision: 0.7702 | Recall: 0.7898 | F1: 0.7799
2026-01-14 11:58:03,302 - INFO - [Metrics for 'normal'] | Precision: 0.8146 | Recall: 0.7967 | F1: 0.8056
2026-01-14 11:58:03,306 - INFO - --------------------------------------------------
2026-01-14 11:58:03,309 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-01-14 11:58:10,201 - INFO - [Train] [7/10] | Loss: 0.3059 | Train Acc: 94.79%
2026-01-14 11:58:13,022 - INFO - [Valid] [7/10] | Loss: 0.5175 | Val Acc: 82.01%
2026-01-14 11:58:13,033 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-01-14 11:58:13,034 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-01-14 11:58:13,038 - INFO - --------------------------------------------------
2026-01-14 11:58:13,041 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-01-14 11:58:19,581 - INFO - [Train] [8/10] | Loss: 0.2873 | Train Acc: 95.39%
2026-01-14 11:58:21,650 - INFO - [Valid] [8/10] | Loss: 0.5463 | Val Acc: 79.94%
2026-01-14 11:58:21,659 - INFO - [Metrics for 'abnormal'] | Precision: 0.7459 | Recall: 0.8599 | F1: 0.7988
2026-01-14 11:58:21,660 - INFO - [Metrics for 'normal'] | Precision: 0.8608 | Recall: 0.7473 | F1: 0.8000
2026-01-14 11:58:21,663 - INFO - --------------------------------------------------
2026-01-14 11:58:21,665 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-01-14 11:58:27,706 - INFO - [Train] [9/10] | Loss: 0.2639 | Train Acc: 96.88%
2026-01-14 11:58:30,101 - INFO - [Valid] [9/10] | Loss: 0.5161 | Val Acc: 82.89%
2026-01-14 11:58:30,114 - INFO - [Metrics for 'abnormal'] | Precision: 0.7964 | Recall: 0.8471 | F1: 0.8210
2026-01-14 11:58:30,115 - INFO - [Metrics for 'normal'] | Precision: 0.8605 | Recall: 0.8132 | F1: 0.8362
2026-01-14 11:58:30,119 - INFO - --------------------------------------------------
2026-01-14 11:58:30,124 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-01-14 11:58:36,226 - INFO - [Train] [10/10] | Loss: 0.2587 | Train Acc: 97.54%
2026-01-14 11:58:38,353 - INFO - [Valid] [10/10] | Loss: 0.5208 | Val Acc: 82.01%
2026-01-14 11:58:38,362 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-01-14 11:58:38,362 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-01-14 11:58:38,366 - INFO - ================================================================================
2026-01-14 11:58:38,367 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-01-14 11:58:38,367 - INFO - ================================================================================
2026-01-14 11:58:38,473 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'을(를) 불러왔습니다.
2026-01-14 11:58:38,474 - INFO - ================================================================================
2026-01-14 11:58:38,474 - INFO - 목표 FLOPs (0.1829 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-01-14 11:58:38,576 - INFO - 원본 모델 FLOPs: 0.5384 GFLOPs
2026-01-14 11:58:38,699 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:38,700 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:38,701 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:39,412 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-01-14 11:58:39,412 - INFO - ==================================================
2026-01-14 11:58:39,500 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.1747 GFLOPs (감소율: 67.56%)
2026-01-14 11:58:39,565 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:39,565 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:39,567 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:40,159 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.2475)에 맞춰 변경되었습니다.
2026-01-14 11:58:40,159 - INFO - ==================================================
2026-01-14 11:58:40,249 - INFO -   [탐색  2] 희소도: 0.2475 -> FLOPs: 0.3329 GFLOPs (감소율: 38.17%)
2026-01-14 11:58:40,315 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:40,316 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:40,317 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:41,167 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.37124999999999997)에 맞춰 변경되었습니다.
2026-01-14 11:58:41,167 - INFO - ==================================================
2026-01-14 11:58:41,256 - INFO -   [탐색  3] 희소도: 0.3712 -> FLOPs: 0.2479 GFLOPs (감소율: 53.96%)
2026-01-14 11:58:41,321 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:41,321 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:41,322 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:41,964 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.433125)에 맞춰 변경되었습니다.
2026-01-14 11:58:41,964 - INFO - ==================================================
2026-01-14 11:58:42,068 - INFO -   [탐색  4] 희소도: 0.4331 -> FLOPs: 0.2093 GFLOPs (감소율: 61.13%)
2026-01-14 11:58:42,200 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:42,201 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:42,202 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:42,933 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4640625)에 맞춰 변경되었습니다.
2026-01-14 11:58:42,933 - INFO - ==================================================
2026-01-14 11:58:43,103 - INFO -   [탐색  5] 희소도: 0.4641 -> FLOPs: 0.1880 GFLOPs (감소율: 65.08%)
2026-01-14 11:58:43,172 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:43,173 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:43,174 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:43,899 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47953124999999996)에 맞춰 변경되었습니다.
2026-01-14 11:58:43,899 - INFO - ==================================================
2026-01-14 11:58:44,005 - INFO -   [탐색  6] 희소도: 0.4795 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-01-14 11:58:44,102 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:44,102 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:44,105 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:45,224 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.471796875)에 맞춰 변경되었습니다.
2026-01-14 11:58:45,225 - INFO - ==================================================
2026-01-14 11:58:45,313 - INFO -   [탐색  7] 희소도: 0.4718 -> FLOPs: 0.1838 GFLOPs (감소율: 65.85%)
2026-01-14 11:58:45,402 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:45,403 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:45,404 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:46,108 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4756640625)에 맞춰 변경되었습니다.
2026-01-14 11:58:46,109 - INFO - ==================================================
2026-01-14 11:58:46,175 - INFO -   [탐색  8] 희소도: 0.4757 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:58:46,239 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:46,240 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:46,241 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:46,844 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47373046875)에 맞춰 변경되었습니다.
2026-01-14 11:58:46,844 - INFO - ==================================================
2026-01-14 11:58:46,909 - INFO -   [탐색  9] 희소도: 0.4737 -> FLOPs: 0.1838 GFLOPs (감소율: 65.85%)
2026-01-14 11:58:46,975 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:46,975 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:46,976 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:47,618 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47469726562500003)에 맞춰 변경되었습니다.
2026-01-14 11:58:47,619 - INFO - ==================================================
2026-01-14 11:58:47,697 - INFO -   [탐색 10] 희소도: 0.4747 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:58:47,782 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:47,783 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:47,785 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:48,541 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47518066406250004)에 맞춰 변경되었습니다.
2026-01-14 11:58:48,541 - INFO - ==================================================
2026-01-14 11:58:48,612 - INFO -   [탐색 11] 희소도: 0.4752 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:58:48,717 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:48,718 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:48,720 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:49,744 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47493896484375003)에 맞춰 변경되었습니다.
2026-01-14 11:58:49,744 - INFO - ==================================================
2026-01-14 11:58:50,109 - INFO -   [탐색 12] 희소도: 0.4749 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:58:50,221 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:50,222 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:50,223 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:51,089 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47505981445312506)에 맞춰 변경되었습니다.
2026-01-14 11:58:51,090 - INFO - ==================================================
2026-01-14 11:58:51,223 - INFO -   [탐색 13] 희소도: 0.4751 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:58:51,350 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:51,351 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:51,353 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:52,491 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749993896484376)에 맞춰 변경되었습니다.
2026-01-14 11:58:52,492 - INFO - ==================================================
2026-01-14 11:58:52,560 - INFO -   [탐색 14] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:58:52,627 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:52,628 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:52,629 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:53,336 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750296020507813)에 맞춰 변경되었습니다.
2026-01-14 11:58:53,336 - INFO - ==================================================
2026-01-14 11:58:53,401 - INFO -   [탐색 15] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:58:53,467 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:53,467 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:53,468 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:54,094 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750144958496094)에 맞춰 변경되었습니다.
2026-01-14 11:58:54,094 - INFO - ==================================================
2026-01-14 11:58:54,156 - INFO -   [탐색 16] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:58:54,532 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:54,533 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:54,534 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:55,233 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750069427490235)에 맞춰 변경되었습니다.
2026-01-14 11:58:55,234 - INFO - ==================================================
2026-01-14 11:58:55,317 - INFO -   [탐색 17] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:58:55,405 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:55,406 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:55,407 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:56,196 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750031661987305)에 맞춰 변경되었습니다.
2026-01-14 11:58:56,196 - INFO - ==================================================
2026-01-14 11:58:56,278 - INFO -   [탐색 18] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:58:56,366 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:56,366 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:56,367 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:57,047 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47500127792358404)에 맞춰 변경되었습니다.
2026-01-14 11:58:57,048 - INFO - ==================================================
2026-01-14 11:58:57,124 - INFO -   [탐색 19] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:58:57,194 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:57,195 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:57,196 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:57,957 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750003337860108)에 맞춰 변경되었습니다.
2026-01-14 11:58:57,958 - INFO - ==================================================
2026-01-14 11:58:58,068 - INFO -   [탐색 20] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:58:58,242 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:58:58,243 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:58:58,245 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:58:59,874 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749998617172242)에 맞춰 변경되었습니다.
2026-01-14 11:58:59,874 - INFO - ==================================================
2026-01-14 11:58:59,959 - INFO -   [탐색 21] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:00,041 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:00,041 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:00,042 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:00,984 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000977516175)에 맞춰 변경되었습니다.
2026-01-14 11:59:00,984 - INFO - ==================================================
2026-01-14 11:59:01,072 - INFO -   [탐색 22] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:01,159 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:01,160 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:01,161 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:01,843 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749999797344209)에 맞춰 변경되었습니다.
2026-01-14 11:59:01,844 - INFO - ==================================================
2026-01-14 11:59:01,913 - INFO -   [탐색 23] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:01,999 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:02,000 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:02,002 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:02,805 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000387430192)에 맞춰 변경되었습니다.
2026-01-14 11:59:02,806 - INFO - ==================================================
2026-01-14 11:59:02,870 - INFO -   [탐색 24] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:02,936 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:02,936 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:02,937 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:03,566 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47500000923872004)에 맞춰 변경되었습니다.
2026-01-14 11:59:03,566 - INFO - ==================================================
2026-01-14 11:59:03,630 - INFO -   [탐색 25] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:03,695 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:03,695 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:03,696 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:05,036 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749999944865705)에 맞춰 변경되었습니다.
2026-01-14 11:59:05,037 - INFO - ==================================================
2026-01-14 11:59:05,115 - INFO -   [탐색 26] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:05,191 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:05,191 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:05,192 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:05,950 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47500000186264524)에 맞춰 변경되었습니다.
2026-01-14 11:59:05,951 - INFO - ==================================================
2026-01-14 11:59:06,026 - INFO -   [탐색 27] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:06,100 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:06,101 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:06,102 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:06,965 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47499999817460786)에 맞춰 변경되었습니다.
2026-01-14 11:59:06,966 - INFO - ==================================================
2026-01-14 11:59:07,053 - INFO -   [탐색 28] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:07,145 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:07,146 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:07,147 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:08,202 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000000186265)에 맞춰 변경되었습니다.
2026-01-14 11:59:08,203 - INFO - ==================================================
2026-01-14 11:59:08,325 - INFO -   [탐색 29] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:08,403 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:08,403 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:08,405 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:09,124 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47499999909661716)에 맞춰 변경되었습니다.
2026-01-14 11:59:09,125 - INFO - ==================================================
2026-01-14 11:59:09,187 - INFO -   [탐색 30] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:09,511 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:09,512 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:09,513 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:10,201 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47499999955762184)에 맞춰 변경되었습니다.
2026-01-14 11:59:10,202 - INFO - ==================================================
2026-01-14 11:59:10,274 - INFO -   [탐색 31] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:10,423 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:10,426 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:10,428 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:11,080 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749999997881242)에 맞춰 변경되었습니다.
2026-01-14 11:59:11,080 - INFO - ==================================================
2026-01-14 11:59:11,144 - INFO -   [탐색 32] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:11,212 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:11,212 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:11,213 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:11,895 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749999999033754)에 맞춰 변경되었습니다.
2026-01-14 11:59:11,895 - INFO - ==================================================
2026-01-14 11:59:11,956 - INFO -   [탐색 33] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:12,021 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:12,022 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:12,023 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:12,819 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47499999996100095)에 맞춰 변경되었습니다.
2026-01-14 11:59:12,819 - INFO - ==================================================
2026-01-14 11:59:13,202 - INFO -   [탐색 34] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:13,344 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:13,344 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:13,346 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:14,771 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47499999998981374)에 맞춰 변경되었습니다.
2026-01-14 11:59:14,772 - INFO - ==================================================
2026-01-14 11:59:14,900 - INFO -   [탐색 35] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:14,971 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:14,972 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:14,973 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:15,980 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47500000000422016)에 맞춰 변경되었습니다.
2026-01-14 11:59:15,980 - INFO - ==================================================
2026-01-14 11:59:16,064 - INFO -   [탐색 36] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:16,163 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:16,163 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:16,165 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:16,964 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749999999970169)에 맞춰 변경되었습니다.
2026-01-14 11:59:16,965 - INFO - ==================================================
2026-01-14 11:59:17,028 - INFO -   [탐색 37] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:17,094 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:17,095 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:17,096 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:17,766 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47500000000061854)에 맞춰 변경되었습니다.
2026-01-14 11:59:17,767 - INFO - ==================================================
2026-01-14 11:59:17,836 - INFO -   [탐색 38] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:17,918 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:17,919 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:17,920 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:18,748 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749999999988177)에 맞춰 변경되었습니다.
2026-01-14 11:59:18,749 - INFO - ==================================================
2026-01-14 11:59:18,834 - INFO -   [탐색 39] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:18,926 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:18,928 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:18,929 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:20,055 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749999999997181)에 맞춰 변경되었습니다.
2026-01-14 11:59:20,056 - INFO - ==================================================
2026-01-14 11:59:20,139 - INFO -   [탐색 40] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:20,240 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:20,244 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:20,247 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:21,063 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000000001683)에 맞춰 변경되었습니다.
2026-01-14 11:59:21,064 - INFO - ==================================================
2026-01-14 11:59:21,151 - INFO -   [탐색 41] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:21,248 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:21,249 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:21,251 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:22,088 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749999999999432)에 맞춰 변경되었습니다.
2026-01-14 11:59:22,088 - INFO - ==================================================
2026-01-14 11:59:22,171 - INFO -   [탐색 42] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:22,274 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:22,275 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:22,276 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:23,111 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000000000557)에 맞춰 변경되었습니다.
2026-01-14 11:59:23,112 - INFO - ==================================================
2026-01-14 11:59:23,195 - INFO -   [탐색 43] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:23,294 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:23,295 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:23,297 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:24,116 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4749999999999994)에 맞춰 변경되었습니다.
2026-01-14 11:59:24,116 - INFO - ==================================================
2026-01-14 11:59:24,206 - INFO -   [탐색 44] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:24,595 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:24,595 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:24,597 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:25,821 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47500000000002757)에 맞춰 변경되었습니다.
2026-01-14 11:59:25,822 - INFO - ==================================================
2026-01-14 11:59:25,904 - INFO -   [탐색 45] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:26,024 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:26,025 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:26,026 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:26,756 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000000000135)에 맞춰 변경되었습니다.
2026-01-14 11:59:26,757 - INFO - ==================================================
2026-01-14 11:59:26,822 - INFO -   [탐색 46] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:26,890 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:26,891 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:26,892 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:27,759 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000000000065)에 맞춰 변경되었습니다.
2026-01-14 11:59:27,760 - INFO - ==================================================
2026-01-14 11:59:27,848 - INFO -   [탐색 47] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:27,938 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:27,939 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:27,940 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:28,781 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475000000000003)에 맞춰 변경되었습니다.
2026-01-14 11:59:28,781 - INFO - ==================================================
2026-01-14 11:59:28,867 - INFO -   [탐색 48] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:28,955 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:28,955 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:28,956 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:30,008 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000000000012)에 맞춰 변경되었습니다.
2026-01-14 11:59:30,009 - INFO - ==================================================
2026-01-14 11:59:30,073 - INFO -   [탐색 49] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:30,155 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:30,155 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:30,157 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:30,813 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000000000003)에 맞춰 변경되었습니다.
2026-01-14 11:59:30,813 - INFO - ==================================================
2026-01-14 11:59:30,878 - INFO -   [탐색 50] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:30,942 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:30,943 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:30,944 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:31,649 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47499999999999987)에 맞춰 변경되었습니다.
2026-01-14 11:59:31,650 - INFO - ==================================================
2026-01-14 11:59:31,727 - INFO -   [탐색 51] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:31,811 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:31,812 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:31,813 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:32,572 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4750000000000001)에 맞춰 변경되었습니다.
2026-01-14 11:59:32,572 - INFO - ==================================================
2026-01-14 11:59:32,655 - INFO -   [탐색 52] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:32,744 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:32,745 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:32,746 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:33,486 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:33,486 - INFO - ==================================================
2026-01-14 11:59:33,559 - INFO -   [탐색 53] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:33,631 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:33,632 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:33,633 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:34,696 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47500000000000003)에 맞춰 변경되었습니다.
2026-01-14 11:59:34,696 - INFO - ==================================================
2026-01-14 11:59:34,771 - INFO -   [탐색 54] 희소도: 0.4750 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-01-14 11:59:34,841 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:34,842 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:34,843 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:35,771 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:35,771 - INFO - ==================================================
2026-01-14 11:59:35,889 - INFO -   [탐색 55] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:35,957 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:35,958 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:35,959 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:37,009 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:37,010 - INFO - ==================================================
2026-01-14 11:59:37,151 - INFO -   [탐색 56] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:37,259 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:37,260 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:37,263 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:37,982 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:37,983 - INFO - ==================================================
2026-01-14 11:59:38,056 - INFO -   [탐색 57] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:38,136 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:38,136 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:38,138 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:39,145 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:39,145 - INFO - ==================================================
2026-01-14 11:59:39,209 - INFO -   [탐색 58] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:39,578 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:39,579 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:39,580 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:40,337 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:40,338 - INFO - ==================================================
2026-01-14 11:59:40,433 - INFO -   [탐색 59] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:40,528 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:40,529 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:40,531 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:41,602 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:41,602 - INFO - ==================================================
2026-01-14 11:59:41,816 - INFO -   [탐색 60] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:41,902 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:41,903 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:41,904 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:43,152 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:43,154 - INFO - ==================================================
2026-01-14 11:59:43,310 - INFO -   [탐색 61] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:43,415 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:43,416 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:43,417 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:44,213 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:44,214 - INFO - ==================================================
2026-01-14 11:59:44,294 - INFO -   [탐색 62] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:44,363 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:44,364 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:44,365 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:45,427 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:45,427 - INFO - ==================================================
2026-01-14 11:59:45,511 - INFO -   [탐색 63] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:45,602 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:45,602 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:45,604 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:46,362 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:46,362 - INFO - ==================================================
2026-01-14 11:59:46,415 - INFO -   [탐색 64] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:46,471 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:46,471 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:46,472 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:47,095 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:47,095 - INFO - ==================================================
2026-01-14 11:59:47,162 - INFO -   [탐색 65] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:47,242 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:47,242 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:47,243 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:48,409 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:48,410 - INFO - ==================================================
2026-01-14 11:59:48,498 - INFO -   [탐색 66] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:48,596 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:48,597 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:48,599 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:50,148 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:50,149 - INFO - ==================================================
2026-01-14 11:59:50,429 - INFO -   [탐색 67] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:51,194 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:51,195 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:51,196 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:52,878 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:52,879 - INFO - ==================================================
2026-01-14 11:59:53,184 - INFO -   [탐색 68] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:53,306 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:53,306 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:53,310 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:54,184 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:54,184 - INFO - ==================================================
2026-01-14 11:59:54,265 - INFO -   [탐색 69] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:54,443 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:54,443 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:54,444 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:55,510 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:55,510 - INFO - ==================================================
2026-01-14 11:59:55,575 - INFO -   [탐색 70] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:55,660 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:55,661 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:55,662 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:56,388 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:56,389 - INFO - ==================================================
2026-01-14 11:59:56,472 - INFO -   [탐색 71] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:56,559 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:56,560 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:56,561 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:57,551 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:57,552 - INFO - ==================================================
2026-01-14 11:59:57,633 - INFO -   [탐색 72] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:57,716 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:57,717 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:57,718 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 11:59:58,623 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 11:59:58,623 - INFO - ==================================================
2026-01-14 11:59:58,705 - INFO -   [탐색 73] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 11:59:58,902 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 11:59:58,902 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 11:59:58,904 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:00,162 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:00,162 - INFO - ==================================================
2026-01-14 12:00:00,254 - INFO -   [탐색 74] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:00,329 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:00,329 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:00,330 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:01,433 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:01,434 - INFO - ==================================================
2026-01-14 12:00:01,526 - INFO -   [탐색 75] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:01,679 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:01,680 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:01,682 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:03,294 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:03,294 - INFO - ==================================================
2026-01-14 12:00:03,369 - INFO -   [탐색 76] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:03,442 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:03,442 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:03,444 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:04,566 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:04,566 - INFO - ==================================================
2026-01-14 12:00:04,630 - INFO -   [탐색 77] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:04,716 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:04,716 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:04,718 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:05,546 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:05,547 - INFO - ==================================================
2026-01-14 12:00:05,633 - INFO -   [탐색 78] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:05,724 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:05,725 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:05,727 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:06,411 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:06,411 - INFO - ==================================================
2026-01-14 12:00:06,593 - INFO -   [탐색 79] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:06,662 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:06,663 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:06,664 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:07,536 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:07,536 - INFO - ==================================================
2026-01-14 12:00:07,637 - INFO -   [탐색 80] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:07,741 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:07,742 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:07,743 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:09,219 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:09,220 - INFO - ==================================================
2026-01-14 12:00:09,297 - INFO -   [탐색 81] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:09,381 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:09,382 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:09,383 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:10,027 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:10,027 - INFO - ==================================================
2026-01-14 12:00:10,208 - INFO -   [탐색 82] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:10,469 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:10,472 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:10,473 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:11,327 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:11,328 - INFO - ==================================================
2026-01-14 12:00:11,401 - INFO -   [탐색 83] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:11,468 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:11,469 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:11,470 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:12,803 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:12,804 - INFO - ==================================================
2026-01-14 12:00:12,929 - INFO -   [탐색 84] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:13,010 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:13,011 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:13,012 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:13,784 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:13,785 - INFO - ==================================================
2026-01-14 12:00:13,867 - INFO -   [탐색 85] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:14,286 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:14,287 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:14,288 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:15,391 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:15,392 - INFO - ==================================================
2026-01-14 12:00:15,458 - INFO -   [탐색 86] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:15,528 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:15,529 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:15,530 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:16,491 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:16,492 - INFO - ==================================================
2026-01-14 12:00:16,576 - INFO -   [탐색 87] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:16,651 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:16,652 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:16,654 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:17,339 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:17,340 - INFO - ==================================================
2026-01-14 12:00:17,404 - INFO -   [탐색 88] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:17,471 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:17,471 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:17,472 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:18,206 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:18,206 - INFO - ==================================================
2026-01-14 12:00:18,278 - INFO -   [탐색 89] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:18,364 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:18,364 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:18,365 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:19,632 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:19,633 - INFO - ==================================================
2026-01-14 12:00:19,699 - INFO -   [탐색 90] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:19,896 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:19,897 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:19,898 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:20,776 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:20,778 - INFO - ==================================================
2026-01-14 12:00:20,935 - INFO -   [탐색 91] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:21,015 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:21,015 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:21,016 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:21,652 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:21,653 - INFO - ==================================================
2026-01-14 12:00:21,722 - INFO -   [탐색 92] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:21,788 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:21,788 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:21,790 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:22,521 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:22,522 - INFO - ==================================================
2026-01-14 12:00:22,621 - INFO -   [탐색 93] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:22,754 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:22,755 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:22,757 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:23,662 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:23,662 - INFO - ==================================================
2026-01-14 12:00:23,730 - INFO -   [탐색 94] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:23,804 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:23,804 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:23,806 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:24,857 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:24,858 - INFO - ==================================================
2026-01-14 12:00:24,952 - INFO -   [탐색 95] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:25,043 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:25,044 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:25,045 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:26,104 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:26,105 - INFO - ==================================================
2026-01-14 12:00:26,189 - INFO -   [탐색 96] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:26,280 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:26,281 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:26,282 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:27,841 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:27,842 - INFO - ==================================================
2026-01-14 12:00:27,912 - INFO -   [탐색 97] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:27,998 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:27,998 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:28,000 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:28,803 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:28,804 - INFO - ==================================================
2026-01-14 12:00:28,907 - INFO -   [탐색 98] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:29,047 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:29,048 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:29,049 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:29,693 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:29,694 - INFO - ==================================================
2026-01-14 12:00:29,765 - INFO -   [탐색 99] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:30,138 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:30,139 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:30,140 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:31,191 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.475)에 맞춰 변경되었습니다.
2026-01-14 12:00:31,202 - INFO - ==================================================
2026-01-14 12:00:31,357 - INFO -   [탐색 100] 희소도: 0.4750 -> FLOPs: 0.1838 GFLOPs (감소율: 65.86%)
2026-01-14 12:00:31,359 - INFO - 탐색 완료. 목표 FLOPs(0.1829)에 가장 근접한 최적 희소도는 0.4757 입니다.
2026-01-14 12:00:31,360 - INFO - ================================================================================
2026-01-14 12:00:31,367 - INFO - 계산된 Pruning 정보(희소도: 0.4757)를 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/pruning_info.yaml'에 저장했습니다.
2026-01-14 12:00:31,453 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-01-14 12:00:31,630 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:00:31,630 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:00:31,631 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:00:32,458 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4756640625)에 맞춰 변경되었습니다.
2026-01-14 12:00:32,459 - INFO - ==================================================
2026-01-14 12:00:32,462 - INFO - ==================================================
2026-01-14 12:00:32,463 - INFO - 모델 파라미터 수:
2026-01-14 12:00:32,463 - INFO -   - 총 파라미터: 320,767 개
2026-01-14 12:00:32,463 - INFO -   - 학습 가능한 파라미터: 320,767 개
2026-01-14 12:00:32,542 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-01-14 12:00:32,700 - INFO - FLOPs가 0.5384 GFLOPs에서 0.1826 GFLOPs로 감소했습니다 (감소율: 66.08%).
2026-01-14 12:00:32,701 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-01-14 12:00:32,701 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:00:32,703 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-01-14 12:00:32,703 - INFO - ==================================================
2026-01-14 12:00:32,703 - INFO - train 모드를 시작합니다.
2026-01-14 12:00:32,703 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:00:32,704 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:00:32,704 - INFO - --------------------------------------------------
2026-01-14 12:00:32,706 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-01-14 12:00:42,041 - INFO - [Train] [11/90] | Loss: 0.5150 | Train Acc: 79.24%
2026-01-14 12:00:44,178 - INFO - [Valid] [11/90] | Loss: 0.5349 | Val Acc: 78.76%
2026-01-14 12:00:44,190 - INFO - [Metrics for 'abnormal'] | Precision: 0.7401 | Recall: 0.8344 | F1: 0.7844
2026-01-14 12:00:44,191 - INFO - [Metrics for 'normal'] | Precision: 0.8395 | Recall: 0.7473 | F1: 0.7907
2026-01-14 12:00:44,255 - INFO - [Best Model Saved] (val loss: 0.5349) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 12:00:44,256 - INFO - --------------------------------------------------
2026-01-14 12:00:44,259 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-01-14 12:00:53,128 - INFO - [Train] [12/90] | Loss: 0.4686 | Train Acc: 83.63%
2026-01-14 12:00:55,912 - INFO - [Valid] [12/90] | Loss: 0.5377 | Val Acc: 79.35%
2026-01-14 12:00:55,924 - INFO - [Metrics for 'abnormal'] | Precision: 0.7919 | Recall: 0.7516 | F1: 0.7712
2026-01-14 12:00:55,924 - INFO - [Metrics for 'normal'] | Precision: 0.7947 | Recall: 0.8297 | F1: 0.8118
2026-01-14 12:00:55,928 - INFO - --------------------------------------------------
2026-01-14 12:00:55,932 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-01-14 12:01:05,859 - INFO - [Train] [13/90] | Loss: 0.4542 | Train Acc: 83.93%
2026-01-14 12:01:08,135 - INFO - [Valid] [13/90] | Loss: 0.5754 | Val Acc: 76.99%
2026-01-14 12:01:08,145 - INFO - [Metrics for 'abnormal'] | Precision: 0.7232 | Recall: 0.8153 | F1: 0.7665
2026-01-14 12:01:08,145 - INFO - [Metrics for 'normal'] | Precision: 0.8210 | Recall: 0.7308 | F1: 0.7733
2026-01-14 12:01:08,151 - INFO - --------------------------------------------------
2026-01-14 12:01:08,153 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-01-14 12:01:16,712 - INFO - [Train] [14/90] | Loss: 0.4278 | Train Acc: 84.23%
2026-01-14 12:01:19,176 - INFO - [Valid] [14/90] | Loss: 0.5751 | Val Acc: 76.70%
2026-01-14 12:01:19,191 - INFO - [Metrics for 'abnormal'] | Precision: 0.7868 | Recall: 0.6815 | F1: 0.7304
2026-01-14 12:01:19,195 - INFO - [Metrics for 'normal'] | Precision: 0.7537 | Recall: 0.8407 | F1: 0.7948
2026-01-14 12:01:19,200 - INFO - --------------------------------------------------
2026-01-14 12:01:19,204 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-01-14 12:01:27,851 - INFO - [Train] [15/90] | Loss: 0.4346 | Train Acc: 84.75%
2026-01-14 12:01:30,296 - INFO - [Valid] [15/90] | Loss: 0.5328 | Val Acc: 78.17%
2026-01-14 12:01:30,308 - INFO - [Metrics for 'abnormal'] | Precision: 0.7345 | Recall: 0.8280 | F1: 0.7784
2026-01-14 12:01:30,308 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7418 | F1: 0.7849
2026-01-14 12:01:30,354 - INFO - [Best Model Saved] (val loss: 0.5328) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 12:01:30,355 - INFO - --------------------------------------------------
2026-01-14 12:01:30,357 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-01-14 12:01:39,663 - INFO - [Train] [16/90] | Loss: 0.4136 | Train Acc: 86.83%
2026-01-14 12:01:42,697 - INFO - [Valid] [16/90] | Loss: 0.5268 | Val Acc: 81.12%
2026-01-14 12:01:42,710 - INFO - [Metrics for 'abnormal'] | Precision: 0.7853 | Recall: 0.8153 | F1: 0.8000
2026-01-14 12:01:42,711 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8077 | F1: 0.8212
2026-01-14 12:01:42,769 - INFO - [Best Model Saved] (val loss: 0.5268) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 12:01:42,770 - INFO - --------------------------------------------------
2026-01-14 12:01:42,773 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-01-14 12:01:52,157 - INFO - [Train] [17/90] | Loss: 0.3827 | Train Acc: 88.84%
2026-01-14 12:01:55,114 - INFO - [Valid] [17/90] | Loss: 0.5248 | Val Acc: 79.35%
2026-01-14 12:01:55,126 - INFO - [Metrics for 'abnormal'] | Precision: 0.8175 | Recall: 0.7134 | F1: 0.7619
2026-01-14 12:01:55,126 - INFO - [Metrics for 'normal'] | Precision: 0.7772 | Recall: 0.8626 | F1: 0.8177
2026-01-14 12:01:55,257 - INFO - [Best Model Saved] (val loss: 0.5248) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 12:01:55,258 - INFO - --------------------------------------------------
2026-01-14 12:01:55,261 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-01-14 12:02:04,717 - INFO - [Train] [18/90] | Loss: 0.3727 | Train Acc: 89.73%
2026-01-14 12:02:07,023 - INFO - [Valid] [18/90] | Loss: 0.5372 | Val Acc: 77.88%
2026-01-14 12:02:07,038 - INFO - [Metrics for 'abnormal'] | Precision: 0.7733 | Recall: 0.7389 | F1: 0.7557
2026-01-14 12:02:07,038 - INFO - [Metrics for 'normal'] | Precision: 0.7831 | Recall: 0.8132 | F1: 0.7978
2026-01-14 12:02:07,043 - INFO - --------------------------------------------------
2026-01-14 12:02:07,047 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-01-14 12:02:15,810 - INFO - [Train] [19/90] | Loss: 0.3519 | Train Acc: 90.77%
2026-01-14 12:02:18,755 - INFO - [Valid] [19/90] | Loss: 0.4822 | Val Acc: 81.42%
2026-01-14 12:02:18,765 - INFO - [Metrics for 'abnormal'] | Precision: 0.7733 | Recall: 0.8471 | F1: 0.8085
2026-01-14 12:02:18,766 - INFO - [Metrics for 'normal'] | Precision: 0.8563 | Recall: 0.7857 | F1: 0.8195
2026-01-14 12:02:18,806 - INFO - [Best Model Saved] (val loss: 0.4822) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 12:02:18,806 - INFO - --------------------------------------------------
2026-01-14 12:02:18,808 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-01-14 12:02:27,838 - INFO - [Train] [20/90] | Loss: 0.3276 | Train Acc: 93.15%
2026-01-14 12:02:30,717 - INFO - [Valid] [20/90] | Loss: 0.5109 | Val Acc: 80.53%
2026-01-14 12:02:30,726 - INFO - [Metrics for 'abnormal'] | Precision: 0.8182 | Recall: 0.7452 | F1: 0.7800
2026-01-14 12:02:30,727 - INFO - [Metrics for 'normal'] | Precision: 0.7959 | Recall: 0.8571 | F1: 0.8254
2026-01-14 12:02:30,729 - INFO - --------------------------------------------------
2026-01-14 12:02:30,731 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-01-14 12:02:39,415 - INFO - [Train] [21/90] | Loss: 0.3290 | Train Acc: 92.93%
2026-01-14 12:02:42,159 - INFO - [Valid] [21/90] | Loss: 0.4795 | Val Acc: 82.30%
2026-01-14 12:02:42,170 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-01-14 12:02:42,171 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-01-14 12:02:42,219 - INFO - [Best Model Saved] (val loss: 0.4795) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 12:02:42,219 - INFO - --------------------------------------------------
2026-01-14 12:02:42,222 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-01-14 12:02:51,071 - INFO - [Train] [22/90] | Loss: 0.3299 | Train Acc: 93.01%
2026-01-14 12:02:53,541 - INFO - [Valid] [22/90] | Loss: 0.5281 | Val Acc: 80.83%
2026-01-14 12:02:53,550 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.7834 | F1: 0.7910
2026-01-14 12:02:53,551 - INFO - [Metrics for 'normal'] | Precision: 0.8162 | Recall: 0.8297 | F1: 0.8229
2026-01-14 12:02:53,554 - INFO - --------------------------------------------------
2026-01-14 12:02:53,557 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-01-14 12:03:01,479 - INFO - [Train] [23/90] | Loss: 0.2960 | Train Acc: 95.68%
2026-01-14 12:03:04,068 - INFO - [Valid] [23/90] | Loss: 0.5280 | Val Acc: 82.01%
2026-01-14 12:03:04,083 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8726 | F1: 0.8179
2026-01-14 12:03:04,084 - INFO - [Metrics for 'normal'] | Precision: 0.8758 | Recall: 0.7747 | F1: 0.8222
2026-01-14 12:03:04,090 - INFO - --------------------------------------------------
2026-01-14 12:03:04,094 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-01-14 12:03:12,420 - INFO - [Train] [24/90] | Loss: 0.2827 | Train Acc: 96.65%
2026-01-14 12:03:15,564 - INFO - [Valid] [24/90] | Loss: 0.5330 | Val Acc: 81.71%
2026-01-14 12:03:15,576 - INFO - [Metrics for 'abnormal'] | Precision: 0.8105 | Recall: 0.7898 | F1: 0.8000
2026-01-14 12:03:15,577 - INFO - [Metrics for 'normal'] | Precision: 0.8226 | Recall: 0.8407 | F1: 0.8315
2026-01-14 12:03:15,581 - INFO - --------------------------------------------------
2026-01-14 12:03:15,585 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-01-14 12:03:23,149 - INFO - [Train] [25/90] | Loss: 0.2926 | Train Acc: 95.46%
2026-01-14 12:03:25,633 - INFO - [Valid] [25/90] | Loss: 0.5024 | Val Acc: 81.42%
2026-01-14 12:03:25,646 - INFO - [Metrics for 'abnormal'] | Precision: 0.8507 | Recall: 0.7261 | F1: 0.7835
2026-01-14 12:03:25,646 - INFO - [Metrics for 'normal'] | Precision: 0.7902 | Recall: 0.8901 | F1: 0.8372
2026-01-14 12:03:25,651 - INFO - --------------------------------------------------
2026-01-14 12:03:25,654 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-01-14 12:03:34,784 - INFO - [Train] [26/90] | Loss: 0.2899 | Train Acc: 96.13%
2026-01-14 12:03:37,465 - INFO - [Valid] [26/90] | Loss: 0.5238 | Val Acc: 80.53%
2026-01-14 12:03:37,475 - INFO - [Metrics for 'abnormal'] | Precision: 0.7974 | Recall: 0.7771 | F1: 0.7871
2026-01-14 12:03:37,476 - INFO - [Metrics for 'normal'] | Precision: 0.8118 | Recall: 0.8297 | F1: 0.8207
2026-01-14 12:03:37,480 - INFO - --------------------------------------------------
2026-01-14 12:03:37,483 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-01-14 12:03:47,987 - INFO - [Train] [27/90] | Loss: 0.2843 | Train Acc: 95.83%
2026-01-14 12:03:51,323 - INFO - [Valid] [27/90] | Loss: 0.5692 | Val Acc: 82.30%
2026-01-14 12:03:51,335 - INFO - [Metrics for 'abnormal'] | Precision: 0.7939 | Recall: 0.8344 | F1: 0.8137
2026-01-14 12:03:51,336 - INFO - [Metrics for 'normal'] | Precision: 0.8506 | Recall: 0.8132 | F1: 0.8315
2026-01-14 12:03:51,340 - INFO - --------------------------------------------------
2026-01-14 12:03:51,344 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-01-14 12:04:00,743 - INFO - [Train] [28/90] | Loss: 0.2841 | Train Acc: 95.98%
2026-01-14 12:04:03,358 - INFO - [Valid] [28/90] | Loss: 0.5361 | Val Acc: 81.42%
2026-01-14 12:04:03,369 - INFO - [Metrics for 'abnormal'] | Precision: 0.7937 | Recall: 0.8089 | F1: 0.8013
2026-01-14 12:04:03,369 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8187 | F1: 0.8255
2026-01-14 12:04:03,372 - INFO - --------------------------------------------------
2026-01-14 12:04:03,375 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-01-14 12:04:12,218 - INFO - [Train] [29/90] | Loss: 0.2524 | Train Acc: 98.07%
2026-01-14 12:04:15,036 - INFO - [Valid] [29/90] | Loss: 0.5733 | Val Acc: 82.01%
2026-01-14 12:04:15,047 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 12:04:15,048 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 12:04:15,051 - INFO - --------------------------------------------------
2026-01-14 12:04:15,054 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-01-14 12:04:23,231 - INFO - [Train] [30/90] | Loss: 0.2616 | Train Acc: 97.77%
2026-01-14 12:04:26,108 - INFO - [Valid] [30/90] | Loss: 0.5738 | Val Acc: 82.60%
2026-01-14 12:04:26,120 - INFO - [Metrics for 'abnormal'] | Precision: 0.8267 | Recall: 0.7898 | F1: 0.8078
2026-01-14 12:04:26,121 - INFO - [Metrics for 'normal'] | Precision: 0.8254 | Recall: 0.8571 | F1: 0.8410
2026-01-14 12:04:26,125 - INFO - --------------------------------------------------
2026-01-14 12:04:26,129 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-01-14 12:04:35,299 - INFO - [Train] [31/90] | Loss: 0.2611 | Train Acc: 97.77%
2026-01-14 12:04:38,100 - INFO - [Valid] [31/90] | Loss: 0.5420 | Val Acc: 82.89%
2026-01-14 12:04:38,111 - INFO - [Metrics for 'abnormal'] | Precision: 0.8367 | Recall: 0.7834 | F1: 0.8092
2026-01-14 12:04:38,112 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.8681 | F1: 0.8449
2026-01-14 12:04:38,115 - INFO - --------------------------------------------------
2026-01-14 12:04:38,119 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-01-14 12:04:47,096 - INFO - [Train] [32/90] | Loss: 0.2717 | Train Acc: 96.95%
2026-01-14 12:04:50,010 - INFO - [Valid] [32/90] | Loss: 0.6093 | Val Acc: 77.88%
2026-01-14 12:04:50,021 - INFO - [Metrics for 'abnormal'] | Precision: 0.8361 | Recall: 0.6497 | F1: 0.7312
2026-01-14 12:04:50,021 - INFO - [Metrics for 'normal'] | Precision: 0.7465 | Recall: 0.8901 | F1: 0.8120
2026-01-14 12:04:50,025 - INFO - --------------------------------------------------
2026-01-14 12:04:50,028 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-01-14 12:04:58,583 - INFO - [Train] [33/90] | Loss: 0.2547 | Train Acc: 97.69%
2026-01-14 12:05:01,259 - INFO - [Valid] [33/90] | Loss: 0.5782 | Val Acc: 79.06%
2026-01-14 12:05:01,306 - INFO - [Metrics for 'abnormal'] | Precision: 0.8028 | Recall: 0.7261 | F1: 0.7625
2026-01-14 12:05:01,307 - INFO - [Metrics for 'normal'] | Precision: 0.7817 | Recall: 0.8462 | F1: 0.8127
2026-01-14 12:05:01,322 - INFO - --------------------------------------------------
2026-01-14 12:05:01,330 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-01-14 12:05:10,211 - INFO - [Train] [34/90] | Loss: 0.2601 | Train Acc: 97.40%
2026-01-14 12:05:12,751 - INFO - [Valid] [34/90] | Loss: 0.5367 | Val Acc: 81.71%
2026-01-14 12:05:12,770 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8471 | F1: 0.8110
2026-01-14 12:05:12,770 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.7912 | F1: 0.8229
2026-01-14 12:05:12,776 - INFO - --------------------------------------------------
2026-01-14 12:05:12,781 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-01-14 12:05:22,165 - INFO - [Train] [35/90] | Loss: 0.2517 | Train Acc: 98.21%
2026-01-14 12:05:25,128 - INFO - [Valid] [35/90] | Loss: 0.5282 | Val Acc: 82.30%
2026-01-14 12:05:25,144 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-01-14 12:05:25,144 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-01-14 12:05:25,157 - INFO - --------------------------------------------------
2026-01-14 12:05:25,175 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-01-14 12:05:35,008 - INFO - [Train] [36/90] | Loss: 0.2512 | Train Acc: 97.69%
2026-01-14 12:05:37,807 - INFO - [Valid] [36/90] | Loss: 0.5710 | Val Acc: 82.01%
2026-01-14 12:05:37,820 - INFO - [Metrics for 'abnormal'] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-01-14 12:05:37,821 - INFO - [Metrics for 'normal'] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-01-14 12:05:37,826 - INFO - --------------------------------------------------
2026-01-14 12:05:37,829 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-01-14 12:05:46,617 - INFO - [Train] [37/90] | Loss: 0.2454 | Train Acc: 98.14%
2026-01-14 12:05:49,339 - INFO - [Valid] [37/90] | Loss: 0.5540 | Val Acc: 84.37%
2026-01-14 12:05:49,350 - INFO - [Metrics for 'abnormal'] | Precision: 0.8250 | Recall: 0.8408 | F1: 0.8328
2026-01-14 12:05:49,351 - INFO - [Metrics for 'normal'] | Precision: 0.8603 | Recall: 0.8462 | F1: 0.8532
2026-01-14 12:05:49,354 - INFO - --------------------------------------------------
2026-01-14 12:05:49,358 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-01-14 12:05:57,527 - INFO - [Train] [38/90] | Loss: 0.2595 | Train Acc: 97.10%
2026-01-14 12:05:59,783 - INFO - [Valid] [38/90] | Loss: 0.5949 | Val Acc: 82.60%
2026-01-14 12:05:59,794 - INFO - [Metrics for 'abnormal'] | Precision: 0.7988 | Recall: 0.8344 | F1: 0.8162
2026-01-14 12:05:59,795 - INFO - [Metrics for 'normal'] | Precision: 0.8514 | Recall: 0.8187 | F1: 0.8347
2026-01-14 12:05:59,799 - INFO - --------------------------------------------------
2026-01-14 12:05:59,802 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-01-14 12:06:08,120 - INFO - [Train] [39/90] | Loss: 0.2501 | Train Acc: 97.77%
2026-01-14 12:06:10,335 - INFO - [Valid] [39/90] | Loss: 0.6199 | Val Acc: 81.71%
2026-01-14 12:06:10,347 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.8089 | F1: 0.8038
2026-01-14 12:06:10,348 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8242 | F1: 0.8287
2026-01-14 12:06:10,353 - INFO - --------------------------------------------------
2026-01-14 12:06:10,357 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-01-14 12:06:18,819 - INFO - [Train] [40/90] | Loss: 0.2379 | Train Acc: 98.96%
2026-01-14 12:06:21,353 - INFO - [Valid] [40/90] | Loss: 0.5623 | Val Acc: 81.12%
2026-01-14 12:06:21,364 - INFO - [Metrics for 'abnormal'] | Precision: 0.7853 | Recall: 0.8153 | F1: 0.8000
2026-01-14 12:06:21,365 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8077 | F1: 0.8212
2026-01-14 12:06:21,369 - INFO - --------------------------------------------------
2026-01-14 12:06:21,372 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-01-14 12:06:30,512 - INFO - [Train] [41/90] | Loss: 0.2432 | Train Acc: 98.29%
2026-01-14 12:06:33,318 - INFO - [Valid] [41/90] | Loss: 0.5835 | Val Acc: 80.53%
2026-01-14 12:06:33,333 - INFO - [Metrics for 'abnormal'] | Precision: 0.7661 | Recall: 0.8344 | F1: 0.7988
2026-01-14 12:06:33,334 - INFO - [Metrics for 'normal'] | Precision: 0.8452 | Recall: 0.7802 | F1: 0.8114
2026-01-14 12:06:33,339 - INFO - --------------------------------------------------
2026-01-14 12:06:33,343 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-01-14 12:06:42,639 - INFO - [Train] [42/90] | Loss: 0.2408 | Train Acc: 98.51%
2026-01-14 12:06:45,047 - INFO - [Valid] [42/90] | Loss: 0.5515 | Val Acc: 82.30%
2026-01-14 12:06:45,057 - INFO - [Metrics for 'abnormal'] | Precision: 0.8089 | Recall: 0.8089 | F1: 0.8089
2026-01-14 12:06:45,058 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8352 | F1: 0.8352
2026-01-14 12:06:45,062 - INFO - --------------------------------------------------
2026-01-14 12:06:45,066 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-01-14 12:06:53,637 - INFO - [Train] [43/90] | Loss: 0.2475 | Train Acc: 98.07%
2026-01-14 12:06:56,193 - INFO - [Valid] [43/90] | Loss: 0.6027 | Val Acc: 81.12%
2026-01-14 12:06:56,203 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.7707 | F1: 0.7908
2026-01-14 12:06:56,204 - INFO - [Metrics for 'normal'] | Precision: 0.8105 | Recall: 0.8462 | F1: 0.8280
2026-01-14 12:06:56,207 - INFO - --------------------------------------------------
2026-01-14 12:06:56,210 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-01-14 12:07:04,833 - INFO - [Train] [44/90] | Loss: 0.2308 | Train Acc: 99.11%
2026-01-14 12:07:07,517 - INFO - [Valid] [44/90] | Loss: 0.5562 | Val Acc: 83.48%
2026-01-14 12:07:07,526 - INFO - [Metrics for 'abnormal'] | Precision: 0.8258 | Recall: 0.8153 | F1: 0.8205
2026-01-14 12:07:07,526 - INFO - [Metrics for 'normal'] | Precision: 0.8424 | Recall: 0.8516 | F1: 0.8470
2026-01-14 12:07:07,529 - INFO - --------------------------------------------------
2026-01-14 12:07:07,531 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-01-14 12:07:15,976 - INFO - [Train] [45/90] | Loss: 0.2360 | Train Acc: 98.66%
2026-01-14 12:07:18,752 - INFO - [Valid] [45/90] | Loss: 0.5780 | Val Acc: 82.30%
2026-01-14 12:07:18,761 - INFO - [Metrics for 'abnormal'] | Precision: 0.8299 | Recall: 0.7771 | F1: 0.8026
2026-01-14 12:07:18,761 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8626 | F1: 0.8396
2026-01-14 12:07:18,765 - INFO - --------------------------------------------------
2026-01-14 12:07:18,768 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-01-14 12:07:26,798 - INFO - [Train] [46/90] | Loss: 0.2398 | Train Acc: 98.44%
2026-01-14 12:07:29,094 - INFO - [Valid] [46/90] | Loss: 0.5124 | Val Acc: 84.07%
2026-01-14 12:07:29,107 - INFO - [Metrics for 'abnormal'] | Precision: 0.8199 | Recall: 0.8408 | F1: 0.8302
2026-01-14 12:07:29,108 - INFO - [Metrics for 'normal'] | Precision: 0.8596 | Recall: 0.8407 | F1: 0.8500
2026-01-14 12:07:29,113 - INFO - --------------------------------------------------
2026-01-14 12:07:29,116 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-01-14 12:07:37,874 - INFO - [Train] [47/90] | Loss: 0.2390 | Train Acc: 98.36%
2026-01-14 12:07:40,829 - INFO - [Valid] [47/90] | Loss: 0.5260 | Val Acc: 82.89%
2026-01-14 12:07:40,864 - INFO - [Metrics for 'abnormal'] | Precision: 0.7895 | Recall: 0.8599 | F1: 0.8232
2026-01-14 12:07:40,874 - INFO - [Metrics for 'normal'] | Precision: 0.8690 | Recall: 0.8022 | F1: 0.8343
2026-01-14 12:07:40,881 - INFO - --------------------------------------------------
2026-01-14 12:07:40,885 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-01-14 12:07:50,324 - INFO - [Train] [48/90] | Loss: 0.2375 | Train Acc: 98.74%
2026-01-14 12:07:52,854 - INFO - [Valid] [48/90] | Loss: 0.5267 | Val Acc: 82.30%
2026-01-14 12:07:52,872 - INFO - [Metrics for 'abnormal'] | Precision: 0.7836 | Recall: 0.8535 | F1: 0.8171
2026-01-14 12:07:52,875 - INFO - [Metrics for 'normal'] | Precision: 0.8631 | Recall: 0.7967 | F1: 0.8286
2026-01-14 12:07:52,884 - INFO - --------------------------------------------------
2026-01-14 12:07:52,893 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-01-14 12:08:01,628 - INFO - [Train] [49/90] | Loss: 0.2298 | Train Acc: 99.18%
2026-01-14 12:08:04,212 - INFO - [Valid] [49/90] | Loss: 0.5662 | Val Acc: 81.12%
2026-01-14 12:08:04,273 - INFO - [Metrics for 'abnormal'] | Precision: 0.8079 | Recall: 0.7771 | F1: 0.7922
2026-01-14 12:08:04,273 - INFO - [Metrics for 'normal'] | Precision: 0.8138 | Recall: 0.8407 | F1: 0.8270
2026-01-14 12:08:04,278 - INFO - --------------------------------------------------
2026-01-14 12:08:04,286 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-01-14 12:08:13,402 - INFO - [Train] [50/90] | Loss: 0.2316 | Train Acc: 98.96%
2026-01-14 12:08:15,687 - INFO - [Valid] [50/90] | Loss: 0.5545 | Val Acc: 81.42%
2026-01-14 12:08:15,699 - INFO - [Metrics for 'abnormal'] | Precision: 0.7831 | Recall: 0.8280 | F1: 0.8050
2026-01-14 12:08:15,700 - INFO - [Metrics for 'normal'] | Precision: 0.8439 | Recall: 0.8022 | F1: 0.8225
2026-01-14 12:08:15,705 - INFO - --------------------------------------------------
2026-01-14 12:08:15,710 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-01-14 12:08:24,675 - INFO - [Train] [51/90] | Loss: 0.2396 | Train Acc: 98.44%
2026-01-14 12:08:27,371 - INFO - [Valid] [51/90] | Loss: 0.5797 | Val Acc: 82.60%
2026-01-14 12:08:27,385 - INFO - [Metrics for 'abnormal'] | Precision: 0.8224 | Recall: 0.7962 | F1: 0.8091
2026-01-14 12:08:27,400 - INFO - [Metrics for 'normal'] | Precision: 0.8289 | Recall: 0.8516 | F1: 0.8401
2026-01-14 12:08:27,406 - INFO - --------------------------------------------------
2026-01-14 12:08:27,411 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-01-14 12:08:36,145 - INFO - [Train] [52/90] | Loss: 0.2260 | Train Acc: 99.11%
2026-01-14 12:08:39,198 - INFO - [Valid] [52/90] | Loss: 0.6187 | Val Acc: 81.12%
2026-01-14 12:08:39,234 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7898 | F1: 0.7949
2026-01-14 12:08:39,235 - INFO - [Metrics for 'normal'] | Precision: 0.8207 | Recall: 0.8297 | F1: 0.8251
2026-01-14 12:08:39,240 - INFO - --------------------------------------------------
2026-01-14 12:08:39,244 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-01-14 12:08:48,572 - INFO - [Train] [53/90] | Loss: 0.2454 | Train Acc: 97.92%
2026-01-14 12:08:51,050 - INFO - [Valid] [53/90] | Loss: 0.5431 | Val Acc: 82.89%
2026-01-14 12:08:51,066 - INFO - [Metrics for 'abnormal'] | Precision: 0.8153 | Recall: 0.8153 | F1: 0.8153
2026-01-14 12:08:51,070 - INFO - [Metrics for 'normal'] | Precision: 0.8407 | Recall: 0.8407 | F1: 0.8407
2026-01-14 12:08:51,073 - INFO - --------------------------------------------------
2026-01-14 12:08:51,076 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-01-14 12:09:00,654 - INFO - [Train] [54/90] | Loss: 0.2315 | Train Acc: 99.18%
2026-01-14 12:09:03,955 - INFO - [Valid] [54/90] | Loss: 0.5558 | Val Acc: 82.89%
2026-01-14 12:09:03,967 - INFO - [Metrics for 'abnormal'] | Precision: 0.8075 | Recall: 0.8280 | F1: 0.8176
2026-01-14 12:09:03,968 - INFO - [Metrics for 'normal'] | Precision: 0.8483 | Recall: 0.8297 | F1: 0.8389
2026-01-14 12:09:03,972 - INFO - --------------------------------------------------
2026-01-14 12:09:03,976 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-01-14 12:09:12,998 - INFO - [Train] [55/90] | Loss: 0.2369 | Train Acc: 98.88%
2026-01-14 12:09:15,627 - INFO - [Valid] [55/90] | Loss: 0.5711 | Val Acc: 81.12%
2026-01-14 12:09:15,643 - INFO - [Metrics for 'abnormal'] | Precision: 0.8039 | Recall: 0.7834 | F1: 0.7935
2026-01-14 12:09:15,644 - INFO - [Metrics for 'normal'] | Precision: 0.8172 | Recall: 0.8352 | F1: 0.8261
2026-01-14 12:09:15,650 - INFO - --------------------------------------------------
2026-01-14 12:09:15,653 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-01-14 12:09:25,093 - INFO - [Train] [56/90] | Loss: 0.2179 | Train Acc: 99.85%
2026-01-14 12:09:26,993 - INFO - [Valid] [56/90] | Loss: 0.5936 | Val Acc: 82.01%
2026-01-14 12:09:27,005 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 12:09:27,006 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 12:09:27,010 - INFO - --------------------------------------------------
2026-01-14 12:09:27,014 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-01-14 12:09:36,745 - INFO - [Train] [57/90] | Loss: 0.2211 | Train Acc: 99.33%
2026-01-14 12:09:38,577 - INFO - [Valid] [57/90] | Loss: 0.5731 | Val Acc: 82.30%
2026-01-14 12:09:38,598 - INFO - [Metrics for 'abnormal'] | Precision: 0.7939 | Recall: 0.8344 | F1: 0.8137
2026-01-14 12:09:38,598 - INFO - [Metrics for 'normal'] | Precision: 0.8506 | Recall: 0.8132 | F1: 0.8315
2026-01-14 12:09:38,602 - INFO - --------------------------------------------------
2026-01-14 12:09:38,606 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-01-14 12:09:47,982 - INFO - [Train] [58/90] | Loss: 0.2385 | Train Acc: 98.14%
2026-01-14 12:09:50,029 - INFO - [Valid] [58/90] | Loss: 0.5541 | Val Acc: 82.30%
2026-01-14 12:09:50,037 - INFO - [Metrics for 'abnormal'] | Precision: 0.8212 | Recall: 0.7898 | F1: 0.8052
2026-01-14 12:09:50,037 - INFO - [Metrics for 'normal'] | Precision: 0.8245 | Recall: 0.8516 | F1: 0.8378
2026-01-14 12:09:50,039 - INFO - --------------------------------------------------
2026-01-14 12:09:50,041 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-01-14 12:10:00,076 - INFO - [Train] [59/90] | Loss: 0.2234 | Train Acc: 99.33%
2026-01-14 12:10:02,455 - INFO - [Valid] [59/90] | Loss: 0.5769 | Val Acc: 81.71%
2026-01-14 12:10:02,467 - INFO - [Metrics for 'abnormal'] | Precision: 0.7950 | Recall: 0.8153 | F1: 0.8050
2026-01-14 12:10:02,467 - INFO - [Metrics for 'normal'] | Precision: 0.8371 | Recall: 0.8187 | F1: 0.8278
2026-01-14 12:10:02,472 - INFO - --------------------------------------------------
2026-01-14 12:10:02,476 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-01-14 12:10:11,511 - INFO - [Train] [60/90] | Loss: 0.2163 | Train Acc: 99.78%
2026-01-14 12:10:14,409 - INFO - [Valid] [60/90] | Loss: 0.5680 | Val Acc: 82.60%
2026-01-14 12:10:14,419 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8280 | F1: 0.8150
2026-01-14 12:10:14,419 - INFO - [Metrics for 'normal'] | Precision: 0.8475 | Recall: 0.8242 | F1: 0.8357
2026-01-14 12:10:14,423 - INFO - --------------------------------------------------
2026-01-14 12:10:14,426 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-01-14 12:10:24,289 - INFO - [Train] [61/90] | Loss: 0.2192 | Train Acc: 99.11%
2026-01-14 12:10:26,657 - INFO - [Valid] [61/90] | Loss: 0.5254 | Val Acc: 84.07%
2026-01-14 12:10:26,686 - INFO - [Metrics for 'abnormal'] | Precision: 0.7977 | Recall: 0.8790 | F1: 0.8364
2026-01-14 12:10:26,686 - INFO - [Metrics for 'normal'] | Precision: 0.8855 | Recall: 0.8077 | F1: 0.8448
2026-01-14 12:10:26,690 - INFO - --------------------------------------------------
2026-01-14 12:10:26,693 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-01-14 12:10:35,044 - INFO - [Train] [62/90] | Loss: 0.2187 | Train Acc: 99.48%
2026-01-14 12:10:37,716 - INFO - [Valid] [62/90] | Loss: 0.5473 | Val Acc: 80.83%
2026-01-14 12:10:37,731 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8217 | F1: 0.7988
2026-01-14 12:10:37,733 - INFO - [Metrics for 'normal'] | Precision: 0.8382 | Recall: 0.7967 | F1: 0.8169
2026-01-14 12:10:37,742 - INFO - --------------------------------------------------
2026-01-14 12:10:37,749 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-01-14 12:10:46,145 - INFO - [Train] [63/90] | Loss: 0.2158 | Train Acc: 99.63%
2026-01-14 12:10:49,285 - INFO - [Valid] [63/90] | Loss: 0.6022 | Val Acc: 82.30%
2026-01-14 12:10:49,301 - INFO - [Metrics for 'abnormal'] | Precision: 0.7975 | Recall: 0.8280 | F1: 0.8125
2026-01-14 12:10:49,302 - INFO - [Metrics for 'normal'] | Precision: 0.8466 | Recall: 0.8187 | F1: 0.8324
2026-01-14 12:10:49,306 - INFO - --------------------------------------------------
2026-01-14 12:10:49,310 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-01-14 12:10:57,797 - INFO - [Train] [64/90] | Loss: 0.2193 | Train Acc: 99.55%
2026-01-14 12:11:00,442 - INFO - [Valid] [64/90] | Loss: 0.5592 | Val Acc: 81.71%
2026-01-14 12:11:00,451 - INFO - [Metrics for 'abnormal'] | Precision: 0.7879 | Recall: 0.8280 | F1: 0.8075
2026-01-14 12:11:00,452 - INFO - [Metrics for 'normal'] | Precision: 0.8448 | Recall: 0.8077 | F1: 0.8258
2026-01-14 12:11:00,455 - INFO - --------------------------------------------------
2026-01-14 12:11:00,458 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-01-14 12:11:08,598 - INFO - [Train] [65/90] | Loss: 0.2118 | Train Acc: 99.78%
2026-01-14 12:11:11,065 - INFO - [Valid] [65/90] | Loss: 0.5679 | Val Acc: 81.71%
2026-01-14 12:11:11,119 - INFO - [Metrics for 'abnormal'] | Precision: 0.8146 | Recall: 0.7834 | F1: 0.7987
2026-01-14 12:11:11,120 - INFO - [Metrics for 'normal'] | Precision: 0.8191 | Recall: 0.8462 | F1: 0.8324
2026-01-14 12:11:11,126 - INFO - --------------------------------------------------
2026-01-14 12:11:11,132 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-01-14 12:11:20,303 - INFO - [Train] [66/90] | Loss: 0.2147 | Train Acc: 99.63%
2026-01-14 12:11:23,000 - INFO - [Valid] [66/90] | Loss: 0.5985 | Val Acc: 82.89%
2026-01-14 12:11:23,010 - INFO - [Metrics for 'abnormal'] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-01-14 12:11:23,011 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-01-14 12:11:23,014 - INFO - --------------------------------------------------
2026-01-14 12:11:23,017 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-01-14 12:11:31,824 - INFO - [Train] [67/90] | Loss: 0.2158 | Train Acc: 99.63%
2026-01-14 12:11:35,611 - INFO - [Valid] [67/90] | Loss: 0.5743 | Val Acc: 81.42%
2026-01-14 12:11:35,623 - INFO - [Metrics for 'abnormal'] | Precision: 0.8013 | Recall: 0.7962 | F1: 0.7987
2026-01-14 12:11:35,624 - INFO - [Metrics for 'normal'] | Precision: 0.8251 | Recall: 0.8297 | F1: 0.8274
2026-01-14 12:11:35,628 - INFO - --------------------------------------------------
2026-01-14 12:11:35,632 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-01-14 12:11:44,615 - INFO - [Train] [68/90] | Loss: 0.2090 | Train Acc: 99.93%
2026-01-14 12:11:47,371 - INFO - [Valid] [68/90] | Loss: 0.5868 | Val Acc: 81.12%
2026-01-14 12:11:47,383 - INFO - [Metrics for 'abnormal'] | Precision: 0.8039 | Recall: 0.7834 | F1: 0.7935
2026-01-14 12:11:47,384 - INFO - [Metrics for 'normal'] | Precision: 0.8172 | Recall: 0.8352 | F1: 0.8261
2026-01-14 12:11:47,388 - INFO - --------------------------------------------------
2026-01-14 12:11:47,391 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-01-14 12:11:56,395 - INFO - [Train] [69/90] | Loss: 0.2124 | Train Acc: 99.78%
2026-01-14 12:11:59,503 - INFO - [Valid] [69/90] | Loss: 0.5738 | Val Acc: 81.71%
2026-01-14 12:11:59,519 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8471 | F1: 0.8110
2026-01-14 12:11:59,520 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.7912 | F1: 0.8229
2026-01-14 12:11:59,534 - INFO - --------------------------------------------------
2026-01-14 12:11:59,538 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-01-14 12:12:08,467 - INFO - [Train] [70/90] | Loss: 0.2142 | Train Acc: 99.85%
2026-01-14 12:12:10,830 - INFO - [Valid] [70/90] | Loss: 0.5795 | Val Acc: 81.71%
2026-01-14 12:12:10,841 - INFO - [Metrics for 'abnormal'] | Precision: 0.7654 | Recall: 0.8726 | F1: 0.8155
2026-01-14 12:12:10,841 - INFO - [Metrics for 'normal'] | Precision: 0.8750 | Recall: 0.7692 | F1: 0.8187
2026-01-14 12:12:10,845 - INFO - --------------------------------------------------
2026-01-14 12:12:10,848 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-01-14 12:12:19,634 - INFO - [Train] [71/90] | Loss: 0.2105 | Train Acc: 99.78%
2026-01-14 12:12:22,130 - INFO - [Valid] [71/90] | Loss: 0.6188 | Val Acc: 79.65%
2026-01-14 12:12:22,146 - INFO - [Metrics for 'abnormal'] | Precision: 0.8014 | Recall: 0.7452 | F1: 0.7723
2026-01-14 12:12:22,150 - INFO - [Metrics for 'normal'] | Precision: 0.7927 | Recall: 0.8407 | F1: 0.8160
2026-01-14 12:12:22,157 - INFO - --------------------------------------------------
2026-01-14 12:12:22,159 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-01-14 12:12:31,081 - INFO - [Train] [72/90] | Loss: 0.2112 | Train Acc: 99.78%
2026-01-14 12:12:33,474 - INFO - [Valid] [72/90] | Loss: 0.5900 | Val Acc: 80.53%
2026-01-14 12:12:33,484 - INFO - [Metrics for 'abnormal'] | Precision: 0.7898 | Recall: 0.7898 | F1: 0.7898
2026-01-14 12:12:33,484 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8187 | F1: 0.8187
2026-01-14 12:12:33,488 - INFO - --------------------------------------------------
2026-01-14 12:12:33,491 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-01-14 12:12:41,943 - INFO - [Train] [73/90] | Loss: 0.2090 | Train Acc: 99.85%
2026-01-14 12:12:43,812 - INFO - [Valid] [73/90] | Loss: 0.5937 | Val Acc: 81.71%
2026-01-14 12:12:43,824 - INFO - [Metrics for 'abnormal'] | Precision: 0.7684 | Recall: 0.8662 | F1: 0.8144
2026-01-14 12:12:43,824 - INFO - [Metrics for 'normal'] | Precision: 0.8704 | Recall: 0.7747 | F1: 0.8198
2026-01-14 12:12:43,829 - INFO - --------------------------------------------------
2026-01-14 12:12:43,832 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-01-14 12:12:53,518 - INFO - [Train] [74/90] | Loss: 0.2090 | Train Acc: 99.78%
2026-01-14 12:12:56,279 - INFO - [Valid] [74/90] | Loss: 0.6172 | Val Acc: 81.71%
2026-01-14 12:12:56,291 - INFO - [Metrics for 'abnormal'] | Precision: 0.8188 | Recall: 0.7771 | F1: 0.7974
2026-01-14 12:12:56,291 - INFO - [Metrics for 'normal'] | Precision: 0.8158 | Recall: 0.8516 | F1: 0.8333
2026-01-14 12:12:56,294 - INFO - --------------------------------------------------
2026-01-14 12:12:56,296 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-01-14 12:13:04,345 - INFO - [Train] [75/90] | Loss: 0.2153 | Train Acc: 99.78%
2026-01-14 12:13:06,810 - INFO - [Valid] [75/90] | Loss: 0.5767 | Val Acc: 81.71%
2026-01-14 12:13:06,822 - INFO - [Metrics for 'abnormal'] | Precision: 0.7950 | Recall: 0.8153 | F1: 0.8050
2026-01-14 12:13:06,823 - INFO - [Metrics for 'normal'] | Precision: 0.8371 | Recall: 0.8187 | F1: 0.8278
2026-01-14 12:13:06,827 - INFO - --------------------------------------------------
2026-01-14 12:13:06,831 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-01-14 12:13:15,604 - INFO - [Train] [76/90] | Loss: 0.2177 | Train Acc: 99.55%
2026-01-14 12:13:19,136 - INFO - [Valid] [76/90] | Loss: 0.5850 | Val Acc: 80.24%
2026-01-14 12:13:19,147 - INFO - [Metrics for 'abnormal'] | Precision: 0.7885 | Recall: 0.7834 | F1: 0.7859
2026-01-14 12:13:19,148 - INFO - [Metrics for 'normal'] | Precision: 0.8142 | Recall: 0.8187 | F1: 0.8164
2026-01-14 12:13:19,152 - INFO - --------------------------------------------------
2026-01-14 12:13:19,157 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-01-14 12:13:27,148 - INFO - [Train] [77/90] | Loss: 0.2188 | Train Acc: 99.26%
2026-01-14 12:13:28,858 - INFO - [Valid] [77/90] | Loss: 0.5743 | Val Acc: 81.12%
2026-01-14 12:13:28,867 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-01-14 12:13:28,868 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-01-14 12:13:28,871 - INFO - --------------------------------------------------
2026-01-14 12:13:28,873 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-01-14 12:13:36,008 - INFO - [Train] [78/90] | Loss: 0.2134 | Train Acc: 99.70%
2026-01-14 12:13:38,135 - INFO - [Valid] [78/90] | Loss: 0.5625 | Val Acc: 81.71%
2026-01-14 12:13:38,152 - INFO - [Metrics for 'abnormal'] | Precision: 0.7811 | Recall: 0.8408 | F1: 0.8098
2026-01-14 12:13:38,154 - INFO - [Metrics for 'normal'] | Precision: 0.8529 | Recall: 0.7967 | F1: 0.8239
2026-01-14 12:13:38,157 - INFO - --------------------------------------------------
2026-01-14 12:13:38,163 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-01-14 12:13:45,729 - INFO - [Train] [79/90] | Loss: 0.2081 | Train Acc: 99.93%
2026-01-14 12:13:48,313 - INFO - [Valid] [79/90] | Loss: 0.5771 | Val Acc: 81.12%
2026-01-14 12:13:48,323 - INFO - [Metrics for 'abnormal'] | Precision: 0.7853 | Recall: 0.8153 | F1: 0.8000
2026-01-14 12:13:48,323 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8077 | F1: 0.8212
2026-01-14 12:13:48,327 - INFO - --------------------------------------------------
2026-01-14 12:13:48,331 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-01-14 12:13:58,490 - INFO - [Train] [80/90] | Loss: 0.2083 | Train Acc: 99.93%
2026-01-14 12:14:00,140 - INFO - [Valid] [80/90] | Loss: 0.5763 | Val Acc: 80.83%
2026-01-14 12:14:00,151 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8217 | F1: 0.7988
2026-01-14 12:14:00,152 - INFO - [Metrics for 'normal'] | Precision: 0.8382 | Recall: 0.7967 | F1: 0.8169
2026-01-14 12:14:00,157 - INFO - --------------------------------------------------
2026-01-14 12:14:00,161 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-01-14 12:14:06,558 - INFO - [Train] [81/90] | Loss: 0.2093 | Train Acc: 99.93%
2026-01-14 12:14:09,068 - INFO - [Valid] [81/90] | Loss: 0.5843 | Val Acc: 82.60%
2026-01-14 12:14:09,076 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.8153 | F1: 0.8127
2026-01-14 12:14:09,077 - INFO - [Metrics for 'normal'] | Precision: 0.8398 | Recall: 0.8352 | F1: 0.8375
2026-01-14 12:14:09,080 - INFO - --------------------------------------------------
2026-01-14 12:14:09,083 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-01-14 12:14:14,329 - INFO - [Train] [82/90] | Loss: 0.2062 | Train Acc: 100.00%
2026-01-14 12:14:15,964 - INFO - [Valid] [82/90] | Loss: 0.5821 | Val Acc: 82.30%
2026-01-14 12:14:15,976 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-01-14 12:14:15,976 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-01-14 12:14:15,980 - INFO - --------------------------------------------------
2026-01-14 12:14:15,983 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-01-14 12:14:21,021 - INFO - [Train] [83/90] | Loss: 0.2097 | Train Acc: 99.93%
2026-01-14 12:14:22,592 - INFO - [Valid] [83/90] | Loss: 0.5774 | Val Acc: 82.60%
2026-01-14 12:14:22,601 - INFO - [Metrics for 'abnormal'] | Precision: 0.7849 | Recall: 0.8599 | F1: 0.8207
2026-01-14 12:14:22,601 - INFO - [Metrics for 'normal'] | Precision: 0.8683 | Recall: 0.7967 | F1: 0.8309
2026-01-14 12:14:22,604 - INFO - --------------------------------------------------
2026-01-14 12:14:22,607 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-01-14 12:14:27,792 - INFO - [Train] [84/90] | Loss: 0.2071 | Train Acc: 99.93%
2026-01-14 12:14:29,426 - INFO - [Valid] [84/90] | Loss: 0.5852 | Val Acc: 82.60%
2026-01-14 12:14:29,437 - INFO - [Metrics for 'abnormal'] | Precision: 0.7849 | Recall: 0.8599 | F1: 0.8207
2026-01-14 12:14:29,437 - INFO - [Metrics for 'normal'] | Precision: 0.8683 | Recall: 0.7967 | F1: 0.8309
2026-01-14 12:14:29,441 - INFO - --------------------------------------------------
2026-01-14 12:14:29,444 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-01-14 12:14:34,304 - INFO - [Train] [85/90] | Loss: 0.2065 | Train Acc: 100.00%
2026-01-14 12:14:35,833 - INFO - [Valid] [85/90] | Loss: 0.5790 | Val Acc: 82.30%
2026-01-14 12:14:35,844 - INFO - [Metrics for 'abnormal'] | Precision: 0.8050 | Recall: 0.8153 | F1: 0.8101
2026-01-14 12:14:35,844 - INFO - [Metrics for 'normal'] | Precision: 0.8389 | Recall: 0.8297 | F1: 0.8343
2026-01-14 12:14:35,848 - INFO - --------------------------------------------------
2026-01-14 12:14:35,852 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-01-14 12:14:41,175 - INFO - [Train] [86/90] | Loss: 0.2057 | Train Acc: 99.85%
2026-01-14 12:14:42,711 - INFO - [Valid] [86/90] | Loss: 0.5743 | Val Acc: 82.01%
2026-01-14 12:14:42,723 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-01-14 12:14:42,723 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-01-14 12:14:42,726 - INFO - --------------------------------------------------
2026-01-14 12:14:42,729 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-01-14 12:14:47,903 - INFO - [Train] [87/90] | Loss: 0.2036 | Train Acc: 100.00%
2026-01-14 12:14:49,389 - INFO - [Valid] [87/90] | Loss: 0.5784 | Val Acc: 82.01%
2026-01-14 12:14:49,398 - INFO - [Metrics for 'abnormal'] | Precision: 0.8158 | Recall: 0.7898 | F1: 0.8026
2026-01-14 12:14:49,398 - INFO - [Metrics for 'normal'] | Precision: 0.8235 | Recall: 0.8462 | F1: 0.8347
2026-01-14 12:14:49,401 - INFO - --------------------------------------------------
2026-01-14 12:14:49,403 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-01-14 12:14:54,525 - INFO - [Train] [88/90] | Loss: 0.2093 | Train Acc: 99.70%
2026-01-14 12:14:55,941 - INFO - [Valid] [88/90] | Loss: 0.5992 | Val Acc: 80.83%
2026-01-14 12:14:55,952 - INFO - [Metrics for 'abnormal'] | Precision: 0.8067 | Recall: 0.7707 | F1: 0.7883
2026-01-14 12:14:55,953 - INFO - [Metrics for 'normal'] | Precision: 0.8095 | Recall: 0.8407 | F1: 0.8248
2026-01-14 12:14:55,956 - INFO - --------------------------------------------------
2026-01-14 12:14:55,960 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-01-14 12:15:01,276 - INFO - [Train] [89/90] | Loss: 0.2073 | Train Acc: 99.78%
2026-01-14 12:15:02,623 - INFO - [Valid] [89/90] | Loss: 0.6062 | Val Acc: 81.12%
2026-01-14 12:15:02,630 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.7707 | F1: 0.7908
2026-01-14 12:15:02,631 - INFO - [Metrics for 'normal'] | Precision: 0.8105 | Recall: 0.8462 | F1: 0.8280
2026-01-14 12:15:02,633 - INFO - --------------------------------------------------
2026-01-14 12:15:02,635 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-01-14 12:15:07,862 - INFO - [Train] [90/90] | Loss: 0.2208 | Train Acc: 99.11%
2026-01-14 12:15:09,959 - INFO - [Valid] [90/90] | Loss: 0.5985 | Val Acc: 82.01%
2026-01-14 12:15:09,970 - INFO - [Metrics for 'abnormal'] | Precision: 0.8117 | Recall: 0.7962 | F1: 0.8039
2026-01-14 12:15:09,970 - INFO - [Metrics for 'normal'] | Precision: 0.8270 | Recall: 0.8407 | F1: 0.8338
2026-01-14 12:15:09,975 - INFO - ==================================================
2026-01-14 12:15:09,976 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-01-14 12:15:09,977 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-01-14 12:15:09,977 - INFO - Baseline 모델 'mobile_vit_xxs'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:15:10,039 - INFO - timm 모델(mobile_vit_xxs)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-01-14 12:15:10,062 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-01-14 12:15:10,065 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-01-14 12:15:10,065 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:15:10,067 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:15:10,819 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4756640625)에 맞춰 변경되었습니다.
2026-01-14 12:15:10,820 - INFO - ==================================================
2026-01-14 12:15:10,903 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/best_model.pth'
2026-01-14 12:15:10,904 - INFO - ==================================================
2026-01-14 12:15:10,904 - INFO - Test 모드를 시작합니다.
2026-01-14 12:15:11,178 - INFO - 연산량 (MACs): 0.0913 GMACs per sample
2026-01-14 12:15:11,179 - INFO - 연산량 (FLOPs): 0.1826 GFLOPs per sample
2026-01-14 12:15:11,179 - INFO - ==================================================
2026-01-14 12:15:11,179 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-01-14 12:15:13,035 - INFO - 샘플 당 평균 Forward Pass 시간: 10.10ms (std: 3.00ms), FPS: 105.38 (std: 22.47) (1개 샘플 x 100회 반복)
2026-01-14 12:15:13,035 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 44.36 MB
2026-01-14 12:15:13,035 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-01-14 12:15:15,358 - INFO - [Test] Loss: 0.3908 | Test Acc: 82.30%
2026-01-14 12:15:15,369 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-01-14 12:15:15,369 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-01-14 12:15:15,822 - INFO - ==================================================
2026-01-14 12:15:15,822 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/confusion_matrix_20260114_115711.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/confusion_matrix_20260114_115711.pdf'
2026-01-14 12:15:15,822 - INFO - ==================================================
2026-01-14 12:15:15,822 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-01-14 12:15:17,440 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/model_fp32_20260114_115711.onnx'에 저장되었습니다. (크기: 1.44 MB)
2026-01-14 12:15:17,863 - INFO - [Model Load] ONNX 모델(FP32) 로드 메모리: 2544.47 MB (증가량: 6.40 MB)
2026-01-14 12:15:17,863 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-01-14 12:15:19,940 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 16.07ms (std: 4.33ms)
2026-01-14 12:15:19,940 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 65.39 FPS (std: 12.69) (1개 샘플 x 100회 반복)
2026-01-14 12:15:19,940 - INFO - [Inference Only] ONNX 런타임 추론 중 최대 CPU 메모리: 2554.48 MB (순수 증가량: 10.02 MB)
2026-01-14 12:15:19,940 - INFO - [Total Process] ONNX 모델(FP32) 전체 메모리 사용량: 2554.48 MB (전체 증가량: 16.42 MB)
2026-01-14 12:15:23,816 - INFO - [Test (ONNX)] | Test Acc (ONNX): 82.30%
2026-01-14 12:15:23,826 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-01-14 12:15:23,827 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-01-14 12:15:24,227 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/val_acc.pdf'
2026-01-14 12:15:24,725 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/train_val_acc.pdf'
2026-01-14 12:15:25,080 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/F1_normal.pdf'
2026-01-14 12:15:25,529 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/val_loss.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/val_loss.pdf'
2026-01-14 12:15:25,863 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/learning_rate.pdf'
2026-01-14 12:15:29,883 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/compile.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260114_115711/graph_20260114_115711/compile.pdf'
