2026-01-14 12:18:36,610 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/log_20260114_121836.log'에 저장됩니다.
2026-01-14 12:18:36,616 - INFO - ==================================================
2026-01-14 12:18:36,616 - INFO - config.yaml:
2026-01-14 12:18:36,616 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_inference_dir: ./pretrained
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      train_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/cau/workspace/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  patch_size: 56
  stride: 56
  cnn_feature_extractor:
    name: efficientnet_b0_feat2
  featured_patch_dim: 24
  emb_dim: 24
  num_heads: 2
  num_decoder_layers: 2
  num_decoder_patches: 1
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: true
  save_attention: true
  num_plot_attention: 5
baseline:
  model_name: xie2019
  use_fpgm_pruning: true
  pruning_flops_target: 0.1829

2026-01-14 12:18:36,616 - INFO - ==================================================
2026-01-14 12:18:36,681 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA RTX PRO 6000 Blackwell Server Edition)
2026-01-14 12:18:36,681 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-01-14 12:18:36,681 - INFO - '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-01-14 12:18:36,691 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-01-14 12:18:36,692 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-01-14 12:18:36,693 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-01-14 12:18:36,693 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:18:37,041 - INFO - ==================================================
2026-01-14 12:18:37,041 - INFO - 모델 파라미터 수:
2026-01-14 12:18:37,041 - INFO -   - 총 파라미터: 9,160,194 개
2026-01-14 12:18:37,042 - INFO -   - 학습 가능한 파라미터: 9,160,194 개
2026-01-14 12:18:37,042 - INFO - ================================================================================
2026-01-14 12:18:37,042 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-01-14 12:18:37,042 - INFO - ================================================================================
2026-01-14 12:18:37,042 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:18:37,043 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-01-14 12:18:37,043 - INFO - ==================================================
2026-01-14 12:18:37,043 - INFO - train 모드를 시작합니다.
2026-01-14 12:18:37,043 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:18:37,043 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:18:37,044 - INFO - --------------------------------------------------
2026-01-14 12:18:37,044 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-01-14 12:18:43,786 - INFO - [Train] [1/10] | Loss: 0.5934 | Train Acc: 73.51%
2026-01-14 12:18:46,661 - INFO - [Valid] [1/10] | Loss: 0.5631 | Val Acc: 76.40%
2026-01-14 12:18:46,683 - INFO - [Metrics for 'abnormal'] | Precision: 0.8080 | Recall: 0.6433 | F1: 0.7163
2026-01-14 12:18:46,685 - INFO - [Metrics for 'normal'] | Precision: 0.7383 | Recall: 0.8681 | F1: 0.7980
2026-01-14 12:18:46,776 - INFO - [Best Model Saved] (val loss: 0.5631) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:18:46,777 - INFO - --------------------------------------------------
2026-01-14 12:18:46,778 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-01-14 12:18:52,500 - INFO - [Train] [2/10] | Loss: 0.5377 | Train Acc: 78.42%
2026-01-14 12:18:54,456 - INFO - [Valid] [2/10] | Loss: 0.5803 | Val Acc: 75.81%
2026-01-14 12:18:54,471 - INFO - [Metrics for 'abnormal'] | Precision: 0.6943 | Recall: 0.8535 | F1: 0.7657
2026-01-14 12:18:54,471 - INFO - [Metrics for 'normal'] | Precision: 0.8425 | Recall: 0.6758 | F1: 0.7500
2026-01-14 12:18:54,476 - INFO - --------------------------------------------------
2026-01-14 12:18:54,477 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-01-14 12:19:01,441 - INFO - [Train] [3/10] | Loss: 0.5169 | Train Acc: 80.65%
2026-01-14 12:19:03,789 - INFO - [Valid] [3/10] | Loss: 0.5382 | Val Acc: 75.81%
2026-01-14 12:19:03,813 - INFO - [Metrics for 'abnormal'] | Precision: 0.7389 | Recall: 0.7389 | F1: 0.7389
2026-01-14 12:19:03,813 - INFO - [Metrics for 'normal'] | Precision: 0.7747 | Recall: 0.7747 | F1: 0.7747
2026-01-14 12:19:03,959 - INFO - [Best Model Saved] (val loss: 0.5382) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:19:03,960 - INFO - --------------------------------------------------
2026-01-14 12:19:03,961 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-01-14 12:19:10,487 - INFO - [Train] [4/10] | Loss: 0.5006 | Train Acc: 81.32%
2026-01-14 12:19:13,010 - INFO - [Valid] [4/10] | Loss: 0.5557 | Val Acc: 76.40%
2026-01-14 12:19:13,023 - INFO - [Metrics for 'abnormal'] | Precision: 0.8235 | Recall: 0.6242 | F1: 0.7101
2026-01-14 12:19:13,023 - INFO - [Metrics for 'normal'] | Precision: 0.7318 | Recall: 0.8846 | F1: 0.8010
2026-01-14 12:19:13,027 - INFO - --------------------------------------------------
2026-01-14 12:19:13,028 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-01-14 12:19:19,797 - INFO - [Train] [5/10] | Loss: 0.4933 | Train Acc: 81.70%
2026-01-14 12:19:22,414 - INFO - [Valid] [5/10] | Loss: 0.5199 | Val Acc: 78.47%
2026-01-14 12:19:22,472 - INFO - [Metrics for 'abnormal'] | Precision: 0.7593 | Recall: 0.7834 | F1: 0.7712
2026-01-14 12:19:22,472 - INFO - [Metrics for 'normal'] | Precision: 0.8079 | Recall: 0.7857 | F1: 0.7967
2026-01-14 12:19:22,595 - INFO - [Best Model Saved] (val loss: 0.5199) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:19:22,595 - INFO - --------------------------------------------------
2026-01-14 12:19:22,596 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-01-14 12:19:30,540 - INFO - [Train] [6/10] | Loss: 0.4734 | Train Acc: 83.71%
2026-01-14 12:19:32,737 - INFO - [Valid] [6/10] | Loss: 0.5169 | Val Acc: 81.42%
2026-01-14 12:19:32,752 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-01-14 12:19:32,753 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-01-14 12:19:32,848 - INFO - [Best Model Saved] (val loss: 0.5169) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:19:32,848 - INFO - --------------------------------------------------
2026-01-14 12:19:32,849 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-01-14 12:19:39,821 - INFO - [Train] [7/10] | Loss: 0.4562 | Train Acc: 84.38%
2026-01-14 12:19:42,559 - INFO - [Valid] [7/10] | Loss: 0.5056 | Val Acc: 80.83%
2026-01-14 12:19:42,569 - INFO - [Metrics for 'abnormal'] | Precision: 0.7875 | Recall: 0.8025 | F1: 0.7950
2026-01-14 12:19:42,570 - INFO - [Metrics for 'normal'] | Precision: 0.8268 | Recall: 0.8132 | F1: 0.8199
2026-01-14 12:19:42,656 - INFO - [Best Model Saved] (val loss: 0.5056) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:19:42,657 - INFO - --------------------------------------------------
2026-01-14 12:19:42,657 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-01-14 12:19:49,748 - INFO - [Train] [8/10] | Loss: 0.4454 | Train Acc: 84.38%
2026-01-14 12:19:51,880 - INFO - [Valid] [8/10] | Loss: 0.5153 | Val Acc: 78.17%
2026-01-14 12:19:51,891 - INFO - [Metrics for 'abnormal'] | Precision: 0.7371 | Recall: 0.8217 | F1: 0.7771
2026-01-14 12:19:51,892 - INFO - [Metrics for 'normal'] | Precision: 0.8293 | Recall: 0.7473 | F1: 0.7861
2026-01-14 12:19:51,895 - INFO - --------------------------------------------------
2026-01-14 12:19:51,896 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-01-14 12:19:58,060 - INFO - [Train] [9/10] | Loss: 0.4453 | Train Acc: 84.52%
2026-01-14 12:20:00,083 - INFO - [Valid] [9/10] | Loss: 0.4986 | Val Acc: 80.53%
2026-01-14 12:20:00,095 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-01-14 12:20:00,095 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-01-14 12:20:00,184 - INFO - [Best Model Saved] (val loss: 0.4986) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:20:00,185 - INFO - --------------------------------------------------
2026-01-14 12:20:00,186 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-01-14 12:20:05,604 - INFO - [Train] [10/10] | Loss: 0.4337 | Train Acc: 85.34%
2026-01-14 12:20:07,643 - INFO - [Valid] [10/10] | Loss: 0.4990 | Val Acc: 81.71%
2026-01-14 12:20:07,655 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8025 | F1: 0.8025
2026-01-14 12:20:07,658 - INFO - [Metrics for 'normal'] | Precision: 0.8297 | Recall: 0.8297 | F1: 0.8297
2026-01-14 12:20:07,661 - INFO - ================================================================================
2026-01-14 12:20:07,662 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-01-14 12:20:07,662 - INFO - ================================================================================
2026-01-14 12:20:07,706 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'을(를) 불러왔습니다.
2026-01-14 12:20:07,707 - INFO - ================================================================================
2026-01-14 12:20:07,707 - INFO - 목표 FLOPs (0.1829 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-01-14 12:20:07,758 - INFO - 원본 모델 FLOPs: 2.8696 GFLOPs
2026-01-14 12:20:07,769 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:07,770 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:08,487 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-01-14 12:20:08,488 - INFO - ==================================================
2026-01-14 12:20:08,495 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 1.3003 GFLOPs (감소율: 54.69%)
2026-01-14 12:20:08,500 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:08,501 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:09,068 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-01-14 12:20:09,068 - INFO - ==================================================
2026-01-14 12:20:09,075 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.6166 GFLOPs (감소율: 78.51%)
2026-01-14 12:20:09,078 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:09,078 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:09,613 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-01-14 12:20:09,613 - INFO - ==================================================
2026-01-14 12:20:09,621 - INFO -   [탐색  3] 희소도: 0.8662 -> FLOPs: 0.3005 GFLOPs (감소율: 89.53%)
2026-01-14 12:20:09,625 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:09,626 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:10,202 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.928125)에 맞춰 변경되었습니다.
2026-01-14 12:20:10,202 - INFO - ==================================================
2026-01-14 12:20:10,208 - INFO -   [탐색  4] 희소도: 0.9281 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:10,212 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:10,212 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:10,794 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8971875)에 맞춰 변경되었습니다.
2026-01-14 12:20:10,795 - INFO - ==================================================
2026-01-14 12:20:10,801 - INFO -   [탐색  5] 희소도: 0.8972 -> FLOPs: 0.2238 GFLOPs (감소율: 92.20%)
2026-01-14 12:20:10,805 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:10,805 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:11,337 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.91265625)에 맞춰 변경되었습니다.
2026-01-14 12:20:11,337 - INFO - ==================================================
2026-01-14 12:20:11,345 - INFO -   [탐색  6] 희소도: 0.9127 -> FLOPs: 0.1858 GFLOPs (감소율: 93.52%)
2026-01-14 12:20:11,351 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:11,351 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:12,203 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.920390625)에 맞춰 변경되었습니다.
2026-01-14 12:20:12,203 - INFO - ==================================================
2026-01-14 12:20:12,210 - INFO -   [탐색  7] 희소도: 0.9204 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:12,215 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:12,215 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:12,794 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9242578125)에 맞춰 변경되었습니다.
2026-01-14 12:20:12,794 - INFO - ==================================================
2026-01-14 12:20:12,800 - INFO -   [탐색  8] 희소도: 0.9243 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:12,805 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:12,805 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:13,433 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.92232421875)에 맞춰 변경되었습니다.
2026-01-14 12:20:13,433 - INFO - ==================================================
2026-01-14 12:20:13,437 - INFO -   [탐색  9] 희소도: 0.9223 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:13,442 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:13,442 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:14,008 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-01-14 12:20:14,009 - INFO - ==================================================
2026-01-14 12:20:14,012 - INFO -   [탐색 10] 희소도: 0.9214 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:14,017 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:14,017 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:14,897 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218408203125)에 맞춰 변경되었습니다.
2026-01-14 12:20:14,898 - INFO - ==================================================
2026-01-14 12:20:14,902 - INFO -   [탐색 11] 희소도: 0.9218 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:14,907 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:14,907 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:15,430 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9220825195312501)에 맞춰 변경되었습니다.
2026-01-14 12:20:15,431 - INFO - ==================================================
2026-01-14 12:20:15,435 - INFO -   [탐색 12] 희소도: 0.9221 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:15,438 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:15,439 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:16,031 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9219616699218751)에 맞춰 변경되었습니다.
2026-01-14 12:20:16,032 - INFO - ==================================================
2026-01-14 12:20:16,036 - INFO -   [탐색 13] 희소도: 0.9220 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:16,040 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:16,040 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:16,665 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9219012451171875)에 맞춰 변경되었습니다.
2026-01-14 12:20:16,665 - INFO - ==================================================
2026-01-14 12:20:16,669 - INFO -   [탐색 14] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:16,673 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:16,673 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:17,237 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218710327148438)에 맞춰 변경되었습니다.
2026-01-14 12:20:17,238 - INFO - ==================================================
2026-01-14 12:20:17,243 - INFO -   [탐색 15] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:17,247 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:17,247 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:18,119 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218861389160157)에 맞춰 변경되었습니다.
2026-01-14 12:20:18,119 - INFO - ==================================================
2026-01-14 12:20:18,125 - INFO -   [탐색 16] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:18,130 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:18,130 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:18,677 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218785858154297)에 맞춰 변경되었습니다.
2026-01-14 12:20:18,677 - INFO - ==================================================
2026-01-14 12:20:18,682 - INFO -   [탐색 17] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:18,690 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:18,691 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:19,447 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218748092651368)에 맞춰 변경되었습니다.
2026-01-14 12:20:19,448 - INFO - ==================================================
2026-01-14 12:20:19,451 - INFO -   [탐색 18] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:19,454 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:19,454 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:20,276 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218766975402832)에 맞춰 변경되었습니다.
2026-01-14 12:20:20,277 - INFO - ==================================================
2026-01-14 12:20:20,282 - INFO -   [탐색 19] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:20,288 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:20,289 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:20,997 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.92187575340271)에 맞춰 변경되었습니다.
2026-01-14 12:20:20,998 - INFO - ==================================================
2026-01-14 12:20:21,004 - INFO -   [탐색 20] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:21,009 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:21,010 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:22,039 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218752813339234)에 맞춰 변경되었습니다.
2026-01-14 12:20:22,039 - INFO - ==================================================
2026-01-14 12:20:22,046 - INFO -   [탐색 21] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:22,049 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:22,050 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:22,695 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750452995301)에 맞춰 변경되었습니다.
2026-01-14 12:20:22,695 - INFO - ==================================================
2026-01-14 12:20:22,699 - INFO -   [탐색 22] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:22,703 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:22,704 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:23,293 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749272823334)에 맞춰 변경되었습니다.
2026-01-14 12:20:23,293 - INFO - ==================================================
2026-01-14 12:20:23,299 - INFO -   [탐색 23] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:23,305 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:23,305 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:23,917 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749862909318)에 맞춰 변경되었습니다.
2026-01-14 12:20:23,917 - INFO - ==================================================
2026-01-14 12:20:23,921 - INFO -   [탐색 24] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:23,925 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:23,925 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:24,502 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750157952309)에 맞춰 변경되었습니다.
2026-01-14 12:20:24,502 - INFO - ==================================================
2026-01-14 12:20:24,506 - INFO -   [탐색 25] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:24,510 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:24,510 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:25,138 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750010430814)에 맞춰 변경되었습니다.
2026-01-14 12:20:25,139 - INFO - ==================================================
2026-01-14 12:20:25,142 - INFO -   [탐색 26] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:25,146 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:25,146 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:25,955 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749936670065)에 맞춰 변경되었습니다.
2026-01-14 12:20:25,955 - INFO - ==================================================
2026-01-14 12:20:25,960 - INFO -   [탐색 27] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:25,965 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:25,965 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:26,607 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921874997355044)에 맞춰 변경되었습니다.
2026-01-14 12:20:26,608 - INFO - ==================================================
2026-01-14 12:20:26,613 - INFO -   [탐색 28] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:26,619 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:26,620 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:27,484 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749991990627)에 맞춰 변경되었습니다.
2026-01-14 12:20:27,487 - INFO - ==================================================
2026-01-14 12:20:27,493 - INFO -   [탐색 29] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:27,498 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:27,500 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:28,293 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875000121072)에 맞춰 변경되었습니다.
2026-01-14 12:20:28,293 - INFO - ==================================================
2026-01-14 12:20:28,297 - INFO -   [탐색 30] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:28,301 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:28,301 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:29,268 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749996600674)에 맞춰 변경되었습니다.
2026-01-14 12:20:29,269 - INFO - ==================================================
2026-01-14 12:20:29,272 - INFO -   [탐색 31] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:29,276 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:29,276 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:29,829 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749998905698)에 맞춰 변경되었습니다.
2026-01-14 12:20:29,829 - INFO - ==================================================
2026-01-14 12:20:29,833 - INFO -   [탐색 32] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:29,835 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:29,836 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:30,562 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000058209)에 맞춰 변경되었습니다.
2026-01-14 12:20:30,562 - INFO - ==================================================
2026-01-14 12:20:30,565 - INFO -   [탐색 33] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:30,569 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:30,569 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:31,157 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999481953)에 맞춰 변경되었습니다.
2026-01-14 12:20:31,157 - INFO - ==================================================
2026-01-14 12:20:31,160 - INFO -   [탐색 34] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:31,164 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:31,164 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:31,665 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999770082)에 맞춰 변경되었습니다.
2026-01-14 12:20:31,665 - INFO - ==================================================
2026-01-14 12:20:31,669 - INFO -   [탐색 35] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:31,673 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:31,674 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:32,565 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999914145)에 맞춰 변경되었습니다.
2026-01-14 12:20:32,565 - INFO - ==================================================
2026-01-14 12:20:32,569 - INFO -   [탐색 36] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:32,573 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:32,573 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:33,069 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999986178)에 맞춰 변경되었습니다.
2026-01-14 12:20:33,069 - INFO - ==================================================
2026-01-14 12:20:33,073 - INFO -   [탐색 37] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:33,077 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:33,077 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:33,751 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000022193)에 맞춰 변경되었습니다.
2026-01-14 12:20:33,752 - INFO - ==================================================
2026-01-14 12:20:33,763 - INFO -   [탐색 38] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:33,767 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:33,768 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:34,708 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000004186)에 맞춰 변경되었습니다.
2026-01-14 12:20:34,709 - INFO - ==================================================
2026-01-14 12:20:34,714 - INFO -   [탐색 39] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:34,718 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:34,718 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:35,429 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999995182)에 맞춰 변경되었습니다.
2026-01-14 12:20:35,430 - INFO - ==================================================
2026-01-14 12:20:35,435 - INFO -   [탐색 40] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:35,444 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:35,449 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:36,464 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999999684)에 맞춰 변경되었습니다.
2026-01-14 12:20:36,465 - INFO - ==================================================
2026-01-14 12:20:36,471 - INFO -   [탐색 41] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:36,477 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:36,478 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:37,066 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000001934)에 맞춰 변경되었습니다.
2026-01-14 12:20:37,067 - INFO - ==================================================
2026-01-14 12:20:37,070 - INFO -   [탐색 42] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:37,073 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:37,073 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:37,730 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000808)에 맞춰 변경되었습니다.
2026-01-14 12:20:37,731 - INFO - ==================================================
2026-01-14 12:20:37,736 - INFO -   [탐색 43] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:37,743 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:37,744 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:38,332 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000246)에 맞춰 변경되었습니다.
2026-01-14 12:20:38,333 - INFO - ==================================================
2026-01-14 12:20:38,338 - INFO -   [탐색 44] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:38,343 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:38,344 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:39,016 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999999964)에 맞춰 변경되었습니다.
2026-01-14 12:20:39,017 - INFO - ==================================================
2026-01-14 12:20:39,026 - INFO -   [탐색 45] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:39,034 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:39,034 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:39,670 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000105)에 맞춰 변경되었습니다.
2026-01-14 12:20:39,671 - INFO - ==================================================
2026-01-14 12:20:39,678 - INFO -   [탐색 46] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:39,684 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:39,685 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:40,573 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000036)에 맞춰 변경되었습니다.
2026-01-14 12:20:40,574 - INFO - ==================================================
2026-01-14 12:20:40,579 - INFO -   [탐색 47] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:40,584 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:40,584 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:41,210 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:41,211 - INFO - ==================================================
2026-01-14 12:20:41,216 - INFO -   [탐색 48] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:41,221 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:41,222 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:41,868 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000018)에 맞춰 변경되었습니다.
2026-01-14 12:20:41,868 - INFO - ==================================================
2026-01-14 12:20:41,873 - INFO -   [탐색 49] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:41,879 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:41,879 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:42,519 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000009)에 맞춰 변경되었습니다.
2026-01-14 12:20:42,520 - INFO - ==================================================
2026-01-14 12:20:42,525 - INFO -   [탐색 50] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:42,531 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:42,531 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:43,408 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000004)에 맞춰 변경되었습니다.
2026-01-14 12:20:43,409 - INFO - ==================================================
2026-01-14 12:20:43,414 - INFO -   [탐색 51] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:43,421 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:43,422 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:44,052 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000002)에 맞춰 변경되었습니다.
2026-01-14 12:20:44,052 - INFO - ==================================================
2026-01-14 12:20:44,058 - INFO -   [탐색 52] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:44,063 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:44,064 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:44,710 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000001)에 맞춰 변경되었습니다.
2026-01-14 12:20:44,711 - INFO - ==================================================
2026-01-14 12:20:44,717 - INFO -   [탐색 53] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-01-14 12:20:44,722 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:44,722 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:45,354 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:45,354 - INFO - ==================================================
2026-01-14 12:20:45,359 - INFO -   [탐색 54] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:45,365 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:45,365 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:46,000 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:46,000 - INFO - ==================================================
2026-01-14 12:20:46,005 - INFO -   [탐색 55] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:46,010 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:46,010 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:47,026 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:47,026 - INFO - ==================================================
2026-01-14 12:20:47,030 - INFO -   [탐색 56] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:47,033 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:47,034 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:47,720 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:47,720 - INFO - ==================================================
2026-01-14 12:20:47,726 - INFO -   [탐색 57] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:47,731 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:47,732 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:48,372 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:48,372 - INFO - ==================================================
2026-01-14 12:20:48,377 - INFO -   [탐색 58] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:48,383 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:48,384 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:49,025 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:49,026 - INFO - ==================================================
2026-01-14 12:20:49,032 - INFO -   [탐색 59] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:49,037 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:49,037 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:49,664 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:49,665 - INFO - ==================================================
2026-01-14 12:20:49,671 - INFO -   [탐색 60] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:49,677 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:49,677 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:50,520 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:50,520 - INFO - ==================================================
2026-01-14 12:20:50,524 - INFO -   [탐색 61] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:50,528 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:50,529 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:51,017 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:51,017 - INFO - ==================================================
2026-01-14 12:20:51,021 - INFO -   [탐색 62] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:51,024 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:51,024 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:51,560 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:51,560 - INFO - ==================================================
2026-01-14 12:20:51,566 - INFO -   [탐색 63] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:51,572 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:51,572 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:52,224 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:52,224 - INFO - ==================================================
2026-01-14 12:20:52,228 - INFO -   [탐색 64] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:52,233 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:52,233 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:52,761 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:52,762 - INFO - ==================================================
2026-01-14 12:20:52,766 - INFO -   [탐색 65] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:52,771 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:52,771 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:53,430 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:53,431 - INFO - ==================================================
2026-01-14 12:20:53,436 - INFO -   [탐색 66] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:53,442 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:53,443 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:54,303 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:54,303 - INFO - ==================================================
2026-01-14 12:20:54,309 - INFO -   [탐색 67] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:54,314 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:54,315 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:54,924 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:54,925 - INFO - ==================================================
2026-01-14 12:20:54,931 - INFO -   [탐색 68] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:54,936 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:54,937 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:55,529 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:55,530 - INFO - ==================================================
2026-01-14 12:20:55,535 - INFO -   [탐색 69] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:55,540 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:55,540 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:56,135 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:56,136 - INFO - ==================================================
2026-01-14 12:20:56,143 - INFO -   [탐색 70] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:56,149 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:56,149 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:56,914 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:56,914 - INFO - ==================================================
2026-01-14 12:20:56,920 - INFO -   [탐색 71] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:56,925 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:56,925 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:57,576 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:57,577 - INFO - ==================================================
2026-01-14 12:20:57,582 - INFO -   [탐색 72] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:57,587 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:57,587 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:58,281 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:58,282 - INFO - ==================================================
2026-01-14 12:20:58,286 - INFO -   [탐색 73] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:58,291 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:58,292 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:58,842 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:58,843 - INFO - ==================================================
2026-01-14 12:20:58,847 - INFO -   [탐색 74] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:58,852 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:58,852 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:59,473 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:20:59,474 - INFO - ==================================================
2026-01-14 12:20:59,479 - INFO -   [탐색 75] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:20:59,485 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:59,485 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:00,373 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:00,374 - INFO - ==================================================
2026-01-14 12:21:00,384 - INFO -   [탐색 76] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:00,394 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:00,395 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:01,035 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:01,036 - INFO - ==================================================
2026-01-14 12:21:01,040 - INFO -   [탐색 77] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:01,046 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:01,047 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:01,693 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:01,697 - INFO - ==================================================
2026-01-14 12:21:01,701 - INFO -   [탐색 78] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:01,705 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:01,705 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:02,256 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:02,257 - INFO - ==================================================
2026-01-14 12:21:02,262 - INFO -   [탐색 79] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:02,268 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:02,269 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:02,863 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:02,864 - INFO - ==================================================
2026-01-14 12:21:02,869 - INFO -   [탐색 80] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:02,874 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:02,875 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:03,751 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:03,751 - INFO - ==================================================
2026-01-14 12:21:03,756 - INFO -   [탐색 81] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:03,761 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:03,762 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:04,422 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:04,423 - INFO - ==================================================
2026-01-14 12:21:04,427 - INFO -   [탐색 82] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:04,432 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:04,433 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:05,049 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:05,049 - INFO - ==================================================
2026-01-14 12:21:05,054 - INFO -   [탐색 83] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:05,059 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:05,059 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:05,639 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:05,640 - INFO - ==================================================
2026-01-14 12:21:05,645 - INFO -   [탐색 84] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:05,650 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:05,651 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:06,214 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:06,214 - INFO - ==================================================
2026-01-14 12:21:06,218 - INFO -   [탐색 85] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:06,222 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:06,222 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:06,835 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:06,835 - INFO - ==================================================
2026-01-14 12:21:06,841 - INFO -   [탐색 86] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:06,846 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:06,846 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:07,686 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:07,686 - INFO - ==================================================
2026-01-14 12:21:07,691 - INFO -   [탐색 87] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:07,695 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:07,696 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:08,344 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:08,345 - INFO - ==================================================
2026-01-14 12:21:08,349 - INFO -   [탐색 88] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:08,354 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:08,355 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:09,214 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:09,215 - INFO - ==================================================
2026-01-14 12:21:09,220 - INFO -   [탐색 89] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:09,225 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:09,226 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:09,854 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:09,855 - INFO - ==================================================
2026-01-14 12:21:09,861 - INFO -   [탐색 90] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:09,866 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:09,867 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:10,773 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:10,773 - INFO - ==================================================
2026-01-14 12:21:10,778 - INFO -   [탐색 91] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:10,783 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:10,784 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:11,344 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:11,344 - INFO - ==================================================
2026-01-14 12:21:11,347 - INFO -   [탐색 92] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:11,350 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:11,351 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:11,894 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:11,895 - INFO - ==================================================
2026-01-14 12:21:11,899 - INFO -   [탐색 93] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:11,903 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:11,903 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:12,505 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:12,506 - INFO - ==================================================
2026-01-14 12:21:12,509 - INFO -   [탐색 94] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:12,512 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:12,513 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:13,027 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:13,027 - INFO - ==================================================
2026-01-14 12:21:13,031 - INFO -   [탐색 95] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:13,034 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:13,034 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:14,287 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:14,287 - INFO - ==================================================
2026-01-14 12:21:14,293 - INFO -   [탐색 96] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:14,298 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:14,299 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:15,001 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:15,001 - INFO - ==================================================
2026-01-14 12:21:15,006 - INFO -   [탐색 97] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:15,010 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:15,011 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:15,607 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:15,608 - INFO - ==================================================
2026-01-14 12:21:15,614 - INFO -   [탐색 98] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:15,620 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:15,620 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:16,246 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:16,247 - INFO - ==================================================
2026-01-14 12:21:16,253 - INFO -   [탐색 99] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:16,258 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:16,259 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:16,884 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-01-14 12:21:16,885 - INFO - ==================================================
2026-01-14 12:21:16,890 - INFO -   [탐색 100] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-01-14 12:21:16,891 - INFO - 탐색 완료. 목표 FLOPs(0.1829)에 가장 근접한 최적 희소도는 0.9214 입니다.
2026-01-14 12:21:16,892 - INFO - ================================================================================
2026-01-14 12:21:16,895 - INFO - 계산된 Pruning 정보(희소도: 0.9214)를 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/pruning_info.yaml'에 저장했습니다.
2026-01-14 12:21:16,903 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-01-14 12:21:16,915 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:16,916 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:17,814 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-01-14 12:21:17,815 - INFO - ==================================================
2026-01-14 12:21:17,815 - INFO - ==================================================
2026-01-14 12:21:17,815 - INFO - 모델 파라미터 수:
2026-01-14 12:21:17,816 - INFO -   - 총 파라미터: 57,792 개
2026-01-14 12:21:17,816 - INFO -   - 학습 가능한 파라미터: 57,792 개
2026-01-14 12:21:17,823 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-01-14 12:21:17,833 - INFO - FLOPs가 2.8696 GFLOPs에서 0.1854 GFLOPs로 감소했습니다 (감소율: 93.54%).
2026-01-14 12:21:17,833 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-01-14 12:21:17,834 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:21:17,834 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-01-14 12:21:17,835 - INFO - ==================================================
2026-01-14 12:21:17,835 - INFO - train 모드를 시작합니다.
2026-01-14 12:21:17,835 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:21:17,835 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:21:17,835 - INFO - --------------------------------------------------
2026-01-14 12:21:17,835 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-01-14 12:21:23,198 - INFO - [Train] [11/90] | Loss: 0.5403 | Train Acc: 77.60%
2026-01-14 12:21:25,297 - INFO - [Valid] [11/90] | Loss: 0.5250 | Val Acc: 76.70%
2026-01-14 12:21:25,310 - INFO - [Metrics for 'abnormal'] | Precision: 0.7378 | Recall: 0.7707 | F1: 0.7539
2026-01-14 12:21:25,311 - INFO - [Metrics for 'normal'] | Precision: 0.7943 | Recall: 0.7637 | F1: 0.7787
2026-01-14 12:21:25,322 - INFO - [Best Model Saved] (val loss: 0.5250) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:21:25,322 - INFO - --------------------------------------------------
2026-01-14 12:21:25,323 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-01-14 12:21:30,870 - INFO - [Train] [12/90] | Loss: 0.4902 | Train Acc: 82.14%
2026-01-14 12:21:33,508 - INFO - [Valid] [12/90] | Loss: 0.5149 | Val Acc: 76.40%
2026-01-14 12:21:33,521 - INFO - [Metrics for 'abnormal'] | Precision: 0.7127 | Recall: 0.8217 | F1: 0.7633
2026-01-14 12:21:33,521 - INFO - [Metrics for 'normal'] | Precision: 0.8228 | Recall: 0.7143 | F1: 0.7647
2026-01-14 12:21:33,531 - INFO - [Best Model Saved] (val loss: 0.5149) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:21:33,531 - INFO - --------------------------------------------------
2026-01-14 12:21:33,532 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-01-14 12:21:39,816 - INFO - [Train] [13/90] | Loss: 0.5021 | Train Acc: 81.32%
2026-01-14 12:21:41,808 - INFO - [Valid] [13/90] | Loss: 0.5066 | Val Acc: 79.06%
2026-01-14 12:21:41,824 - INFO - [Metrics for 'abnormal'] | Precision: 0.7654 | Recall: 0.7898 | F1: 0.7774
2026-01-14 12:21:41,824 - INFO - [Metrics for 'normal'] | Precision: 0.8136 | Recall: 0.7912 | F1: 0.8022
2026-01-14 12:21:41,835 - INFO - [Best Model Saved] (val loss: 0.5066) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:21:41,836 - INFO - --------------------------------------------------
2026-01-14 12:21:41,837 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-01-14 12:21:48,150 - INFO - [Train] [14/90] | Loss: 0.4872 | Train Acc: 83.11%
2026-01-14 12:21:50,834 - INFO - [Valid] [14/90] | Loss: 0.5234 | Val Acc: 76.11%
2026-01-14 12:21:50,846 - INFO - [Metrics for 'abnormal'] | Precision: 0.7135 | Recall: 0.8089 | F1: 0.7582
2026-01-14 12:21:50,847 - INFO - [Metrics for 'normal'] | Precision: 0.8137 | Recall: 0.7198 | F1: 0.7638
2026-01-14 12:21:50,851 - INFO - --------------------------------------------------
2026-01-14 12:21:50,852 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-01-14 12:21:57,387 - INFO - [Train] [15/90] | Loss: 0.4744 | Train Acc: 82.22%
2026-01-14 12:21:59,769 - INFO - [Valid] [15/90] | Loss: 0.5205 | Val Acc: 77.29%
2026-01-14 12:21:59,781 - INFO - [Metrics for 'abnormal'] | Precision: 0.7247 | Recall: 0.8217 | F1: 0.7701
2026-01-14 12:21:59,782 - INFO - [Metrics for 'normal'] | Precision: 0.8261 | Recall: 0.7308 | F1: 0.7755
2026-01-14 12:21:59,786 - INFO - --------------------------------------------------
2026-01-14 12:21:59,787 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-01-14 12:22:06,699 - INFO - [Train] [16/90] | Loss: 0.4691 | Train Acc: 83.41%
2026-01-14 12:22:09,121 - INFO - [Valid] [16/90] | Loss: 0.5149 | Val Acc: 77.29%
2026-01-14 12:22:09,137 - INFO - [Metrics for 'abnormal'] | Precision: 0.7222 | Recall: 0.8280 | F1: 0.7715
2026-01-14 12:22:09,137 - INFO - [Metrics for 'normal'] | Precision: 0.8302 | Recall: 0.7253 | F1: 0.7742
2026-01-14 12:22:09,151 - INFO - --------------------------------------------------
2026-01-14 12:22:09,151 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-01-14 12:22:16,105 - INFO - [Train] [17/90] | Loss: 0.4656 | Train Acc: 82.74%
2026-01-14 12:22:18,609 - INFO - [Valid] [17/90] | Loss: 0.4991 | Val Acc: 81.12%
2026-01-14 12:22:18,686 - INFO - [Metrics for 'abnormal'] | Precision: 0.8079 | Recall: 0.7771 | F1: 0.7922
2026-01-14 12:22:18,693 - INFO - [Metrics for 'normal'] | Precision: 0.8138 | Recall: 0.8407 | F1: 0.8270
2026-01-14 12:22:18,714 - INFO - [Best Model Saved] (val loss: 0.4991) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:22:18,718 - INFO - --------------------------------------------------
2026-01-14 12:22:18,718 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-01-14 12:22:25,316 - INFO - [Train] [18/90] | Loss: 0.4724 | Train Acc: 84.52%
2026-01-14 12:22:27,501 - INFO - [Valid] [18/90] | Loss: 0.4983 | Val Acc: 79.35%
2026-01-14 12:22:27,539 - INFO - [Metrics for 'abnormal'] | Precision: 0.7669 | Recall: 0.7962 | F1: 0.7812
2026-01-14 12:22:27,539 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.7912 | F1: 0.8045
2026-01-14 12:22:27,561 - INFO - [Best Model Saved] (val loss: 0.4983) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:22:27,561 - INFO - --------------------------------------------------
2026-01-14 12:22:27,561 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-01-14 12:22:34,791 - INFO - [Train] [19/90] | Loss: 0.4637 | Train Acc: 82.96%
2026-01-14 12:22:36,953 - INFO - [Valid] [19/90] | Loss: 0.5081 | Val Acc: 78.47%
2026-01-14 12:22:36,970 - INFO - [Metrics for 'abnormal'] | Precision: 0.7530 | Recall: 0.7962 | F1: 0.7740
2026-01-14 12:22:36,971 - INFO - [Metrics for 'normal'] | Precision: 0.8150 | Recall: 0.7747 | F1: 0.7944
2026-01-14 12:22:36,976 - INFO - --------------------------------------------------
2026-01-14 12:22:36,976 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-01-14 12:22:44,286 - INFO - [Train] [20/90] | Loss: 0.4517 | Train Acc: 84.60%
2026-01-14 12:22:46,038 - INFO - [Valid] [20/90] | Loss: 0.4879 | Val Acc: 82.30%
2026-01-14 12:22:46,064 - INFO - [Metrics for 'abnormal'] | Precision: 0.8255 | Recall: 0.7834 | F1: 0.8039
2026-01-14 12:22:46,064 - INFO - [Metrics for 'normal'] | Precision: 0.8211 | Recall: 0.8571 | F1: 0.8387
2026-01-14 12:22:46,079 - INFO - [Best Model Saved] (val loss: 0.4879) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:22:46,079 - INFO - --------------------------------------------------
2026-01-14 12:22:46,079 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-01-14 12:22:53,919 - INFO - [Train] [21/90] | Loss: 0.4517 | Train Acc: 84.75%
2026-01-14 12:22:55,486 - INFO - [Valid] [21/90] | Loss: 0.4997 | Val Acc: 81.12%
2026-01-14 12:22:55,511 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-01-14 12:22:55,511 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-01-14 12:22:55,517 - INFO - --------------------------------------------------
2026-01-14 12:22:55,517 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-01-14 12:23:04,008 - INFO - [Train] [22/90] | Loss: 0.4705 | Train Acc: 84.23%
2026-01-14 12:23:06,722 - INFO - [Valid] [22/90] | Loss: 0.5017 | Val Acc: 81.42%
2026-01-14 12:23:06,766 - INFO - [Metrics for 'abnormal'] | Precision: 0.8219 | Recall: 0.7643 | F1: 0.7921
2026-01-14 12:23:06,768 - INFO - [Metrics for 'normal'] | Precision: 0.8083 | Recall: 0.8571 | F1: 0.8320
2026-01-14 12:23:06,771 - INFO - --------------------------------------------------
2026-01-14 12:23:06,772 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-01-14 12:23:15,096 - INFO - [Train] [23/90] | Loss: 0.4503 | Train Acc: 84.82%
2026-01-14 12:23:17,611 - INFO - [Valid] [23/90] | Loss: 0.5067 | Val Acc: 79.94%
2026-01-14 12:23:17,622 - INFO - [Metrics for 'abnormal'] | Precision: 0.7834 | Recall: 0.7834 | F1: 0.7834
2026-01-14 12:23:17,623 - INFO - [Metrics for 'normal'] | Precision: 0.8132 | Recall: 0.8132 | F1: 0.8132
2026-01-14 12:23:17,627 - INFO - --------------------------------------------------
2026-01-14 12:23:17,628 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-01-14 12:23:25,332 - INFO - [Train] [24/90] | Loss: 0.4445 | Train Acc: 85.79%
2026-01-14 12:23:28,366 - INFO - [Valid] [24/90] | Loss: 0.5214 | Val Acc: 78.17%
2026-01-14 12:23:28,410 - INFO - [Metrics for 'abnormal'] | Precision: 0.7268 | Recall: 0.8471 | F1: 0.7824
2026-01-14 12:23:28,410 - INFO - [Metrics for 'normal'] | Precision: 0.8462 | Recall: 0.7253 | F1: 0.7811
2026-01-14 12:23:28,422 - INFO - --------------------------------------------------
2026-01-14 12:23:28,430 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-01-14 12:23:36,250 - INFO - [Train] [25/90] | Loss: 0.4507 | Train Acc: 86.38%
2026-01-14 12:23:39,455 - INFO - [Valid] [25/90] | Loss: 0.5000 | Val Acc: 82.30%
2026-01-14 12:23:39,467 - INFO - [Metrics for 'abnormal'] | Precision: 0.8255 | Recall: 0.7834 | F1: 0.8039
2026-01-14 12:23:39,468 - INFO - [Metrics for 'normal'] | Precision: 0.8211 | Recall: 0.8571 | F1: 0.8387
2026-01-14 12:23:39,473 - INFO - --------------------------------------------------
2026-01-14 12:23:39,474 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-01-14 12:23:48,294 - INFO - [Train] [26/90] | Loss: 0.4426 | Train Acc: 86.01%
2026-01-14 12:23:50,356 - INFO - [Valid] [26/90] | Loss: 0.5153 | Val Acc: 77.88%
2026-01-14 12:23:50,389 - INFO - [Metrics for 'abnormal'] | Precision: 0.7303 | Recall: 0.8280 | F1: 0.7761
2026-01-14 12:23:50,389 - INFO - [Metrics for 'normal'] | Precision: 0.8323 | Recall: 0.7363 | F1: 0.7813
2026-01-14 12:23:50,398 - INFO - --------------------------------------------------
2026-01-14 12:23:50,400 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-01-14 12:24:00,759 - INFO - [Train] [27/90] | Loss: 0.4307 | Train Acc: 86.24%
2026-01-14 12:24:03,156 - INFO - [Valid] [27/90] | Loss: 0.5175 | Val Acc: 82.01%
2026-01-14 12:24:03,166 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.7643 | F1: 0.7973
2026-01-14 12:24:03,166 - INFO - [Metrics for 'normal'] | Precision: 0.8103 | Recall: 0.8681 | F1: 0.8382
2026-01-14 12:24:03,170 - INFO - --------------------------------------------------
2026-01-14 12:24:03,170 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-01-14 12:24:13,912 - INFO - [Train] [28/90] | Loss: 0.4405 | Train Acc: 85.94%
2026-01-14 12:24:16,226 - INFO - [Valid] [28/90] | Loss: 0.4995 | Val Acc: 78.76%
2026-01-14 12:24:16,250 - INFO - [Metrics for 'abnormal'] | Precision: 0.7485 | Recall: 0.8153 | F1: 0.7805
2026-01-14 12:24:16,250 - INFO - [Metrics for 'normal'] | Precision: 0.8274 | Recall: 0.7637 | F1: 0.7943
2026-01-14 12:24:16,257 - INFO - --------------------------------------------------
2026-01-14 12:24:16,259 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-01-14 12:24:26,229 - INFO - [Train] [29/90] | Loss: 0.4194 | Train Acc: 86.01%
2026-01-14 12:24:28,507 - INFO - [Valid] [29/90] | Loss: 0.4985 | Val Acc: 79.06%
2026-01-14 12:24:28,544 - INFO - [Metrics for 'abnormal'] | Precision: 0.7590 | Recall: 0.8025 | F1: 0.7802
2026-01-14 12:24:28,545 - INFO - [Metrics for 'normal'] | Precision: 0.8208 | Recall: 0.7802 | F1: 0.8000
2026-01-14 12:24:28,550 - INFO - --------------------------------------------------
2026-01-14 12:24:28,551 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-01-14 12:24:39,137 - INFO - [Train] [30/90] | Loss: 0.4181 | Train Acc: 87.65%
2026-01-14 12:24:41,330 - INFO - [Valid] [30/90] | Loss: 0.5145 | Val Acc: 80.53%
2026-01-14 12:24:41,386 - INFO - [Metrics for 'abnormal'] | Precision: 0.7791 | Recall: 0.8089 | F1: 0.7937
2026-01-14 12:24:41,390 - INFO - [Metrics for 'normal'] | Precision: 0.8295 | Recall: 0.8022 | F1: 0.8156
2026-01-14 12:24:41,396 - INFO - --------------------------------------------------
2026-01-14 12:24:41,397 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-01-14 12:24:51,344 - INFO - [Train] [31/90] | Loss: 0.4197 | Train Acc: 87.05%
2026-01-14 12:24:53,906 - INFO - [Valid] [31/90] | Loss: 0.5221 | Val Acc: 78.17%
2026-01-14 12:24:53,931 - INFO - [Metrics for 'abnormal'] | Precision: 0.7371 | Recall: 0.8217 | F1: 0.7771
2026-01-14 12:24:53,932 - INFO - [Metrics for 'normal'] | Precision: 0.8293 | Recall: 0.7473 | F1: 0.7861
2026-01-14 12:24:53,937 - INFO - --------------------------------------------------
2026-01-14 12:24:53,938 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-01-14 12:25:03,501 - INFO - [Train] [32/90] | Loss: 0.4160 | Train Acc: 88.32%
2026-01-14 12:25:05,988 - INFO - [Valid] [32/90] | Loss: 0.4897 | Val Acc: 82.89%
2026-01-14 12:25:06,032 - INFO - [Metrics for 'abnormal'] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-01-14 12:25:06,034 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-01-14 12:25:06,062 - INFO - --------------------------------------------------
2026-01-14 12:25:06,063 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-01-14 12:25:16,185 - INFO - [Train] [33/90] | Loss: 0.4059 | Train Acc: 87.50%
2026-01-14 12:25:19,778 - INFO - [Valid] [33/90] | Loss: 0.5090 | Val Acc: 82.01%
2026-01-14 12:25:19,791 - INFO - [Metrics for 'abnormal'] | Precision: 0.7963 | Recall: 0.8217 | F1: 0.8088
2026-01-14 12:25:19,792 - INFO - [Metrics for 'normal'] | Precision: 0.8418 | Recall: 0.8187 | F1: 0.8301
2026-01-14 12:25:19,796 - INFO - --------------------------------------------------
2026-01-14 12:25:19,797 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-01-14 12:25:28,955 - INFO - [Train] [34/90] | Loss: 0.4096 | Train Acc: 88.32%
2026-01-14 12:25:31,893 - INFO - [Valid] [34/90] | Loss: 0.5025 | Val Acc: 82.01%
2026-01-14 12:25:32,019 - INFO - [Metrics for 'abnormal'] | Precision: 0.8380 | Recall: 0.7580 | F1: 0.7960
2026-01-14 12:25:32,019 - INFO - [Metrics for 'normal'] | Precision: 0.8071 | Recall: 0.8736 | F1: 0.8391
2026-01-14 12:25:32,024 - INFO - --------------------------------------------------
2026-01-14 12:25:32,024 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-01-14 12:25:40,941 - INFO - [Train] [35/90] | Loss: 0.4130 | Train Acc: 88.17%
2026-01-14 12:25:43,835 - INFO - [Valid] [35/90] | Loss: 0.5092 | Val Acc: 79.65%
2026-01-14 12:25:43,849 - INFO - [Metrics for 'abnormal'] | Precision: 0.7683 | Recall: 0.8025 | F1: 0.7850
2026-01-14 12:25:43,850 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.7912 | F1: 0.8067
2026-01-14 12:25:43,856 - INFO - --------------------------------------------------
2026-01-14 12:25:43,857 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-01-14 12:25:52,705 - INFO - [Train] [36/90] | Loss: 0.3930 | Train Acc: 89.29%
2026-01-14 12:25:55,498 - INFO - [Valid] [36/90] | Loss: 0.5118 | Val Acc: 81.42%
2026-01-14 12:25:55,510 - INFO - [Metrics for 'abnormal'] | Precision: 0.8013 | Recall: 0.7962 | F1: 0.7987
2026-01-14 12:25:55,511 - INFO - [Metrics for 'normal'] | Precision: 0.8251 | Recall: 0.8297 | F1: 0.8274
2026-01-14 12:25:55,514 - INFO - --------------------------------------------------
2026-01-14 12:25:55,516 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-01-14 12:26:04,970 - INFO - [Train] [37/90] | Loss: 0.3907 | Train Acc: 89.58%
2026-01-14 12:26:08,160 - INFO - [Valid] [37/90] | Loss: 0.5129 | Val Acc: 80.83%
2026-01-14 12:26:08,170 - INFO - [Metrics for 'abnormal'] | Precision: 0.7556 | Recall: 0.8662 | F1: 0.8071
2026-01-14 12:26:08,171 - INFO - [Metrics for 'normal'] | Precision: 0.8679 | Recall: 0.7582 | F1: 0.8094
2026-01-14 12:26:08,174 - INFO - --------------------------------------------------
2026-01-14 12:26:08,175 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-01-14 12:26:16,763 - INFO - [Train] [38/90] | Loss: 0.3911 | Train Acc: 89.06%
2026-01-14 12:26:19,787 - INFO - [Valid] [38/90] | Loss: 0.5107 | Val Acc: 82.01%
2026-01-14 12:26:19,805 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 12:26:19,807 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 12:26:19,815 - INFO - --------------------------------------------------
2026-01-14 12:26:19,818 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-01-14 12:26:28,735 - INFO - [Train] [39/90] | Loss: 0.3805 | Train Acc: 89.81%
2026-01-14 12:26:31,170 - INFO - [Valid] [39/90] | Loss: 0.5276 | Val Acc: 81.12%
2026-01-14 12:26:31,181 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-01-14 12:26:31,181 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-01-14 12:26:31,185 - INFO - --------------------------------------------------
2026-01-14 12:26:31,186 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-01-14 12:26:41,108 - INFO - [Train] [40/90] | Loss: 0.3769 | Train Acc: 91.00%
2026-01-14 12:26:43,688 - INFO - [Valid] [40/90] | Loss: 0.5016 | Val Acc: 82.01%
2026-01-14 12:26:43,737 - INFO - [Metrics for 'abnormal'] | Precision: 0.7963 | Recall: 0.8217 | F1: 0.8088
2026-01-14 12:26:43,737 - INFO - [Metrics for 'normal'] | Precision: 0.8418 | Recall: 0.8187 | F1: 0.8301
2026-01-14 12:26:43,746 - INFO - --------------------------------------------------
2026-01-14 12:26:43,746 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-01-14 12:26:53,039 - INFO - [Train] [41/90] | Loss: 0.3777 | Train Acc: 90.40%
2026-01-14 12:26:55,092 - INFO - [Valid] [41/90] | Loss: 0.5043 | Val Acc: 82.01%
2026-01-14 12:26:55,102 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-01-14 12:26:55,103 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-01-14 12:26:55,108 - INFO - --------------------------------------------------
2026-01-14 12:26:55,112 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-01-14 12:27:05,134 - INFO - [Train] [42/90] | Loss: 0.3801 | Train Acc: 89.81%
2026-01-14 12:27:07,181 - INFO - [Valid] [42/90] | Loss: 0.5117 | Val Acc: 80.53%
2026-01-14 12:27:07,197 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-01-14 12:27:07,198 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-01-14 12:27:07,202 - INFO - --------------------------------------------------
2026-01-14 12:27:07,203 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-01-14 12:27:17,410 - INFO - [Train] [43/90] | Loss: 0.3694 | Train Acc: 91.37%
2026-01-14 12:27:19,623 - INFO - [Valid] [43/90] | Loss: 0.5094 | Val Acc: 82.60%
2026-01-14 12:27:19,636 - INFO - [Metrics for 'abnormal'] | Precision: 0.8141 | Recall: 0.8089 | F1: 0.8115
2026-01-14 12:27:19,637 - INFO - [Metrics for 'normal'] | Precision: 0.8361 | Recall: 0.8407 | F1: 0.8384
2026-01-14 12:27:19,643 - INFO - --------------------------------------------------
2026-01-14 12:27:19,644 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-01-14 12:27:29,184 - INFO - [Train] [44/90] | Loss: 0.3577 | Train Acc: 92.04%
2026-01-14 12:27:31,319 - INFO - [Valid] [44/90] | Loss: 0.5264 | Val Acc: 81.12%
2026-01-14 12:27:31,336 - INFO - [Metrics for 'abnormal'] | Precision: 0.7962 | Recall: 0.7962 | F1: 0.7962
2026-01-14 12:27:31,337 - INFO - [Metrics for 'normal'] | Precision: 0.8242 | Recall: 0.8242 | F1: 0.8242
2026-01-14 12:27:31,341 - INFO - --------------------------------------------------
2026-01-14 12:27:31,342 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-01-14 12:27:41,277 - INFO - [Train] [45/90] | Loss: 0.3618 | Train Acc: 90.77%
2026-01-14 12:27:43,777 - INFO - [Valid] [45/90] | Loss: 0.5335 | Val Acc: 80.53%
2026-01-14 12:27:43,789 - INFO - [Metrics for 'abnormal'] | Precision: 0.7791 | Recall: 0.8089 | F1: 0.7937
2026-01-14 12:27:43,789 - INFO - [Metrics for 'normal'] | Precision: 0.8295 | Recall: 0.8022 | F1: 0.8156
2026-01-14 12:27:43,793 - INFO - --------------------------------------------------
2026-01-14 12:27:43,794 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-01-14 12:27:52,138 - INFO - [Train] [46/90] | Loss: 0.3620 | Train Acc: 91.89%
2026-01-14 12:27:54,778 - INFO - [Valid] [46/90] | Loss: 0.5254 | Val Acc: 80.53%
2026-01-14 12:27:54,807 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.8471 | F1: 0.8012
2026-01-14 12:27:54,807 - INFO - [Metrics for 'normal'] | Precision: 0.8537 | Recall: 0.7692 | F1: 0.8092
2026-01-14 12:27:54,813 - INFO - --------------------------------------------------
2026-01-14 12:27:54,813 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-01-14 12:28:03,221 - INFO - [Train] [47/90] | Loss: 0.3647 | Train Acc: 91.82%
2026-01-14 12:28:05,653 - INFO - [Valid] [47/90] | Loss: 0.5397 | Val Acc: 78.76%
2026-01-14 12:28:05,663 - INFO - [Metrics for 'abnormal'] | Precision: 0.7322 | Recall: 0.8535 | F1: 0.7882
2026-01-14 12:28:05,663 - INFO - [Metrics for 'normal'] | Precision: 0.8526 | Recall: 0.7308 | F1: 0.7870
2026-01-14 12:28:05,667 - INFO - --------------------------------------------------
2026-01-14 12:28:05,668 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-01-14 12:28:14,999 - INFO - [Train] [48/90] | Loss: 0.3481 | Train Acc: 92.71%
2026-01-14 12:28:18,080 - INFO - [Valid] [48/90] | Loss: 0.5338 | Val Acc: 81.12%
2026-01-14 12:28:18,105 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-01-14 12:28:18,110 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-01-14 12:28:18,114 - INFO - --------------------------------------------------
2026-01-14 12:28:18,120 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-01-14 12:28:26,551 - INFO - [Train] [49/90] | Loss: 0.3486 | Train Acc: 91.82%
2026-01-14 12:28:29,427 - INFO - [Valid] [49/90] | Loss: 0.5145 | Val Acc: 83.19%
2026-01-14 12:28:29,437 - INFO - [Metrics for 'abnormal'] | Precision: 0.8425 | Recall: 0.7834 | F1: 0.8119
2026-01-14 12:28:29,437 - INFO - [Metrics for 'normal'] | Precision: 0.8238 | Recall: 0.8736 | F1: 0.8480
2026-01-14 12:28:29,440 - INFO - --------------------------------------------------
2026-01-14 12:28:29,441 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-01-14 12:28:38,297 - INFO - [Train] [50/90] | Loss: 0.3528 | Train Acc: 92.49%
2026-01-14 12:28:41,168 - INFO - [Valid] [50/90] | Loss: 0.5209 | Val Acc: 82.60%
2026-01-14 12:28:41,183 - INFO - [Metrics for 'abnormal'] | Precision: 0.7917 | Recall: 0.8471 | F1: 0.8185
2026-01-14 12:28:41,185 - INFO - [Metrics for 'normal'] | Precision: 0.8596 | Recall: 0.8077 | F1: 0.8329
2026-01-14 12:28:41,189 - INFO - --------------------------------------------------
2026-01-14 12:28:41,190 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-01-14 12:28:49,760 - INFO - [Train] [51/90] | Loss: 0.3425 | Train Acc: 93.23%
2026-01-14 12:28:52,302 - INFO - [Valid] [51/90] | Loss: 0.5135 | Val Acc: 83.19%
2026-01-14 12:28:52,312 - INFO - [Metrics for 'abnormal'] | Precision: 0.8165 | Recall: 0.8217 | F1: 0.8190
2026-01-14 12:28:52,313 - INFO - [Metrics for 'normal'] | Precision: 0.8453 | Recall: 0.8407 | F1: 0.8430
2026-01-14 12:28:52,316 - INFO - --------------------------------------------------
2026-01-14 12:28:52,317 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-01-14 12:29:00,712 - INFO - [Train] [52/90] | Loss: 0.3437 | Train Acc: 93.15%
2026-01-14 12:29:03,933 - INFO - [Valid] [52/90] | Loss: 0.5107 | Val Acc: 83.19%
2026-01-14 12:29:03,942 - INFO - [Metrics for 'abnormal'] | Precision: 0.7976 | Recall: 0.8535 | F1: 0.8246
2026-01-14 12:29:03,943 - INFO - [Metrics for 'normal'] | Precision: 0.8655 | Recall: 0.8132 | F1: 0.8385
2026-01-14 12:29:03,947 - INFO - --------------------------------------------------
2026-01-14 12:29:03,947 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-01-14 12:29:12,735 - INFO - [Train] [53/90] | Loss: 0.3380 | Train Acc: 93.30%
2026-01-14 12:29:15,779 - INFO - [Valid] [53/90] | Loss: 0.5353 | Val Acc: 82.60%
2026-01-14 12:29:15,791 - INFO - [Metrics for 'abnormal'] | Precision: 0.8224 | Recall: 0.7962 | F1: 0.8091
2026-01-14 12:29:15,791 - INFO - [Metrics for 'normal'] | Precision: 0.8289 | Recall: 0.8516 | F1: 0.8401
2026-01-14 12:29:15,795 - INFO - --------------------------------------------------
2026-01-14 12:29:15,796 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-01-14 12:29:23,433 - INFO - [Train] [54/90] | Loss: 0.3418 | Train Acc: 92.78%
2026-01-14 12:29:25,639 - INFO - [Valid] [54/90] | Loss: 0.5348 | Val Acc: 81.12%
2026-01-14 12:29:25,651 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-01-14 12:29:25,652 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-01-14 12:29:25,658 - INFO - --------------------------------------------------
2026-01-14 12:29:25,659 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-01-14 12:29:35,366 - INFO - [Train] [55/90] | Loss: 0.3343 | Train Acc: 93.38%
2026-01-14 12:29:37,694 - INFO - [Valid] [55/90] | Loss: 0.5085 | Val Acc: 81.42%
2026-01-14 12:29:37,708 - INFO - [Metrics for 'abnormal'] | Precision: 0.7798 | Recall: 0.8344 | F1: 0.8062
2026-01-14 12:29:37,712 - INFO - [Metrics for 'normal'] | Precision: 0.8480 | Recall: 0.7967 | F1: 0.8215
2026-01-14 12:29:37,718 - INFO - --------------------------------------------------
2026-01-14 12:29:37,718 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-01-14 12:29:46,809 - INFO - [Train] [56/90] | Loss: 0.3297 | Train Acc: 93.90%
2026-01-14 12:29:49,036 - INFO - [Valid] [56/90] | Loss: 0.5168 | Val Acc: 81.12%
2026-01-14 12:29:49,077 - INFO - [Metrics for 'abnormal'] | Precision: 0.7719 | Recall: 0.8408 | F1: 0.8049
2026-01-14 12:29:49,077 - INFO - [Metrics for 'normal'] | Precision: 0.8512 | Recall: 0.7857 | F1: 0.8171
2026-01-14 12:29:49,086 - INFO - --------------------------------------------------
2026-01-14 12:29:49,086 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-01-14 12:29:58,099 - INFO - [Train] [57/90] | Loss: 0.3315 | Train Acc: 92.93%
2026-01-14 12:30:00,114 - INFO - [Valid] [57/90] | Loss: 0.5065 | Val Acc: 82.30%
2026-01-14 12:30:00,162 - INFO - [Metrics for 'abnormal'] | Precision: 0.7904 | Recall: 0.8408 | F1: 0.8148
2026-01-14 12:30:00,165 - INFO - [Metrics for 'normal'] | Precision: 0.8547 | Recall: 0.8077 | F1: 0.8305
2026-01-14 12:30:00,175 - INFO - --------------------------------------------------
2026-01-14 12:30:00,176 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-01-14 12:30:09,255 - INFO - [Train] [58/90] | Loss: 0.3298 | Train Acc: 93.45%
2026-01-14 12:30:11,737 - INFO - [Valid] [58/90] | Loss: 0.5326 | Val Acc: 80.83%
2026-01-14 12:30:11,745 - INFO - [Metrics for 'abnormal'] | Precision: 0.7614 | Recall: 0.8535 | F1: 0.8048
2026-01-14 12:30:11,745 - INFO - [Metrics for 'normal'] | Precision: 0.8589 | Recall: 0.7692 | F1: 0.8116
2026-01-14 12:30:11,748 - INFO - --------------------------------------------------
2026-01-14 12:30:11,748 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-01-14 12:30:20,039 - INFO - [Train] [59/90] | Loss: 0.3292 | Train Acc: 93.82%
2026-01-14 12:30:22,732 - INFO - [Valid] [59/90] | Loss: 0.5175 | Val Acc: 82.01%
2026-01-14 12:30:22,755 - INFO - [Metrics for 'abnormal'] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-01-14 12:30:22,755 - INFO - [Metrics for 'normal'] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-01-14 12:30:22,760 - INFO - --------------------------------------------------
2026-01-14 12:30:22,761 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-01-14 12:30:30,976 - INFO - [Train] [60/90] | Loss: 0.3209 | Train Acc: 94.05%
2026-01-14 12:30:33,939 - INFO - [Valid] [60/90] | Loss: 0.5200 | Val Acc: 82.01%
2026-01-14 12:30:33,960 - INFO - [Metrics for 'abnormal'] | Precision: 0.7759 | Recall: 0.8599 | F1: 0.8157
2026-01-14 12:30:33,961 - INFO - [Metrics for 'normal'] | Precision: 0.8667 | Recall: 0.7857 | F1: 0.8242
2026-01-14 12:30:33,965 - INFO - --------------------------------------------------
2026-01-14 12:30:33,966 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-01-14 12:30:41,795 - INFO - [Train] [61/90] | Loss: 0.3174 | Train Acc: 94.27%
2026-01-14 12:30:44,556 - INFO - [Valid] [61/90] | Loss: 0.5251 | Val Acc: 82.60%
2026-01-14 12:30:44,574 - INFO - [Metrics for 'abnormal'] | Precision: 0.7849 | Recall: 0.8599 | F1: 0.8207
2026-01-14 12:30:44,576 - INFO - [Metrics for 'normal'] | Precision: 0.8683 | Recall: 0.7967 | F1: 0.8309
2026-01-14 12:30:44,582 - INFO - --------------------------------------------------
2026-01-14 12:30:44,583 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-01-14 12:30:52,290 - INFO - [Train] [62/90] | Loss: 0.3153 | Train Acc: 95.09%
2026-01-14 12:30:55,191 - INFO - [Valid] [62/90] | Loss: 0.5321 | Val Acc: 82.01%
2026-01-14 12:30:55,221 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-01-14 12:30:55,224 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-01-14 12:30:55,229 - INFO - --------------------------------------------------
2026-01-14 12:30:55,230 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-01-14 12:31:02,525 - INFO - [Train] [63/90] | Loss: 0.3169 | Train Acc: 94.72%
2026-01-14 12:31:05,877 - INFO - [Valid] [63/90] | Loss: 0.5242 | Val Acc: 82.30%
2026-01-14 12:31:05,900 - INFO - [Metrics for 'abnormal'] | Precision: 0.7939 | Recall: 0.8344 | F1: 0.8137
2026-01-14 12:31:05,901 - INFO - [Metrics for 'normal'] | Precision: 0.8506 | Recall: 0.8132 | F1: 0.8315
2026-01-14 12:31:05,906 - INFO - --------------------------------------------------
2026-01-14 12:31:05,907 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-01-14 12:31:14,892 - INFO - [Train] [64/90] | Loss: 0.3075 | Train Acc: 95.46%
2026-01-14 12:31:17,101 - INFO - [Valid] [64/90] | Loss: 0.5198 | Val Acc: 81.12%
2026-01-14 12:31:17,140 - INFO - [Metrics for 'abnormal'] | Precision: 0.7751 | Recall: 0.8344 | F1: 0.8037
2026-01-14 12:31:17,140 - INFO - [Metrics for 'normal'] | Precision: 0.8471 | Recall: 0.7912 | F1: 0.8182
2026-01-14 12:31:17,144 - INFO - --------------------------------------------------
2026-01-14 12:31:17,144 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-01-14 12:31:26,195 - INFO - [Train] [65/90] | Loss: 0.3066 | Train Acc: 95.01%
2026-01-14 12:31:28,400 - INFO - [Valid] [65/90] | Loss: 0.5403 | Val Acc: 82.60%
2026-01-14 12:31:28,414 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-01-14 12:31:28,415 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-01-14 12:31:28,419 - INFO - --------------------------------------------------
2026-01-14 12:31:28,420 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-01-14 12:31:37,763 - INFO - [Train] [66/90] | Loss: 0.3126 | Train Acc: 95.09%
2026-01-14 12:31:40,001 - INFO - [Valid] [66/90] | Loss: 0.5215 | Val Acc: 81.12%
2026-01-14 12:31:40,026 - INFO - [Metrics for 'abnormal'] | Precision: 0.7657 | Recall: 0.8535 | F1: 0.8072
2026-01-14 12:31:40,027 - INFO - [Metrics for 'normal'] | Precision: 0.8598 | Recall: 0.7747 | F1: 0.8150
2026-01-14 12:31:40,031 - INFO - --------------------------------------------------
2026-01-14 12:31:40,032 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-01-14 12:31:48,210 - INFO - [Train] [67/90] | Loss: 0.3007 | Train Acc: 95.98%
2026-01-14 12:31:50,541 - INFO - [Valid] [67/90] | Loss: 0.5266 | Val Acc: 82.01%
2026-01-14 12:31:50,558 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 12:31:50,558 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 12:31:50,563 - INFO - --------------------------------------------------
2026-01-14 12:31:50,564 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-01-14 12:31:59,886 - INFO - [Train] [68/90] | Loss: 0.3082 | Train Acc: 95.83%
2026-01-14 12:32:02,451 - INFO - [Valid] [68/90] | Loss: 0.5227 | Val Acc: 82.01%
2026-01-14 12:32:02,465 - INFO - [Metrics for 'abnormal'] | Precision: 0.7759 | Recall: 0.8599 | F1: 0.8157
2026-01-14 12:32:02,465 - INFO - [Metrics for 'normal'] | Precision: 0.8667 | Recall: 0.7857 | F1: 0.8242
2026-01-14 12:32:02,469 - INFO - --------------------------------------------------
2026-01-14 12:32:02,470 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-01-14 12:32:10,793 - INFO - [Train] [69/90] | Loss: 0.3009 | Train Acc: 95.91%
2026-01-14 12:32:14,012 - INFO - [Valid] [69/90] | Loss: 0.5273 | Val Acc: 80.53%
2026-01-14 12:32:14,025 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.8471 | F1: 0.8012
2026-01-14 12:32:14,025 - INFO - [Metrics for 'normal'] | Precision: 0.8537 | Recall: 0.7692 | F1: 0.8092
2026-01-14 12:32:14,030 - INFO - --------------------------------------------------
2026-01-14 12:32:14,030 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-01-14 12:32:22,057 - INFO - [Train] [70/90] | Loss: 0.3089 | Train Acc: 95.24%
2026-01-14 12:32:24,832 - INFO - [Valid] [70/90] | Loss: 0.5309 | Val Acc: 82.30%
2026-01-14 12:32:24,841 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8662 | F1: 0.8193
2026-01-14 12:32:24,841 - INFO - [Metrics for 'normal'] | Precision: 0.8720 | Recall: 0.7857 | F1: 0.8266
2026-01-14 12:32:24,845 - INFO - --------------------------------------------------
2026-01-14 12:32:24,845 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-01-14 12:32:32,962 - INFO - [Train] [71/90] | Loss: 0.3019 | Train Acc: 95.68%
2026-01-14 12:32:35,755 - INFO - [Valid] [71/90] | Loss: 0.5227 | Val Acc: 82.01%
2026-01-14 12:32:35,766 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 12:32:35,766 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 12:32:35,770 - INFO - --------------------------------------------------
2026-01-14 12:32:35,771 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-01-14 12:32:44,698 - INFO - [Train] [72/90] | Loss: 0.3021 | Train Acc: 95.39%
2026-01-14 12:32:47,522 - INFO - [Valid] [72/90] | Loss: 0.5224 | Val Acc: 80.83%
2026-01-14 12:32:47,535 - INFO - [Metrics for 'abnormal'] | Precision: 0.7805 | Recall: 0.8153 | F1: 0.7975
2026-01-14 12:32:47,536 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8022 | F1: 0.8179
2026-01-14 12:32:47,541 - INFO - --------------------------------------------------
2026-01-14 12:32:47,542 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-01-14 12:32:56,302 - INFO - [Train] [73/90] | Loss: 0.3034 | Train Acc: 95.31%
2026-01-14 12:32:59,952 - INFO - [Valid] [73/90] | Loss: 0.5259 | Val Acc: 81.71%
2026-01-14 12:32:59,965 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8471 | F1: 0.8110
2026-01-14 12:32:59,966 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.7912 | F1: 0.8229
2026-01-14 12:32:59,971 - INFO - --------------------------------------------------
2026-01-14 12:32:59,972 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-01-14 12:33:08,135 - INFO - [Train] [74/90] | Loss: 0.2951 | Train Acc: 96.35%
2026-01-14 12:33:11,454 - INFO - [Valid] [74/90] | Loss: 0.5221 | Val Acc: 82.60%
2026-01-14 12:33:11,473 - INFO - [Metrics for 'abnormal'] | Precision: 0.7952 | Recall: 0.8408 | F1: 0.8173
2026-01-14 12:33:11,476 - INFO - [Metrics for 'normal'] | Precision: 0.8555 | Recall: 0.8132 | F1: 0.8338
2026-01-14 12:33:11,482 - INFO - --------------------------------------------------
2026-01-14 12:33:11,483 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-01-14 12:33:20,352 - INFO - [Train] [75/90] | Loss: 0.2924 | Train Acc: 96.21%
2026-01-14 12:33:23,186 - INFO - [Valid] [75/90] | Loss: 0.5178 | Val Acc: 82.89%
2026-01-14 12:33:23,211 - INFO - [Metrics for 'abnormal'] | Precision: 0.8037 | Recall: 0.8344 | F1: 0.8187
2026-01-14 12:33:23,211 - INFO - [Metrics for 'normal'] | Precision: 0.8523 | Recall: 0.8242 | F1: 0.8380
2026-01-14 12:33:23,215 - INFO - --------------------------------------------------
2026-01-14 12:33:23,219 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-01-14 12:33:31,919 - INFO - [Train] [76/90] | Loss: 0.2952 | Train Acc: 96.73%
2026-01-14 12:33:34,977 - INFO - [Valid] [76/90] | Loss: 0.5275 | Val Acc: 82.89%
2026-01-14 12:33:35,020 - INFO - [Metrics for 'abnormal'] | Precision: 0.8075 | Recall: 0.8280 | F1: 0.8176
2026-01-14 12:33:35,020 - INFO - [Metrics for 'normal'] | Precision: 0.8483 | Recall: 0.8297 | F1: 0.8389
2026-01-14 12:33:35,031 - INFO - --------------------------------------------------
2026-01-14 12:33:35,032 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-01-14 12:33:42,539 - INFO - [Train] [77/90] | Loss: 0.2938 | Train Acc: 95.83%
2026-01-14 12:33:45,334 - INFO - [Valid] [77/90] | Loss: 0.5208 | Val Acc: 82.30%
2026-01-14 12:33:45,345 - INFO - [Metrics for 'abnormal'] | Precision: 0.7975 | Recall: 0.8280 | F1: 0.8125
2026-01-14 12:33:45,346 - INFO - [Metrics for 'normal'] | Precision: 0.8466 | Recall: 0.8187 | F1: 0.8324
2026-01-14 12:33:45,349 - INFO - --------------------------------------------------
2026-01-14 12:33:45,350 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-01-14 12:33:54,599 - INFO - [Train] [78/90] | Loss: 0.2903 | Train Acc: 96.73%
2026-01-14 12:33:57,203 - INFO - [Valid] [78/90] | Loss: 0.5223 | Val Acc: 82.01%
2026-01-14 12:33:57,213 - INFO - [Metrics for 'abnormal'] | Precision: 0.7791 | Recall: 0.8535 | F1: 0.8146
2026-01-14 12:33:57,213 - INFO - [Metrics for 'normal'] | Precision: 0.8623 | Recall: 0.7912 | F1: 0.8252
2026-01-14 12:33:57,217 - INFO - --------------------------------------------------
2026-01-14 12:33:57,218 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-01-14 12:34:06,379 - INFO - [Train] [79/90] | Loss: 0.2931 | Train Acc: 95.83%
2026-01-14 12:34:08,462 - INFO - [Valid] [79/90] | Loss: 0.5123 | Val Acc: 83.48%
2026-01-14 12:34:08,538 - INFO - [Metrics for 'abnormal'] | Precision: 0.8137 | Recall: 0.8344 | F1: 0.8239
2026-01-14 12:34:08,538 - INFO - [Metrics for 'normal'] | Precision: 0.8539 | Recall: 0.8352 | F1: 0.8444
2026-01-14 12:34:08,541 - INFO - --------------------------------------------------
2026-01-14 12:34:08,542 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-01-14 12:34:18,360 - INFO - [Train] [80/90] | Loss: 0.2892 | Train Acc: 96.65%
2026-01-14 12:34:20,665 - INFO - [Valid] [80/90] | Loss: 0.5230 | Val Acc: 82.89%
2026-01-14 12:34:20,676 - INFO - [Metrics for 'abnormal'] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-01-14 12:34:20,676 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-01-14 12:34:20,681 - INFO - --------------------------------------------------
2026-01-14 12:34:20,682 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-01-14 12:34:30,151 - INFO - [Train] [81/90] | Loss: 0.2847 | Train Acc: 97.54%
2026-01-14 12:34:32,492 - INFO - [Valid] [81/90] | Loss: 0.5293 | Val Acc: 83.19%
2026-01-14 12:34:32,512 - INFO - [Metrics for 'abnormal'] | Precision: 0.8165 | Recall: 0.8217 | F1: 0.8190
2026-01-14 12:34:32,516 - INFO - [Metrics for 'normal'] | Precision: 0.8453 | Recall: 0.8407 | F1: 0.8430
2026-01-14 12:34:32,520 - INFO - --------------------------------------------------
2026-01-14 12:34:32,523 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-01-14 12:34:41,836 - INFO - [Train] [82/90] | Loss: 0.2895 | Train Acc: 96.73%
2026-01-14 12:34:43,992 - INFO - [Valid] [82/90] | Loss: 0.5293 | Val Acc: 82.89%
2026-01-14 12:34:44,005 - INFO - [Metrics for 'abnormal'] | Precision: 0.8037 | Recall: 0.8344 | F1: 0.8187
2026-01-14 12:34:44,005 - INFO - [Metrics for 'normal'] | Precision: 0.8523 | Recall: 0.8242 | F1: 0.8380
2026-01-14 12:34:44,010 - INFO - --------------------------------------------------
2026-01-14 12:34:44,011 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-01-14 12:34:52,694 - INFO - [Train] [83/90] | Loss: 0.2858 | Train Acc: 96.88%
2026-01-14 12:34:55,252 - INFO - [Valid] [83/90] | Loss: 0.5315 | Val Acc: 82.60%
2026-01-14 12:34:55,272 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-01-14 12:34:55,274 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-01-14 12:34:55,278 - INFO - --------------------------------------------------
2026-01-14 12:34:55,278 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-01-14 12:35:05,601 - INFO - [Train] [84/90] | Loss: 0.2874 | Train Acc: 97.02%
2026-01-14 12:35:08,060 - INFO - [Valid] [84/90] | Loss: 0.5239 | Val Acc: 82.01%
2026-01-14 12:35:08,072 - INFO - [Metrics for 'abnormal'] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-01-14 12:35:08,073 - INFO - [Metrics for 'normal'] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-01-14 12:35:08,077 - INFO - --------------------------------------------------
2026-01-14 12:35:08,078 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-01-14 12:35:16,959 - INFO - [Train] [85/90] | Loss: 0.2906 | Train Acc: 96.28%
2026-01-14 12:35:20,607 - INFO - [Valid] [85/90] | Loss: 0.5254 | Val Acc: 82.01%
2026-01-14 12:35:20,616 - INFO - [Metrics for 'abnormal'] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-01-14 12:35:20,617 - INFO - [Metrics for 'normal'] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-01-14 12:35:20,620 - INFO - --------------------------------------------------
2026-01-14 12:35:20,621 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-01-14 12:35:28,919 - INFO - [Train] [86/90] | Loss: 0.2855 | Train Acc: 96.35%
2026-01-14 12:35:30,973 - INFO - [Valid] [86/90] | Loss: 0.5245 | Val Acc: 83.19%
2026-01-14 12:35:30,985 - INFO - [Metrics for 'abnormal'] | Precision: 0.8125 | Recall: 0.8280 | F1: 0.8202
2026-01-14 12:35:30,986 - INFO - [Metrics for 'normal'] | Precision: 0.8492 | Recall: 0.8352 | F1: 0.8421
2026-01-14 12:35:30,990 - INFO - --------------------------------------------------
2026-01-14 12:35:30,991 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-01-14 12:35:39,921 - INFO - [Train] [87/90] | Loss: 0.2840 | Train Acc: 96.80%
2026-01-14 12:35:42,123 - INFO - [Valid] [87/90] | Loss: 0.5246 | Val Acc: 82.89%
2026-01-14 12:35:42,136 - INFO - [Metrics for 'abnormal'] | Precision: 0.8075 | Recall: 0.8280 | F1: 0.8176
2026-01-14 12:35:42,136 - INFO - [Metrics for 'normal'] | Precision: 0.8483 | Recall: 0.8297 | F1: 0.8389
2026-01-14 12:35:42,141 - INFO - --------------------------------------------------
2026-01-14 12:35:42,143 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-01-14 12:35:52,873 - INFO - [Train] [88/90] | Loss: 0.2822 | Train Acc: 97.32%
2026-01-14 12:35:56,180 - INFO - [Valid] [88/90] | Loss: 0.5259 | Val Acc: 83.19%
2026-01-14 12:35:56,190 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8471 | F1: 0.8235
2026-01-14 12:35:56,190 - INFO - [Metrics for 'normal'] | Precision: 0.8613 | Recall: 0.8187 | F1: 0.8394
2026-01-14 12:35:56,194 - INFO - --------------------------------------------------
2026-01-14 12:35:56,194 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-01-14 12:36:03,410 - INFO - [Train] [89/90] | Loss: 0.2795 | Train Acc: 97.32%
2026-01-14 12:36:05,332 - INFO - [Valid] [89/90] | Loss: 0.5258 | Val Acc: 82.30%
2026-01-14 12:36:05,345 - INFO - [Metrics for 'abnormal'] | Precision: 0.7836 | Recall: 0.8535 | F1: 0.8171
2026-01-14 12:36:05,346 - INFO - [Metrics for 'normal'] | Precision: 0.8631 | Recall: 0.7967 | F1: 0.8286
2026-01-14 12:36:05,351 - INFO - --------------------------------------------------
2026-01-14 12:36:05,352 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-01-14 12:36:12,336 - INFO - [Train] [90/90] | Loss: 0.2814 | Train Acc: 97.25%
2026-01-14 12:36:14,160 - INFO - [Valid] [90/90] | Loss: 0.5251 | Val Acc: 82.60%
2026-01-14 12:36:14,172 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-01-14 12:36:14,173 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-01-14 12:36:14,177 - INFO - ==================================================
2026-01-14 12:36:14,178 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-01-14 12:36:14,179 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-01-14 12:36:14,179 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:36:14,284 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-01-14 12:36:14,286 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:36:14,287 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:36:15,003 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-01-14 12:36:15,003 - INFO - ==================================================
2026-01-14 12:36:15,008 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/best_model.pth'
2026-01-14 12:36:15,008 - INFO - ==================================================
2026-01-14 12:36:15,008 - INFO - Test 모드를 시작합니다.
2026-01-14 12:36:15,367 - INFO - 연산량 (MACs): 0.0927 GMACs per sample
2026-01-14 12:36:15,368 - INFO - 연산량 (FLOPs): 0.1854 GFLOPs per sample
2026-01-14 12:36:15,368 - INFO - ==================================================
2026-01-14 12:36:15,368 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-01-14 12:36:16,000 - INFO - 샘플 당 평균 Forward Pass 시간: 0.49ms (std: 0.20ms), FPS: 2258.98 (std: 1009.05) (1개 샘플 x 100회 반복)
2026-01-14 12:36:16,001 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 160.08 MB
2026-01-14 12:36:16,001 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-01-14 12:36:19,020 - INFO - [Test] Loss: 0.4283 | Test Acc: 82.30%
2026-01-14 12:36:19,050 - INFO - [Metrics for 'abnormal'] | Precision: 0.8255 | Recall: 0.7834 | F1: 0.8039
2026-01-14 12:36:19,050 - INFO - [Metrics for 'normal'] | Precision: 0.8211 | Recall: 0.8571 | F1: 0.8387
2026-01-14 12:36:19,725 - INFO - ==================================================
2026-01-14 12:36:19,725 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/confusion_matrix_20260114_121836.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/confusion_matrix_20260114_121836.pdf'
2026-01-14 12:36:19,725 - INFO - ==================================================
2026-01-14 12:36:19,725 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-01-14 12:36:19,968 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/model_fp32_20260114_121836.onnx'에 저장되었습니다. (크기: 0.22 MB)
2026-01-14 12:36:20,542 - INFO - [Model Load] ONNX 모델(FP32) 로드 메모리: 2417.35 MB (증가량: 7.45 MB)
2026-01-14 12:36:20,542 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-01-14 12:36:21,381 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 4.21ms (std: 8.88ms)
2026-01-14 12:36:21,382 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 463.68 FPS (std: 263.21) (1개 샘플 x 100회 반복)
2026-01-14 12:36:21,382 - INFO - [Inference Only] ONNX 런타임 추론 중 최대 CPU 메모리: 2423.35 MB (순수 증가량: 6.00 MB)
2026-01-14 12:36:21,382 - INFO - [Total Process] ONNX 모델(FP32) 전체 메모리 사용량: 2423.35 MB (전체 증가량: 13.45 MB)
2026-01-14 12:36:24,650 - INFO - [Test (ONNX)] | Test Acc (ONNX): 82.30%
2026-01-14 12:36:24,678 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8255 | Recall: 0.7834 | F1: 0.8039
2026-01-14 12:36:24,678 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8211 | Recall: 0.8571 | F1: 0.8387
2026-01-14 12:36:25,484 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/val_acc.pdf'
2026-01-14 12:36:26,040 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/train_val_acc.pdf'
2026-01-14 12:36:26,479 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/F1_normal.pdf'
2026-01-14 12:36:27,170 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/val_loss.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/val_loss.pdf'
2026-01-14 12:36:27,853 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/learning_rate.pdf'
2026-01-14 12:36:33,839 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/compile.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260114_121836/graph_20260114_121836/compile.pdf'
