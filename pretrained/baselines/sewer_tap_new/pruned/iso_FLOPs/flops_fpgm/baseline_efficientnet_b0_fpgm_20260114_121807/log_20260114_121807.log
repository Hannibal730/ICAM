2026-01-14 12:18:07,286 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/log_20260114_121807.log'에 저장됩니다.
2026-01-14 12:18:07,291 - INFO - ==================================================
2026-01-14 12:18:07,291 - INFO - config.yaml:
2026-01-14 12:18:07,292 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_inference_dir: ./pretrained
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      train_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/cau/workspace/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  patch_size: 56
  stride: 56
  cnn_feature_extractor:
    name: efficientnet_b0_feat2
  featured_patch_dim: 24
  emb_dim: 24
  num_heads: 2
  num_decoder_layers: 2
  num_decoder_patches: 1
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: true
  save_attention: true
  num_plot_attention: 5
baseline:
  model_name: efficientnet_b0
  use_fpgm_pruning: true
  pruning_flops_target: 0.1829

2026-01-14 12:18:07,292 - INFO - ==================================================
2026-01-14 12:18:07,480 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA RTX PRO 6000 Blackwell Server Edition)
2026-01-14 12:18:07,481 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-01-14 12:18:07,481 - INFO - '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-01-14 12:18:07,489 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-01-14 12:18:07,489 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-01-14 12:18:07,490 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-01-14 12:18:07,490 - INFO - Baseline 모델 'efficientnet_b0'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:18:07,798 - INFO - ==================================================
2026-01-14 12:18:07,799 - INFO - 모델 파라미터 수:
2026-01-14 12:18:07,799 - INFO -   - 총 파라미터: 4,010,110 개
2026-01-14 12:18:07,799 - INFO -   - 학습 가능한 파라미터: 4,010,110 개
2026-01-14 12:18:07,799 - INFO - ================================================================================
2026-01-14 12:18:07,799 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-01-14 12:18:07,799 - INFO - ================================================================================
2026-01-14 12:18:07,799 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:18:07,801 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-01-14 12:18:07,801 - INFO - ==================================================
2026-01-14 12:18:07,801 - INFO - train 모드를 시작합니다.
2026-01-14 12:18:07,802 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:18:07,802 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:18:07,802 - INFO - --------------------------------------------------
2026-01-14 12:18:07,804 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-01-14 12:18:13,636 - INFO - [Train] [1/10] | Loss: 0.5891 | Train Acc: 73.29%
2026-01-14 12:18:16,140 - INFO - [Valid] [1/10] | Loss: 0.6040 | Val Acc: 76.40%
2026-01-14 12:18:16,158 - INFO - [Metrics for 'abnormal'] | Precision: 0.8598 | Recall: 0.5860 | F1: 0.6970
2026-01-14 12:18:16,158 - INFO - [Metrics for 'normal'] | Precision: 0.7198 | Recall: 0.9176 | F1: 0.8068
2026-01-14 12:18:16,231 - INFO - [Best Model Saved] (val loss: 0.6040) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:18:16,232 - INFO - --------------------------------------------------
2026-01-14 12:18:16,235 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-01-14 12:18:20,904 - INFO - [Train] [2/10] | Loss: 0.5420 | Train Acc: 77.75%
2026-01-14 12:18:22,427 - INFO - [Valid] [2/10] | Loss: 0.7251 | Val Acc: 79.06%
2026-01-14 12:18:22,439 - INFO - [Metrics for 'abnormal'] | Precision: 0.8308 | Recall: 0.6879 | F1: 0.7526
2026-01-14 12:18:22,439 - INFO - [Metrics for 'normal'] | Precision: 0.7656 | Recall: 0.8791 | F1: 0.8184
2026-01-14 12:18:22,443 - INFO - --------------------------------------------------
2026-01-14 12:18:22,445 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-01-14 12:18:27,571 - INFO - [Train] [3/10] | Loss: 0.5165 | Train Acc: 79.84%
2026-01-14 12:18:29,010 - INFO - [Valid] [3/10] | Loss: 0.5287 | Val Acc: 79.35%
2026-01-14 12:18:29,023 - INFO - [Metrics for 'abnormal'] | Precision: 0.7736 | Recall: 0.7834 | F1: 0.7785
2026-01-14 12:18:29,023 - INFO - [Metrics for 'normal'] | Precision: 0.8111 | Recall: 0.8022 | F1: 0.8066
2026-01-14 12:18:29,091 - INFO - [Best Model Saved] (val loss: 0.5287) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:18:29,091 - INFO - --------------------------------------------------
2026-01-14 12:18:29,093 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-01-14 12:18:34,182 - INFO - [Train] [4/10] | Loss: 0.4682 | Train Acc: 83.85%
2026-01-14 12:18:35,589 - INFO - [Valid] [4/10] | Loss: 0.5136 | Val Acc: 76.70%
2026-01-14 12:18:35,603 - INFO - [Metrics for 'abnormal'] | Precision: 0.6840 | Recall: 0.9236 | F1: 0.7859
2026-01-14 12:18:35,603 - INFO - [Metrics for 'normal'] | Precision: 0.9055 | Recall: 0.6319 | F1: 0.7443
2026-01-14 12:18:35,683 - INFO - [Best Model Saved] (val loss: 0.5136) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:18:35,683 - INFO - --------------------------------------------------
2026-01-14 12:18:35,686 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-01-14 12:18:41,375 - INFO - [Train] [5/10] | Loss: 0.4490 | Train Acc: 83.78%
2026-01-14 12:18:43,115 - INFO - [Valid] [5/10] | Loss: 0.4837 | Val Acc: 79.94%
2026-01-14 12:18:43,127 - INFO - [Metrics for 'abnormal'] | Precision: 0.8069 | Recall: 0.7452 | F1: 0.7748
2026-01-14 12:18:43,127 - INFO - [Metrics for 'normal'] | Precision: 0.7938 | Recall: 0.8462 | F1: 0.8191
2026-01-14 12:18:43,188 - INFO - [Best Model Saved] (val loss: 0.4837) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:18:43,188 - INFO - --------------------------------------------------
2026-01-14 12:18:43,191 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-01-14 12:18:48,611 - INFO - [Train] [6/10] | Loss: 0.4372 | Train Acc: 84.15%
2026-01-14 12:18:50,329 - INFO - [Valid] [6/10] | Loss: 0.5192 | Val Acc: 81.12%
2026-01-14 12:18:50,342 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.7707 | F1: 0.7908
2026-01-14 12:18:50,342 - INFO - [Metrics for 'normal'] | Precision: 0.8105 | Recall: 0.8462 | F1: 0.8280
2026-01-14 12:18:50,347 - INFO - --------------------------------------------------
2026-01-14 12:18:50,350 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-01-14 12:18:56,772 - INFO - [Train] [7/10] | Loss: 0.4095 | Train Acc: 86.83%
2026-01-14 12:18:58,570 - INFO - [Valid] [7/10] | Loss: 0.4739 | Val Acc: 84.66%
2026-01-14 12:18:58,581 - INFO - [Metrics for 'abnormal'] | Precision: 0.8344 | Recall: 0.8344 | F1: 0.8344
2026-01-14 12:18:58,581 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.8571 | F1: 0.8571
2026-01-14 12:18:58,647 - INFO - [Best Model Saved] (val loss: 0.4739) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:18:58,649 - INFO - --------------------------------------------------
2026-01-14 12:18:58,651 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-01-14 12:19:05,720 - INFO - [Train] [8/10] | Loss: 0.3735 | Train Acc: 89.29%
2026-01-14 12:19:07,603 - INFO - [Valid] [8/10] | Loss: 0.4604 | Val Acc: 82.89%
2026-01-14 12:19:07,617 - INFO - [Metrics for 'abnormal'] | Precision: 0.8235 | Recall: 0.8025 | F1: 0.8129
2026-01-14 12:19:07,618 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8516 | F1: 0.8424
2026-01-14 12:19:07,710 - INFO - [Best Model Saved] (val loss: 0.4604) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:19:07,711 - INFO - --------------------------------------------------
2026-01-14 12:19:07,713 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-01-14 12:19:15,453 - INFO - [Train] [9/10] | Loss: 0.3467 | Train Acc: 91.00%
2026-01-14 12:19:18,194 - INFO - [Valid] [9/10] | Loss: 0.4581 | Val Acc: 84.66%
2026-01-14 12:19:18,206 - INFO - [Metrics for 'abnormal'] | Precision: 0.8344 | Recall: 0.8344 | F1: 0.8344
2026-01-14 12:19:18,207 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.8571 | F1: 0.8571
2026-01-14 12:19:18,292 - INFO - [Best Model Saved] (val loss: 0.4581) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:19:18,292 - INFO - --------------------------------------------------
2026-01-14 12:19:18,294 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-01-14 12:19:27,307 - INFO - [Train] [10/10] | Loss: 0.3209 | Train Acc: 93.75%
2026-01-14 12:19:30,844 - INFO - [Valid] [10/10] | Loss: 0.4834 | Val Acc: 83.19%
2026-01-14 12:19:30,869 - INFO - [Metrics for 'abnormal'] | Precision: 0.9032 | Recall: 0.7134 | F1: 0.7972
2026-01-14 12:19:30,873 - INFO - [Metrics for 'normal'] | Precision: 0.7907 | Recall: 0.9341 | F1: 0.8564
2026-01-14 12:19:30,888 - INFO - ================================================================================
2026-01-14 12:19:30,889 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-01-14 12:19:30,889 - INFO - ================================================================================
2026-01-14 12:19:31,595 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'을(를) 불러왔습니다.
2026-01-14 12:19:31,596 - INFO - ================================================================================
2026-01-14 12:19:31,597 - INFO - 목표 FLOPs (0.1829 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-01-14 12:19:31,737 - INFO - 원본 모델 FLOPs: 0.8277 GFLOPs
2026-01-14 12:19:31,933 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:31,934 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:32,866 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-01-14 12:19:32,867 - INFO - ==================================================
2026-01-14 12:19:33,008 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.2459 GFLOPs (감소율: 70.30%)
2026-01-14 12:19:33,120 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:33,121 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:33,484 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-01-14 12:19:33,485 - INFO - ==================================================
2026-01-14 12:19:33,571 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.0817 GFLOPs (감소율: 90.13%)
2026-01-14 12:19:33,631 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:33,632 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:34,142 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.6187499999999999)에 맞춰 변경되었습니다.
2026-01-14 12:19:34,142 - INFO - ==================================================
2026-01-14 12:19:34,257 - INFO -   [탐색  3] 희소도: 0.6187 -> FLOPs: 0.1535 GFLOPs (감소율: 81.46%)
2026-01-14 12:19:34,358 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:34,359 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:34,761 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.556875)에 맞춰 변경되었습니다.
2026-01-14 12:19:34,761 - INFO - ==================================================
2026-01-14 12:19:34,864 - INFO -   [탐색  4] 희소도: 0.5569 -> FLOPs: 0.1960 GFLOPs (감소율: 76.32%)
2026-01-14 12:19:35,440 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:35,441 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:35,874 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5878125)에 맞춰 변경되었습니다.
2026-01-14 12:19:35,875 - INFO - ==================================================
2026-01-14 12:19:35,988 - INFO -   [탐색  5] 희소도: 0.5878 -> FLOPs: 0.1727 GFLOPs (감소율: 79.13%)
2026-01-14 12:19:36,070 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:36,070 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:36,686 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5723437499999999)에 맞춰 변경되었습니다.
2026-01-14 12:19:36,687 - INFO - ==================================================
2026-01-14 12:19:36,799 - INFO -   [탐색  6] 희소도: 0.5723 -> FLOPs: 0.1841 GFLOPs (감소율: 77.76%)
2026-01-14 12:19:36,879 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:36,879 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:37,546 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.580078125)에 맞춰 변경되었습니다.
2026-01-14 12:19:37,546 - INFO - ==================================================
2026-01-14 12:19:37,659 - INFO -   [탐색  7] 희소도: 0.5801 -> FLOPs: 0.1790 GFLOPs (감소율: 78.37%)
2026-01-14 12:19:37,752 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:37,753 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:38,389 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5762109375)에 맞춰 변경되었습니다.
2026-01-14 12:19:38,389 - INFO - ==================================================
2026-01-14 12:19:38,488 - INFO -   [탐색  8] 희소도: 0.5762 -> FLOPs: 0.1810 GFLOPs (감소율: 78.13%)
2026-01-14 12:19:38,573 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:38,578 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:39,115 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.57427734375)에 맞춰 변경되었습니다.
2026-01-14 12:19:39,115 - INFO - ==================================================
2026-01-14 12:19:39,240 - INFO -   [탐색  9] 희소도: 0.5743 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:39,319 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:39,320 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:40,132 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5733105468749999)에 맞춰 변경되었습니다.
2026-01-14 12:19:40,133 - INFO - ==================================================
2026-01-14 12:19:40,228 - INFO -   [탐색 10] 희소도: 0.5733 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:40,300 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:40,301 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:41,266 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737939453124999)에 맞춰 변경되었습니다.
2026-01-14 12:19:41,266 - INFO - ==================================================
2026-01-14 12:19:41,349 - INFO -   [탐색 11] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:41,440 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:41,440 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:42,272 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5735522460937499)에 맞춰 변경되었습니다.
2026-01-14 12:19:42,273 - INFO - ==================================================
2026-01-14 12:19:42,337 - INFO -   [탐색 12] 희소도: 0.5736 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:42,403 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:42,404 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:42,879 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5736730957031249)에 맞춰 변경되었습니다.
2026-01-14 12:19:42,879 - INFO - ==================================================
2026-01-14 12:19:42,949 - INFO -   [탐색 13] 희소도: 0.5737 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:43,040 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:43,041 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:43,629 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737335205078125)에 맞춰 변경되었습니다.
2026-01-14 12:19:43,629 - INFO - ==================================================
2026-01-14 12:19:43,713 - INFO -   [탐색 14] 희소도: 0.5737 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:43,794 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:43,795 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:44,279 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737637329101561)에 맞춰 변경되었습니다.
2026-01-14 12:19:44,280 - INFO - ==================================================
2026-01-14 12:19:44,355 - INFO -   [탐색 15] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:44,432 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:44,433 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:44,962 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737788391113281)에 맞춰 변경되었습니다.
2026-01-14 12:19:44,962 - INFO - ==================================================
2026-01-14 12:19:45,074 - INFO -   [탐색 16] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:45,178 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:45,179 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:45,819 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.573786392211914)에 맞춰 변경되었습니다.
2026-01-14 12:19:45,820 - INFO - ==================================================
2026-01-14 12:19:45,914 - INFO -   [탐색 17] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:46,016 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:46,016 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:46,428 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.573782615661621)에 맞춰 변경되었습니다.
2026-01-14 12:19:46,429 - INFO - ==================================================
2026-01-14 12:19:46,513 - INFO -   [탐색 18] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:46,600 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:46,601 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:47,586 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737845039367675)에 맞춰 변경되었습니다.
2026-01-14 12:19:47,587 - INFO - ==================================================
2026-01-14 12:19:47,672 - INFO -   [탐색 19] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:47,747 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:47,747 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:48,219 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737854480743407)에 맞춰 변경되었습니다.
2026-01-14 12:19:48,220 - INFO - ==================================================
2026-01-14 12:19:48,295 - INFO -   [탐색 20] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:48,386 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:48,386 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:49,124 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737849760055541)에 맞춰 변경되었습니다.
2026-01-14 12:19:49,125 - INFO - ==================================================
2026-01-14 12:19:49,244 - INFO -   [탐색 21] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:49,357 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:49,357 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:49,935 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847399711609)에 맞춰 변경되었습니다.
2026-01-14 12:19:49,936 - INFO - ==================================================
2026-01-14 12:19:50,013 - INFO -   [탐색 22] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:50,091 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:50,092 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:50,590 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737846219539642)에 맞춰 변경되었습니다.
2026-01-14 12:19:50,591 - INFO - ==================================================
2026-01-14 12:19:50,666 - INFO -   [탐색 23] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:50,747 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:50,748 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:51,301 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737846809625625)에 맞춰 변경되었습니다.
2026-01-14 12:19:51,302 - INFO - ==================================================
2026-01-14 12:19:51,376 - INFO -   [탐색 24] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:51,438 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:51,439 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:51,909 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847104668616)에 맞춰 변경되었습니다.
2026-01-14 12:19:51,909 - INFO - ==================================================
2026-01-14 12:19:52,000 - INFO -   [탐색 25] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:52,081 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:52,082 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:52,957 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847252190112)에 맞춰 변경되었습니다.
2026-01-14 12:19:52,957 - INFO - ==================================================
2026-01-14 12:19:53,056 - INFO -   [탐색 26] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:53,145 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:53,145 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:53,540 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847178429365)에 맞춰 변경되었습니다.
2026-01-14 12:19:53,541 - INFO - ==================================================
2026-01-14 12:19:53,617 - INFO -   [탐색 27] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:53,698 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:53,698 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:54,293 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847215309739)에 맞춰 변경되었습니다.
2026-01-14 12:19:54,294 - INFO - ==================================================
2026-01-14 12:19:54,369 - INFO -   [탐색 28] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:54,450 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:54,451 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:54,896 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847233749926)에 맞춰 변경되었습니다.
2026-01-14 12:19:54,897 - INFO - ==================================================
2026-01-14 12:19:54,974 - INFO -   [탐색 29] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:55,059 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:55,060 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:55,522 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847224529833)에 맞춰 변경되었습니다.
2026-01-14 12:19:55,523 - INFO - ==================================================
2026-01-14 12:19:55,658 - INFO -   [탐색 30] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:55,824 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:55,825 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:56,320 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847219919786)에 맞춰 변경되었습니다.
2026-01-14 12:19:56,321 - INFO - ==================================================
2026-01-14 12:19:56,392 - INFO -   [탐색 31] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:56,472 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:56,473 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:56,941 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.573784722222481)에 맞춰 변경되었습니다.
2026-01-14 12:19:56,941 - INFO - ==================================================
2026-01-14 12:19:57,011 - INFO -   [탐색 32] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:19:57,088 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:57,089 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:57,943 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847221072299)에 맞춰 변경되었습니다.
2026-01-14 12:19:57,944 - INFO - ==================================================
2026-01-14 12:19:58,020 - INFO -   [탐색 33] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:58,097 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:58,098 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:58,563 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847221648554)에 맞춰 변경되었습니다.
2026-01-14 12:19:58,564 - INFO - ==================================================
2026-01-14 12:19:58,643 - INFO -   [탐색 34] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:58,719 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:58,720 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:19:59,282 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847221936683)에 맞춰 변경되었습니다.
2026-01-14 12:19:59,283 - INFO - ==================================================
2026-01-14 12:19:59,364 - INFO -   [탐색 35] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:19:59,461 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:19:59,462 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:00,070 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222080746)에 맞춰 변경되었습니다.
2026-01-14 12:20:00,070 - INFO - ==================================================
2026-01-14 12:20:00,154 - INFO -   [탐색 36] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:00,238 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:00,239 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:00,605 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222152779)에 맞춰 변경되었습니다.
2026-01-14 12:20:00,605 - INFO - ==================================================
2026-01-14 12:20:00,677 - INFO -   [탐색 37] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:00,761 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:00,762 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:01,442 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222188794)에 맞춰 변경되었습니다.
2026-01-14 12:20:01,443 - INFO - ==================================================
2026-01-14 12:20:01,530 - INFO -   [탐색 38] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:01,612 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:01,612 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:02,372 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222206802)에 맞춰 변경되었습니다.
2026-01-14 12:20:02,372 - INFO - ==================================================
2026-01-14 12:20:02,467 - INFO -   [탐색 39] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:02,570 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:02,571 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:03,309 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222215806)에 맞춰 변경되었습니다.
2026-01-14 12:20:03,310 - INFO - ==================================================
2026-01-14 12:20:03,383 - INFO -   [탐색 40] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:03,460 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:03,461 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:04,039 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222220308)에 맞춰 변경되었습니다.
2026-01-14 12:20:04,040 - INFO - ==================================================
2026-01-14 12:20:04,165 - INFO -   [탐색 41] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:04,296 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:04,296 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:04,882 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222558)에 맞춰 변경되었습니다.
2026-01-14 12:20:04,883 - INFO - ==================================================
2026-01-14 12:20:04,958 - INFO -   [탐색 42] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:05,041 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:05,042 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:05,717 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222221433)에 맞춰 변경되었습니다.
2026-01-14 12:20:05,718 - INFO - ==================================================
2026-01-14 12:20:05,800 - INFO -   [탐색 43] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:05,884 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:05,885 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:06,502 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222221996)에 맞춰 변경되었습니다.
2026-01-14 12:20:06,502 - INFO - ==================================================
2026-01-14 12:20:06,637 - INFO -   [탐색 44] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:06,816 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:06,817 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:07,375 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222276)에 맞춰 변경되었습니다.
2026-01-14 12:20:07,375 - INFO - ==================================================
2026-01-14 12:20:07,446 - INFO -   [탐색 45] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:07,535 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:07,538 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:08,053 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222137)에 맞춰 변경되었습니다.
2026-01-14 12:20:08,053 - INFO - ==================================================
2026-01-14 12:20:08,126 - INFO -   [탐색 46] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:08,209 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:08,210 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:09,202 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222207)에 맞춰 변경되었습니다.
2026-01-14 12:20:09,203 - INFO - ==================================================
2026-01-14 12:20:09,260 - INFO -   [탐색 47] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:09,322 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:09,322 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:09,759 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222241)에 맞춰 변경되었습니다.
2026-01-14 12:20:09,759 - INFO - ==================================================
2026-01-14 12:20:09,819 - INFO -   [탐색 48] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:09,876 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:09,877 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:10,267 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:10,268 - INFO - ==================================================
2026-01-14 12:20:10,340 - INFO -   [탐색 49] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:10,427 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:10,427 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:10,896 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222214)에 맞춰 변경되었습니다.
2026-01-14 12:20:10,896 - INFO - ==================================================
2026-01-14 12:20:10,959 - INFO -   [탐색 50] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:11,035 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:11,036 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:11,411 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222219)에 맞춰 변경되었습니다.
2026-01-14 12:20:11,411 - INFO - ==================================================
2026-01-14 12:20:11,484 - INFO -   [탐색 51] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:11,560 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:11,560 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:11,961 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222221)에 맞춰 변경되었습니다.
2026-01-14 12:20:11,961 - INFO - ==================================================
2026-01-14 12:20:12,032 - INFO -   [탐색 52] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:12,107 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:12,107 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:12,521 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222222)에 맞춰 변경되었습니다.
2026-01-14 12:20:12,522 - INFO - ==================================================
2026-01-14 12:20:12,595 - INFO -   [탐색 53] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:20:12,673 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:12,674 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:13,508 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:13,509 - INFO - ==================================================
2026-01-14 12:20:13,583 - INFO -   [탐색 54] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:13,659 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:13,660 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:14,287 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:14,290 - INFO - ==================================================
2026-01-14 12:20:14,367 - INFO -   [탐색 55] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:14,444 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:14,445 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:15,047 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:15,048 - INFO - ==================================================
2026-01-14 12:20:15,195 - INFO -   [탐색 56] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:15,283 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:15,284 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:15,948 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:15,949 - INFO - ==================================================
2026-01-14 12:20:16,024 - INFO -   [탐색 57] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:16,095 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:16,095 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:16,538 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:16,539 - INFO - ==================================================
2026-01-14 12:20:16,617 - INFO -   [탐색 58] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:16,696 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:16,696 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:17,182 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:17,182 - INFO - ==================================================
2026-01-14 12:20:17,232 - INFO -   [탐색 59] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:17,299 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:17,299 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:17,687 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:17,688 - INFO - ==================================================
2026-01-14 12:20:17,805 - INFO -   [탐색 60] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:17,899 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:17,899 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:18,598 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:18,599 - INFO - ==================================================
2026-01-14 12:20:18,661 - INFO -   [탐색 61] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:18,718 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:18,718 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:19,207 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:19,209 - INFO - ==================================================
2026-01-14 12:20:19,284 - INFO -   [탐색 62] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:19,364 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:19,365 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:19,858 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:19,859 - INFO - ==================================================
2026-01-14 12:20:19,937 - INFO -   [탐색 63] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:20,018 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:20,018 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:20,564 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:20,565 - INFO - ==================================================
2026-01-14 12:20:20,646 - INFO -   [탐색 64] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:20,722 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:20,723 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:21,214 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:21,215 - INFO - ==================================================
2026-01-14 12:20:21,331 - INFO -   [탐색 65] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:21,450 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:21,451 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:21,990 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:21,991 - INFO - ==================================================
2026-01-14 12:20:22,062 - INFO -   [탐색 66] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:22,138 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:22,139 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:22,796 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:22,797 - INFO - ==================================================
2026-01-14 12:20:22,867 - INFO -   [탐색 67] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:22,943 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:22,943 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:23,632 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:23,632 - INFO - ==================================================
2026-01-14 12:20:23,696 - INFO -   [탐색 68] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:23,768 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:23,768 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:24,260 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:24,260 - INFO - ==================================================
2026-01-14 12:20:24,329 - INFO -   [탐색 69] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:24,405 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:24,406 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:24,858 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:24,858 - INFO - ==================================================
2026-01-14 12:20:25,013 - INFO -   [탐색 70] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:25,090 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:25,091 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:25,499 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:25,500 - INFO - ==================================================
2026-01-14 12:20:25,573 - INFO -   [탐색 71] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:25,650 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:25,651 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:26,106 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:26,106 - INFO - ==================================================
2026-01-14 12:20:26,177 - INFO -   [탐색 72] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:26,253 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:26,254 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:26,707 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:26,707 - INFO - ==================================================
2026-01-14 12:20:26,779 - INFO -   [탐색 73] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:26,855 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:26,856 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:27,280 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:27,281 - INFO - ==================================================
2026-01-14 12:20:27,354 - INFO -   [탐색 74] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:27,430 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:27,431 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:28,257 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:28,258 - INFO - ==================================================
2026-01-14 12:20:28,340 - INFO -   [탐색 75] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:28,447 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:28,447 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:29,063 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:29,064 - INFO - ==================================================
2026-01-14 12:20:29,133 - INFO -   [탐색 76] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:29,212 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:29,212 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:29,795 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:29,796 - INFO - ==================================================
2026-01-14 12:20:29,866 - INFO -   [탐색 77] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:29,941 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:29,941 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:30,359 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:30,359 - INFO - ==================================================
2026-01-14 12:20:30,455 - INFO -   [탐색 78] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:30,544 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:30,545 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:30,990 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:30,990 - INFO - ==================================================
2026-01-14 12:20:31,050 - INFO -   [탐색 79] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:31,139 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:31,139 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:31,530 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:31,530 - INFO - ==================================================
2026-01-14 12:20:31,601 - INFO -   [탐색 80] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:31,675 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:31,676 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:32,043 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:32,043 - INFO - ==================================================
2026-01-14 12:20:32,116 - INFO -   [탐색 81] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:32,192 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:32,193 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:32,863 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:32,863 - INFO - ==================================================
2026-01-14 12:20:32,938 - INFO -   [탐색 82] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:33,007 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:33,007 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:33,352 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:33,352 - INFO - ==================================================
2026-01-14 12:20:33,425 - INFO -   [탐색 83] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:33,518 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:33,519 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:33,975 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:33,976 - INFO - ==================================================
2026-01-14 12:20:34,050 - INFO -   [탐색 84] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:34,156 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:34,160 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:34,748 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:34,749 - INFO - ==================================================
2026-01-14 12:20:34,822 - INFO -   [탐색 85] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:34,889 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:34,890 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:35,316 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:35,317 - INFO - ==================================================
2026-01-14 12:20:35,389 - INFO -   [탐색 86] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:35,469 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:35,470 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:35,985 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:35,986 - INFO - ==================================================
2026-01-14 12:20:36,055 - INFO -   [탐색 87] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:36,133 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:36,133 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:36,664 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:36,665 - INFO - ==================================================
2026-01-14 12:20:36,736 - INFO -   [탐색 88] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:36,817 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:36,817 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:37,598 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:37,600 - INFO - ==================================================
2026-01-14 12:20:37,675 - INFO -   [탐색 89] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:37,746 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:37,747 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:38,194 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:38,194 - INFO - ==================================================
2026-01-14 12:20:38,273 - INFO -   [탐색 90] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:38,339 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:38,340 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:38,714 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:38,715 - INFO - ==================================================
2026-01-14 12:20:38,786 - INFO -   [탐색 91] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:38,861 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:38,862 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:39,238 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:39,238 - INFO - ==================================================
2026-01-14 12:20:39,318 - INFO -   [탐색 92] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:39,396 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:39,397 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:39,819 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:39,819 - INFO - ==================================================
2026-01-14 12:20:39,897 - INFO -   [탐색 93] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:39,981 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:39,981 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:40,369 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:40,370 - INFO - ==================================================
2026-01-14 12:20:40,451 - INFO -   [탐색 94] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:40,530 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:40,531 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:40,953 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:40,954 - INFO - ==================================================
2026-01-14 12:20:41,031 - INFO -   [탐색 95] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:41,111 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:41,111 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:41,898 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:41,899 - INFO - ==================================================
2026-01-14 12:20:41,981 - INFO -   [탐색 96] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:42,062 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:42,063 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:42,543 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:42,543 - INFO - ==================================================
2026-01-14 12:20:42,625 - INFO -   [탐색 97] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:42,711 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:42,712 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:43,196 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:43,197 - INFO - ==================================================
2026-01-14 12:20:43,275 - INFO -   [탐색 98] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:43,360 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:43,360 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:43,856 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:43,856 - INFO - ==================================================
2026-01-14 12:20:43,943 - INFO -   [탐색 99] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:44,025 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:44,026 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:44,492 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:20:44,492 - INFO - ==================================================
2026-01-14 12:20:44,566 - INFO -   [탐색 100] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:20:44,567 - INFO - 탐색 완료. 목표 FLOPs(0.1829)에 가장 근접한 최적 희소도는 0.5738 입니다.
2026-01-14 12:20:44,567 - INFO - ================================================================================
2026-01-14 12:20:44,571 - INFO - 계산된 Pruning 정보(희소도: 0.5738)를 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/pruning_info.yaml'에 저장했습니다.
2026-01-14 12:20:44,645 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-01-14 12:20:44,808 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:44,808 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:45,362 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737939453124999)에 맞춰 변경되었습니다.
2026-01-14 12:20:45,363 - INFO - ==================================================
2026-01-14 12:20:45,366 - INFO - ==================================================
2026-01-14 12:20:45,366 - INFO - 모델 파라미터 수:
2026-01-14 12:20:45,366 - INFO -   - 총 파라미터: 775,493 개
2026-01-14 12:20:45,366 - INFO -   - 학습 가능한 파라미터: 775,493 개
2026-01-14 12:20:45,438 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-01-14 12:20:45,594 - INFO - FLOPs가 0.8277 GFLOPs에서 0.1829 GFLOPs로 감소했습니다 (감소율: 77.91%).
2026-01-14 12:20:45,595 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-01-14 12:20:45,595 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:20:45,597 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-01-14 12:20:45,598 - INFO - ==================================================
2026-01-14 12:20:45,598 - INFO - train 모드를 시작합니다.
2026-01-14 12:20:45,598 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:20:45,598 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:20:45,598 - INFO - --------------------------------------------------
2026-01-14 12:20:45,601 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-01-14 12:20:51,164 - INFO - [Train] [11/90] | Loss: 0.5297 | Train Acc: 77.31%
2026-01-14 12:20:52,626 - INFO - [Valid] [11/90] | Loss: 0.5845 | Val Acc: 74.63%
2026-01-14 12:20:52,636 - INFO - [Metrics for 'abnormal'] | Precision: 0.6859 | Recall: 0.8344 | F1: 0.7529
2026-01-14 12:20:52,636 - INFO - [Metrics for 'normal'] | Precision: 0.8243 | Recall: 0.6703 | F1: 0.7394
2026-01-14 12:20:52,682 - INFO - [Best Model Saved] (val loss: 0.5845) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:20:52,683 - INFO - --------------------------------------------------
2026-01-14 12:20:52,685 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-01-14 12:20:57,800 - INFO - [Train] [12/90] | Loss: 0.4939 | Train Acc: 81.70%
2026-01-14 12:20:59,366 - INFO - [Valid] [12/90] | Loss: 0.4992 | Val Acc: 81.71%
2026-01-14 12:20:59,374 - INFO - [Metrics for 'abnormal'] | Precision: 0.8188 | Recall: 0.7771 | F1: 0.7974
2026-01-14 12:20:59,374 - INFO - [Metrics for 'normal'] | Precision: 0.8158 | Recall: 0.8516 | F1: 0.8333
2026-01-14 12:20:59,415 - INFO - [Best Model Saved] (val loss: 0.4992) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:20:59,416 - INFO - --------------------------------------------------
2026-01-14 12:20:59,419 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-01-14 12:21:04,649 - INFO - [Train] [13/90] | Loss: 0.4757 | Train Acc: 82.37%
2026-01-14 12:21:06,081 - INFO - [Valid] [13/90] | Loss: 0.5229 | Val Acc: 78.76%
2026-01-14 12:21:06,091 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.7580 | F1: 0.7677
2026-01-14 12:21:06,091 - INFO - [Metrics for 'normal'] | Precision: 0.7957 | Recall: 0.8132 | F1: 0.8043
2026-01-14 12:21:06,095 - INFO - --------------------------------------------------
2026-01-14 12:21:06,097 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-01-14 12:21:11,529 - INFO - [Train] [14/90] | Loss: 0.4561 | Train Acc: 83.26%
2026-01-14 12:21:13,050 - INFO - [Valid] [14/90] | Loss: 0.4725 | Val Acc: 82.01%
2026-01-14 12:21:13,059 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-01-14 12:21:13,060 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-01-14 12:21:13,100 - INFO - [Best Model Saved] (val loss: 0.4725) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:21:13,100 - INFO - --------------------------------------------------
2026-01-14 12:21:13,102 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-01-14 12:21:18,858 - INFO - [Train] [15/90] | Loss: 0.4397 | Train Acc: 83.18%
2026-01-14 12:21:20,801 - INFO - [Valid] [15/90] | Loss: 0.4715 | Val Acc: 82.89%
2026-01-14 12:21:20,833 - INFO - [Metrics for 'abnormal'] | Precision: 0.8194 | Recall: 0.8089 | F1: 0.8141
2026-01-14 12:21:20,834 - INFO - [Metrics for 'normal'] | Precision: 0.8370 | Recall: 0.8462 | F1: 0.8415
2026-01-14 12:21:20,903 - INFO - [Best Model Saved] (val loss: 0.4715) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:21:20,903 - INFO - --------------------------------------------------
2026-01-14 12:21:20,905 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-01-14 12:21:27,595 - INFO - [Train] [16/90] | Loss: 0.4428 | Train Acc: 84.75%
2026-01-14 12:21:29,281 - INFO - [Valid] [16/90] | Loss: 0.4897 | Val Acc: 79.94%
2026-01-14 12:21:29,296 - INFO - [Metrics for 'abnormal'] | Precision: 0.8069 | Recall: 0.7452 | F1: 0.7748
2026-01-14 12:21:29,297 - INFO - [Metrics for 'normal'] | Precision: 0.7938 | Recall: 0.8462 | F1: 0.8191
2026-01-14 12:21:29,301 - INFO - --------------------------------------------------
2026-01-14 12:21:29,305 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-01-14 12:21:35,286 - INFO - [Train] [17/90] | Loss: 0.4183 | Train Acc: 86.24%
2026-01-14 12:21:37,119 - INFO - [Valid] [17/90] | Loss: 0.4887 | Val Acc: 82.60%
2026-01-14 12:21:37,133 - INFO - [Metrics for 'abnormal'] | Precision: 0.8451 | Recall: 0.7643 | F1: 0.8027
2026-01-14 12:21:37,134 - INFO - [Metrics for 'normal'] | Precision: 0.8122 | Recall: 0.8791 | F1: 0.8443
2026-01-14 12:21:37,139 - INFO - --------------------------------------------------
2026-01-14 12:21:37,142 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-01-14 12:21:43,504 - INFO - [Train] [18/90] | Loss: 0.3887 | Train Acc: 88.54%
2026-01-14 12:21:45,252 - INFO - [Valid] [18/90] | Loss: 0.4648 | Val Acc: 83.19%
2026-01-14 12:21:45,264 - INFO - [Metrics for 'abnormal'] | Precision: 0.8125 | Recall: 0.8280 | F1: 0.8202
2026-01-14 12:21:45,265 - INFO - [Metrics for 'normal'] | Precision: 0.8492 | Recall: 0.8352 | F1: 0.8421
2026-01-14 12:21:45,332 - INFO - [Best Model Saved] (val loss: 0.4648) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:21:45,333 - INFO - --------------------------------------------------
2026-01-14 12:21:45,335 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-01-14 12:21:53,382 - INFO - [Train] [19/90] | Loss: 0.4002 | Train Acc: 87.95%
2026-01-14 12:21:55,515 - INFO - [Valid] [19/90] | Loss: 0.4651 | Val Acc: 83.48%
2026-01-14 12:21:55,529 - INFO - [Metrics for 'abnormal'] | Precision: 0.8176 | Recall: 0.8280 | F1: 0.8228
2026-01-14 12:21:55,530 - INFO - [Metrics for 'normal'] | Precision: 0.8500 | Recall: 0.8407 | F1: 0.8453
2026-01-14 12:21:55,535 - INFO - --------------------------------------------------
2026-01-14 12:21:55,538 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-01-14 12:22:04,010 - INFO - [Train] [20/90] | Loss: 0.3999 | Train Acc: 87.28%
2026-01-14 12:22:06,027 - INFO - [Valid] [20/90] | Loss: 0.4766 | Val Acc: 84.37%
2026-01-14 12:22:06,037 - INFO - [Metrics for 'abnormal'] | Precision: 0.8210 | Recall: 0.8471 | F1: 0.8339
2026-01-14 12:22:06,038 - INFO - [Metrics for 'normal'] | Precision: 0.8644 | Recall: 0.8407 | F1: 0.8524
2026-01-14 12:22:06,042 - INFO - --------------------------------------------------
2026-01-14 12:22:06,044 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-01-14 12:22:14,124 - INFO - [Train] [21/90] | Loss: 0.3868 | Train Acc: 88.62%
2026-01-14 12:22:16,061 - INFO - [Valid] [21/90] | Loss: 0.4821 | Val Acc: 82.60%
2026-01-14 12:22:16,074 - INFO - [Metrics for 'abnormal'] | Precision: 0.8500 | Recall: 0.7580 | F1: 0.8013
2026-01-14 12:22:16,074 - INFO - [Metrics for 'normal'] | Precision: 0.8090 | Recall: 0.8846 | F1: 0.8451
2026-01-14 12:22:16,078 - INFO - --------------------------------------------------
2026-01-14 12:22:16,080 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-01-14 12:22:23,873 - INFO - [Train] [22/90] | Loss: 0.3705 | Train Acc: 89.58%
2026-01-14 12:22:25,871 - INFO - [Valid] [22/90] | Loss: 0.4820 | Val Acc: 80.53%
2026-01-14 12:22:25,885 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-01-14 12:22:25,886 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-01-14 12:22:25,890 - INFO - --------------------------------------------------
2026-01-14 12:22:25,893 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-01-14 12:22:34,549 - INFO - [Train] [23/90] | Loss: 0.3727 | Train Acc: 88.32%
2026-01-14 12:22:36,274 - INFO - [Valid] [23/90] | Loss: 0.4499 | Val Acc: 84.07%
2026-01-14 12:22:36,286 - INFO - [Metrics for 'abnormal'] | Precision: 0.8411 | Recall: 0.8089 | F1: 0.8247
2026-01-14 12:22:36,287 - INFO - [Metrics for 'normal'] | Precision: 0.8404 | Recall: 0.8681 | F1: 0.8541
2026-01-14 12:22:36,356 - INFO - [Best Model Saved] (val loss: 0.4499) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:22:36,356 - INFO - --------------------------------------------------
2026-01-14 12:22:36,358 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-01-14 12:22:44,687 - INFO - [Train] [24/90] | Loss: 0.3517 | Train Acc: 91.52%
2026-01-14 12:22:46,657 - INFO - [Valid] [24/90] | Loss: 0.5118 | Val Acc: 80.83%
2026-01-14 12:22:46,672 - INFO - [Metrics for 'abnormal'] | Precision: 0.7614 | Recall: 0.8535 | F1: 0.8048
2026-01-14 12:22:46,672 - INFO - [Metrics for 'normal'] | Precision: 0.8589 | Recall: 0.7692 | F1: 0.8116
2026-01-14 12:22:46,677 - INFO - --------------------------------------------------
2026-01-14 12:22:46,680 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-01-14 12:22:55,216 - INFO - [Train] [25/90] | Loss: 0.3590 | Train Acc: 90.48%
2026-01-14 12:22:57,912 - INFO - [Valid] [25/90] | Loss: 0.4440 | Val Acc: 84.37%
2026-01-14 12:22:57,925 - INFO - [Metrics for 'abnormal'] | Precision: 0.8467 | Recall: 0.8089 | F1: 0.8274
2026-01-14 12:22:57,925 - INFO - [Metrics for 'normal'] | Precision: 0.8413 | Recall: 0.8736 | F1: 0.8571
2026-01-14 12:22:58,002 - INFO - [Best Model Saved] (val loss: 0.4440) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:22:58,003 - INFO - --------------------------------------------------
2026-01-14 12:22:58,005 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-01-14 12:23:07,818 - INFO - [Train] [26/90] | Loss: 0.3270 | Train Acc: 91.59%
2026-01-14 12:23:10,541 - INFO - [Valid] [26/90] | Loss: 0.4768 | Val Acc: 81.12%
2026-01-14 12:23:10,553 - INFO - [Metrics for 'abnormal'] | Precision: 0.8908 | Recall: 0.6752 | F1: 0.7681
2026-01-14 12:23:10,554 - INFO - [Metrics for 'normal'] | Precision: 0.7682 | Recall: 0.9286 | F1: 0.8408
2026-01-14 12:23:10,558 - INFO - --------------------------------------------------
2026-01-14 12:23:10,560 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-01-14 12:23:20,270 - INFO - [Train] [27/90] | Loss: 0.3407 | Train Acc: 92.41%
2026-01-14 12:23:22,850 - INFO - [Valid] [27/90] | Loss: 0.4826 | Val Acc: 81.71%
2026-01-14 12:23:22,864 - INFO - [Metrics for 'abnormal'] | Precision: 0.7879 | Recall: 0.8280 | F1: 0.8075
2026-01-14 12:23:22,866 - INFO - [Metrics for 'normal'] | Precision: 0.8448 | Recall: 0.8077 | F1: 0.8258
2026-01-14 12:23:22,871 - INFO - --------------------------------------------------
2026-01-14 12:23:22,874 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-01-14 12:23:32,209 - INFO - [Train] [28/90] | Loss: 0.3353 | Train Acc: 92.26%
2026-01-14 12:23:34,935 - INFO - [Valid] [28/90] | Loss: 0.5096 | Val Acc: 82.60%
2026-01-14 12:23:34,948 - INFO - [Metrics for 'abnormal'] | Precision: 0.7722 | Recall: 0.8854 | F1: 0.8249
2026-01-14 12:23:34,949 - INFO - [Metrics for 'normal'] | Precision: 0.8868 | Recall: 0.7747 | F1: 0.8270
2026-01-14 12:23:34,954 - INFO - --------------------------------------------------
2026-01-14 12:23:34,957 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-01-14 12:23:44,799 - INFO - [Train] [29/90] | Loss: 0.3419 | Train Acc: 92.26%
2026-01-14 12:23:47,772 - INFO - [Valid] [29/90] | Loss: 0.5111 | Val Acc: 84.37%
2026-01-14 12:23:47,796 - INFO - [Metrics for 'abnormal'] | Precision: 0.8514 | Recall: 0.8025 | F1: 0.8262
2026-01-14 12:23:47,796 - INFO - [Metrics for 'normal'] | Precision: 0.8377 | Recall: 0.8791 | F1: 0.8579
2026-01-14 12:23:47,807 - INFO - --------------------------------------------------
2026-01-14 12:23:47,811 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-01-14 12:23:58,496 - INFO - [Train] [30/90] | Loss: 0.3150 | Train Acc: 93.82%
2026-01-14 12:24:00,786 - INFO - [Valid] [30/90] | Loss: 0.5192 | Val Acc: 82.30%
2026-01-14 12:24:00,799 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-01-14 12:24:00,800 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-01-14 12:24:00,805 - INFO - --------------------------------------------------
2026-01-14 12:24:00,808 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-01-14 12:24:11,620 - INFO - [Train] [31/90] | Loss: 0.3114 | Train Acc: 93.75%
2026-01-14 12:24:13,586 - INFO - [Valid] [31/90] | Loss: 0.5422 | Val Acc: 83.78%
2026-01-14 12:24:13,598 - INFO - [Metrics for 'abnormal'] | Precision: 0.7898 | Recall: 0.8854 | F1: 0.8348
2026-01-14 12:24:13,598 - INFO - [Metrics for 'normal'] | Precision: 0.8896 | Recall: 0.7967 | F1: 0.8406
2026-01-14 12:24:13,602 - INFO - --------------------------------------------------
2026-01-14 12:24:13,605 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-01-14 12:24:23,085 - INFO - [Train] [32/90] | Loss: 0.3028 | Train Acc: 93.60%
2026-01-14 12:24:27,145 - INFO - [Valid] [32/90] | Loss: 0.4415 | Val Acc: 83.48%
2026-01-14 12:24:27,157 - INFO - [Metrics for 'abnormal'] | Precision: 0.8582 | Recall: 0.7707 | F1: 0.8121
2026-01-14 12:24:27,157 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8901 | F1: 0.8526
2026-01-14 12:24:27,205 - INFO - [Best Model Saved] (val loss: 0.4415) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:24:27,205 - INFO - --------------------------------------------------
2026-01-14 12:24:27,208 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-01-14 12:24:37,040 - INFO - [Train] [33/90] | Loss: 0.3092 | Train Acc: 93.97%
2026-01-14 12:24:39,288 - INFO - [Valid] [33/90] | Loss: 0.4817 | Val Acc: 84.66%
2026-01-14 12:24:39,302 - INFO - [Metrics for 'abnormal'] | Precision: 0.8571 | Recall: 0.8025 | F1: 0.8289
2026-01-14 12:24:39,302 - INFO - [Metrics for 'normal'] | Precision: 0.8385 | Recall: 0.8846 | F1: 0.8610
2026-01-14 12:24:39,307 - INFO - --------------------------------------------------
2026-01-14 12:24:39,310 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-01-14 12:24:49,460 - INFO - [Train] [34/90] | Loss: 0.2926 | Train Acc: 95.01%
2026-01-14 12:24:51,712 - INFO - [Valid] [34/90] | Loss: 0.4854 | Val Acc: 84.96%
2026-01-14 12:24:51,724 - INFO - [Metrics for 'abnormal'] | Precision: 0.8533 | Recall: 0.8153 | F1: 0.8339
2026-01-14 12:24:51,725 - INFO - [Metrics for 'normal'] | Precision: 0.8466 | Recall: 0.8791 | F1: 0.8625
2026-01-14 12:24:51,729 - INFO - --------------------------------------------------
2026-01-14 12:24:51,732 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-01-14 12:25:01,588 - INFO - [Train] [35/90] | Loss: 0.2904 | Train Acc: 95.09%
2026-01-14 12:25:04,096 - INFO - [Valid] [35/90] | Loss: 0.4409 | Val Acc: 87.91%
2026-01-14 12:25:04,122 - INFO - [Metrics for 'abnormal'] | Precision: 0.8718 | Recall: 0.8662 | F1: 0.8690
2026-01-14 12:25:04,125 - INFO - [Metrics for 'normal'] | Precision: 0.8852 | Recall: 0.8901 | F1: 0.8877
2026-01-14 12:25:04,236 - INFO - [Best Model Saved] (val loss: 0.4409) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:25:04,237 - INFO - --------------------------------------------------
2026-01-14 12:25:04,240 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-01-14 12:25:14,696 - INFO - [Train] [36/90] | Loss: 0.2817 | Train Acc: 95.39%
2026-01-14 12:25:16,943 - INFO - [Valid] [36/90] | Loss: 0.4921 | Val Acc: 83.78%
2026-01-14 12:25:16,955 - INFO - [Metrics for 'abnormal'] | Precision: 0.8269 | Recall: 0.8217 | F1: 0.8243
2026-01-14 12:25:16,956 - INFO - [Metrics for 'normal'] | Precision: 0.8470 | Recall: 0.8516 | F1: 0.8493
2026-01-14 12:25:16,961 - INFO - --------------------------------------------------
2026-01-14 12:25:16,964 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-01-14 12:25:27,373 - INFO - [Train] [37/90] | Loss: 0.2839 | Train Acc: 95.68%
2026-01-14 12:25:29,795 - INFO - [Valid] [37/90] | Loss: 0.4662 | Val Acc: 83.78%
2026-01-14 12:25:29,815 - INFO - [Metrics for 'abnormal'] | Precision: 0.8446 | Recall: 0.7962 | F1: 0.8197
2026-01-14 12:25:29,815 - INFO - [Metrics for 'normal'] | Precision: 0.8325 | Recall: 0.8736 | F1: 0.8525
2026-01-14 12:25:29,822 - INFO - --------------------------------------------------
2026-01-14 12:25:29,828 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-01-14 12:25:40,210 - INFO - [Train] [38/90] | Loss: 0.2856 | Train Acc: 95.61%
2026-01-14 12:25:43,989 - INFO - [Valid] [38/90] | Loss: 0.4937 | Val Acc: 84.66%
2026-01-14 12:25:44,018 - INFO - [Metrics for 'abnormal'] | Precision: 0.8182 | Recall: 0.8599 | F1: 0.8385
2026-01-14 12:25:44,018 - INFO - [Metrics for 'normal'] | Precision: 0.8736 | Recall: 0.8352 | F1: 0.8539
2026-01-14 12:25:44,025 - INFO - --------------------------------------------------
2026-01-14 12:25:44,031 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-01-14 12:25:53,247 - INFO - [Train] [39/90] | Loss: 0.2846 | Train Acc: 95.54%
2026-01-14 12:25:55,955 - INFO - [Valid] [39/90] | Loss: 0.5448 | Val Acc: 83.19%
2026-01-14 12:25:55,994 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8471 | F1: 0.8235
2026-01-14 12:25:55,994 - INFO - [Metrics for 'normal'] | Precision: 0.8613 | Recall: 0.8187 | F1: 0.8394
2026-01-14 12:25:56,000 - INFO - --------------------------------------------------
2026-01-14 12:25:56,004 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-01-14 12:26:06,032 - INFO - [Train] [40/90] | Loss: 0.2752 | Train Acc: 96.13%
2026-01-14 12:26:09,384 - INFO - [Valid] [40/90] | Loss: 0.4977 | Val Acc: 80.83%
2026-01-14 12:26:09,409 - INFO - [Metrics for 'abnormal'] | Precision: 0.7347 | Recall: 0.9172 | F1: 0.8159
2026-01-14 12:26:09,410 - INFO - [Metrics for 'normal'] | Precision: 0.9091 | Recall: 0.7143 | F1: 0.8000
2026-01-14 12:26:09,420 - INFO - --------------------------------------------------
2026-01-14 12:26:09,427 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-01-14 12:26:17,746 - INFO - [Train] [41/90] | Loss: 0.2800 | Train Acc: 95.83%
2026-01-14 12:26:20,575 - INFO - [Valid] [41/90] | Loss: 0.4661 | Val Acc: 85.84%
2026-01-14 12:26:20,601 - INFO - [Metrics for 'abnormal'] | Precision: 0.8428 | Recall: 0.8535 | F1: 0.8481
2026-01-14 12:26:20,602 - INFO - [Metrics for 'normal'] | Precision: 0.8722 | Recall: 0.8626 | F1: 0.8674
2026-01-14 12:26:20,609 - INFO - --------------------------------------------------
2026-01-14 12:26:20,617 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-01-14 12:26:29,012 - INFO - [Train] [42/90] | Loss: 0.2731 | Train Acc: 95.54%
2026-01-14 12:26:32,487 - INFO - [Valid] [42/90] | Loss: 0.4882 | Val Acc: 85.25%
2026-01-14 12:26:32,510 - INFO - [Metrics for 'abnormal'] | Precision: 0.8242 | Recall: 0.8662 | F1: 0.8447
2026-01-14 12:26:32,510 - INFO - [Metrics for 'normal'] | Precision: 0.8793 | Recall: 0.8407 | F1: 0.8596
2026-01-14 12:26:32,521 - INFO - --------------------------------------------------
2026-01-14 12:26:32,529 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-01-14 12:26:41,573 - INFO - [Train] [43/90] | Loss: 0.2631 | Train Acc: 96.73%
2026-01-14 12:26:44,837 - INFO - [Valid] [43/90] | Loss: 0.4768 | Val Acc: 85.25%
2026-01-14 12:26:44,849 - INFO - [Metrics for 'abnormal'] | Precision: 0.8408 | Recall: 0.8408 | F1: 0.8408
2026-01-14 12:26:44,850 - INFO - [Metrics for 'normal'] | Precision: 0.8626 | Recall: 0.8626 | F1: 0.8626
2026-01-14 12:26:44,855 - INFO - --------------------------------------------------
2026-01-14 12:26:44,858 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-01-14 12:26:53,685 - INFO - [Train] [44/90] | Loss: 0.2569 | Train Acc: 97.17%
2026-01-14 12:26:56,388 - INFO - [Valid] [44/90] | Loss: 0.4840 | Val Acc: 84.66%
2026-01-14 12:26:56,420 - INFO - [Metrics for 'abnormal'] | Precision: 0.8523 | Recall: 0.8089 | F1: 0.8301
2026-01-14 12:26:56,421 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.8791 | F1: 0.8602
2026-01-14 12:26:56,426 - INFO - --------------------------------------------------
2026-01-14 12:26:56,430 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-01-14 12:27:06,928 - INFO - [Train] [45/90] | Loss: 0.2628 | Train Acc: 96.43%
2026-01-14 12:27:11,159 - INFO - [Valid] [45/90] | Loss: 0.4981 | Val Acc: 83.48%
2026-01-14 12:27:11,172 - INFO - [Metrics for 'abnormal'] | Precision: 0.8024 | Recall: 0.8535 | F1: 0.8272
2026-01-14 12:27:11,172 - INFO - [Metrics for 'normal'] | Precision: 0.8663 | Recall: 0.8187 | F1: 0.8418
2026-01-14 12:27:11,190 - INFO - --------------------------------------------------
2026-01-14 12:27:11,193 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-01-14 12:27:19,807 - INFO - [Train] [46/90] | Loss: 0.2459 | Train Acc: 97.54%
2026-01-14 12:27:22,693 - INFO - [Valid] [46/90] | Loss: 0.4725 | Val Acc: 85.84%
2026-01-14 12:27:22,706 - INFO - [Metrics for 'abnormal'] | Precision: 0.8428 | Recall: 0.8535 | F1: 0.8481
2026-01-14 12:27:22,707 - INFO - [Metrics for 'normal'] | Precision: 0.8722 | Recall: 0.8626 | F1: 0.8674
2026-01-14 12:27:22,711 - INFO - --------------------------------------------------
2026-01-14 12:27:22,715 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-01-14 12:27:32,156 - INFO - [Train] [47/90] | Loss: 0.2452 | Train Acc: 98.21%
2026-01-14 12:27:35,627 - INFO - [Valid] [47/90] | Loss: 0.4625 | Val Acc: 84.37%
2026-01-14 12:27:35,638 - INFO - [Metrics for 'abnormal'] | Precision: 0.8377 | Recall: 0.8217 | F1: 0.8296
2026-01-14 12:27:35,639 - INFO - [Metrics for 'normal'] | Precision: 0.8486 | Recall: 0.8626 | F1: 0.8556
2026-01-14 12:27:35,645 - INFO - --------------------------------------------------
2026-01-14 12:27:35,648 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-01-14 12:27:44,971 - INFO - [Train] [48/90] | Loss: 0.2460 | Train Acc: 97.40%
2026-01-14 12:27:47,638 - INFO - [Valid] [48/90] | Loss: 0.4866 | Val Acc: 84.37%
2026-01-14 12:27:47,663 - INFO - [Metrics for 'abnormal'] | Precision: 0.8059 | Recall: 0.8726 | F1: 0.8379
2026-01-14 12:27:47,663 - INFO - [Metrics for 'normal'] | Precision: 0.8817 | Recall: 0.8187 | F1: 0.8490
2026-01-14 12:27:47,667 - INFO - --------------------------------------------------
2026-01-14 12:27:47,670 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-01-14 12:27:56,367 - INFO - [Train] [49/90] | Loss: 0.2405 | Train Acc: 97.77%
2026-01-14 12:28:00,533 - INFO - [Valid] [49/90] | Loss: 0.4971 | Val Acc: 84.37%
2026-01-14 12:28:00,554 - INFO - [Metrics for 'abnormal'] | Precision: 0.8095 | Recall: 0.8662 | F1: 0.8369
2026-01-14 12:28:00,555 - INFO - [Metrics for 'normal'] | Precision: 0.8772 | Recall: 0.8242 | F1: 0.8499
2026-01-14 12:28:00,560 - INFO - --------------------------------------------------
2026-01-14 12:28:00,563 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-01-14 12:28:09,536 - INFO - [Train] [50/90] | Loss: 0.2425 | Train Acc: 97.69%
2026-01-14 12:28:12,484 - INFO - [Valid] [50/90] | Loss: 0.4891 | Val Acc: 84.37%
2026-01-14 12:28:12,496 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.8280 | F1: 0.8307
2026-01-14 12:28:12,497 - INFO - [Metrics for 'normal'] | Precision: 0.8525 | Recall: 0.8571 | F1: 0.8548
2026-01-14 12:28:12,598 - INFO - --------------------------------------------------
2026-01-14 12:28:12,602 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-01-14 12:28:21,516 - INFO - [Train] [51/90] | Loss: 0.2429 | Train Acc: 97.62%
2026-01-14 12:28:24,395 - INFO - [Valid] [51/90] | Loss: 0.4974 | Val Acc: 83.19%
2026-01-14 12:28:24,428 - INFO - [Metrics for 'abnormal'] | Precision: 0.8165 | Recall: 0.8217 | F1: 0.8190
2026-01-14 12:28:24,428 - INFO - [Metrics for 'normal'] | Precision: 0.8453 | Recall: 0.8407 | F1: 0.8430
2026-01-14 12:28:24,442 - INFO - --------------------------------------------------
2026-01-14 12:28:24,444 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-01-14 12:28:34,087 - INFO - [Train] [52/90] | Loss: 0.2297 | Train Acc: 98.81%
2026-01-14 12:28:36,618 - INFO - [Valid] [52/90] | Loss: 0.5011 | Val Acc: 83.78%
2026-01-14 12:28:36,628 - INFO - [Metrics for 'abnormal'] | Precision: 0.8072 | Recall: 0.8535 | F1: 0.8297
2026-01-14 12:28:36,629 - INFO - [Metrics for 'normal'] | Precision: 0.8671 | Recall: 0.8242 | F1: 0.8451
2026-01-14 12:28:36,633 - INFO - --------------------------------------------------
2026-01-14 12:28:36,638 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-01-14 12:28:46,565 - INFO - [Train] [53/90] | Loss: 0.2627 | Train Acc: 96.21%
2026-01-14 12:28:49,648 - INFO - [Valid] [53/90] | Loss: 0.4517 | Val Acc: 86.14%
2026-01-14 12:28:49,664 - INFO - [Metrics for 'abnormal'] | Precision: 0.8819 | Recall: 0.8089 | F1: 0.8439
2026-01-14 12:28:49,665 - INFO - [Metrics for 'normal'] | Precision: 0.8462 | Recall: 0.9066 | F1: 0.8753
2026-01-14 12:28:49,669 - INFO - --------------------------------------------------
2026-01-14 12:28:49,671 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-01-14 12:28:59,623 - INFO - [Train] [54/90] | Loss: 0.2329 | Train Acc: 98.51%
2026-01-14 12:29:02,227 - INFO - [Valid] [54/90] | Loss: 0.4572 | Val Acc: 86.73%
2026-01-14 12:29:02,240 - INFO - [Metrics for 'abnormal'] | Precision: 0.8544 | Recall: 0.8599 | F1: 0.8571
2026-01-14 12:29:02,240 - INFO - [Metrics for 'normal'] | Precision: 0.8785 | Recall: 0.8736 | F1: 0.8760
2026-01-14 12:29:02,245 - INFO - --------------------------------------------------
2026-01-14 12:29:02,247 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-01-14 12:29:11,773 - INFO - [Train] [55/90] | Loss: 0.2258 | Train Acc: 98.59%
2026-01-14 12:29:14,263 - INFO - [Valid] [55/90] | Loss: 0.5191 | Val Acc: 84.66%
2026-01-14 12:29:14,276 - INFO - [Metrics for 'abnormal'] | Precision: 0.8431 | Recall: 0.8217 | F1: 0.8323
2026-01-14 12:29:14,276 - INFO - [Metrics for 'normal'] | Precision: 0.8495 | Recall: 0.8681 | F1: 0.8587
2026-01-14 12:29:14,281 - INFO - --------------------------------------------------
2026-01-14 12:29:14,284 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-01-14 12:29:23,094 - INFO - [Train] [56/90] | Loss: 0.2285 | Train Acc: 98.21%
2026-01-14 12:29:26,086 - INFO - [Valid] [56/90] | Loss: 0.4964 | Val Acc: 83.78%
2026-01-14 12:29:26,099 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8662 | F1: 0.8318
2026-01-14 12:29:26,099 - INFO - [Metrics for 'normal'] | Precision: 0.8757 | Recall: 0.8132 | F1: 0.8433
2026-01-14 12:29:26,104 - INFO - --------------------------------------------------
2026-01-14 12:29:26,107 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-01-14 12:29:35,827 - INFO - [Train] [57/90] | Loss: 0.2221 | Train Acc: 98.88%
2026-01-14 12:29:38,219 - INFO - [Valid] [57/90] | Loss: 0.5014 | Val Acc: 84.37%
2026-01-14 12:29:38,236 - INFO - [Metrics for 'abnormal'] | Precision: 0.8768 | Recall: 0.7707 | F1: 0.8203
2026-01-14 12:29:38,240 - INFO - [Metrics for 'normal'] | Precision: 0.8209 | Recall: 0.9066 | F1: 0.8616
2026-01-14 12:29:38,244 - INFO - --------------------------------------------------
2026-01-14 12:29:38,249 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-01-14 12:29:47,503 - INFO - [Train] [58/90] | Loss: 0.2232 | Train Acc: 98.59%
2026-01-14 12:29:50,053 - INFO - [Valid] [58/90] | Loss: 0.5250 | Val Acc: 83.78%
2026-01-14 12:29:50,065 - INFO - [Metrics for 'abnormal'] | Precision: 0.8592 | Recall: 0.7771 | F1: 0.8161
2026-01-14 12:29:50,066 - INFO - [Metrics for 'normal'] | Precision: 0.8223 | Recall: 0.8901 | F1: 0.8549
2026-01-14 12:29:50,072 - INFO - --------------------------------------------------
2026-01-14 12:29:50,075 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-01-14 12:29:59,352 - INFO - [Train] [59/90] | Loss: 0.2175 | Train Acc: 99.18%
2026-01-14 12:30:01,639 - INFO - [Valid] [59/90] | Loss: 0.4908 | Val Acc: 84.37%
2026-01-14 12:30:01,652 - INFO - [Metrics for 'abnormal'] | Precision: 0.8421 | Recall: 0.8153 | F1: 0.8285
2026-01-14 12:30:01,653 - INFO - [Metrics for 'normal'] | Precision: 0.8449 | Recall: 0.8681 | F1: 0.8564
2026-01-14 12:30:01,657 - INFO - --------------------------------------------------
2026-01-14 12:30:01,661 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-01-14 12:30:11,379 - INFO - [Train] [60/90] | Loss: 0.2166 | Train Acc: 99.03%
2026-01-14 12:30:14,269 - INFO - [Valid] [60/90] | Loss: 0.5206 | Val Acc: 84.07%
2026-01-14 12:30:14,295 - INFO - [Metrics for 'abnormal'] | Precision: 0.8047 | Recall: 0.8662 | F1: 0.8344
2026-01-14 12:30:14,296 - INFO - [Metrics for 'normal'] | Precision: 0.8765 | Recall: 0.8187 | F1: 0.8466
2026-01-14 12:30:14,306 - INFO - --------------------------------------------------
2026-01-14 12:30:14,312 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-01-14 12:30:23,567 - INFO - [Train] [61/90] | Loss: 0.2155 | Train Acc: 99.33%
2026-01-14 12:30:26,529 - INFO - [Valid] [61/90] | Loss: 0.5008 | Val Acc: 84.96%
2026-01-14 12:30:26,542 - INFO - [Metrics for 'abnormal'] | Precision: 0.8681 | Recall: 0.7962 | F1: 0.8306
2026-01-14 12:30:26,543 - INFO - [Metrics for 'normal'] | Precision: 0.8359 | Recall: 0.8956 | F1: 0.8647
2026-01-14 12:30:26,548 - INFO - --------------------------------------------------
2026-01-14 12:30:26,551 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-01-14 12:30:35,988 - INFO - [Train] [62/90] | Loss: 0.2238 | Train Acc: 98.59%
2026-01-14 12:30:38,889 - INFO - [Valid] [62/90] | Loss: 0.5063 | Val Acc: 84.07%
2026-01-14 12:30:38,914 - INFO - [Metrics for 'abnormal'] | Precision: 0.8323 | Recall: 0.8217 | F1: 0.8269
2026-01-14 12:30:38,914 - INFO - [Metrics for 'normal'] | Precision: 0.8478 | Recall: 0.8571 | F1: 0.8525
2026-01-14 12:30:38,926 - INFO - --------------------------------------------------
2026-01-14 12:30:38,929 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-01-14 12:30:48,389 - INFO - [Train] [63/90] | Loss: 0.2105 | Train Acc: 99.55%
2026-01-14 12:30:51,191 - INFO - [Valid] [63/90] | Loss: 0.5051 | Val Acc: 84.07%
2026-01-14 12:30:51,208 - INFO - [Metrics for 'abnormal'] | Precision: 0.8503 | Recall: 0.7962 | F1: 0.8224
2026-01-14 12:30:51,209 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8791 | F1: 0.8556
2026-01-14 12:30:51,213 - INFO - --------------------------------------------------
2026-01-14 12:30:51,216 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-01-14 12:31:00,556 - INFO - [Train] [64/90] | Loss: 0.2221 | Train Acc: 98.59%
2026-01-14 12:31:03,964 - INFO - [Valid] [64/90] | Loss: 0.5162 | Val Acc: 84.37%
2026-01-14 12:31:03,976 - INFO - [Metrics for 'abnormal'] | Precision: 0.8377 | Recall: 0.8217 | F1: 0.8296
2026-01-14 12:31:03,976 - INFO - [Metrics for 'normal'] | Precision: 0.8486 | Recall: 0.8626 | F1: 0.8556
2026-01-14 12:31:03,980 - INFO - --------------------------------------------------
2026-01-14 12:31:03,983 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-01-14 12:31:13,095 - INFO - [Train] [65/90] | Loss: 0.2134 | Train Acc: 99.33%
2026-01-14 12:31:15,648 - INFO - [Valid] [65/90] | Loss: 0.5200 | Val Acc: 84.37%
2026-01-14 12:31:15,669 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.8280 | F1: 0.8307
2026-01-14 12:31:15,670 - INFO - [Metrics for 'normal'] | Precision: 0.8525 | Recall: 0.8571 | F1: 0.8548
2026-01-14 12:31:15,676 - INFO - --------------------------------------------------
2026-01-14 12:31:15,680 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-01-14 12:31:24,972 - INFO - [Train] [66/90] | Loss: 0.2119 | Train Acc: 99.33%
2026-01-14 12:31:27,452 - INFO - [Valid] [66/90] | Loss: 0.4890 | Val Acc: 84.96%
2026-01-14 12:31:27,464 - INFO - [Metrics for 'abnormal'] | Precision: 0.8354 | Recall: 0.8408 | F1: 0.8381
2026-01-14 12:31:27,464 - INFO - [Metrics for 'normal'] | Precision: 0.8619 | Recall: 0.8571 | F1: 0.8595
2026-01-14 12:31:27,467 - INFO - --------------------------------------------------
2026-01-14 12:31:27,470 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-01-14 12:31:37,354 - INFO - [Train] [67/90] | Loss: 0.2105 | Train Acc: 99.55%
2026-01-14 12:31:40,077 - INFO - [Valid] [67/90] | Loss: 0.4890 | Val Acc: 85.55%
2026-01-14 12:31:40,090 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.8599 | F1: 0.8464
2026-01-14 12:31:40,090 - INFO - [Metrics for 'normal'] | Precision: 0.8757 | Recall: 0.8516 | F1: 0.8635
2026-01-14 12:31:40,095 - INFO - --------------------------------------------------
2026-01-14 12:31:40,098 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-01-14 12:31:49,588 - INFO - [Train] [68/90] | Loss: 0.2134 | Train Acc: 99.33%
2026-01-14 12:31:52,355 - INFO - [Valid] [68/90] | Loss: 0.4902 | Val Acc: 84.96%
2026-01-14 12:31:52,368 - INFO - [Metrics for 'abnormal'] | Precision: 0.8630 | Recall: 0.8025 | F1: 0.8317
2026-01-14 12:31:52,369 - INFO - [Metrics for 'normal'] | Precision: 0.8394 | Recall: 0.8901 | F1: 0.8640
2026-01-14 12:31:52,373 - INFO - --------------------------------------------------
2026-01-14 12:31:52,384 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-01-14 12:32:01,610 - INFO - [Train] [69/90] | Loss: 0.2104 | Train Acc: 99.55%
2026-01-14 12:32:04,494 - INFO - [Valid] [69/90] | Loss: 0.5204 | Val Acc: 82.60%
2026-01-14 12:32:04,509 - INFO - [Metrics for 'abnormal'] | Precision: 0.8182 | Recall: 0.8025 | F1: 0.8103
2026-01-14 12:32:04,509 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8462 | F1: 0.8392
2026-01-14 12:32:04,514 - INFO - --------------------------------------------------
2026-01-14 12:32:04,517 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-01-14 12:32:12,726 - INFO - [Train] [70/90] | Loss: 0.2132 | Train Acc: 99.26%
2026-01-14 12:32:14,569 - INFO - [Valid] [70/90] | Loss: 0.5155 | Val Acc: 83.48%
2026-01-14 12:32:14,584 - INFO - [Metrics for 'abnormal'] | Precision: 0.8301 | Recall: 0.8089 | F1: 0.8194
2026-01-14 12:32:14,584 - INFO - [Metrics for 'normal'] | Precision: 0.8387 | Recall: 0.8571 | F1: 0.8478
2026-01-14 12:32:14,588 - INFO - --------------------------------------------------
2026-01-14 12:32:14,591 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-01-14 12:32:24,623 - INFO - [Train] [71/90] | Loss: 0.2133 | Train Acc: 99.18%
2026-01-14 12:32:27,496 - INFO - [Valid] [71/90] | Loss: 0.4982 | Val Acc: 84.96%
2026-01-14 12:32:27,513 - INFO - [Metrics for 'abnormal'] | Precision: 0.8533 | Recall: 0.8153 | F1: 0.8339
2026-01-14 12:32:27,514 - INFO - [Metrics for 'normal'] | Precision: 0.8466 | Recall: 0.8791 | F1: 0.8625
2026-01-14 12:32:27,519 - INFO - --------------------------------------------------
2026-01-14 12:32:27,522 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-01-14 12:32:36,305 - INFO - [Train] [72/90] | Loss: 0.2139 | Train Acc: 99.26%
2026-01-14 12:32:39,925 - INFO - [Valid] [72/90] | Loss: 0.5200 | Val Acc: 83.48%
2026-01-14 12:32:39,937 - INFO - [Metrics for 'abnormal'] | Precision: 0.8098 | Recall: 0.8408 | F1: 0.8250
2026-01-14 12:32:39,937 - INFO - [Metrics for 'normal'] | Precision: 0.8580 | Recall: 0.8297 | F1: 0.8436
2026-01-14 12:32:39,941 - INFO - --------------------------------------------------
2026-01-14 12:32:39,944 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-01-14 12:32:51,634 - INFO - [Train] [73/90] | Loss: 0.2077 | Train Acc: 99.63%
2026-01-14 12:32:54,042 - INFO - [Valid] [73/90] | Loss: 0.4922 | Val Acc: 85.55%
2026-01-14 12:32:54,056 - INFO - [Metrics for 'abnormal'] | Precision: 0.8803 | Recall: 0.7962 | F1: 0.8361
2026-01-14 12:32:54,057 - INFO - [Metrics for 'normal'] | Precision: 0.8376 | Recall: 0.9066 | F1: 0.8707
2026-01-14 12:32:54,062 - INFO - --------------------------------------------------
2026-01-14 12:32:54,066 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-01-14 12:33:02,841 - INFO - [Train] [74/90] | Loss: 0.2094 | Train Acc: 99.40%
2026-01-14 12:33:05,922 - INFO - [Valid] [74/90] | Loss: 0.4766 | Val Acc: 84.96%
2026-01-14 12:33:05,934 - INFO - [Metrics for 'abnormal'] | Precision: 0.8487 | Recall: 0.8217 | F1: 0.8350
2026-01-14 12:33:05,934 - INFO - [Metrics for 'normal'] | Precision: 0.8503 | Recall: 0.8736 | F1: 0.8618
2026-01-14 12:33:05,939 - INFO - --------------------------------------------------
2026-01-14 12:33:05,940 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-01-14 12:33:15,632 - INFO - [Train] [75/90] | Loss: 0.2061 | Train Acc: 99.63%
2026-01-14 12:33:18,352 - INFO - [Valid] [75/90] | Loss: 0.4889 | Val Acc: 84.37%
2026-01-14 12:33:18,376 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.8280 | F1: 0.8307
2026-01-14 12:33:18,377 - INFO - [Metrics for 'normal'] | Precision: 0.8525 | Recall: 0.8571 | F1: 0.8548
2026-01-14 12:33:18,381 - INFO - --------------------------------------------------
2026-01-14 12:33:18,383 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-01-14 12:33:26,854 - INFO - [Train] [76/90] | Loss: 0.2087 | Train Acc: 99.48%
2026-01-14 12:33:29,827 - INFO - [Valid] [76/90] | Loss: 0.4900 | Val Acc: 84.96%
2026-01-14 12:33:29,839 - INFO - [Metrics for 'abnormal'] | Precision: 0.8442 | Recall: 0.8280 | F1: 0.8360
2026-01-14 12:33:29,839 - INFO - [Metrics for 'normal'] | Precision: 0.8541 | Recall: 0.8681 | F1: 0.8610
2026-01-14 12:33:29,846 - INFO - --------------------------------------------------
2026-01-14 12:33:29,848 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-01-14 12:33:39,036 - INFO - [Train] [77/90] | Loss: 0.2082 | Train Acc: 99.48%
2026-01-14 12:33:42,496 - INFO - [Valid] [77/90] | Loss: 0.4690 | Val Acc: 85.55%
2026-01-14 12:33:42,506 - INFO - [Metrics for 'abnormal'] | Precision: 0.8462 | Recall: 0.8408 | F1: 0.8435
2026-01-14 12:33:42,507 - INFO - [Metrics for 'normal'] | Precision: 0.8634 | Recall: 0.8681 | F1: 0.8658
2026-01-14 12:33:42,510 - INFO - --------------------------------------------------
2026-01-14 12:33:42,512 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-01-14 12:33:53,255 - INFO - [Train] [78/90] | Loss: 0.2076 | Train Acc: 99.70%
2026-01-14 12:33:55,593 - INFO - [Valid] [78/90] | Loss: 0.4632 | Val Acc: 86.14%
2026-01-14 12:33:55,607 - INFO - [Metrics for 'abnormal'] | Precision: 0.8571 | Recall: 0.8408 | F1: 0.8489
2026-01-14 12:33:55,607 - INFO - [Metrics for 'normal'] | Precision: 0.8649 | Recall: 0.8791 | F1: 0.8719
2026-01-14 12:33:55,612 - INFO - --------------------------------------------------
2026-01-14 12:33:55,616 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-01-14 12:34:06,178 - INFO - [Train] [79/90] | Loss: 0.2046 | Train Acc: 99.78%
2026-01-14 12:34:08,836 - INFO - [Valid] [79/90] | Loss: 0.4930 | Val Acc: 85.25%
2026-01-14 12:34:08,848 - INFO - [Metrics for 'abnormal'] | Precision: 0.8849 | Recall: 0.7834 | F1: 0.8311
2026-01-14 12:34:08,849 - INFO - [Metrics for 'normal'] | Precision: 0.8300 | Recall: 0.9121 | F1: 0.8691
2026-01-14 12:34:08,853 - INFO - --------------------------------------------------
2026-01-14 12:34:08,856 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-01-14 12:34:18,137 - INFO - [Train] [80/90] | Loss: 0.2091 | Train Acc: 99.40%
2026-01-14 12:34:20,644 - INFO - [Valid] [80/90] | Loss: 0.5086 | Val Acc: 84.37%
2026-01-14 12:34:20,654 - INFO - [Metrics for 'abnormal'] | Precision: 0.8467 | Recall: 0.8089 | F1: 0.8274
2026-01-14 12:34:20,654 - INFO - [Metrics for 'normal'] | Precision: 0.8413 | Recall: 0.8736 | F1: 0.8571
2026-01-14 12:34:20,658 - INFO - --------------------------------------------------
2026-01-14 12:34:20,660 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-01-14 12:34:30,420 - INFO - [Train] [81/90] | Loss: 0.2067 | Train Acc: 99.55%
2026-01-14 12:34:33,115 - INFO - [Valid] [81/90] | Loss: 0.4929 | Val Acc: 85.84%
2026-01-14 12:34:33,127 - INFO - [Metrics for 'abnormal'] | Precision: 0.8609 | Recall: 0.8280 | F1: 0.8442
2026-01-14 12:34:33,128 - INFO - [Metrics for 'normal'] | Precision: 0.8564 | Recall: 0.8846 | F1: 0.8703
2026-01-14 12:34:33,132 - INFO - --------------------------------------------------
2026-01-14 12:34:33,136 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-01-14 12:34:42,528 - INFO - [Train] [82/90] | Loss: 0.2068 | Train Acc: 99.63%
2026-01-14 12:34:44,707 - INFO - [Valid] [82/90] | Loss: 0.4940 | Val Acc: 84.37%
2026-01-14 12:34:44,728 - INFO - [Metrics for 'abnormal'] | Precision: 0.8611 | Recall: 0.7898 | F1: 0.8239
2026-01-14 12:34:44,731 - INFO - [Metrics for 'normal'] | Precision: 0.8308 | Recall: 0.8901 | F1: 0.8594
2026-01-14 12:34:44,736 - INFO - --------------------------------------------------
2026-01-14 12:34:44,744 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-01-14 12:34:53,531 - INFO - [Train] [83/90] | Loss: 0.2052 | Train Acc: 99.63%
2026-01-14 12:34:56,075 - INFO - [Valid] [83/90] | Loss: 0.4831 | Val Acc: 84.66%
2026-01-14 12:34:56,091 - INFO - [Metrics for 'abnormal'] | Precision: 0.8261 | Recall: 0.8471 | F1: 0.8365
2026-01-14 12:34:56,091 - INFO - [Metrics for 'normal'] | Precision: 0.8652 | Recall: 0.8462 | F1: 0.8556
2026-01-14 12:34:56,097 - INFO - --------------------------------------------------
2026-01-14 12:34:56,100 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-01-14 12:35:06,287 - INFO - [Train] [84/90] | Loss: 0.2092 | Train Acc: 99.40%
2026-01-14 12:35:08,560 - INFO - [Valid] [84/90] | Loss: 0.4740 | Val Acc: 86.14%
2026-01-14 12:35:08,569 - INFO - [Metrics for 'abnormal'] | Precision: 0.8526 | Recall: 0.8471 | F1: 0.8498
2026-01-14 12:35:08,569 - INFO - [Metrics for 'normal'] | Precision: 0.8689 | Recall: 0.8736 | F1: 0.8712
2026-01-14 12:35:08,572 - INFO - --------------------------------------------------
2026-01-14 12:35:08,574 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-01-14 12:35:18,417 - INFO - [Train] [85/90] | Loss: 0.2039 | Train Acc: 99.70%
2026-01-14 12:35:20,605 - INFO - [Valid] [85/90] | Loss: 0.4841 | Val Acc: 84.66%
2026-01-14 12:35:20,614 - INFO - [Metrics for 'abnormal'] | Precision: 0.8344 | Recall: 0.8344 | F1: 0.8344
2026-01-14 12:35:20,615 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.8571 | F1: 0.8571
2026-01-14 12:35:20,618 - INFO - --------------------------------------------------
2026-01-14 12:35:20,621 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-01-14 12:35:29,888 - INFO - [Train] [86/90] | Loss: 0.2111 | Train Acc: 99.40%
2026-01-14 12:35:32,067 - INFO - [Valid] [86/90] | Loss: 0.4882 | Val Acc: 84.66%
2026-01-14 12:35:32,088 - INFO - [Metrics for 'abnormal'] | Precision: 0.8431 | Recall: 0.8217 | F1: 0.8323
2026-01-14 12:35:32,092 - INFO - [Metrics for 'normal'] | Precision: 0.8495 | Recall: 0.8681 | F1: 0.8587
2026-01-14 12:35:32,097 - INFO - --------------------------------------------------
2026-01-14 12:35:32,103 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-01-14 12:35:40,973 - INFO - [Train] [87/90] | Loss: 0.2056 | Train Acc: 99.78%
2026-01-14 12:35:43,699 - INFO - [Valid] [87/90] | Loss: 0.4822 | Val Acc: 86.14%
2026-01-14 12:35:43,717 - INFO - [Metrics for 'abnormal'] | Precision: 0.8571 | Recall: 0.8408 | F1: 0.8489
2026-01-14 12:35:43,717 - INFO - [Metrics for 'normal'] | Precision: 0.8649 | Recall: 0.8791 | F1: 0.8719
2026-01-14 12:35:43,720 - INFO - --------------------------------------------------
2026-01-14 12:35:43,722 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-01-14 12:35:54,690 - INFO - [Train] [88/90] | Loss: 0.2031 | Train Acc: 99.85%
2026-01-14 12:35:56,654 - INFO - [Valid] [88/90] | Loss: 0.4948 | Val Acc: 84.66%
2026-01-14 12:35:56,665 - INFO - [Metrics for 'abnormal'] | Precision: 0.8523 | Recall: 0.8089 | F1: 0.8301
2026-01-14 12:35:56,665 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.8791 | F1: 0.8602
2026-01-14 12:35:56,668 - INFO - --------------------------------------------------
2026-01-14 12:35:56,671 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-01-14 12:36:03,904 - INFO - [Train] [89/90] | Loss: 0.2031 | Train Acc: 99.78%
2026-01-14 12:36:05,811 - INFO - [Valid] [89/90] | Loss: 0.4866 | Val Acc: 84.37%
2026-01-14 12:36:05,841 - INFO - [Metrics for 'abnormal'] | Precision: 0.8421 | Recall: 0.8153 | F1: 0.8285
2026-01-14 12:36:05,841 - INFO - [Metrics for 'normal'] | Precision: 0.8449 | Recall: 0.8681 | F1: 0.8564
2026-01-14 12:36:05,846 - INFO - --------------------------------------------------
2026-01-14 12:36:05,849 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-01-14 12:36:13,566 - INFO - [Train] [90/90] | Loss: 0.2032 | Train Acc: 99.85%
2026-01-14 12:36:15,307 - INFO - [Valid] [90/90] | Loss: 0.4851 | Val Acc: 84.66%
2026-01-14 12:36:15,318 - INFO - [Metrics for 'abnormal'] | Precision: 0.8477 | Recall: 0.8153 | F1: 0.8312
2026-01-14 12:36:15,319 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8736 | F1: 0.8595
2026-01-14 12:36:15,324 - INFO - ==================================================
2026-01-14 12:36:15,325 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-01-14 12:36:15,325 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-01-14 12:36:15,326 - INFO - Baseline 모델 'efficientnet_b0'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:36:15,558 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-01-14 12:36:15,561 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:36:15,562 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:36:16,345 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737939453124999)에 맞춰 변경되었습니다.
2026-01-14 12:36:16,346 - INFO - ==================================================
2026-01-14 12:36:16,484 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/best_model.pth'
2026-01-14 12:36:16,485 - INFO - ==================================================
2026-01-14 12:36:16,486 - INFO - Test 모드를 시작합니다.
2026-01-14 12:36:16,785 - INFO - 연산량 (MACs): 0.0914 GMACs per sample
2026-01-14 12:36:16,786 - INFO - 연산량 (FLOPs): 0.1829 GFLOPs per sample
2026-01-14 12:36:16,786 - INFO - ==================================================
2026-01-14 12:36:16,786 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-01-14 12:36:18,432 - INFO - 샘플 당 평균 Forward Pass 시간: 10.08ms (std: 5.02ms), FPS: 110.35 (std: 25.90) (1개 샘플 x 100회 반복)
2026-01-14 12:36:18,432 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 98.43 MB
2026-01-14 12:36:18,432 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-01-14 12:36:22,094 - INFO - [Test] Loss: 0.3512 | Test Acc: 87.91%
2026-01-14 12:36:22,109 - INFO - [Metrics for 'abnormal'] | Precision: 0.8718 | Recall: 0.8662 | F1: 0.8690
2026-01-14 12:36:22,110 - INFO - [Metrics for 'normal'] | Precision: 0.8852 | Recall: 0.8901 | F1: 0.8877
2026-01-14 12:36:22,809 - INFO - ==================================================
2026-01-14 12:36:22,810 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/confusion_matrix_20260114_121807.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/confusion_matrix_20260114_121807.pdf'
2026-01-14 12:36:22,810 - INFO - ==================================================
2026-01-14 12:36:22,810 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-01-14 12:36:30,823 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/model_fp32_20260114_121807.onnx'에 저장되었습니다. (크기: 2.99 MB)
2026-01-14 12:36:31,186 - INFO - [Model Load] ONNX 모델(FP32) 로드 메모리: 2584.57 MB (증가량: 7.52 MB)
2026-01-14 12:36:31,187 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-01-14 12:36:34,662 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 20.38ms (std: 15.99ms)
2026-01-14 12:36:34,662 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 69.09 FPS (std: 33.22) (1개 샘플 x 100회 반복)
2026-01-14 12:36:34,663 - INFO - [Inference Only] ONNX 런타임 추론 중 최대 CPU 메모리: 2587.11 MB (순수 증가량: 2.54 MB)
2026-01-14 12:36:34,663 - INFO - [Total Process] ONNX 모델(FP32) 전체 메모리 사용량: 2587.11 MB (전체 증가량: 10.06 MB)
2026-01-14 12:36:41,838 - INFO - [Test (ONNX)] | Test Acc (ONNX): 87.91%
2026-01-14 12:36:41,851 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8718 | Recall: 0.8662 | F1: 0.8690
2026-01-14 12:36:41,852 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8852 | Recall: 0.8901 | F1: 0.8877
2026-01-14 12:36:42,447 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/val_acc.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/val_acc.pdf'
2026-01-14 12:36:43,069 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/train_val_acc.pdf'
2026-01-14 12:36:43,556 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/F1_normal.pdf'
2026-01-14 12:36:44,107 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/val_loss.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/val_loss.pdf'
2026-01-14 12:36:44,719 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/learning_rate.pdf'
2026-01-14 12:36:50,133 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/compile.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_fpgm_20260114_121807/graph_20260114_121807/compile.pdf'
