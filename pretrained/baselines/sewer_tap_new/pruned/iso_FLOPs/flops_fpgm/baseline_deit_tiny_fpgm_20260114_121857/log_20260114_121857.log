2026-01-14 12:18:57,738 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/log_20260114_121857.log'에 저장됩니다.
2026-01-14 12:18:57,745 - INFO - ==================================================
2026-01-14 12:18:57,745 - INFO - config.yaml:
2026-01-14 12:18:57,746 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_inference_dir: ./pretrained
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      train_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/cau/workspace/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  patch_size: 56
  stride: 56
  cnn_feature_extractor:
    name: efficientnet_b0_feat2
  featured_patch_dim: 24
  emb_dim: 24
  num_heads: 2
  num_decoder_layers: 2
  num_decoder_patches: 1
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: true
  save_attention: true
  num_plot_attention: 5
baseline:
  model_name: deit_tiny
  use_fpgm_pruning: true
  pruning_flops_target: 0.1829

2026-01-14 12:18:57,747 - INFO - ==================================================
2026-01-14 12:18:57,811 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA RTX PRO 6000 Blackwell Server Edition)
2026-01-14 12:18:57,812 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-01-14 12:18:57,812 - INFO - '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-01-14 12:18:57,826 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-01-14 12:18:57,827 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-01-14 12:18:57,828 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-01-14 12:18:57,828 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:19:03,182 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-01-14 12:19:03,185 - INFO - ==================================================
2026-01-14 12:19:03,185 - INFO - 모델 파라미터 수:
2026-01-14 12:19:03,186 - INFO -   - 총 파라미터: 5,524,802 개
2026-01-14 12:19:03,186 - INFO -   - 학습 가능한 파라미터: 5,524,802 개
2026-01-14 12:19:03,186 - INFO - ================================================================================
2026-01-14 12:19:03,186 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-01-14 12:19:03,186 - INFO - ================================================================================
2026-01-14 12:19:03,186 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:19:03,188 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-01-14 12:19:03,189 - INFO - ==================================================
2026-01-14 12:19:03,190 - INFO - train 모드를 시작합니다.
2026-01-14 12:19:03,190 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:19:03,191 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:19:03,191 - INFO - --------------------------------------------------
2026-01-14 12:19:03,194 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-01-14 12:19:12,360 - INFO - [Train] [1/10] | Loss: 0.7051 | Train Acc: 60.49%
2026-01-14 12:19:15,750 - INFO - [Valid] [1/10] | Loss: 0.6723 | Val Acc: 59.88%
2026-01-14 12:19:15,787 - INFO - [Metrics for 'abnormal'] | Precision: 0.5484 | Recall: 0.7580 | F1: 0.6364
2026-01-14 12:19:15,787 - INFO - [Metrics for 'normal'] | Precision: 0.6885 | Recall: 0.4615 | F1: 0.5526
2026-01-14 12:19:15,879 - INFO - [Best Model Saved] (val loss: 0.6723) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:19:15,880 - INFO - --------------------------------------------------
2026-01-14 12:19:15,883 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-01-14 12:19:24,303 - INFO - [Train] [2/10] | Loss: 0.6453 | Train Acc: 65.33%
2026-01-14 12:19:26,675 - INFO - [Valid] [2/10] | Loss: 0.6489 | Val Acc: 63.72%
2026-01-14 12:19:26,686 - INFO - [Metrics for 'abnormal'] | Precision: 0.7297 | Recall: 0.3439 | F1: 0.4675
2026-01-14 12:19:26,686 - INFO - [Metrics for 'normal'] | Precision: 0.6113 | Recall: 0.8901 | F1: 0.7248
2026-01-14 12:19:26,757 - INFO - [Best Model Saved] (val loss: 0.6489) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:19:26,757 - INFO - --------------------------------------------------
2026-01-14 12:19:26,759 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-01-14 12:19:34,414 - INFO - [Train] [3/10] | Loss: 0.5860 | Train Acc: 71.58%
2026-01-14 12:19:36,332 - INFO - [Valid] [3/10] | Loss: 0.6053 | Val Acc: 67.55%
2026-01-14 12:19:36,342 - INFO - [Metrics for 'abnormal'] | Precision: 0.6169 | Recall: 0.7898 | F1: 0.6927
2026-01-14 12:19:36,342 - INFO - [Metrics for 'normal'] | Precision: 0.7609 | Recall: 0.5769 | F1: 0.6562
2026-01-14 12:19:36,402 - INFO - [Best Model Saved] (val loss: 0.6053) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:19:36,402 - INFO - --------------------------------------------------
2026-01-14 12:19:36,404 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-01-14 12:19:43,813 - INFO - [Train] [4/10] | Loss: 0.5638 | Train Acc: 74.40%
2026-01-14 12:19:46,277 - INFO - [Valid] [4/10] | Loss: 0.5746 | Val Acc: 72.86%
2026-01-14 12:19:46,286 - INFO - [Metrics for 'abnormal'] | Precision: 0.6650 | Recall: 0.8344 | F1: 0.7401
2026-01-14 12:19:46,286 - INFO - [Metrics for 'normal'] | Precision: 0.8169 | Recall: 0.6374 | F1: 0.7160
2026-01-14 12:19:46,446 - INFO - [Best Model Saved] (val loss: 0.5746) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:19:46,447 - INFO - --------------------------------------------------
2026-01-14 12:19:46,448 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-01-14 12:19:52,684 - INFO - [Train] [5/10] | Loss: 0.5164 | Train Acc: 79.02%
2026-01-14 12:19:54,746 - INFO - [Valid] [5/10] | Loss: 0.5757 | Val Acc: 74.34%
2026-01-14 12:19:54,772 - INFO - [Metrics for 'abnormal'] | Precision: 0.6683 | Recall: 0.8854 | F1: 0.7616
2026-01-14 12:19:54,773 - INFO - [Metrics for 'normal'] | Precision: 0.8626 | Recall: 0.6209 | F1: 0.7220
2026-01-14 12:19:54,779 - INFO - --------------------------------------------------
2026-01-14 12:19:54,781 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-01-14 12:20:01,515 - INFO - [Train] [6/10] | Loss: 0.5160 | Train Acc: 78.12%
2026-01-14 12:20:03,468 - INFO - [Valid] [6/10] | Loss: 0.5585 | Val Acc: 72.57%
2026-01-14 12:20:03,478 - INFO - [Metrics for 'abnormal'] | Precision: 0.6600 | Recall: 0.8408 | F1: 0.7395
2026-01-14 12:20:03,478 - INFO - [Metrics for 'normal'] | Precision: 0.8201 | Recall: 0.6264 | F1: 0.7103
2026-01-14 12:20:03,533 - INFO - [Best Model Saved] (val loss: 0.5585) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:20:03,534 - INFO - --------------------------------------------------
2026-01-14 12:20:03,536 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-01-14 12:20:09,411 - INFO - [Train] [7/10] | Loss: 0.4942 | Train Acc: 80.36%
2026-01-14 12:20:10,853 - INFO - [Valid] [7/10] | Loss: 0.5586 | Val Acc: 77.58%
2026-01-14 12:20:10,862 - INFO - [Metrics for 'abnormal'] | Precision: 0.8462 | Recall: 0.6306 | F1: 0.7226
2026-01-14 12:20:10,863 - INFO - [Metrics for 'normal'] | Precision: 0.7387 | Recall: 0.9011 | F1: 0.8119
2026-01-14 12:20:10,867 - INFO - --------------------------------------------------
2026-01-14 12:20:10,869 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-01-14 12:20:16,058 - INFO - [Train] [8/10] | Loss: 0.4856 | Train Acc: 80.88%
2026-01-14 12:20:17,492 - INFO - [Valid] [8/10] | Loss: 0.5266 | Val Acc: 78.47%
2026-01-14 12:20:17,501 - INFO - [Metrics for 'abnormal'] | Precision: 0.8231 | Recall: 0.6815 | F1: 0.7456
2026-01-14 12:20:17,501 - INFO - [Metrics for 'normal'] | Precision: 0.7608 | Recall: 0.8736 | F1: 0.8133
2026-01-14 12:20:17,545 - INFO - [Best Model Saved] (val loss: 0.5266) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:20:17,546 - INFO - --------------------------------------------------
2026-01-14 12:20:17,548 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-01-14 12:20:23,080 - INFO - [Train] [9/10] | Loss: 0.4839 | Train Acc: 80.73%
2026-01-14 12:20:24,586 - INFO - [Valid] [9/10] | Loss: 0.5262 | Val Acc: 77.88%
2026-01-14 12:20:24,595 - INFO - [Metrics for 'abnormal'] | Precision: 0.7887 | Recall: 0.7134 | F1: 0.7492
2026-01-14 12:20:24,595 - INFO - [Metrics for 'normal'] | Precision: 0.7716 | Recall: 0.8352 | F1: 0.8021
2026-01-14 12:20:24,648 - INFO - [Best Model Saved] (val loss: 0.5262) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:20:24,648 - INFO - --------------------------------------------------
2026-01-14 12:20:24,650 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-01-14 12:20:30,013 - INFO - [Train] [10/10] | Loss: 0.4762 | Train Acc: 81.70%
2026-01-14 12:20:31,488 - INFO - [Valid] [10/10] | Loss: 0.5295 | Val Acc: 76.11%
2026-01-14 12:20:31,496 - INFO - [Metrics for 'abnormal'] | Precision: 0.7405 | Recall: 0.7452 | F1: 0.7429
2026-01-14 12:20:31,496 - INFO - [Metrics for 'normal'] | Precision: 0.7790 | Recall: 0.7747 | F1: 0.7769
2026-01-14 12:20:31,500 - INFO - ================================================================================
2026-01-14 12:20:31,500 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-01-14 12:20:31,501 - INFO - ================================================================================
2026-01-14 12:20:31,554 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'을(를) 불러왔습니다.
2026-01-14 12:20:31,555 - INFO - ================================================================================
2026-01-14 12:20:31,555 - INFO - 목표 FLOPs (0.1829 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-01-14 12:20:31,598 - INFO - 원본 모델 FLOPs: 2.1493 GFLOPs
2026-01-14 12:20:31,720 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:31,721 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:31,722 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:32,457 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-01-14 12:20:32,458 - INFO - ==================================================
2026-01-14 12:20:32,500 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.7288 GFLOPs (감소율: 66.09%)
2026-01-14 12:20:32,562 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:32,562 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:32,563 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:33,005 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-01-14 12:20:33,005 - INFO - ==================================================
2026-01-14 12:20:33,036 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.2840 GFLOPs (감소율: 86.79%)
2026-01-14 12:20:33,081 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:33,082 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:33,082 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:34,130 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-01-14 12:20:34,131 - INFO - ==================================================
2026-01-14 12:20:34,166 - INFO -   [탐색  3] 희소도: 0.8662 -> FLOPs: 0.1224 GFLOPs (감소율: 94.30%)
2026-01-14 12:20:34,214 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:34,215 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:34,216 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:34,740 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.804375)에 맞춰 변경되었습니다.
2026-01-14 12:20:34,741 - INFO - ==================================================
2026-01-14 12:20:34,773 - INFO -   [탐색  4] 희소도: 0.8044 -> FLOPs: 0.1980 GFLOPs (감소율: 90.79%)
2026-01-14 12:20:34,826 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:34,826 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:34,827 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:35,339 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8353124999999999)에 맞춰 변경되었습니다.
2026-01-14 12:20:35,340 - INFO - ==================================================
2026-01-14 12:20:35,387 - INFO -   [탐색  5] 희소도: 0.8353 -> FLOPs: 0.1588 GFLOPs (감소율: 92.61%)
2026-01-14 12:20:35,435 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:35,435 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:35,437 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:35,951 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81984375)에 맞춰 변경되었습니다.
2026-01-14 12:20:35,952 - INFO - ==================================================
2026-01-14 12:20:35,997 - INFO -   [탐색  6] 희소도: 0.8198 -> FLOPs: 0.1781 GFLOPs (감소율: 91.72%)
2026-01-14 12:20:36,058 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:36,059 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:36,059 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:36,646 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8121093749999999)에 맞춰 변경되었습니다.
2026-01-14 12:20:36,646 - INFO - ==================================================
2026-01-14 12:20:36,689 - INFO -   [탐색  7] 희소도: 0.8121 -> FLOPs: 0.1906 GFLOPs (감소율: 91.13%)
2026-01-14 12:20:36,753 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:36,754 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:36,755 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:37,636 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8159765625)에 맞춰 변경되었습니다.
2026-01-14 12:20:37,637 - INFO - ==================================================
2026-01-14 12:20:37,662 - INFO -   [탐색  8] 희소도: 0.8160 -> FLOPs: 0.1843 GFLOPs (감소율: 91.43%)
2026-01-14 12:20:37,723 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:37,725 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:37,726 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:38,198 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81791015625)에 맞춰 변경되었습니다.
2026-01-14 12:20:38,198 - INFO - ==================================================
2026-01-14 12:20:38,230 - INFO -   [탐색  9] 희소도: 0.8179 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:38,275 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:38,276 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:38,276 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:38,785 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-01-14 12:20:38,785 - INFO - ==================================================
2026-01-14 12:20:38,825 - INFO -   [탐색 10] 희소도: 0.8169 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:38,893 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:38,893 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:38,894 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:39,377 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8174267578125)에 맞춰 변경되었습니다.
2026-01-14 12:20:39,377 - INFO - ==================================================
2026-01-14 12:20:39,409 - INFO -   [탐색 11] 희소도: 0.8174 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:39,453 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:39,454 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:39,455 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:39,944 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81766845703125)에 맞춰 변경되었습니다.
2026-01-14 12:20:39,945 - INFO - ==================================================
2026-01-14 12:20:39,977 - INFO -   [탐색 12] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:40,020 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:40,021 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:40,022 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:40,844 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.817789306640625)에 맞춰 변경되었습니다.
2026-01-14 12:20:40,845 - INFO - ==================================================
2026-01-14 12:20:40,879 - INFO -   [탐색 13] 희소도: 0.8178 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:40,930 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:40,930 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:40,932 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:41,396 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177288818359375)에 맞춰 변경되었습니다.
2026-01-14 12:20:41,397 - INFO - ==================================================
2026-01-14 12:20:41,428 - INFO -   [탐색 14] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:41,470 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:41,471 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:41,472 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:42,136 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8176986694335937)에 맞춰 변경되었습니다.
2026-01-14 12:20:42,137 - INFO - ==================================================
2026-01-14 12:20:42,175 - INFO -   [탐색 15] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:42,232 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:42,233 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:42,234 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:42,816 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177137756347657)에 맞춰 변경되었습니다.
2026-01-14 12:20:42,817 - INFO - ==================================================
2026-01-14 12:20:42,855 - INFO -   [탐색 16] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:42,923 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:42,924 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:42,926 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:43,571 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177062225341797)에 맞춰 변경되었습니다.
2026-01-14 12:20:43,572 - INFO - ==================================================
2026-01-14 12:20:43,614 - INFO -   [탐색 17] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:43,677 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:43,678 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:43,679 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:44,700 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177099990844727)에 맞춰 변경되었습니다.
2026-01-14 12:20:44,701 - INFO - ==================================================
2026-01-14 12:20:44,732 - INFO -   [탐색 18] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:44,777 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:44,778 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:44,778 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:45,217 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177081108093263)에 맞춰 변경되었습니다.
2026-01-14 12:20:45,217 - INFO - ==================================================
2026-01-14 12:20:45,242 - INFO -   [탐색 19] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:45,278 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:45,278 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:45,278 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:45,685 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177090549468995)에 맞춰 변경되었습니다.
2026-01-14 12:20:45,686 - INFO - ==================================================
2026-01-14 12:20:45,717 - INFO -   [탐색 20] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:45,783 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:45,785 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:45,786 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:46,546 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177085828781129)에 맞춰 변경되었습니다.
2026-01-14 12:20:46,546 - INFO - ==================================================
2026-01-14 12:20:46,577 - INFO -   [탐색 21] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:46,619 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:46,620 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:46,620 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:47,155 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083468437196)에 맞춰 변경되었습니다.
2026-01-14 12:20:47,156 - INFO - ==================================================
2026-01-14 12:20:47,198 - INFO -   [탐색 22] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:47,259 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:47,260 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:47,261 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:48,299 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082288265229)에 맞춰 변경되었습니다.
2026-01-14 12:20:48,299 - INFO - ==================================================
2026-01-14 12:20:48,332 - INFO -   [탐색 23] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:48,401 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:48,401 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:48,402 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:49,008 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082878351213)에 맞춰 변경되었습니다.
2026-01-14 12:20:49,009 - INFO - ==================================================
2026-01-14 12:20:49,060 - INFO -   [탐색 24] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:49,172 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:49,172 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:49,173 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:49,836 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083173394204)에 맞춰 변경되었습니다.
2026-01-14 12:20:49,836 - INFO - ==================================================
2026-01-14 12:20:49,871 - INFO -   [탐색 25] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:49,932 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:49,933 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:49,934 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:50,504 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833209157)에 맞춰 변경되었습니다.
2026-01-14 12:20:50,505 - INFO - ==================================================
2026-01-14 12:20:50,546 - INFO -   [탐색 26] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:50,597 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:50,597 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:50,598 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:51,351 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083394676448)에 맞춰 변경되었습니다.
2026-01-14 12:20:51,352 - INFO - ==================================================
2026-01-14 12:20:51,392 - INFO -   [탐색 27] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:51,442 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:51,442 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:51,443 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:52,026 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083357796073)에 맞춰 변경되었습니다.
2026-01-14 12:20:52,027 - INFO - ==================================================
2026-01-14 12:20:52,068 - INFO -   [탐색 28] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:52,123 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:52,124 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:52,124 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:52,570 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083339355886)에 맞춰 변경되었습니다.
2026-01-14 12:20:52,570 - INFO - ==================================================
2026-01-14 12:20:52,601 - INFO -   [탐색 29] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:52,643 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:52,643 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:52,644 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:53,138 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083330135793)에 맞춰 변경되었습니다.
2026-01-14 12:20:53,138 - INFO - ==================================================
2026-01-14 12:20:53,173 - INFO -   [탐색 30] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:53,225 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:53,225 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:53,226 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:53,754 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083334745839)에 맞춰 변경되었습니다.
2026-01-14 12:20:53,754 - INFO - ==================================================
2026-01-14 12:20:53,789 - INFO -   [탐색 31] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:53,853 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:53,853 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:53,854 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:55,028 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083332440815)에 맞춰 변경되었습니다.
2026-01-14 12:20:55,028 - INFO - ==================================================
2026-01-14 12:20:55,066 - INFO -   [탐색 32] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:55,115 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:55,116 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:55,117 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:55,616 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333593327)에 맞춰 변경되었습니다.
2026-01-14 12:20:55,617 - INFO - ==================================================
2026-01-14 12:20:55,648 - INFO -   [탐색 33] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:55,692 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:55,693 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:55,694 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:56,261 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333017071)에 맞춰 변경되었습니다.
2026-01-14 12:20:56,261 - INFO - ==================================================
2026-01-14 12:20:56,301 - INFO -   [탐색 34] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:56,359 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:56,359 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:56,360 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:56,941 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833333052)에 맞춰 변경되었습니다.
2026-01-14 12:20:56,941 - INFO - ==================================================
2026-01-14 12:20:56,995 - INFO -   [탐색 35] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:57,051 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:57,052 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:57,053 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:57,517 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333449263)에 맞춰 변경되었습니다.
2026-01-14 12:20:57,518 - INFO - ==================================================
2026-01-14 12:20:57,549 - INFO -   [탐색 36] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:57,593 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:57,593 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:57,594 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:58,494 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333377231)에 맞춰 변경되었습니다.
2026-01-14 12:20:58,494 - INFO - ==================================================
2026-01-14 12:20:58,533 - INFO -   [탐색 37] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:58,582 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:58,583 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:58,584 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:58,992 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333341215)에 맞춰 변경되었습니다.
2026-01-14 12:20:58,993 - INFO - ==================================================
2026-01-14 12:20:59,024 - INFO -   [탐색 38] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:20:59,066 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:59,067 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:59,068 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:59,445 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333323207)에 맞춰 변경되었습니다.
2026-01-14 12:20:59,446 - INFO - ==================================================
2026-01-14 12:20:59,476 - INFO -   [탐색 39] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:20:59,520 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:20:59,520 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:20:59,521 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:20:59,954 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332211)에 맞춰 변경되었습니다.
2026-01-14 12:20:59,956 - INFO - ==================================================
2026-01-14 12:20:59,996 - INFO -   [탐색 40] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:00,043 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:00,044 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:00,045 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:00,563 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333336713)에 맞춰 변경되었습니다.
2026-01-14 12:21:00,563 - INFO - ==================================================
2026-01-14 12:21:00,639 - INFO -   [탐색 41] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:21:00,752 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:00,753 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:00,754 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:01,646 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333334463)에 맞춰 변경되었습니다.
2026-01-14 12:21:01,647 - INFO - ==================================================
2026-01-14 12:21:01,694 - INFO -   [탐색 42] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:21:01,751 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:01,751 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:01,752 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:02,433 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333337)에 맞춰 변경되었습니다.
2026-01-14 12:21:02,434 - INFO - ==================================================
2026-01-14 12:21:02,477 - INFO -   [탐색 43] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:21:02,530 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:02,530 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:02,531 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:02,965 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332774)에 맞춰 변경되었습니다.
2026-01-14 12:21:02,966 - INFO - ==================================================
2026-01-14 12:21:03,000 - INFO -   [탐색 44] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:03,044 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:03,045 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:03,046 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:03,531 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333055)에 맞춰 변경되었습니다.
2026-01-14 12:21:03,532 - INFO - ==================================================
2026-01-14 12:21:03,562 - INFO -   [탐색 45] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:03,613 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:03,615 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:03,616 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:04,099 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333196)에 맞춰 변경되었습니다.
2026-01-14 12:21:04,100 - INFO - ==================================================
2026-01-14 12:21:04,130 - INFO -   [탐색 46] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:04,173 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:04,173 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:04,174 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:04,898 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333266)에 맞춰 변경되었습니다.
2026-01-14 12:21:04,899 - INFO - ==================================================
2026-01-14 12:21:04,930 - INFO -   [탐색 47] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:04,977 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:04,978 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:04,979 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:05,382 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333302)에 맞춰 변경되었습니다.
2026-01-14 12:21:05,383 - INFO - ==================================================
2026-01-14 12:21:05,414 - INFO -   [탐색 48] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:05,458 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:05,458 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:05,459 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:05,950 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333319)에 맞춰 변경되었습니다.
2026-01-14 12:21:05,950 - INFO - ==================================================
2026-01-14 12:21:05,980 - INFO -   [탐색 49] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:06,020 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:06,021 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:06,021 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:06,518 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333328)에 맞춰 변경되었습니다.
2026-01-14 12:21:06,519 - INFO - ==================================================
2026-01-14 12:21:06,569 - INFO -   [탐색 50] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:06,629 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:06,630 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:06,631 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:07,648 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:07,649 - INFO - ==================================================
2026-01-14 12:21:07,688 - INFO -   [탐색 51] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:07,731 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:07,731 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:07,732 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:08,298 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333335)에 맞춰 변경되었습니다.
2026-01-14 12:21:08,299 - INFO - ==================================================
2026-01-14 12:21:08,336 - INFO -   [탐색 52] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:21:08,388 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:08,388 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:08,389 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:08,988 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333334)에 맞춰 변경되었습니다.
2026-01-14 12:21:08,989 - INFO - ==================================================
2026-01-14 12:21:09,029 - INFO -   [탐색 53] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-01-14 12:21:09,084 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:09,085 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:09,086 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:09,761 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:09,762 - INFO - ==================================================
2026-01-14 12:21:09,821 - INFO -   [탐색 54] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:09,887 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:09,888 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:09,889 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:10,605 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:10,605 - INFO - ==================================================
2026-01-14 12:21:10,632 - INFO -   [탐색 55] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:10,676 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:10,676 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:10,677 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:11,421 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:11,422 - INFO - ==================================================
2026-01-14 12:21:11,452 - INFO -   [탐색 56] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:11,498 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:11,498 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:11,499 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:12,083 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:12,084 - INFO - ==================================================
2026-01-14 12:21:12,132 - INFO -   [탐색 57] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:12,204 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:12,206 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:12,207 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:12,659 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:12,660 - INFO - ==================================================
2026-01-14 12:21:12,683 - INFO -   [탐색 58] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:12,721 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:12,721 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:12,721 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:13,078 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:13,078 - INFO - ==================================================
2026-01-14 12:21:13,118 - INFO -   [탐색 59] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:13,179 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:13,180 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:13,181 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:13,717 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:13,717 - INFO - ==================================================
2026-01-14 12:21:13,755 - INFO -   [탐색 60] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:13,808 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:13,809 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:13,810 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:14,883 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:14,884 - INFO - ==================================================
2026-01-14 12:21:14,918 - INFO -   [탐색 61] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:14,965 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:14,966 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:14,967 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:15,730 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:15,730 - INFO - ==================================================
2026-01-14 12:21:15,766 - INFO -   [탐색 62] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:15,822 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:15,823 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:15,824 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:16,560 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:16,560 - INFO - ==================================================
2026-01-14 12:21:16,608 - INFO -   [탐색 63] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:16,663 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:16,663 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:16,664 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:17,228 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:17,229 - INFO - ==================================================
2026-01-14 12:21:17,267 - INFO -   [탐색 64] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:17,320 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:17,320 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:17,321 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:17,904 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:17,905 - INFO - ==================================================
2026-01-14 12:21:17,951 - INFO -   [탐색 65] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:18,016 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:18,017 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:18,018 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:18,896 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:18,896 - INFO - ==================================================
2026-01-14 12:21:18,985 - INFO -   [탐색 66] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:19,090 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:19,091 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:19,092 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:19,684 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:19,685 - INFO - ==================================================
2026-01-14 12:21:19,725 - INFO -   [탐색 67] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:19,780 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:19,781 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:19,782 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:20,324 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:20,325 - INFO - ==================================================
2026-01-14 12:21:20,367 - INFO -   [탐색 68] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:20,410 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:20,410 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:20,411 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:20,942 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:20,943 - INFO - ==================================================
2026-01-14 12:21:20,981 - INFO -   [탐색 69] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:21,076 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:21,078 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:21,079 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:21,767 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:21,768 - INFO - ==================================================
2026-01-14 12:21:21,804 - INFO -   [탐색 70] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:21,858 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:21,858 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:21,859 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:22,726 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:22,727 - INFO - ==================================================
2026-01-14 12:21:22,765 - INFO -   [탐색 71] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:22,815 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:22,816 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:22,817 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:23,346 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:23,347 - INFO - ==================================================
2026-01-14 12:21:23,385 - INFO -   [탐색 72] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:23,444 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:23,444 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:23,446 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:24,008 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:24,008 - INFO - ==================================================
2026-01-14 12:21:24,047 - INFO -   [탐색 73] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:24,104 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:24,105 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:24,106 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:24,695 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:24,695 - INFO - ==================================================
2026-01-14 12:21:24,734 - INFO -   [탐색 74] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:24,790 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:24,791 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:24,793 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:25,306 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:25,306 - INFO - ==================================================
2026-01-14 12:21:25,358 - INFO -   [탐색 75] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:25,429 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:25,430 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:25,431 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:26,322 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:26,323 - INFO - ==================================================
2026-01-14 12:21:26,364 - INFO -   [탐색 76] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:26,421 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:26,422 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:26,423 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:27,002 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:27,003 - INFO - ==================================================
2026-01-14 12:21:27,042 - INFO -   [탐색 77] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:27,101 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:27,101 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:27,102 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:27,688 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:27,688 - INFO - ==================================================
2026-01-14 12:21:27,730 - INFO -   [탐색 78] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:27,791 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:27,792 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:27,793 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:28,414 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:28,415 - INFO - ==================================================
2026-01-14 12:21:28,466 - INFO -   [탐색 79] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:28,526 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:28,527 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:28,528 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:29,386 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:29,387 - INFO - ==================================================
2026-01-14 12:21:29,426 - INFO -   [탐색 80] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:29,488 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:29,489 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:29,490 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:30,049 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:30,049 - INFO - ==================================================
2026-01-14 12:21:30,088 - INFO -   [탐색 81] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:30,143 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:30,143 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:30,144 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:30,712 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:30,713 - INFO - ==================================================
2026-01-14 12:21:30,751 - INFO -   [탐색 82] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:30,806 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:30,809 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:30,810 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:31,400 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:31,401 - INFO - ==================================================
2026-01-14 12:21:31,442 - INFO -   [탐색 83] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:31,532 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:31,532 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:31,533 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:32,280 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:32,280 - INFO - ==================================================
2026-01-14 12:21:32,323 - INFO -   [탐색 84] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:32,385 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:32,386 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:32,387 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:33,518 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:33,522 - INFO - ==================================================
2026-01-14 12:21:33,572 - INFO -   [탐색 85] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:33,641 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:33,642 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:33,644 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:34,302 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:34,303 - INFO - ==================================================
2026-01-14 12:21:34,341 - INFO -   [탐색 86] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:34,404 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:34,406 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:34,407 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:34,965 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:34,966 - INFO - ==================================================
2026-01-14 12:21:35,004 - INFO -   [탐색 87] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:35,060 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:35,061 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:35,062 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:35,674 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:35,675 - INFO - ==================================================
2026-01-14 12:21:35,715 - INFO -   [탐색 88] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:35,780 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:35,781 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:35,783 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:36,305 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:36,307 - INFO - ==================================================
2026-01-14 12:21:36,342 - INFO -   [탐색 89] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:36,392 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:36,392 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:36,394 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:37,234 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:37,234 - INFO - ==================================================
2026-01-14 12:21:37,273 - INFO -   [탐색 90] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:37,328 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:37,329 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:37,330 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:37,989 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:37,989 - INFO - ==================================================
2026-01-14 12:21:38,023 - INFO -   [탐색 91] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:38,095 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:38,095 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:38,096 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:38,585 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:38,586 - INFO - ==================================================
2026-01-14 12:21:38,621 - INFO -   [탐색 92] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:38,685 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:38,686 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:38,687 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:39,331 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:39,332 - INFO - ==================================================
2026-01-14 12:21:39,376 - INFO -   [탐색 93] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:39,442 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:39,443 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:39,443 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:40,040 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:40,040 - INFO - ==================================================
2026-01-14 12:21:40,090 - INFO -   [탐색 94] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:40,155 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:40,155 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:40,156 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:40,924 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:40,925 - INFO - ==================================================
2026-01-14 12:21:40,963 - INFO -   [탐색 95] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:41,017 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:41,017 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:41,018 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:41,652 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:41,653 - INFO - ==================================================
2026-01-14 12:21:41,693 - INFO -   [탐색 96] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:41,741 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:41,742 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:41,742 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:42,187 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:42,187 - INFO - ==================================================
2026-01-14 12:21:42,231 - INFO -   [탐색 97] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:42,291 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:42,291 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:42,292 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:42,754 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:42,755 - INFO - ==================================================
2026-01-14 12:21:42,789 - INFO -   [탐색 98] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:42,839 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:42,840 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:42,841 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:43,411 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:43,411 - INFO - ==================================================
2026-01-14 12:21:43,449 - INFO -   [탐색 99] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:43,505 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:43,506 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:43,507 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:44,402 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-01-14 12:21:44,403 - INFO - ==================================================
2026-01-14 12:21:44,444 - INFO -   [탐색 100] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-01-14 12:21:44,445 - INFO - 탐색 완료. 목표 FLOPs(0.1829)에 가장 근접한 최적 희소도는 0.8169 입니다.
2026-01-14 12:21:44,445 - INFO - ================================================================================
2026-01-14 12:21:44,449 - INFO - 계산된 Pruning 정보(희소도: 0.8169)를 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/pruning_info.yaml'에 저장했습니다.
2026-01-14 12:21:44,488 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-01-14 12:21:44,586 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:21:44,587 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:21:44,587 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:21:45,162 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-01-14 12:21:45,163 - INFO - ==================================================
2026-01-14 12:21:45,165 - INFO - ==================================================
2026-01-14 12:21:45,166 - INFO - 모델 파라미터 수:
2026-01-14 12:21:45,166 - INFO -   - 총 파라미터: 485,259 개
2026-01-14 12:21:45,167 - INFO -   - 학습 가능한 파라미터: 485,259 개
2026-01-14 12:21:45,210 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-01-14 12:21:45,311 - INFO - FLOPs가 2.1493 GFLOPs에서 0.1840 GFLOPs로 감소했습니다 (감소율: 91.44%).
2026-01-14 12:21:45,312 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-01-14 12:21:45,312 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:21:45,315 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-01-14 12:21:45,315 - INFO - ==================================================
2026-01-14 12:21:45,316 - INFO - train 모드를 시작합니다.
2026-01-14 12:21:45,316 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:21:45,316 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:21:45,316 - INFO - --------------------------------------------------
2026-01-14 12:21:45,319 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-01-14 12:21:54,404 - INFO - [Train] [11/90] | Loss: 0.5274 | Train Acc: 78.87%
2026-01-14 12:21:56,454 - INFO - [Valid] [11/90] | Loss: 0.5706 | Val Acc: 75.22%
2026-01-14 12:21:56,478 - INFO - [Metrics for 'abnormal'] | Precision: 0.8174 | Recall: 0.5987 | F1: 0.6912
2026-01-14 12:21:56,478 - INFO - [Metrics for 'normal'] | Precision: 0.7188 | Recall: 0.8846 | F1: 0.7931
2026-01-14 12:21:56,506 - INFO - [Best Model Saved] (val loss: 0.5706) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:21:56,507 - INFO - --------------------------------------------------
2026-01-14 12:21:56,509 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-01-14 12:22:04,940 - INFO - [Train] [12/90] | Loss: 0.5036 | Train Acc: 80.28%
2026-01-14 12:22:07,007 - INFO - [Valid] [12/90] | Loss: 0.5455 | Val Acc: 74.63%
2026-01-14 12:22:07,033 - INFO - [Metrics for 'abnormal'] | Precision: 0.7383 | Recall: 0.7006 | F1: 0.7190
2026-01-14 12:22:07,033 - INFO - [Metrics for 'normal'] | Precision: 0.7526 | Recall: 0.7857 | F1: 0.7688
2026-01-14 12:22:07,093 - INFO - [Best Model Saved] (val loss: 0.5455) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:22:07,093 - INFO - --------------------------------------------------
2026-01-14 12:22:07,097 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-01-14 12:22:15,134 - INFO - [Train] [13/90] | Loss: 0.5009 | Train Acc: 80.21%
2026-01-14 12:22:17,178 - INFO - [Valid] [13/90] | Loss: 0.5558 | Val Acc: 76.40%
2026-01-14 12:22:17,194 - INFO - [Metrics for 'abnormal'] | Precision: 0.8080 | Recall: 0.6433 | F1: 0.7163
2026-01-14 12:22:17,194 - INFO - [Metrics for 'normal'] | Precision: 0.7383 | Recall: 0.8681 | F1: 0.7980
2026-01-14 12:22:17,202 - INFO - --------------------------------------------------
2026-01-14 12:22:17,204 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-01-14 12:22:25,080 - INFO - [Train] [14/90] | Loss: 0.4944 | Train Acc: 81.47%
2026-01-14 12:22:27,143 - INFO - [Valid] [14/90] | Loss: 0.5422 | Val Acc: 76.70%
2026-01-14 12:22:27,157 - INFO - [Metrics for 'abnormal'] | Precision: 0.7378 | Recall: 0.7707 | F1: 0.7539
2026-01-14 12:22:27,157 - INFO - [Metrics for 'normal'] | Precision: 0.7943 | Recall: 0.7637 | F1: 0.7787
2026-01-14 12:22:27,196 - INFO - [Best Model Saved] (val loss: 0.5422) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:22:27,197 - INFO - --------------------------------------------------
2026-01-14 12:22:27,199 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-01-14 12:22:35,693 - INFO - [Train] [15/90] | Loss: 0.4938 | Train Acc: 80.65%
2026-01-14 12:22:37,867 - INFO - [Valid] [15/90] | Loss: 0.5587 | Val Acc: 76.11%
2026-01-14 12:22:37,889 - INFO - [Metrics for 'abnormal'] | Precision: 0.7676 | Recall: 0.6943 | F1: 0.7291
2026-01-14 12:22:37,890 - INFO - [Metrics for 'normal'] | Precision: 0.7563 | Recall: 0.8187 | F1: 0.7863
2026-01-14 12:22:37,895 - INFO - --------------------------------------------------
2026-01-14 12:22:37,906 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-01-14 12:22:45,278 - INFO - [Train] [16/90] | Loss: 0.4894 | Train Acc: 80.80%
2026-01-14 12:22:47,536 - INFO - [Valid] [16/90] | Loss: 0.5405 | Val Acc: 77.29%
2026-01-14 12:22:47,549 - INFO - [Metrics for 'abnormal'] | Precision: 0.7299 | Recall: 0.8089 | F1: 0.7674
2026-01-14 12:22:47,549 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.7418 | F1: 0.7781
2026-01-14 12:22:47,587 - INFO - [Best Model Saved] (val loss: 0.5405) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:22:47,587 - INFO - --------------------------------------------------
2026-01-14 12:22:47,590 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-01-14 12:22:55,329 - INFO - [Train] [17/90] | Loss: 0.4836 | Train Acc: 81.18%
2026-01-14 12:22:58,585 - INFO - [Valid] [17/90] | Loss: 0.5707 | Val Acc: 74.04%
2026-01-14 12:22:58,617 - INFO - [Metrics for 'abnormal'] | Precision: 0.6683 | Recall: 0.8726 | F1: 0.7569
2026-01-14 12:22:58,617 - INFO - [Metrics for 'normal'] | Precision: 0.8507 | Recall: 0.6264 | F1: 0.7215
2026-01-14 12:22:58,629 - INFO - --------------------------------------------------
2026-01-14 12:22:58,635 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-01-14 12:23:08,684 - INFO - [Train] [18/90] | Loss: 0.4774 | Train Acc: 81.99%
2026-01-14 12:23:11,541 - INFO - [Valid] [18/90] | Loss: 0.5444 | Val Acc: 75.81%
2026-01-14 12:23:11,564 - INFO - [Metrics for 'abnormal'] | Precision: 0.7389 | Recall: 0.7389 | F1: 0.7389
2026-01-14 12:23:11,567 - INFO - [Metrics for 'normal'] | Precision: 0.7747 | Recall: 0.7747 | F1: 0.7747
2026-01-14 12:23:11,575 - INFO - --------------------------------------------------
2026-01-14 12:23:11,577 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-01-14 12:23:21,690 - INFO - [Train] [19/90] | Loss: 0.4701 | Train Acc: 81.85%
2026-01-14 12:23:24,031 - INFO - [Valid] [19/90] | Loss: 0.5388 | Val Acc: 74.63%
2026-01-14 12:23:24,043 - INFO - [Metrics for 'abnormal'] | Precision: 0.7205 | Recall: 0.7389 | F1: 0.7296
2026-01-14 12:23:24,044 - INFO - [Metrics for 'normal'] | Precision: 0.7697 | Recall: 0.7527 | F1: 0.7611
2026-01-14 12:23:24,200 - INFO - [Best Model Saved] (val loss: 0.5388) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:23:24,200 - INFO - --------------------------------------------------
2026-01-14 12:23:24,202 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-01-14 12:23:33,687 - INFO - [Train] [20/90] | Loss: 0.4789 | Train Acc: 83.04%
2026-01-14 12:23:36,109 - INFO - [Valid] [20/90] | Loss: 0.5328 | Val Acc: 77.29%
2026-01-14 12:23:36,122 - INFO - [Metrics for 'abnormal'] | Precision: 0.7532 | Recall: 0.7580 | F1: 0.7556
2026-01-14 12:23:36,123 - INFO - [Metrics for 'normal'] | Precision: 0.7901 | Recall: 0.7857 | F1: 0.7879
2026-01-14 12:23:36,159 - INFO - [Best Model Saved] (val loss: 0.5328) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:23:36,159 - INFO - --------------------------------------------------
2026-01-14 12:23:36,161 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-01-14 12:23:46,464 - INFO - [Train] [21/90] | Loss: 0.4708 | Train Acc: 82.07%
2026-01-14 12:23:49,093 - INFO - [Valid] [21/90] | Loss: 0.5379 | Val Acc: 76.40%
2026-01-14 12:23:49,104 - INFO - [Metrics for 'abnormal'] | Precision: 0.7251 | Recall: 0.7898 | F1: 0.7561
2026-01-14 12:23:49,104 - INFO - [Metrics for 'normal'] | Precision: 0.8036 | Recall: 0.7418 | F1: 0.7714
2026-01-14 12:23:49,108 - INFO - --------------------------------------------------
2026-01-14 12:23:49,110 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-01-14 12:23:59,971 - INFO - [Train] [22/90] | Loss: 0.4718 | Train Acc: 82.07%
2026-01-14 12:24:02,315 - INFO - [Valid] [22/90] | Loss: 0.5465 | Val Acc: 75.81%
2026-01-14 12:24:02,325 - INFO - [Metrics for 'abnormal'] | Precision: 0.8099 | Recall: 0.6242 | F1: 0.7050
2026-01-14 12:24:02,326 - INFO - [Metrics for 'normal'] | Precision: 0.7294 | Recall: 0.8736 | F1: 0.7950
2026-01-14 12:24:02,329 - INFO - --------------------------------------------------
2026-01-14 12:24:02,332 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-01-14 12:24:13,201 - INFO - [Train] [23/90] | Loss: 0.4753 | Train Acc: 81.85%
2026-01-14 12:24:15,206 - INFO - [Valid] [23/90] | Loss: 0.5390 | Val Acc: 75.81%
2026-01-14 12:24:15,223 - INFO - [Metrics for 'abnormal'] | Precision: 0.7820 | Recall: 0.6624 | F1: 0.7172
2026-01-14 12:24:15,226 - INFO - [Metrics for 'normal'] | Precision: 0.7427 | Recall: 0.8407 | F1: 0.7887
2026-01-14 12:24:15,233 - INFO - --------------------------------------------------
2026-01-14 12:24:15,235 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-01-14 12:24:25,605 - INFO - [Train] [24/90] | Loss: 0.4772 | Train Acc: 81.85%
2026-01-14 12:24:27,509 - INFO - [Valid] [24/90] | Loss: 0.5171 | Val Acc: 76.99%
2026-01-14 12:24:27,519 - INFO - [Metrics for 'abnormal'] | Precision: 0.7484 | Recall: 0.7580 | F1: 0.7532
2026-01-14 12:24:27,520 - INFO - [Metrics for 'normal'] | Precision: 0.7889 | Recall: 0.7802 | F1: 0.7845
2026-01-14 12:24:27,549 - INFO - [Best Model Saved] (val loss: 0.5171) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:24:27,550 - INFO - --------------------------------------------------
2026-01-14 12:24:27,552 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-01-14 12:24:37,991 - INFO - [Train] [25/90] | Loss: 0.4711 | Train Acc: 82.89%
2026-01-14 12:24:40,133 - INFO - [Valid] [25/90] | Loss: 0.5190 | Val Acc: 76.40%
2026-01-14 12:24:40,146 - INFO - [Metrics for 'abnormal'] | Precision: 0.7655 | Recall: 0.7070 | F1: 0.7351
2026-01-14 12:24:40,146 - INFO - [Metrics for 'normal'] | Precision: 0.7629 | Recall: 0.8132 | F1: 0.7872
2026-01-14 12:24:40,152 - INFO - --------------------------------------------------
2026-01-14 12:24:40,154 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-01-14 12:24:49,587 - INFO - [Train] [26/90] | Loss: 0.4620 | Train Acc: 83.56%
2026-01-14 12:24:52,345 - INFO - [Valid] [26/90] | Loss: 0.5174 | Val Acc: 78.76%
2026-01-14 12:24:52,356 - INFO - [Metrics for 'abnormal'] | Precision: 0.7348 | Recall: 0.8471 | F1: 0.7870
2026-01-14 12:24:52,357 - INFO - [Metrics for 'normal'] | Precision: 0.8481 | Recall: 0.7363 | F1: 0.7882
2026-01-14 12:24:52,362 - INFO - --------------------------------------------------
2026-01-14 12:24:52,364 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-01-14 12:25:02,981 - INFO - [Train] [27/90] | Loss: 0.4669 | Train Acc: 81.70%
2026-01-14 12:25:05,159 - INFO - [Valid] [27/90] | Loss: 0.5294 | Val Acc: 78.17%
2026-01-14 12:25:05,170 - INFO - [Metrics for 'abnormal'] | Precision: 0.7371 | Recall: 0.8217 | F1: 0.7771
2026-01-14 12:25:05,171 - INFO - [Metrics for 'normal'] | Precision: 0.8293 | Recall: 0.7473 | F1: 0.7861
2026-01-14 12:25:05,175 - INFO - --------------------------------------------------
2026-01-14 12:25:05,178 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-01-14 12:25:14,328 - INFO - [Train] [28/90] | Loss: 0.4521 | Train Acc: 83.56%
2026-01-14 12:25:16,322 - INFO - [Valid] [28/90] | Loss: 0.5243 | Val Acc: 78.47%
2026-01-14 12:25:16,336 - INFO - [Metrics for 'abnormal'] | Precision: 0.7625 | Recall: 0.7771 | F1: 0.7697
2026-01-14 12:25:16,337 - INFO - [Metrics for 'normal'] | Precision: 0.8045 | Recall: 0.7912 | F1: 0.7978
2026-01-14 12:25:16,341 - INFO - --------------------------------------------------
2026-01-14 12:25:16,343 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-01-14 12:25:25,819 - INFO - [Train] [29/90] | Loss: 0.4696 | Train Acc: 81.85%
2026-01-14 12:25:28,137 - INFO - [Valid] [29/90] | Loss: 0.5002 | Val Acc: 77.58%
2026-01-14 12:25:28,165 - INFO - [Metrics for 'abnormal'] | Precision: 0.7580 | Recall: 0.7580 | F1: 0.7580
2026-01-14 12:25:28,165 - INFO - [Metrics for 'normal'] | Precision: 0.7912 | Recall: 0.7912 | F1: 0.7912
2026-01-14 12:25:28,222 - INFO - [Best Model Saved] (val loss: 0.5002) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:25:28,223 - INFO - --------------------------------------------------
2026-01-14 12:25:28,225 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-01-14 12:25:37,964 - INFO - [Train] [30/90] | Loss: 0.4572 | Train Acc: 83.18%
2026-01-14 12:25:40,293 - INFO - [Valid] [30/90] | Loss: 0.5029 | Val Acc: 78.47%
2026-01-14 12:25:40,305 - INFO - [Metrics for 'abnormal'] | Precision: 0.7838 | Recall: 0.7389 | F1: 0.7607
2026-01-14 12:25:40,305 - INFO - [Metrics for 'normal'] | Precision: 0.7853 | Recall: 0.8242 | F1: 0.8043
2026-01-14 12:25:40,310 - INFO - --------------------------------------------------
2026-01-14 12:25:40,312 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-01-14 12:25:51,130 - INFO - [Train] [31/90] | Loss: 0.4465 | Train Acc: 84.23%
2026-01-14 12:25:54,060 - INFO - [Valid] [31/90] | Loss: 0.4987 | Val Acc: 78.17%
2026-01-14 12:25:54,087 - INFO - [Metrics for 'abnormal'] | Precision: 0.8029 | Recall: 0.7006 | F1: 0.7483
2026-01-14 12:25:54,091 - INFO - [Metrics for 'normal'] | Precision: 0.7673 | Recall: 0.8516 | F1: 0.8073
2026-01-14 12:25:54,154 - INFO - [Best Model Saved] (val loss: 0.4987) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:25:54,154 - INFO - --------------------------------------------------
2026-01-14 12:25:54,159 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-01-14 12:26:03,817 - INFO - [Train] [32/90] | Loss: 0.4504 | Train Acc: 83.48%
2026-01-14 12:26:06,081 - INFO - [Valid] [32/90] | Loss: 0.5571 | Val Acc: 74.93%
2026-01-14 12:26:06,107 - INFO - [Metrics for 'abnormal'] | Precision: 0.6667 | Recall: 0.9172 | F1: 0.7721
2026-01-14 12:26:06,111 - INFO - [Metrics for 'normal'] | Precision: 0.8943 | Recall: 0.6044 | F1: 0.7213
2026-01-14 12:26:06,116 - INFO - --------------------------------------------------
2026-01-14 12:26:06,118 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-01-14 12:26:16,102 - INFO - [Train] [33/90] | Loss: 0.4545 | Train Acc: 83.63%
2026-01-14 12:26:18,514 - INFO - [Valid] [33/90] | Loss: 0.5164 | Val Acc: 79.35%
2026-01-14 12:26:18,527 - INFO - [Metrics for 'abnormal'] | Precision: 0.7514 | Recall: 0.8280 | F1: 0.7879
2026-01-14 12:26:18,528 - INFO - [Metrics for 'normal'] | Precision: 0.8373 | Recall: 0.7637 | F1: 0.7989
2026-01-14 12:26:18,533 - INFO - --------------------------------------------------
2026-01-14 12:26:18,536 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-01-14 12:26:28,501 - INFO - [Train] [34/90] | Loss: 0.4454 | Train Acc: 83.78%
2026-01-14 12:26:30,847 - INFO - [Valid] [34/90] | Loss: 0.5271 | Val Acc: 78.17%
2026-01-14 12:26:30,859 - INFO - [Metrics for 'abnormal'] | Precision: 0.7546 | Recall: 0.7834 | F1: 0.7688
2026-01-14 12:26:30,860 - INFO - [Metrics for 'normal'] | Precision: 0.8068 | Recall: 0.7802 | F1: 0.7933
2026-01-14 12:26:30,865 - INFO - --------------------------------------------------
2026-01-14 12:26:30,868 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-01-14 12:26:40,211 - INFO - [Train] [35/90] | Loss: 0.4431 | Train Acc: 84.30%
2026-01-14 12:26:42,698 - INFO - [Valid] [35/90] | Loss: 0.4979 | Val Acc: 77.58%
2026-01-14 12:26:42,729 - INFO - [Metrics for 'abnormal'] | Precision: 0.7485 | Recall: 0.7771 | F1: 0.7625
2026-01-14 12:26:42,731 - INFO - [Metrics for 'normal'] | Precision: 0.8011 | Recall: 0.7747 | F1: 0.7877
2026-01-14 12:26:42,769 - INFO - [Best Model Saved] (val loss: 0.4979) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:26:42,769 - INFO - --------------------------------------------------
2026-01-14 12:26:42,772 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-01-14 12:26:52,783 - INFO - [Train] [36/90] | Loss: 0.4398 | Train Acc: 83.78%
2026-01-14 12:26:55,173 - INFO - [Valid] [36/90] | Loss: 0.5147 | Val Acc: 80.24%
2026-01-14 12:26:55,198 - INFO - [Metrics for 'abnormal'] | Precision: 0.8082 | Recall: 0.7516 | F1: 0.7789
2026-01-14 12:26:55,202 - INFO - [Metrics for 'normal'] | Precision: 0.7979 | Recall: 0.8462 | F1: 0.8213
2026-01-14 12:26:55,209 - INFO - --------------------------------------------------
2026-01-14 12:26:55,212 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-01-14 12:27:05,284 - INFO - [Train] [37/90] | Loss: 0.4325 | Train Acc: 85.42%
2026-01-14 12:27:07,554 - INFO - [Valid] [37/90] | Loss: 0.4953 | Val Acc: 79.35%
2026-01-14 12:27:07,584 - INFO - [Metrics for 'abnormal'] | Precision: 0.7669 | Recall: 0.7962 | F1: 0.7812
2026-01-14 12:27:07,584 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.7912 | F1: 0.8045
2026-01-14 12:27:07,649 - INFO - [Best Model Saved] (val loss: 0.4953) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:27:07,650 - INFO - --------------------------------------------------
2026-01-14 12:27:07,652 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-01-14 12:27:18,224 - INFO - [Train] [38/90] | Loss: 0.4287 | Train Acc: 85.27%
2026-01-14 12:27:20,451 - INFO - [Valid] [38/90] | Loss: 0.4969 | Val Acc: 79.65%
2026-01-14 12:27:20,482 - INFO - [Metrics for 'abnormal'] | Precision: 0.7683 | Recall: 0.8025 | F1: 0.7850
2026-01-14 12:27:20,482 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.7912 | F1: 0.8067
2026-01-14 12:27:20,504 - INFO - --------------------------------------------------
2026-01-14 12:27:20,506 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-01-14 12:27:31,235 - INFO - [Train] [39/90] | Loss: 0.4266 | Train Acc: 84.97%
2026-01-14 12:27:34,178 - INFO - [Valid] [39/90] | Loss: 0.5106 | Val Acc: 79.65%
2026-01-14 12:27:34,209 - INFO - [Metrics for 'abnormal'] | Precision: 0.7340 | Recall: 0.8790 | F1: 0.8000
2026-01-14 12:27:34,209 - INFO - [Metrics for 'normal'] | Precision: 0.8742 | Recall: 0.7253 | F1: 0.7928
2026-01-14 12:27:34,223 - INFO - --------------------------------------------------
2026-01-14 12:27:34,230 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-01-14 12:27:43,804 - INFO - [Train] [40/90] | Loss: 0.4267 | Train Acc: 84.30%
2026-01-14 12:27:47,486 - INFO - [Valid] [40/90] | Loss: 0.4935 | Val Acc: 79.94%
2026-01-14 12:27:47,514 - INFO - [Metrics for 'abnormal'] | Precision: 0.7764 | Recall: 0.7962 | F1: 0.7862
2026-01-14 12:27:47,518 - INFO - [Metrics for 'normal'] | Precision: 0.8202 | Recall: 0.8022 | F1: 0.8111
2026-01-14 12:27:47,575 - INFO - [Best Model Saved] (val loss: 0.4935) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:27:47,577 - INFO - --------------------------------------------------
2026-01-14 12:27:47,579 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-01-14 12:27:56,535 - INFO - [Train] [41/90] | Loss: 0.4159 | Train Acc: 85.94%
2026-01-14 12:27:59,597 - INFO - [Valid] [41/90] | Loss: 0.5007 | Val Acc: 77.58%
2026-01-14 12:27:59,627 - INFO - [Metrics for 'abnormal'] | Precision: 0.7613 | Recall: 0.7516 | F1: 0.7564
2026-01-14 12:27:59,627 - INFO - [Metrics for 'normal'] | Precision: 0.7880 | Recall: 0.7967 | F1: 0.7923
2026-01-14 12:27:59,635 - INFO - --------------------------------------------------
2026-01-14 12:27:59,641 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-01-14 12:28:08,014 - INFO - [Train] [42/90] | Loss: 0.4239 | Train Acc: 86.01%
2026-01-14 12:28:12,152 - INFO - [Valid] [42/90] | Loss: 0.5154 | Val Acc: 76.99%
2026-01-14 12:28:12,165 - INFO - [Metrics for 'abnormal'] | Precision: 0.8160 | Recall: 0.6497 | F1: 0.7234
2026-01-14 12:28:12,166 - INFO - [Metrics for 'normal'] | Precision: 0.7430 | Recall: 0.8736 | F1: 0.8030
2026-01-14 12:28:12,170 - INFO - --------------------------------------------------
2026-01-14 12:28:12,172 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-01-14 12:28:20,757 - INFO - [Train] [43/90] | Loss: 0.4154 | Train Acc: 85.49%
2026-01-14 12:28:23,965 - INFO - [Valid] [43/90] | Loss: 0.4934 | Val Acc: 80.53%
2026-01-14 12:28:23,987 - INFO - [Metrics for 'abnormal'] | Precision: 0.7542 | Recall: 0.8599 | F1: 0.8036
2026-01-14 12:28:23,988 - INFO - [Metrics for 'normal'] | Precision: 0.8625 | Recall: 0.7582 | F1: 0.8070
2026-01-14 12:28:24,023 - INFO - [Best Model Saved] (val loss: 0.4934) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:28:24,024 - INFO - --------------------------------------------------
2026-01-14 12:28:24,026 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-01-14 12:28:33,536 - INFO - [Train] [44/90] | Loss: 0.4108 | Train Acc: 86.31%
2026-01-14 12:28:37,009 - INFO - [Valid] [44/90] | Loss: 0.5003 | Val Acc: 81.42%
2026-01-14 12:28:37,023 - INFO - [Metrics for 'abnormal'] | Precision: 0.7582 | Recall: 0.8790 | F1: 0.8142
2026-01-14 12:28:37,024 - INFO - [Metrics for 'normal'] | Precision: 0.8790 | Recall: 0.7582 | F1: 0.8142
2026-01-14 12:28:37,028 - INFO - --------------------------------------------------
2026-01-14 12:28:37,031 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-01-14 12:28:46,737 - INFO - [Train] [45/90] | Loss: 0.4153 | Train Acc: 86.46%
2026-01-14 12:28:50,108 - INFO - [Valid] [45/90] | Loss: 0.5098 | Val Acc: 79.65%
2026-01-14 12:28:50,133 - INFO - [Metrics for 'abnormal'] | Precision: 0.7716 | Recall: 0.7962 | F1: 0.7837
2026-01-14 12:28:50,133 - INFO - [Metrics for 'normal'] | Precision: 0.8192 | Recall: 0.7967 | F1: 0.8078
2026-01-14 12:28:50,152 - INFO - --------------------------------------------------
2026-01-14 12:28:50,155 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-01-14 12:28:59,046 - INFO - [Train] [46/90] | Loss: 0.4186 | Train Acc: 86.38%
2026-01-14 12:29:02,293 - INFO - [Valid] [46/90] | Loss: 0.5134 | Val Acc: 80.83%
2026-01-14 12:29:02,318 - INFO - [Metrics for 'abnormal'] | Precision: 0.7527 | Recall: 0.8726 | F1: 0.8083
2026-01-14 12:29:02,319 - INFO - [Metrics for 'normal'] | Precision: 0.8726 | Recall: 0.7527 | F1: 0.8083
2026-01-14 12:29:02,324 - INFO - --------------------------------------------------
2026-01-14 12:29:02,327 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-01-14 12:29:10,732 - INFO - [Train] [47/90] | Loss: 0.4052 | Train Acc: 86.46%
2026-01-14 12:29:13,274 - INFO - [Valid] [47/90] | Loss: 0.4875 | Val Acc: 80.53%
2026-01-14 12:29:13,289 - INFO - [Metrics for 'abnormal'] | Precision: 0.8013 | Recall: 0.7707 | F1: 0.7857
2026-01-14 12:29:13,290 - INFO - [Metrics for 'normal'] | Precision: 0.8085 | Recall: 0.8352 | F1: 0.8216
2026-01-14 12:29:13,326 - INFO - [Best Model Saved] (val loss: 0.4875) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:29:13,327 - INFO - --------------------------------------------------
2026-01-14 12:29:13,329 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-01-14 12:29:21,896 - INFO - [Train] [48/90] | Loss: 0.4156 | Train Acc: 85.86%
2026-01-14 12:29:25,033 - INFO - [Valid] [48/90] | Loss: 0.5126 | Val Acc: 81.12%
2026-01-14 12:29:25,045 - INFO - [Metrics for 'abnormal'] | Precision: 0.7541 | Recall: 0.8790 | F1: 0.8118
2026-01-14 12:29:25,045 - INFO - [Metrics for 'normal'] | Precision: 0.8782 | Recall: 0.7527 | F1: 0.8107
2026-01-14 12:29:25,049 - INFO - --------------------------------------------------
2026-01-14 12:29:25,052 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-01-14 12:29:34,378 - INFO - [Train] [49/90] | Loss: 0.4008 | Train Acc: 86.46%
2026-01-14 12:29:36,540 - INFO - [Valid] [49/90] | Loss: 0.5120 | Val Acc: 79.35%
2026-01-14 12:29:36,575 - INFO - [Metrics for 'abnormal'] | Precision: 0.7636 | Recall: 0.8025 | F1: 0.7826
2026-01-14 12:29:36,575 - INFO - [Metrics for 'normal'] | Precision: 0.8218 | Recall: 0.7857 | F1: 0.8034
2026-01-14 12:29:36,584 - INFO - --------------------------------------------------
2026-01-14 12:29:36,590 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-01-14 12:29:46,876 - INFO - [Train] [50/90] | Loss: 0.3986 | Train Acc: 87.28%
2026-01-14 12:29:49,499 - INFO - [Valid] [50/90] | Loss: 0.5074 | Val Acc: 80.53%
2026-01-14 12:29:49,521 - INFO - [Metrics for 'abnormal'] | Precision: 0.7571 | Recall: 0.8535 | F1: 0.8024
2026-01-14 12:29:49,522 - INFO - [Metrics for 'normal'] | Precision: 0.8580 | Recall: 0.7637 | F1: 0.8081
2026-01-14 12:29:49,527 - INFO - --------------------------------------------------
2026-01-14 12:29:49,530 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-01-14 12:29:59,748 - INFO - [Train] [51/90] | Loss: 0.3907 | Train Acc: 87.87%
2026-01-14 12:30:02,743 - INFO - [Valid] [51/90] | Loss: 0.4986 | Val Acc: 81.42%
2026-01-14 12:30:02,772 - INFO - [Metrics for 'abnormal'] | Precision: 0.7831 | Recall: 0.8280 | F1: 0.8050
2026-01-14 12:30:02,773 - INFO - [Metrics for 'normal'] | Precision: 0.8439 | Recall: 0.8022 | F1: 0.8225
2026-01-14 12:30:02,777 - INFO - --------------------------------------------------
2026-01-14 12:30:02,779 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-01-14 12:30:12,108 - INFO - [Train] [52/90] | Loss: 0.3899 | Train Acc: 88.39%
2026-01-14 12:30:14,930 - INFO - [Valid] [52/90] | Loss: 0.4852 | Val Acc: 80.83%
2026-01-14 12:30:14,950 - INFO - [Metrics for 'abnormal'] | Precision: 0.8108 | Recall: 0.7643 | F1: 0.7869
2026-01-14 12:30:14,954 - INFO - [Metrics for 'normal'] | Precision: 0.8063 | Recall: 0.8462 | F1: 0.8257
2026-01-14 12:30:15,015 - INFO - [Best Model Saved] (val loss: 0.4852) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:30:15,015 - INFO - --------------------------------------------------
2026-01-14 12:30:15,018 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-01-14 12:30:22,949 - INFO - [Train] [53/90] | Loss: 0.3981 | Train Acc: 86.98%
2026-01-14 12:30:25,712 - INFO - [Valid] [53/90] | Loss: 0.4980 | Val Acc: 79.65%
2026-01-14 12:30:25,725 - INFO - [Metrics for 'abnormal'] | Precision: 0.7785 | Recall: 0.7834 | F1: 0.7810
2026-01-14 12:30:25,726 - INFO - [Metrics for 'normal'] | Precision: 0.8122 | Recall: 0.8077 | F1: 0.8099
2026-01-14 12:30:25,731 - INFO - --------------------------------------------------
2026-01-14 12:30:25,735 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-01-14 12:30:35,733 - INFO - [Train] [54/90] | Loss: 0.3837 | Train Acc: 88.02%
2026-01-14 12:30:38,714 - INFO - [Valid] [54/90] | Loss: 0.5101 | Val Acc: 80.24%
2026-01-14 12:30:38,726 - INFO - [Metrics for 'abnormal'] | Precision: 0.7679 | Recall: 0.8217 | F1: 0.7938
2026-01-14 12:30:38,726 - INFO - [Metrics for 'normal'] | Precision: 0.8363 | Recall: 0.7857 | F1: 0.8102
2026-01-14 12:30:38,730 - INFO - --------------------------------------------------
2026-01-14 12:30:38,732 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-01-14 12:30:47,606 - INFO - [Train] [55/90] | Loss: 0.3799 | Train Acc: 87.43%
2026-01-14 12:30:50,171 - INFO - [Valid] [55/90] | Loss: 0.5009 | Val Acc: 79.06%
2026-01-14 12:30:50,183 - INFO - [Metrics for 'abnormal'] | Precision: 0.7654 | Recall: 0.7898 | F1: 0.7774
2026-01-14 12:30:50,183 - INFO - [Metrics for 'normal'] | Precision: 0.8136 | Recall: 0.7912 | F1: 0.8022
2026-01-14 12:30:50,188 - INFO - --------------------------------------------------
2026-01-14 12:30:50,190 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-01-14 12:30:59,399 - INFO - [Train] [56/90] | Loss: 0.3879 | Train Acc: 87.87%
2026-01-14 12:31:01,781 - INFO - [Valid] [56/90] | Loss: 0.5145 | Val Acc: 79.94%
2026-01-14 12:31:01,807 - INFO - [Metrics for 'abnormal'] | Precision: 0.7908 | Recall: 0.7707 | F1: 0.7806
2026-01-14 12:31:01,807 - INFO - [Metrics for 'normal'] | Precision: 0.8065 | Recall: 0.8242 | F1: 0.8152
2026-01-14 12:31:01,818 - INFO - --------------------------------------------------
2026-01-14 12:31:01,823 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-01-14 12:31:10,872 - INFO - [Train] [57/90] | Loss: 0.3754 | Train Acc: 88.91%
2026-01-14 12:31:13,661 - INFO - [Valid] [57/90] | Loss: 0.5065 | Val Acc: 79.06%
2026-01-14 12:31:13,686 - INFO - [Metrics for 'abnormal'] | Precision: 0.7500 | Recall: 0.8217 | F1: 0.7842
2026-01-14 12:31:13,686 - INFO - [Metrics for 'normal'] | Precision: 0.8323 | Recall: 0.7637 | F1: 0.7966
2026-01-14 12:31:13,693 - INFO - --------------------------------------------------
2026-01-14 12:31:13,699 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-01-14 12:31:22,626 - INFO - [Train] [58/90] | Loss: 0.3853 | Train Acc: 88.02%
2026-01-14 12:31:25,529 - INFO - [Valid] [58/90] | Loss: 0.6672 | Val Acc: 67.55%
2026-01-14 12:31:25,541 - INFO - [Metrics for 'abnormal'] | Precision: 0.6083 | Recall: 0.8408 | F1: 0.7059
2026-01-14 12:31:25,542 - INFO - [Metrics for 'normal'] | Precision: 0.7951 | Recall: 0.5330 | F1: 0.6382
2026-01-14 12:31:25,547 - INFO - --------------------------------------------------
2026-01-14 12:31:25,549 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-01-14 12:31:35,641 - INFO - [Train] [59/90] | Loss: 0.4588 | Train Acc: 83.26%
2026-01-14 12:31:37,970 - INFO - [Valid] [59/90] | Loss: 0.5261 | Val Acc: 78.17%
2026-01-14 12:31:37,982 - INFO - [Metrics for 'abnormal'] | Precision: 0.7196 | Recall: 0.8662 | F1: 0.7861
2026-01-14 12:31:37,982 - INFO - [Metrics for 'normal'] | Precision: 0.8600 | Recall: 0.7088 | F1: 0.7771
2026-01-14 12:31:37,986 - INFO - --------------------------------------------------
2026-01-14 12:31:37,989 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-01-14 12:31:46,960 - INFO - [Train] [60/90] | Loss: 0.4025 | Train Acc: 87.05%
2026-01-14 12:31:49,593 - INFO - [Valid] [60/90] | Loss: 0.5089 | Val Acc: 79.35%
2026-01-14 12:31:49,605 - INFO - [Metrics for 'abnormal'] | Precision: 0.7574 | Recall: 0.8153 | F1: 0.7853
2026-01-14 12:31:49,605 - INFO - [Metrics for 'normal'] | Precision: 0.8294 | Recall: 0.7747 | F1: 0.8011
2026-01-14 12:31:49,609 - INFO - --------------------------------------------------
2026-01-14 12:31:49,611 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-01-14 12:31:57,829 - INFO - [Train] [61/90] | Loss: 0.3905 | Train Acc: 87.95%
2026-01-14 12:32:00,727 - INFO - [Valid] [61/90] | Loss: 0.5048 | Val Acc: 78.47%
2026-01-14 12:32:00,774 - INFO - [Metrics for 'abnormal'] | Precision: 0.7838 | Recall: 0.7389 | F1: 0.7607
2026-01-14 12:32:00,774 - INFO - [Metrics for 'normal'] | Precision: 0.7853 | Recall: 0.8242 | F1: 0.8043
2026-01-14 12:32:00,778 - INFO - --------------------------------------------------
2026-01-14 12:32:00,779 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-01-14 12:32:09,200 - INFO - [Train] [62/90] | Loss: 0.3893 | Train Acc: 87.65%
2026-01-14 12:32:11,971 - INFO - [Valid] [62/90] | Loss: 0.5312 | Val Acc: 79.35%
2026-01-14 12:32:11,983 - INFO - [Metrics for 'abnormal'] | Precision: 0.7959 | Recall: 0.7452 | F1: 0.7697
2026-01-14 12:32:11,984 - INFO - [Metrics for 'normal'] | Precision: 0.7917 | Recall: 0.8352 | F1: 0.8128
2026-01-14 12:32:11,990 - INFO - --------------------------------------------------
2026-01-14 12:32:11,993 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-01-14 12:32:20,602 - INFO - [Train] [63/90] | Loss: 0.3758 | Train Acc: 88.91%
2026-01-14 12:32:23,772 - INFO - [Valid] [63/90] | Loss: 0.5010 | Val Acc: 79.94%
2026-01-14 12:32:23,796 - INFO - [Metrics for 'abnormal'] | Precision: 0.7799 | Recall: 0.7898 | F1: 0.7848
2026-01-14 12:32:23,796 - INFO - [Metrics for 'normal'] | Precision: 0.8167 | Recall: 0.8077 | F1: 0.8122
2026-01-14 12:32:23,802 - INFO - --------------------------------------------------
2026-01-14 12:32:23,804 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-01-14 12:32:32,128 - INFO - [Train] [64/90] | Loss: 0.3597 | Train Acc: 89.36%
2026-01-14 12:32:34,739 - INFO - [Valid] [64/90] | Loss: 0.5122 | Val Acc: 80.53%
2026-01-14 12:32:34,750 - INFO - [Metrics for 'abnormal'] | Precision: 0.7791 | Recall: 0.8089 | F1: 0.7937
2026-01-14 12:32:34,750 - INFO - [Metrics for 'normal'] | Precision: 0.8295 | Recall: 0.8022 | F1: 0.8156
2026-01-14 12:32:34,755 - INFO - --------------------------------------------------
2026-01-14 12:32:34,757 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-01-14 12:32:43,831 - INFO - [Train] [65/90] | Loss: 0.3717 | Train Acc: 89.06%
2026-01-14 12:32:46,693 - INFO - [Valid] [65/90] | Loss: 0.4948 | Val Acc: 82.01%
2026-01-14 12:32:46,706 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-01-14 12:32:46,707 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-01-14 12:32:46,711 - INFO - --------------------------------------------------
2026-01-14 12:32:46,715 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-01-14 12:32:55,971 - INFO - [Train] [66/90] | Loss: 0.3635 | Train Acc: 89.51%
2026-01-14 12:32:58,693 - INFO - [Valid] [66/90] | Loss: 0.4867 | Val Acc: 80.24%
2026-01-14 12:32:58,705 - INFO - [Metrics for 'abnormal'] | Precision: 0.7885 | Recall: 0.7834 | F1: 0.7859
2026-01-14 12:32:58,706 - INFO - [Metrics for 'normal'] | Precision: 0.8142 | Recall: 0.8187 | F1: 0.8164
2026-01-14 12:32:58,710 - INFO - --------------------------------------------------
2026-01-14 12:32:58,714 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-01-14 12:33:07,293 - INFO - [Train] [67/90] | Loss: 0.3535 | Train Acc: 90.92%
2026-01-14 12:33:10,207 - INFO - [Valid] [67/90] | Loss: 0.5094 | Val Acc: 80.83%
2026-01-14 12:33:10,240 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-01-14 12:33:10,246 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-01-14 12:33:10,252 - INFO - --------------------------------------------------
2026-01-14 12:33:10,259 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-01-14 12:33:19,059 - INFO - [Train] [68/90] | Loss: 0.3563 | Train Acc: 89.73%
2026-01-14 12:33:21,706 - INFO - [Valid] [68/90] | Loss: 0.5051 | Val Acc: 80.53%
2026-01-14 12:33:21,720 - INFO - [Metrics for 'abnormal'] | Precision: 0.7542 | Recall: 0.8599 | F1: 0.8036
2026-01-14 12:33:21,721 - INFO - [Metrics for 'normal'] | Precision: 0.8625 | Recall: 0.7582 | F1: 0.8070
2026-01-14 12:33:21,725 - INFO - --------------------------------------------------
2026-01-14 12:33:21,728 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-01-14 12:33:30,961 - INFO - [Train] [69/90] | Loss: 0.3536 | Train Acc: 89.96%
2026-01-14 12:33:33,340 - INFO - [Valid] [69/90] | Loss: 0.5147 | Val Acc: 80.24%
2026-01-14 12:33:33,350 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7643 | F1: 0.7818
2026-01-14 12:33:33,351 - INFO - [Metrics for 'normal'] | Precision: 0.8042 | Recall: 0.8352 | F1: 0.8194
2026-01-14 12:33:33,354 - INFO - --------------------------------------------------
2026-01-14 12:33:33,357 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-01-14 12:33:42,530 - INFO - [Train] [70/90] | Loss: 0.3542 | Train Acc: 89.96%
2026-01-14 12:33:45,266 - INFO - [Valid] [70/90] | Loss: 0.5170 | Val Acc: 80.24%
2026-01-14 12:33:45,280 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8025 | F1: 0.7900
2026-01-14 12:33:45,281 - INFO - [Metrics for 'normal'] | Precision: 0.8249 | Recall: 0.8022 | F1: 0.8134
2026-01-14 12:33:45,286 - INFO - --------------------------------------------------
2026-01-14 12:33:45,288 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-01-14 12:33:54,666 - INFO - [Train] [71/90] | Loss: 0.3423 | Train Acc: 90.77%
2026-01-14 12:33:57,080 - INFO - [Valid] [71/90] | Loss: 0.5081 | Val Acc: 81.71%
2026-01-14 12:33:57,108 - INFO - [Metrics for 'abnormal'] | Precision: 0.7914 | Recall: 0.8217 | F1: 0.8063
2026-01-14 12:33:57,108 - INFO - [Metrics for 'normal'] | Precision: 0.8409 | Recall: 0.8132 | F1: 0.8268
2026-01-14 12:33:57,112 - INFO - --------------------------------------------------
2026-01-14 12:33:57,115 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-01-14 12:34:07,724 - INFO - [Train] [72/90] | Loss: 0.3457 | Train Acc: 90.70%
2026-01-14 12:34:09,568 - INFO - [Valid] [72/90] | Loss: 0.5507 | Val Acc: 78.47%
2026-01-14 12:34:09,582 - INFO - [Metrics for 'abnormal'] | Precision: 0.7258 | Recall: 0.8599 | F1: 0.7872
2026-01-14 12:34:09,583 - INFO - [Metrics for 'normal'] | Precision: 0.8562 | Recall: 0.7198 | F1: 0.7821
2026-01-14 12:34:09,588 - INFO - --------------------------------------------------
2026-01-14 12:34:09,591 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-01-14 12:34:19,516 - INFO - [Train] [73/90] | Loss: 0.3487 | Train Acc: 90.03%
2026-01-14 12:34:22,077 - INFO - [Valid] [73/90] | Loss: 0.5346 | Val Acc: 80.83%
2026-01-14 12:34:22,088 - INFO - [Metrics for 'abnormal'] | Precision: 0.7738 | Recall: 0.8280 | F1: 0.8000
2026-01-14 12:34:22,089 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.7912 | F1: 0.8159
2026-01-14 12:34:22,094 - INFO - --------------------------------------------------
2026-01-14 12:34:22,096 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-01-14 12:34:31,647 - INFO - [Train] [74/90] | Loss: 0.3486 | Train Acc: 90.48%
2026-01-14 12:34:34,050 - INFO - [Valid] [74/90] | Loss: 0.5075 | Val Acc: 79.65%
2026-01-14 12:34:34,078 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.7707 | F1: 0.7781
2026-01-14 12:34:34,079 - INFO - [Metrics for 'normal'] | Precision: 0.8054 | Recall: 0.8187 | F1: 0.8120
2026-01-14 12:34:34,083 - INFO - --------------------------------------------------
2026-01-14 12:34:34,084 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-01-14 12:34:43,577 - INFO - [Train] [75/90] | Loss: 0.3382 | Train Acc: 91.44%
2026-01-14 12:34:45,971 - INFO - [Valid] [75/90] | Loss: 0.5052 | Val Acc: 80.24%
2026-01-14 12:34:45,985 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7643 | F1: 0.7818
2026-01-14 12:34:45,986 - INFO - [Metrics for 'normal'] | Precision: 0.8042 | Recall: 0.8352 | F1: 0.8194
2026-01-14 12:34:45,992 - INFO - --------------------------------------------------
2026-01-14 12:34:45,995 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-01-14 12:34:55,033 - INFO - [Train] [76/90] | Loss: 0.3467 | Train Acc: 90.40%
2026-01-14 12:34:57,273 - INFO - [Valid] [76/90] | Loss: 0.5096 | Val Acc: 79.65%
2026-01-14 12:34:57,307 - INFO - [Metrics for 'abnormal'] | Precision: 0.7716 | Recall: 0.7962 | F1: 0.7837
2026-01-14 12:34:57,309 - INFO - [Metrics for 'normal'] | Precision: 0.8192 | Recall: 0.7967 | F1: 0.8078
2026-01-14 12:34:57,318 - INFO - --------------------------------------------------
2026-01-14 12:34:57,324 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-01-14 12:35:07,428 - INFO - [Train] [77/90] | Loss: 0.3367 | Train Acc: 90.92%
2026-01-14 12:35:09,961 - INFO - [Valid] [77/90] | Loss: 0.5197 | Val Acc: 79.65%
2026-01-14 12:35:09,987 - INFO - [Metrics for 'abnormal'] | Precision: 0.7683 | Recall: 0.8025 | F1: 0.7850
2026-01-14 12:35:09,987 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.7912 | F1: 0.8067
2026-01-14 12:35:09,995 - INFO - --------------------------------------------------
2026-01-14 12:35:10,000 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-01-14 12:35:19,315 - INFO - [Train] [78/90] | Loss: 0.3281 | Train Acc: 92.11%
2026-01-14 12:35:22,094 - INFO - [Valid] [78/90] | Loss: 0.5249 | Val Acc: 79.94%
2026-01-14 12:35:22,172 - INFO - [Metrics for 'abnormal'] | Precision: 0.7602 | Recall: 0.8280 | F1: 0.7927
2026-01-14 12:35:22,172 - INFO - [Metrics for 'normal'] | Precision: 0.8393 | Recall: 0.7747 | F1: 0.8057
2026-01-14 12:35:22,176 - INFO - --------------------------------------------------
2026-01-14 12:35:22,179 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-01-14 12:35:30,827 - INFO - [Train] [79/90] | Loss: 0.3262 | Train Acc: 91.96%
2026-01-14 12:35:33,363 - INFO - [Valid] [79/90] | Loss: 0.5331 | Val Acc: 79.65%
2026-01-14 12:35:33,387 - INFO - [Metrics for 'abnormal'] | Precision: 0.7529 | Recall: 0.8344 | F1: 0.7915
2026-01-14 12:35:33,387 - INFO - [Metrics for 'normal'] | Precision: 0.8424 | Recall: 0.7637 | F1: 0.8012
2026-01-14 12:35:33,395 - INFO - --------------------------------------------------
2026-01-14 12:35:33,405 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-01-14 12:35:41,897 - INFO - [Train] [80/90] | Loss: 0.3245 | Train Acc: 92.49%
2026-01-14 12:35:44,385 - INFO - [Valid] [80/90] | Loss: 0.5150 | Val Acc: 79.35%
2026-01-14 12:35:44,394 - INFO - [Metrics for 'abnormal'] | Precision: 0.7636 | Recall: 0.8025 | F1: 0.7826
2026-01-14 12:35:44,394 - INFO - [Metrics for 'normal'] | Precision: 0.8218 | Recall: 0.7857 | F1: 0.8034
2026-01-14 12:35:44,397 - INFO - --------------------------------------------------
2026-01-14 12:35:44,399 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-01-14 12:35:55,321 - INFO - [Train] [81/90] | Loss: 0.3268 | Train Acc: 92.34%
2026-01-14 12:35:57,714 - INFO - [Valid] [81/90] | Loss: 0.5229 | Val Acc: 80.53%
2026-01-14 12:35:57,726 - INFO - [Metrics for 'abnormal'] | Precision: 0.7898 | Recall: 0.7898 | F1: 0.7898
2026-01-14 12:35:57,727 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8187 | F1: 0.8187
2026-01-14 12:35:57,731 - INFO - --------------------------------------------------
2026-01-14 12:35:57,734 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-01-14 12:36:05,167 - INFO - [Train] [82/90] | Loss: 0.3327 | Train Acc: 91.82%
2026-01-14 12:36:07,642 - INFO - [Valid] [82/90] | Loss: 0.5216 | Val Acc: 80.53%
2026-01-14 12:36:07,658 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-01-14 12:36:07,660 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-01-14 12:36:07,664 - INFO - --------------------------------------------------
2026-01-14 12:36:07,666 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-01-14 12:36:13,848 - INFO - [Train] [83/90] | Loss: 0.3226 | Train Acc: 92.34%
2026-01-14 12:36:15,600 - INFO - [Valid] [83/90] | Loss: 0.5198 | Val Acc: 79.94%
2026-01-14 12:36:15,614 - INFO - [Metrics for 'abnormal'] | Precision: 0.7764 | Recall: 0.7962 | F1: 0.7862
2026-01-14 12:36:15,615 - INFO - [Metrics for 'normal'] | Precision: 0.8202 | Recall: 0.8022 | F1: 0.8111
2026-01-14 12:36:15,619 - INFO - --------------------------------------------------
2026-01-14 12:36:15,621 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-01-14 12:36:21,669 - INFO - [Train] [84/90] | Loss: 0.3261 | Train Acc: 91.96%
2026-01-14 12:36:24,941 - INFO - [Valid] [84/90] | Loss: 0.5268 | Val Acc: 79.94%
2026-01-14 12:36:24,961 - INFO - [Metrics for 'abnormal'] | Precision: 0.7764 | Recall: 0.7962 | F1: 0.7862
2026-01-14 12:36:24,965 - INFO - [Metrics for 'normal'] | Precision: 0.8202 | Recall: 0.8022 | F1: 0.8111
2026-01-14 12:36:24,971 - INFO - --------------------------------------------------
2026-01-14 12:36:24,973 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-01-14 12:36:31,008 - INFO - [Train] [85/90] | Loss: 0.3153 | Train Acc: 92.78%
2026-01-14 12:36:34,039 - INFO - [Valid] [85/90] | Loss: 0.5479 | Val Acc: 79.65%
2026-01-14 12:36:34,049 - INFO - [Metrics for 'abnormal'] | Precision: 0.7716 | Recall: 0.7962 | F1: 0.7837
2026-01-14 12:36:34,050 - INFO - [Metrics for 'normal'] | Precision: 0.8192 | Recall: 0.7967 | F1: 0.8078
2026-01-14 12:36:34,054 - INFO - --------------------------------------------------
2026-01-14 12:36:34,056 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-01-14 12:36:43,826 - INFO - [Train] [86/90] | Loss: 0.3236 | Train Acc: 92.19%
2026-01-14 12:36:45,653 - INFO - [Valid] [86/90] | Loss: 0.5423 | Val Acc: 80.53%
2026-01-14 12:36:45,665 - INFO - [Metrics for 'abnormal'] | Precision: 0.7661 | Recall: 0.8344 | F1: 0.7988
2026-01-14 12:36:45,665 - INFO - [Metrics for 'normal'] | Precision: 0.8452 | Recall: 0.7802 | F1: 0.8114
2026-01-14 12:36:45,669 - INFO - --------------------------------------------------
2026-01-14 12:36:45,672 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-01-14 12:36:50,951 - INFO - [Train] [87/90] | Loss: 0.3265 | Train Acc: 92.19%
2026-01-14 12:36:52,426 - INFO - [Valid] [87/90] | Loss: 0.5532 | Val Acc: 79.35%
2026-01-14 12:36:52,434 - INFO - [Metrics for 'abnormal'] | Precision: 0.7544 | Recall: 0.8217 | F1: 0.7866
2026-01-14 12:36:52,434 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7692 | F1: 0.8000
2026-01-14 12:36:52,436 - INFO - --------------------------------------------------
2026-01-14 12:36:52,438 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-01-14 12:36:57,511 - INFO - [Train] [88/90] | Loss: 0.3197 | Train Acc: 92.78%
2026-01-14 12:36:59,141 - INFO - [Valid] [88/90] | Loss: 0.5252 | Val Acc: 79.35%
2026-01-14 12:36:59,153 - INFO - [Metrics for 'abnormal'] | Precision: 0.7736 | Recall: 0.7834 | F1: 0.7785
2026-01-14 12:36:59,153 - INFO - [Metrics for 'normal'] | Precision: 0.8111 | Recall: 0.8022 | F1: 0.8066
2026-01-14 12:36:59,157 - INFO - --------------------------------------------------
2026-01-14 12:36:59,159 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-01-14 12:37:04,015 - INFO - [Train] [89/90] | Loss: 0.3261 | Train Acc: 91.96%
2026-01-14 12:37:05,486 - INFO - [Valid] [89/90] | Loss: 0.5271 | Val Acc: 80.24%
2026-01-14 12:37:05,498 - INFO - [Metrics for 'abnormal'] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-01-14 12:37:05,499 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-01-14 12:37:05,504 - INFO - --------------------------------------------------
2026-01-14 12:37:05,507 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-01-14 12:37:10,484 - INFO - [Train] [90/90] | Loss: 0.3205 | Train Acc: 92.86%
2026-01-14 12:37:11,976 - INFO - [Valid] [90/90] | Loss: 0.5287 | Val Acc: 80.53%
2026-01-14 12:37:11,989 - INFO - [Metrics for 'abnormal'] | Precision: 0.7725 | Recall: 0.8217 | F1: 0.7963
2026-01-14 12:37:11,990 - INFO - [Metrics for 'normal'] | Precision: 0.8372 | Recall: 0.7912 | F1: 0.8136
2026-01-14 12:37:11,995 - INFO - ==================================================
2026-01-14 12:37:11,996 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-01-14 12:37:11,996 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-01-14 12:37:11,997 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:37:12,910 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-01-14 12:37:12,911 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-01-14 12:37:12,913 - INFO - FPGM Pruning을 시작합니다.
2026-01-14 12:37:12,913 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:37:12,914 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-01-14 12:37:13,364 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-01-14 12:37:13,364 - INFO - ==================================================
2026-01-14 12:37:13,407 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/best_model.pth'
2026-01-14 12:37:13,408 - INFO - ==================================================
2026-01-14 12:37:13,408 - INFO - Test 모드를 시작합니다.
2026-01-14 12:37:13,540 - INFO - 연산량 (MACs): 0.0920 GMACs per sample
2026-01-14 12:37:13,541 - INFO - 연산량 (FLOPs): 0.1840 GFLOPs per sample
2026-01-14 12:37:13,541 - INFO - ==================================================
2026-01-14 12:37:13,541 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-01-14 12:37:14,767 - INFO - 샘플 당 평균 Forward Pass 시간: 6.14ms (std: 1.69ms), FPS: 173.87 (std: 43.65) (1개 샘플 x 100회 반복)
2026-01-14 12:37:14,768 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 112.93 MB
2026-01-14 12:37:14,768 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-01-14 12:37:17,108 - INFO - [Test] Loss: 0.4229 | Test Acc: 80.83%
2026-01-14 12:37:17,118 - INFO - [Metrics for 'abnormal'] | Precision: 0.8108 | Recall: 0.7643 | F1: 0.7869
2026-01-14 12:37:17,118 - INFO - [Metrics for 'normal'] | Precision: 0.8063 | Recall: 0.8462 | F1: 0.8257
2026-01-14 12:37:17,646 - INFO - ==================================================
2026-01-14 12:37:17,647 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/confusion_matrix_20260114_121857.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/confusion_matrix_20260114_121857.pdf'
2026-01-14 12:37:17,647 - INFO - ==================================================
2026-01-14 12:37:17,647 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-01-14 12:37:19,558 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/model_fp32_20260114_121857.onnx'에 저장되었습니다. (크기: 1.99 MB)
2026-01-14 12:37:19,997 - INFO - [Model Load] ONNX 모델(FP32) 로드 메모리: 2393.72 MB (증가량: 6.94 MB)
2026-01-14 12:37:19,999 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-01-14 12:37:21,747 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 12.39ms (std: 6.65ms)
2026-01-14 12:37:21,747 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 93.65 FPS (std: 28.99) (1개 샘플 x 100회 반복)
2026-01-14 12:37:21,748 - INFO - [Inference Only] ONNX 런타임 추론 중 최대 CPU 메모리: 2396.78 MB (순수 증가량: 3.06 MB)
2026-01-14 12:37:21,748 - INFO - [Total Process] ONNX 모델(FP32) 전체 메모리 사용량: 2396.78 MB (전체 증가량: 10.00 MB)
2026-01-14 12:37:27,352 - INFO - [Test (ONNX)] | Test Acc (ONNX): 80.83%
2026-01-14 12:37:27,366 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8108 | Recall: 0.7643 | F1: 0.7869
2026-01-14 12:37:27,367 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8063 | Recall: 0.8462 | F1: 0.8257
2026-01-14 12:37:27,875 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/val_acc.pdf'
2026-01-14 12:37:28,330 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/train_val_acc.pdf'
2026-01-14 12:37:28,787 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/F1_normal.pdf'
2026-01-14 12:37:29,349 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/val_loss.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/val_loss.pdf'
2026-01-14 12:37:29,801 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/learning_rate.pdf'
2026-01-14 12:37:34,273 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/compile.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260114_121857/graph_20260114_121857/compile.pdf'
