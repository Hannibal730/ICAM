2026-01-14 12:39:44,092 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/log_20260114_123944.log'에 저장됩니다.
2026-01-14 12:39:44,100 - INFO - ==================================================
2026-01-14 12:39:44,100 - INFO - config.yaml:
2026-01-14 12:39:44,101 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_inference_dir: ./pretrained
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      train_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/cau/workspace/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  patch_size: 56
  stride: 56
  cnn_feature_extractor:
    name: efficientnet_b0_feat2
  featured_patch_dim: 24
  emb_dim: 24
  num_heads: 2
  num_decoder_layers: 2
  num_decoder_patches: 1
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: true
  save_attention: true
  num_plot_attention: 5
baseline:
  model_name: mobilenet_v4_s
  use_wanda_pruning: true
  num_wanda_calib_samples: 1353
  pruning_flops_target: 0.1829

2026-01-14 12:39:44,102 - INFO - ==================================================
2026-01-14 12:39:44,156 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA RTX PRO 6000 Blackwell Server Edition)
2026-01-14 12:39:44,157 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-01-14 12:39:44,157 - INFO - '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-01-14 12:39:44,168 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-01-14 12:39:44,168 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-01-14 12:39:44,169 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-01-14 12:39:44,169 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:39:44,429 - INFO - ==================================================
2026-01-14 12:39:44,430 - INFO - 모델 파라미터 수:
2026-01-14 12:39:44,430 - INFO -   - 총 파라미터: 2,495,586 개
2026-01-14 12:39:44,430 - INFO -   - 학습 가능한 파라미터: 2,495,586 개
2026-01-14 12:39:44,431 - INFO - ================================================================================
2026-01-14 12:39:44,431 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-01-14 12:39:44,431 - INFO - ================================================================================
2026-01-14 12:39:44,431 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:39:44,432 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-01-14 12:39:44,432 - INFO - ==================================================
2026-01-14 12:39:44,433 - INFO - train 모드를 시작합니다.
2026-01-14 12:39:44,433 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:39:44,433 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:39:44,433 - INFO - --------------------------------------------------
2026-01-14 12:39:44,434 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-01-14 12:39:50,561 - INFO - [Train] [1/10] | Loss: 2.9033 | Train Acc: 65.70%
2026-01-14 12:39:53,402 - INFO - [Valid] [1/10] | Loss: 1.1888 | Val Acc: 62.83%
2026-01-14 12:39:53,417 - INFO - [Metrics for 'abnormal'] | Precision: 0.6824 | Recall: 0.3694 | F1: 0.4793
2026-01-14 12:39:53,417 - INFO - [Metrics for 'normal'] | Precision: 0.6102 | Recall: 0.8516 | F1: 0.7110
2026-01-14 12:39:53,468 - INFO - [Best Model Saved] (val loss: 1.1888) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:39:53,469 - INFO - --------------------------------------------------
2026-01-14 12:39:53,471 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-01-14 12:39:58,428 - INFO - [Train] [2/10] | Loss: 0.7415 | Train Acc: 70.54%
2026-01-14 12:40:00,139 - INFO - [Valid] [2/10] | Loss: 0.6254 | Val Acc: 74.34%
2026-01-14 12:40:00,149 - INFO - [Metrics for 'abnormal'] | Precision: 0.7823 | Recall: 0.6178 | F1: 0.6904
2026-01-14 12:40:00,149 - INFO - [Metrics for 'normal'] | Precision: 0.7209 | Recall: 0.8516 | F1: 0.7809
2026-01-14 12:40:00,202 - INFO - [Best Model Saved] (val loss: 0.6254) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:40:00,203 - INFO - --------------------------------------------------
2026-01-14 12:40:00,206 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-01-14 12:40:05,933 - INFO - [Train] [3/10] | Loss: 0.6828 | Train Acc: 73.66%
2026-01-14 12:40:07,746 - INFO - [Valid] [3/10] | Loss: 0.6367 | Val Acc: 73.16%
2026-01-14 12:40:07,756 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.5605 | F1: 0.6592
2026-01-14 12:40:07,757 - INFO - [Metrics for 'normal'] | Precision: 0.6987 | Recall: 0.8791 | F1: 0.7786
2026-01-14 12:40:07,761 - INFO - --------------------------------------------------
2026-01-14 12:40:07,763 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-01-14 12:40:14,749 - INFO - [Train] [4/10] | Loss: 0.5912 | Train Acc: 75.52%
2026-01-14 12:40:17,031 - INFO - [Valid] [4/10] | Loss: 0.7026 | Val Acc: 69.03%
2026-01-14 12:40:17,042 - INFO - [Metrics for 'abnormal'] | Precision: 0.6831 | Recall: 0.6178 | F1: 0.6488
2026-01-14 12:40:17,043 - INFO - [Metrics for 'normal'] | Precision: 0.6954 | Recall: 0.7527 | F1: 0.7230
2026-01-14 12:40:17,047 - INFO - --------------------------------------------------
2026-01-14 12:40:17,050 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-01-14 12:40:25,097 - INFO - [Train] [5/10] | Loss: 0.6106 | Train Acc: 74.40%
2026-01-14 12:40:27,424 - INFO - [Valid] [5/10] | Loss: 0.6907 | Val Acc: 68.14%
2026-01-14 12:40:27,434 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.4076 | F1: 0.5424
2026-01-14 12:40:27,434 - INFO - [Metrics for 'normal'] | Precision: 0.6423 | Recall: 0.9176 | F1: 0.7557
2026-01-14 12:40:27,438 - INFO - --------------------------------------------------
2026-01-14 12:40:27,440 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-01-14 12:40:36,775 - INFO - [Train] [6/10] | Loss: 0.5258 | Train Acc: 79.99%
2026-01-14 12:40:38,751 - INFO - [Valid] [6/10] | Loss: 0.6006 | Val Acc: 78.47%
2026-01-14 12:40:38,760 - INFO - [Metrics for 'abnormal'] | Precision: 0.7333 | Recall: 0.8408 | F1: 0.7834
2026-01-14 12:40:38,760 - INFO - [Metrics for 'normal'] | Precision: 0.8428 | Recall: 0.7363 | F1: 0.7859
2026-01-14 12:40:38,811 - INFO - [Best Model Saved] (val loss: 0.6006) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:40:38,812 - INFO - --------------------------------------------------
2026-01-14 12:40:38,813 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-01-14 12:40:49,788 - INFO - [Train] [7/10] | Loss: 0.5135 | Train Acc: 80.21%
2026-01-14 12:40:51,718 - INFO - [Valid] [7/10] | Loss: 0.6269 | Val Acc: 69.91%
2026-01-14 12:40:51,727 - INFO - [Metrics for 'abnormal'] | Precision: 0.6267 | Recall: 0.8662 | F1: 0.7273
2026-01-14 12:40:51,728 - INFO - [Metrics for 'normal'] | Precision: 0.8279 | Recall: 0.5549 | F1: 0.6645
2026-01-14 12:40:51,732 - INFO - --------------------------------------------------
2026-01-14 12:40:51,735 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-01-14 12:41:01,608 - INFO - [Train] [8/10] | Loss: 0.4772 | Train Acc: 81.92%
2026-01-14 12:41:03,848 - INFO - [Valid] [8/10] | Loss: 0.6294 | Val Acc: 72.86%
2026-01-14 12:41:03,860 - INFO - [Metrics for 'abnormal'] | Precision: 0.6419 | Recall: 0.9363 | F1: 0.7617
2026-01-14 12:41:03,860 - INFO - [Metrics for 'normal'] | Precision: 0.9091 | Recall: 0.5495 | F1: 0.6849
2026-01-14 12:41:03,865 - INFO - --------------------------------------------------
2026-01-14 12:41:03,868 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-01-14 12:41:12,080 - INFO - [Train] [9/10] | Loss: 0.4454 | Train Acc: 84.15%
2026-01-14 12:41:14,707 - INFO - [Valid] [9/10] | Loss: 0.5438 | Val Acc: 76.99%
2026-01-14 12:41:14,718 - INFO - [Metrics for 'abnormal'] | Precision: 0.8264 | Recall: 0.6369 | F1: 0.7194
2026-01-14 12:41:14,718 - INFO - [Metrics for 'normal'] | Precision: 0.7385 | Recall: 0.8846 | F1: 0.8050
2026-01-14 12:41:14,791 - INFO - [Best Model Saved] (val loss: 0.5438) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:41:14,791 - INFO - --------------------------------------------------
2026-01-14 12:41:14,793 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-01-14 12:41:22,348 - INFO - [Train] [10/10] | Loss: 0.4464 | Train Acc: 85.04%
2026-01-14 12:41:25,512 - INFO - [Valid] [10/10] | Loss: 0.5281 | Val Acc: 79.06%
2026-01-14 12:41:25,528 - INFO - [Metrics for 'abnormal'] | Precision: 0.8209 | Recall: 0.7006 | F1: 0.7560
2026-01-14 12:41:25,529 - INFO - [Metrics for 'normal'] | Precision: 0.7707 | Recall: 0.8681 | F1: 0.8165
2026-01-14 12:41:25,589 - INFO - [Best Model Saved] (val loss: 0.5281) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:41:25,590 - INFO - ================================================================================
2026-01-14 12:41:25,590 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-01-14 12:41:25,591 - INFO - ================================================================================
2026-01-14 12:41:25,679 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'을(를) 불러왔습니다.
2026-01-14 12:41:25,679 - INFO - ================================================================================
2026-01-14 12:41:25,679 - INFO - 목표 FLOPs (0.1829 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-01-14 12:41:25,782 - INFO - 원본 모델 FLOPs: 0.3853 GFLOPs
2026-01-14 12:41:25,916 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:41:25,917 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:41:25,920 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:41:34,394 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:41:34,825 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-01-14 12:41:34,826 - INFO - ==================================================
2026-01-14 12:41:35,275 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.1092 GFLOPs (감소율: 71.66%)
2026-01-14 12:41:35,368 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:41:35,368 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:41:35,373 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:41:44,284 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:41:44,835 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.2475)에 맞춰 변경되었습니다.
2026-01-14 12:41:44,836 - INFO - ==================================================
2026-01-14 12:41:44,899 - INFO -   [탐색  2] 희소도: 0.2475 -> FLOPs: 0.2263 GFLOPs (감소율: 41.26%)
2026-01-14 12:41:44,953 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:41:44,954 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:41:44,958 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:41:54,309 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:41:54,488 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.37124999999999997)에 맞춰 변경되었습니다.
2026-01-14 12:41:54,489 - INFO - ==================================================
2026-01-14 12:41:54,572 - INFO -   [탐색  3] 희소도: 0.3712 -> FLOPs: 0.1626 GFLOPs (감소율: 57.81%)
2026-01-14 12:41:54,626 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:41:54,627 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:41:54,629 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:04,327 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:04,653 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.30937499999999996)에 맞춰 변경되었습니다.
2026-01-14 12:42:04,654 - INFO - ==================================================
2026-01-14 12:42:04,711 - INFO -   [탐색  4] 희소도: 0.3094 -> FLOPs: 0.1932 GFLOPs (감소율: 49.86%)
2026-01-14 12:42:04,801 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:04,801 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:04,804 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:13,438 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:13,594 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.34031249999999996)에 맞춰 변경되었습니다.
2026-01-14 12:42:13,596 - INFO - ==================================================
2026-01-14 12:42:13,669 - INFO -   [탐색  5] 희소도: 0.3403 -> FLOPs: 0.1775 GFLOPs (감소율: 53.92%)
2026-01-14 12:42:13,726 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:13,726 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:13,729 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:22,868 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:23,018 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32484375)에 맞춰 변경되었습니다.
2026-01-14 12:42:23,019 - INFO - ==================================================
2026-01-14 12:42:23,093 - INFO -   [탐색  6] 희소도: 0.3248 -> FLOPs: 0.1825 GFLOPs (감소율: 52.64%)
2026-01-14 12:42:23,504 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:23,505 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:23,508 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:31,422 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:31,621 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.31710937499999997)에 맞춰 변경되었습니다.
2026-01-14 12:42:31,622 - INFO - ==================================================
2026-01-14 12:42:31,682 - INFO -   [탐색  7] 희소도: 0.3171 -> FLOPs: 0.1856 GFLOPs (감소율: 51.83%)
2026-01-14 12:42:31,748 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:31,748 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:31,752 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:40,358 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:40,517 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32097656249999995)에 맞춰 변경되었습니다.
2026-01-14 12:42:40,517 - INFO - ==================================================
2026-01-14 12:42:40,597 - INFO -   [탐색  8] 희소도: 0.3210 -> FLOPs: 0.1844 GFLOPs (감소율: 52.14%)
2026-01-14 12:42:40,672 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:40,672 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:40,676 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:49,823 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:50,050 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291015624999997)에 맞춰 변경되었습니다.
2026-01-14 12:42:50,050 - INFO - ==================================================
2026-01-14 12:42:50,109 - INFO -   [탐색  9] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:42:50,172 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:50,176 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:50,179 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:57,871 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:58,115 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.323876953125)에 맞춰 변경되었습니다.
2026-01-14 12:42:58,117 - INFO - ==================================================
2026-01-14 12:42:58,258 - INFO -   [탐색 10] 희소도: 0.3239 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:42:58,609 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:58,609 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:58,612 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:07,563 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:07,765 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3233935546875)에 맞춰 변경되었습니다.
2026-01-14 12:43:07,765 - INFO - ==================================================
2026-01-14 12:43:07,813 - INFO -   [탐색 11] 희소도: 0.3234 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:43:07,891 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:07,892 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:07,894 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:16,873 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:17,025 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32315185546874997)에 맞춰 변경되었습니다.
2026-01-14 12:43:17,025 - INFO - ==================================================
2026-01-14 12:43:17,086 - INFO -   [탐색 12] 희소도: 0.3232 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:43:17,145 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:17,146 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:17,149 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:27,419 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:27,630 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32303100585937494)에 맞춰 변경되었습니다.
2026-01-14 12:43:27,631 - INFO - ==================================================
2026-01-14 12:43:27,709 - INFO -   [탐색 13] 희소도: 0.3230 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:43:27,819 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:27,828 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:27,833 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:36,014 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:36,304 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32297058105468746)에 맞춰 변경되었습니다.
2026-01-14 12:43:36,304 - INFO - ==================================================
2026-01-14 12:43:36,365 - INFO -   [탐색 14] 희소도: 0.3230 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:43:36,820 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:36,824 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:36,827 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:45,190 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:45,380 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229403686523437)에 맞춰 변경되었습니다.
2026-01-14 12:43:45,381 - INFO - ==================================================
2026-01-14 12:43:45,444 - INFO -   [탐색 15] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:43:45,521 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:45,521 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:45,525 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:54,605 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:54,832 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229252624511718)에 맞춰 변경되었습니다.
2026-01-14 12:43:54,833 - INFO - ==================================================
2026-01-14 12:43:54,895 - INFO -   [탐색 16] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:43:54,958 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:54,959 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:54,963 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:03,887 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:04,043 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229177093505859)에 맞춰 변경되었습니다.
2026-01-14 12:44:04,043 - INFO - ==================================================
2026-01-14 12:44:04,101 - INFO -   [탐색 17] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:44:04,631 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:04,632 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:04,636 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:12,396 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:12,542 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229139328002929)에 맞춰 변경되었습니다.
2026-01-14 12:44:12,543 - INFO - ==================================================
2026-01-14 12:44:12,589 - INFO -   [탐색 18] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:44:12,648 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:12,649 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:12,652 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:21,822 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:21,978 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291582107543937)에 맞춰 변경되었습니다.
2026-01-14 12:44:21,979 - INFO - ==================================================
2026-01-14 12:44:22,035 - INFO -   [탐색 19] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:44:22,103 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:22,104 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:22,108 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:30,846 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:31,040 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229167652130126)에 맞춰 변경되었습니다.
2026-01-14 12:44:31,041 - INFO - ==================================================
2026-01-14 12:44:31,102 - INFO -   [탐색 20] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:44:31,170 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:31,171 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:31,175 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:40,642 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:40,815 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.322916293144226)에 맞춰 변경되었습니다.
2026-01-14 12:44:40,816 - INFO - ==================================================
2026-01-14 12:44:40,875 - INFO -   [탐색 21] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:44:40,950 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:40,951 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:40,955 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:49,640 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:49,935 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229165291786193)에 맞춰 변경되었습니다.
2026-01-14 12:44:49,936 - INFO - ==================================================
2026-01-14 12:44:50,028 - INFO -   [탐색 22] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:44:50,102 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:50,103 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:50,107 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:59,586 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:59,744 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166471958159)에 맞춰 변경되었습니다.
2026-01-14 12:44:59,744 - INFO - ==================================================
2026-01-14 12:44:59,801 - INFO -   [탐색 23] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:44:59,868 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:59,869 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:59,872 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:08,572 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:08,734 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291670620441426)에 맞춰 변경되었습니다.
2026-01-14 12:45:08,735 - INFO - ==================================================
2026-01-14 12:45:08,782 - INFO -   [탐색 24] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:45:08,847 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:08,847 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:08,851 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:19,457 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:19,756 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166767001151)에 맞춰 변경되었습니다.
2026-01-14 12:45:19,756 - INFO - ==================================================
2026-01-14 12:45:19,857 - INFO -   [탐색 25] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:45:19,911 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:19,912 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:19,915 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:28,209 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:28,410 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666194796553)에 맞춰 변경되었습니다.
2026-01-14 12:45:28,411 - INFO - ==================================================
2026-01-14 12:45:28,467 - INFO -   [탐색 26] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:45:28,522 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:28,522 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:28,525 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:37,863 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:38,005 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166693240403)에 맞춰 변경되었습니다.
2026-01-14 12:45:38,005 - INFO - ==================================================
2026-01-14 12:45:38,059 - INFO -   [탐색 27] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:45:38,117 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:38,117 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:38,119 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:47,405 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:47,656 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166656360029)에 맞춰 변경되었습니다.
2026-01-14 12:45:47,656 - INFO - ==================================================
2026-01-14 12:45:47,726 - INFO -   [탐색 28] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:45:47,810 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:47,810 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:47,814 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:55,838 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:56,329 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666748002157)에 맞춰 변경되었습니다.
2026-01-14 12:45:56,329 - INFO - ==================================================
2026-01-14 12:45:56,860 - INFO -   [탐색 29] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:45:56,956 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:56,956 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:56,960 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:05,917 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:06,212 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166665580122)에 맞춰 변경되었습니다.
2026-01-14 12:46:06,212 - INFO - ==================================================
2026-01-14 12:46:06,269 - INFO -   [탐색 30] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:46:06,330 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:06,330 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:06,333 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:15,444 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:15,735 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166670190169)에 맞춰 변경되었습니다.
2026-01-14 12:46:15,735 - INFO - ==================================================
2026-01-14 12:46:15,802 - INFO -   [탐색 31] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:46:15,877 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:15,878 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:15,882 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:26,096 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:26,282 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666678851455)에 맞춰 변경되었습니다.
2026-01-14 12:46:26,283 - INFO - ==================================================
2026-01-14 12:46:26,323 - INFO -   [탐색 32] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:46:26,404 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:26,405 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:26,407 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:36,452 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:36,614 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666667326335)에 맞춰 변경되었습니다.
2026-01-14 12:46:36,614 - INFO - ==================================================
2026-01-14 12:46:36,672 - INFO -   [탐색 33] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:46:36,744 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:36,745 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:36,749 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:46,764 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:46,900 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666156378)에 맞춰 변경되었습니다.
2026-01-14 12:46:46,900 - INFO - ==================================================
2026-01-14 12:46:46,944 - INFO -   [탐색 34] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:46:47,020 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:47,020 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:47,024 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:56,785 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:56,962 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666664445057)에 맞춰 변경되었습니다.
2026-01-14 12:46:56,962 - INFO - ==================================================
2026-01-14 12:46:57,023 - INFO -   [탐색 35] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:46:57,079 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:57,079 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:57,081 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:06,081 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:06,231 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.322916666658857)에 맞춰 변경되었습니다.
2026-01-14 12:47:06,231 - INFO - ==================================================
2026-01-14 12:47:06,269 - INFO -   [탐색 36] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:47:06,320 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:06,321 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:06,324 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:16,366 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:16,523 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666606017)에 맞춰 변경되었습니다.
2026-01-14 12:47:16,523 - INFO - ==================================================
2026-01-14 12:47:16,580 - INFO -   [탐색 37] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:47:16,649 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:16,650 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:16,653 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:25,683 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:25,837 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666696618)에 맞춰 변경되었습니다.
2026-01-14 12:47:25,838 - INFO - ==================================================
2026-01-14 12:47:25,897 - INFO -   [탐색 38] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:47:25,971 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:25,971 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:25,974 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:36,103 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:36,557 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.322916666667861)에 맞춰 변경되었습니다.
2026-01-14 12:47:36,558 - INFO - ==================================================
2026-01-14 12:47:36,613 - INFO -   [탐색 39] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:47:36,665 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:36,666 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:36,670 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:45,922 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:46,165 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666669606)에 맞춰 변경되었습니다.
2026-01-14 12:47:46,165 - INFO - ==================================================
2026-01-14 12:47:46,214 - INFO -   [탐색 40] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:47:46,270 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:46,271 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:46,273 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:54,194 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:54,374 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666665104)에 맞춰 변경되었습니다.
2026-01-14 12:47:54,375 - INFO - ==================================================
2026-01-14 12:47:54,437 - INFO -   [탐색 41] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:47:54,513 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:54,514 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:54,517 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:03,176 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:03,449 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666667355)에 맞춰 변경되었습니다.
2026-01-14 12:48:03,449 - INFO - ==================================================
2026-01-14 12:48:03,500 - INFO -   [탐색 42] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:48:03,569 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:03,569 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:03,573 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:11,952 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:12,213 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.322916666666623)에 맞춰 변경되었습니다.
2026-01-14 12:48:12,214 - INFO - ==================================================
2026-01-14 12:48:12,339 - INFO -   [탐색 43] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:48:12,432 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:12,433 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:12,437 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:21,236 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:21,587 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666666793)에 맞춰 변경되었습니다.
2026-01-14 12:48:21,588 - INFO - ==================================================
2026-01-14 12:48:21,644 - INFO -   [탐색 44] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:48:21,711 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:21,712 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:21,716 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:31,036 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:31,149 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666665114)에 맞춰 변경되었습니다.
2026-01-14 12:48:31,150 - INFO - ==================================================
2026-01-14 12:48:31,207 - INFO -   [탐색 45] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:48:31,278 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:31,278 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:31,281 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:39,731 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:39,856 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666666652)에 맞춰 변경되었습니다.
2026-01-14 12:48:39,857 - INFO - ==================================================
2026-01-14 12:48:39,901 - INFO -   [탐색 46] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:48:39,953 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:39,953 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:39,956 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:48,297 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:48,565 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666667224)에 맞춰 변경되었습니다.
2026-01-14 12:48:48,566 - INFO - ==================================================
2026-01-14 12:48:48,631 - INFO -   [탐색 47] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:48:48,702 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:48,703 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:48,707 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:57,600 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:57,816 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666874)에 맞춰 변경되었습니다.
2026-01-14 12:48:57,816 - INFO - ==================================================
2026-01-14 12:48:58,031 - INFO -   [탐색 48] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:48:58,155 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:58,155 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:58,159 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:06,886 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:07,187 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666696)에 맞춰 변경되었습니다.
2026-01-14 12:49:07,188 - INFO - ==================================================
2026-01-14 12:49:07,262 - INFO -   [탐색 49] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:49:07,644 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:07,645 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:07,648 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:16,473 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:16,645 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666666661)에 맞춰 변경되었습니다.
2026-01-14 12:49:16,646 - INFO - ==================================================
2026-01-14 12:49:16,728 - INFO -   [탐색 50] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:49:16,866 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:16,867 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:16,871 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:24,977 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:25,166 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666666665)에 맞춰 변경되었습니다.
2026-01-14 12:49:25,167 - INFO - ==================================================
2026-01-14 12:49:25,225 - INFO -   [탐색 51] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.18%)
2026-01-14 12:49:25,296 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:25,296 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:25,300 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:34,132 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:34,294 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666674)에 맞춰 변경되었습니다.
2026-01-14 12:49:34,294 - INFO - ==================================================
2026-01-14 12:49:34,354 - INFO -   [탐색 52] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.19%)
2026-01-14 12:49:34,420 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:34,422 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:34,425 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:43,462 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:43,623 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:49:43,624 - INFO - ==================================================
2026-01-14 12:49:43,679 - INFO -   [탐색 53] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:49:43,749 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:43,750 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:43,754 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:51,987 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:52,157 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3229166666666668)에 맞춰 변경되었습니다.
2026-01-14 12:49:52,157 - INFO - ==================================================
2026-01-14 12:49:52,207 - INFO -   [탐색 54] 희소도: 0.3229 -> FLOPs: 0.1842 GFLOPs (감소율: 52.19%)
2026-01-14 12:49:52,259 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:52,260 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:52,263 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:00,426 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:00,580 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:50:00,580 - INFO - ==================================================
2026-01-14 12:50:00,650 - INFO -   [탐색 55] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:50:00,758 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:00,758 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:00,762 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:10,242 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:10,435 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:50:10,435 - INFO - ==================================================
2026-01-14 12:50:10,491 - INFO -   [탐색 56] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:50:10,559 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:10,560 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:10,564 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:18,657 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:18,841 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:50:18,842 - INFO - ==================================================
2026-01-14 12:50:18,895 - INFO -   [탐색 57] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:50:18,969 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:18,970 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:18,975 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:28,356 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:28,499 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:50:28,499 - INFO - ==================================================
2026-01-14 12:50:28,554 - INFO -   [탐색 58] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:50:28,625 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:28,626 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:28,631 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:38,189 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:38,413 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:50:38,414 - INFO - ==================================================
2026-01-14 12:50:38,482 - INFO -   [탐색 59] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:50:38,530 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:38,530 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:38,532 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:47,032 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:47,186 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:50:47,186 - INFO - ==================================================
2026-01-14 12:50:47,246 - INFO -   [탐색 60] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:50:47,613 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:47,614 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:47,616 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:57,323 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:57,656 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:50:57,657 - INFO - ==================================================
2026-01-14 12:50:57,738 - INFO -   [탐색 61] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:50:57,826 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:57,826 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:57,831 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:07,410 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:07,618 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:51:07,619 - INFO - ==================================================
2026-01-14 12:51:07,679 - INFO -   [탐색 62] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:51:07,758 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:07,758 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:07,762 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:16,240 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:16,419 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:51:16,420 - INFO - ==================================================
2026-01-14 12:51:16,481 - INFO -   [탐색 63] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:51:16,555 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:16,555 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:16,559 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:25,617 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:25,786 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:51:25,787 - INFO - ==================================================
2026-01-14 12:51:25,840 - INFO -   [탐색 64] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:51:25,911 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:25,912 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:25,916 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:34,463 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:34,604 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:51:34,605 - INFO - ==================================================
2026-01-14 12:51:34,682 - INFO -   [탐색 65] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:51:34,744 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:34,744 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:34,749 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:44,056 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:44,290 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:51:44,291 - INFO - ==================================================
2026-01-14 12:51:44,339 - INFO -   [탐색 66] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:51:44,399 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:44,400 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:44,404 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:53,343 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:53,779 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:51:53,779 - INFO - ==================================================
2026-01-14 12:51:53,880 - INFO -   [탐색 67] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:51:53,987 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:53,991 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:53,994 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:02,670 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:03,028 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:52:03,037 - INFO - ==================================================
2026-01-14 12:52:03,209 - INFO -   [탐색 68] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:52:03,311 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:03,312 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:03,316 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:12,097 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:12,249 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:52:12,249 - INFO - ==================================================
2026-01-14 12:52:12,299 - INFO -   [탐색 69] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:52:12,360 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:12,361 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:12,364 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:21,815 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:22,069 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:52:22,070 - INFO - ==================================================
2026-01-14 12:52:22,127 - INFO -   [탐색 70] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:52:22,196 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:22,197 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:22,201 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:31,774 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:32,220 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:52:32,220 - INFO - ==================================================
2026-01-14 12:52:32,290 - INFO -   [탐색 71] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:52:32,373 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:32,373 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:32,377 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:42,517 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:43,218 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:52:43,218 - INFO - ==================================================
2026-01-14 12:52:43,298 - INFO -   [탐색 72] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:52:43,416 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:43,416 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:43,421 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:52,941 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:53,104 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:52:53,105 - INFO - ==================================================
2026-01-14 12:52:53,164 - INFO -   [탐색 73] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:52:53,255 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:53,255 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:53,259 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:02,940 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:03,341 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:53:03,341 - INFO - ==================================================
2026-01-14 12:53:03,406 - INFO -   [탐색 74] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:53:03,482 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:03,483 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:03,487 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:12,087 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:12,443 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:53:12,444 - INFO - ==================================================
2026-01-14 12:53:12,671 - INFO -   [탐색 75] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:53:12,740 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:12,741 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:12,745 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:21,677 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:21,932 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:53:21,933 - INFO - ==================================================
2026-01-14 12:53:22,085 - INFO -   [탐색 76] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:53:22,170 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:22,171 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:22,182 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:31,109 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:31,275 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:53:31,275 - INFO - ==================================================
2026-01-14 12:53:31,371 - INFO -   [탐색 77] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:53:31,478 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:31,479 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:31,483 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:40,502 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:40,881 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:53:40,882 - INFO - ==================================================
2026-01-14 12:53:41,019 - INFO -   [탐색 78] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:53:41,150 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:41,151 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:41,155 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:49,897 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:50,344 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:53:50,345 - INFO - ==================================================
2026-01-14 12:53:50,390 - INFO -   [탐색 79] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:53:50,472 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:50,473 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:50,476 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:59,434 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:59,546 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:53:59,547 - INFO - ==================================================
2026-01-14 12:53:59,601 - INFO -   [탐색 80] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:53:59,670 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:59,671 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:59,674 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:08,408 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:08,565 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:54:08,565 - INFO - ==================================================
2026-01-14 12:54:08,622 - INFO -   [탐색 81] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:54:08,694 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:08,695 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:08,699 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:17,695 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:17,847 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:54:17,847 - INFO - ==================================================
2026-01-14 12:54:17,909 - INFO -   [탐색 82] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:54:18,094 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:18,094 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:18,098 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:28,517 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:28,871 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:54:28,875 - INFO - ==================================================
2026-01-14 12:54:28,989 - INFO -   [탐색 83] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:54:29,070 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:29,071 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:29,075 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:38,450 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:39,049 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:54:39,049 - INFO - ==================================================
2026-01-14 12:54:39,113 - INFO -   [탐색 84] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:54:39,190 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:39,191 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:39,195 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:49,025 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:49,185 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:54:49,186 - INFO - ==================================================
2026-01-14 12:54:49,332 - INFO -   [탐색 85] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:54:49,417 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:49,417 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:49,421 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:58,781 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:58,895 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:54:58,895 - INFO - ==================================================
2026-01-14 12:54:58,942 - INFO -   [탐색 86] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:54:58,996 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:58,997 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:58,999 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:08,301 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:08,473 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:55:08,474 - INFO - ==================================================
2026-01-14 12:55:08,530 - INFO -   [탐색 87] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:55:08,602 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:08,603 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:08,607 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:17,839 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:18,175 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:55:18,175 - INFO - ==================================================
2026-01-14 12:55:18,351 - INFO -   [탐색 88] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:55:18,536 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:18,536 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:18,543 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:28,657 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:28,837 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:55:28,838 - INFO - ==================================================
2026-01-14 12:55:29,100 - INFO -   [탐색 89] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:55:29,172 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:29,173 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:29,175 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:37,393 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:37,666 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:55:37,670 - INFO - ==================================================
2026-01-14 12:55:37,757 - INFO -   [탐색 90] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:55:37,847 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:37,848 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:37,851 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:47,022 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:47,395 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:55:47,396 - INFO - ==================================================
2026-01-14 12:55:47,565 - INFO -   [탐색 91] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:55:47,760 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:47,760 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:47,768 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:57,553 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:57,734 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:55:57,735 - INFO - ==================================================
2026-01-14 12:55:57,785 - INFO -   [탐색 92] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:55:57,939 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:57,939 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:57,977 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:07,191 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:07,407 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:56:07,408 - INFO - ==================================================
2026-01-14 12:56:07,459 - INFO -   [탐색 93] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:56:07,517 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:07,518 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:07,527 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:16,722 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:17,916 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:56:17,917 - INFO - ==================================================
2026-01-14 12:56:17,977 - INFO -   [탐색 94] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:56:18,087 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:18,087 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:18,115 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:26,041 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:26,252 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:56:26,252 - INFO - ==================================================
2026-01-14 12:56:26,307 - INFO -   [탐색 95] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:56:26,378 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:26,379 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:26,384 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:34,918 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:35,091 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:56:35,092 - INFO - ==================================================
2026-01-14 12:56:35,149 - INFO -   [탐색 96] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:56:35,218 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:35,219 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:35,223 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:43,897 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:44,051 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:56:44,051 - INFO - ==================================================
2026-01-14 12:56:44,112 - INFO -   [탐색 97] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:56:44,177 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:44,177 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:44,181 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:53,326 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:53,566 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:56:53,566 - INFO - ==================================================
2026-01-14 12:56:53,755 - INFO -   [탐색 98] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:56:53,814 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:53,814 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:53,817 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:57:02,488 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:57:02,653 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:57:02,653 - INFO - ==================================================
2026-01-14 12:57:02,718 - INFO -   [탐색 99] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:57:02,786 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:57:02,787 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:57:02,791 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:57:12,218 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:57:12,458 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32291666666666685)에 맞춰 변경되었습니다.
2026-01-14 12:57:12,459 - INFO - ==================================================
2026-01-14 12:57:12,521 - INFO -   [탐색 100] 희소도: 0.3229 -> FLOPs: 0.1826 GFLOPs (감소율: 52.61%)
2026-01-14 12:57:12,522 - INFO - 탐색 완료. 목표 FLOPs(0.1829)에 가장 근접한 최적 희소도는 0.3234 입니다.
2026-01-14 12:57:12,522 - INFO - ================================================================================
2026-01-14 12:57:12,528 - INFO - 계산된 Pruning 정보(희소도: 0.3234)를 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/pruning_info.yaml'에 저장했습니다.
2026-01-14 12:57:12,609 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-01-14 12:57:13,007 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:57:13,007 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:57:13,011 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:57:21,357 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:57:21,588 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3233935546875)에 맞춰 변경되었습니다.
2026-01-14 12:57:21,589 - INFO - ==================================================
2026-01-14 12:57:21,592 - INFO - ==================================================
2026-01-14 12:57:21,593 - INFO - 모델 파라미터 수:
2026-01-14 12:57:21,594 - INFO -   - 총 파라미터: 1,156,016 개
2026-01-14 12:57:21,595 - INFO -   - 학습 가능한 파라미터: 1,156,016 개
2026-01-14 12:57:21,658 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-01-14 12:57:21,790 - INFO - FLOPs가 0.3853 GFLOPs에서 0.1826 GFLOPs로 감소했습니다 (감소율: 52.61%).
2026-01-14 12:57:21,791 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-01-14 12:57:21,791 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:57:21,793 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-01-14 12:57:21,794 - INFO - ==================================================
2026-01-14 12:57:21,794 - INFO - train 모드를 시작합니다.
2026-01-14 12:57:21,794 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:57:21,795 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:57:21,795 - INFO - --------------------------------------------------
2026-01-14 12:57:21,797 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-01-14 12:57:31,840 - INFO - [Train] [11/90] | Loss: 0.8126 | Train Acc: 67.34%
2026-01-14 12:57:34,649 - INFO - [Valid] [11/90] | Loss: 1.4775 | Val Acc: 64.31%
2026-01-14 12:57:34,673 - INFO - [Metrics for 'abnormal'] | Precision: 0.6698 | Recall: 0.4522 | F1: 0.5399
2026-01-14 12:57:34,673 - INFO - [Metrics for 'normal'] | Precision: 0.6309 | Recall: 0.8077 | F1: 0.7084
2026-01-14 12:57:34,766 - INFO - [Best Model Saved] (val loss: 1.4775) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:57:34,766 - INFO - --------------------------------------------------
2026-01-14 12:57:34,770 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-01-14 12:57:43,925 - INFO - [Train] [12/90] | Loss: 0.5966 | Train Acc: 73.14%
2026-01-14 12:57:47,187 - INFO - [Valid] [12/90] | Loss: 0.6681 | Val Acc: 72.57%
2026-01-14 12:57:47,196 - INFO - [Metrics for 'abnormal'] | Precision: 0.6416 | Recall: 0.9236 | F1: 0.7572
2026-01-14 12:57:47,197 - INFO - [Metrics for 'normal'] | Precision: 0.8938 | Recall: 0.5549 | F1: 0.6847
2026-01-14 12:57:47,242 - INFO - [Best Model Saved] (val loss: 0.6681) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:57:47,243 - INFO - --------------------------------------------------
2026-01-14 12:57:47,245 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-01-14 12:57:56,504 - INFO - [Train] [13/90] | Loss: 0.5839 | Train Acc: 75.97%
2026-01-14 12:57:59,606 - INFO - [Valid] [13/90] | Loss: 0.6530 | Val Acc: 75.52%
2026-01-14 12:57:59,619 - INFO - [Metrics for 'abnormal'] | Precision: 0.6832 | Recall: 0.8790 | F1: 0.7688
2026-01-14 12:57:59,619 - INFO - [Metrics for 'normal'] | Precision: 0.8613 | Recall: 0.6484 | F1: 0.7398
2026-01-14 12:57:59,671 - INFO - [Best Model Saved] (val loss: 0.6530) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:57:59,671 - INFO - --------------------------------------------------
2026-01-14 12:57:59,673 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-01-14 12:58:09,179 - INFO - [Train] [14/90] | Loss: 0.5257 | Train Acc: 78.72%
2026-01-14 12:58:10,934 - INFO - [Valid] [14/90] | Loss: 1.0984 | Val Acc: 59.29%
2026-01-14 12:58:10,943 - INFO - [Metrics for 'abnormal'] | Precision: 0.5333 | Recall: 0.9682 | F1: 0.6878
2026-01-14 12:58:10,943 - INFO - [Metrics for 'normal'] | Precision: 0.9074 | Recall: 0.2692 | F1: 0.4153
2026-01-14 12:58:10,946 - INFO - --------------------------------------------------
2026-01-14 12:58:10,948 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-01-14 12:58:20,851 - INFO - [Train] [15/90] | Loss: 0.5457 | Train Acc: 79.99%
2026-01-14 12:58:22,562 - INFO - [Valid] [15/90] | Loss: 0.6136 | Val Acc: 74.93%
2026-01-14 12:58:22,573 - INFO - [Metrics for 'abnormal'] | Precision: 0.8673 | Recall: 0.5414 | F1: 0.6667
2026-01-14 12:58:22,573 - INFO - [Metrics for 'normal'] | Precision: 0.7012 | Recall: 0.9286 | F1: 0.7991
2026-01-14 12:58:22,636 - INFO - [Best Model Saved] (val loss: 0.6136) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:58:22,636 - INFO - --------------------------------------------------
2026-01-14 12:58:22,638 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-01-14 12:58:32,692 - INFO - [Train] [16/90] | Loss: 0.5351 | Train Acc: 78.79%
2026-01-14 12:58:35,169 - INFO - [Valid] [16/90] | Loss: 0.5445 | Val Acc: 74.93%
2026-01-14 12:58:35,197 - INFO - [Metrics for 'abnormal'] | Precision: 0.6765 | Recall: 0.8790 | F1: 0.7645
2026-01-14 12:58:35,197 - INFO - [Metrics for 'normal'] | Precision: 0.8593 | Recall: 0.6374 | F1: 0.7319
2026-01-14 12:58:35,248 - INFO - [Best Model Saved] (val loss: 0.5445) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:58:35,248 - INFO - --------------------------------------------------
2026-01-14 12:58:35,257 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-01-14 12:58:45,611 - INFO - [Train] [17/90] | Loss: 0.5448 | Train Acc: 78.87%
2026-01-14 12:58:48,335 - INFO - [Valid] [17/90] | Loss: 0.5019 | Val Acc: 81.12%
2026-01-14 12:58:48,345 - INFO - [Metrics for 'abnormal'] | Precision: 0.7818 | Recall: 0.8217 | F1: 0.8012
2026-01-14 12:58:48,346 - INFO - [Metrics for 'normal'] | Precision: 0.8391 | Recall: 0.8022 | F1: 0.8202
2026-01-14 12:58:48,415 - INFO - [Best Model Saved] (val loss: 0.5019) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:58:48,415 - INFO - --------------------------------------------------
2026-01-14 12:58:48,418 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-01-14 12:58:57,499 - INFO - [Train] [18/90] | Loss: 0.5366 | Train Acc: 77.83%
2026-01-14 12:59:00,195 - INFO - [Valid] [18/90] | Loss: 0.5090 | Val Acc: 79.35%
2026-01-14 12:59:00,208 - INFO - [Metrics for 'abnormal'] | Precision: 0.7544 | Recall: 0.8217 | F1: 0.7866
2026-01-14 12:59:00,209 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7692 | F1: 0.8000
2026-01-14 12:59:00,213 - INFO - --------------------------------------------------
2026-01-14 12:59:00,216 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-01-14 12:59:09,364 - INFO - [Train] [19/90] | Loss: 0.5154 | Train Acc: 79.99%
2026-01-14 12:59:11,974 - INFO - [Valid] [19/90] | Loss: 0.5311 | Val Acc: 82.30%
2026-01-14 12:59:11,985 - INFO - [Metrics for 'abnormal'] | Precision: 0.8255 | Recall: 0.7834 | F1: 0.8039
2026-01-14 12:59:11,986 - INFO - [Metrics for 'normal'] | Precision: 0.8211 | Recall: 0.8571 | F1: 0.8387
2026-01-14 12:59:11,989 - INFO - --------------------------------------------------
2026-01-14 12:59:11,992 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-01-14 12:59:22,262 - INFO - [Train] [20/90] | Loss: 0.4753 | Train Acc: 82.66%
2026-01-14 12:59:24,327 - INFO - [Valid] [20/90] | Loss: 0.5436 | Val Acc: 76.70%
2026-01-14 12:59:24,352 - INFO - [Metrics for 'abnormal'] | Precision: 0.6970 | Recall: 0.8790 | F1: 0.7775
2026-01-14 12:59:24,356 - INFO - [Metrics for 'normal'] | Precision: 0.8652 | Recall: 0.6703 | F1: 0.7554
2026-01-14 12:59:24,363 - INFO - --------------------------------------------------
2026-01-14 12:59:24,366 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-01-14 12:59:33,679 - INFO - [Train] [21/90] | Loss: 0.4580 | Train Acc: 84.67%
2026-01-14 12:59:35,619 - INFO - [Valid] [21/90] | Loss: 0.7060 | Val Acc: 73.45%
2026-01-14 12:59:35,633 - INFO - [Metrics for 'abnormal'] | Precision: 0.8526 | Recall: 0.5159 | F1: 0.6429
2026-01-14 12:59:35,633 - INFO - [Metrics for 'normal'] | Precision: 0.6885 | Recall: 0.9231 | F1: 0.7887
2026-01-14 12:59:35,637 - INFO - --------------------------------------------------
2026-01-14 12:59:35,639 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-01-14 12:59:45,120 - INFO - [Train] [22/90] | Loss: 0.4763 | Train Acc: 83.56%
2026-01-14 12:59:47,612 - INFO - [Valid] [22/90] | Loss: 0.6720 | Val Acc: 69.32%
2026-01-14 12:59:47,622 - INFO - [Metrics for 'abnormal'] | Precision: 0.6497 | Recall: 0.7325 | F1: 0.6886
2026-01-14 12:59:47,623 - INFO - [Metrics for 'normal'] | Precision: 0.7407 | Recall: 0.6593 | F1: 0.6977
2026-01-14 12:59:47,626 - INFO - --------------------------------------------------
2026-01-14 12:59:47,629 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-01-14 12:59:56,216 - INFO - [Train] [23/90] | Loss: 0.4345 | Train Acc: 86.61%
2026-01-14 12:59:58,617 - INFO - [Valid] [23/90] | Loss: 0.4907 | Val Acc: 80.24%
2026-01-14 12:59:58,634 - INFO - [Metrics for 'abnormal'] | Precision: 0.8462 | Recall: 0.7006 | F1: 0.7666
2026-01-14 12:59:58,636 - INFO - [Metrics for 'normal'] | Precision: 0.7751 | Recall: 0.8901 | F1: 0.8286
2026-01-14 12:59:58,695 - INFO - [Best Model Saved] (val loss: 0.4907) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 12:59:58,696 - INFO - --------------------------------------------------
2026-01-14 12:59:58,698 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-01-14 13:00:09,562 - INFO - [Train] [24/90] | Loss: 0.4372 | Train Acc: 86.68%
2026-01-14 13:00:13,091 - INFO - [Valid] [24/90] | Loss: 0.5395 | Val Acc: 76.70%
2026-01-14 13:00:13,103 - INFO - [Metrics for 'abnormal'] | Precision: 0.7216 | Recall: 0.8089 | F1: 0.7628
2026-01-14 13:00:13,103 - INFO - [Metrics for 'normal'] | Precision: 0.8160 | Recall: 0.7308 | F1: 0.7710
2026-01-14 13:00:13,108 - INFO - --------------------------------------------------
2026-01-14 13:00:13,110 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-01-14 13:00:21,779 - INFO - [Train] [25/90] | Loss: 0.4202 | Train Acc: 86.98%
2026-01-14 13:00:24,592 - INFO - [Valid] [25/90] | Loss: 0.5360 | Val Acc: 79.65%
2026-01-14 13:00:24,617 - INFO - [Metrics for 'abnormal'] | Precision: 0.8014 | Recall: 0.7452 | F1: 0.7723
2026-01-14 13:00:24,619 - INFO - [Metrics for 'normal'] | Precision: 0.7927 | Recall: 0.8407 | F1: 0.8160
2026-01-14 13:00:24,628 - INFO - --------------------------------------------------
2026-01-14 13:00:24,630 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-01-14 13:00:32,996 - INFO - [Train] [26/90] | Loss: 0.4096 | Train Acc: 87.20%
2026-01-14 13:00:35,874 - INFO - [Valid] [26/90] | Loss: 0.8351 | Val Acc: 64.60%
2026-01-14 13:00:35,912 - INFO - [Metrics for 'abnormal'] | Precision: 0.5678 | Recall: 0.9873 | F1: 0.7209
2026-01-14 13:00:35,912 - INFO - [Metrics for 'normal'] | Precision: 0.9697 | Recall: 0.3516 | F1: 0.5161
2026-01-14 13:00:35,929 - INFO - --------------------------------------------------
2026-01-14 13:00:35,933 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-01-14 13:00:46,726 - INFO - [Train] [27/90] | Loss: 0.3824 | Train Acc: 88.47%
2026-01-14 13:00:48,898 - INFO - [Valid] [27/90] | Loss: 0.5467 | Val Acc: 76.99%
2026-01-14 13:00:48,910 - INFO - [Metrics for 'abnormal'] | Precision: 0.8376 | Recall: 0.6242 | F1: 0.7153
2026-01-14 13:00:48,911 - INFO - [Metrics for 'normal'] | Precision: 0.7342 | Recall: 0.8956 | F1: 0.8069
2026-01-14 13:00:48,915 - INFO - --------------------------------------------------
2026-01-14 13:00:48,918 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-01-14 13:00:58,265 - INFO - [Train] [28/90] | Loss: 0.3690 | Train Acc: 90.92%
2026-01-14 13:01:00,662 - INFO - [Valid] [28/90] | Loss: 0.5040 | Val Acc: 83.19%
2026-01-14 13:01:00,681 - INFO - [Metrics for 'abnormal'] | Precision: 0.8247 | Recall: 0.8089 | F1: 0.8167
2026-01-14 13:01:00,683 - INFO - [Metrics for 'normal'] | Precision: 0.8378 | Recall: 0.8516 | F1: 0.8447
2026-01-14 13:01:00,689 - INFO - --------------------------------------------------
2026-01-14 13:01:00,692 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-01-14 13:01:10,198 - INFO - [Train] [29/90] | Loss: 0.3591 | Train Acc: 91.15%
2026-01-14 13:01:12,696 - INFO - [Valid] [29/90] | Loss: 0.6464 | Val Acc: 79.06%
2026-01-14 13:01:12,733 - INFO - [Metrics for 'abnormal'] | Precision: 0.7108 | Recall: 0.9236 | F1: 0.8033
2026-01-14 13:01:12,737 - INFO - [Metrics for 'normal'] | Precision: 0.9111 | Recall: 0.6758 | F1: 0.7760
2026-01-14 13:01:12,745 - INFO - --------------------------------------------------
2026-01-14 13:01:12,753 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-01-14 13:01:22,385 - INFO - [Train] [30/90] | Loss: 0.3599 | Train Acc: 90.70%
2026-01-14 13:01:24,820 - INFO - [Valid] [30/90] | Loss: 0.5031 | Val Acc: 81.12%
2026-01-14 13:01:24,832 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7898 | F1: 0.7949
2026-01-14 13:01:24,832 - INFO - [Metrics for 'normal'] | Precision: 0.8207 | Recall: 0.8297 | F1: 0.8251
2026-01-14 13:01:24,837 - INFO - --------------------------------------------------
2026-01-14 13:01:24,840 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-01-14 13:01:33,866 - INFO - [Train] [31/90] | Loss: 0.3536 | Train Acc: 91.74%
2026-01-14 13:01:36,139 - INFO - [Valid] [31/90] | Loss: 0.5059 | Val Acc: 81.42%
2026-01-14 13:01:36,157 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-01-14 13:01:36,162 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-01-14 13:01:36,170 - INFO - --------------------------------------------------
2026-01-14 13:01:36,172 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-01-14 13:01:44,337 - INFO - [Train] [32/90] | Loss: 0.3434 | Train Acc: 91.67%
2026-01-14 13:01:47,533 - INFO - [Valid] [32/90] | Loss: 0.5196 | Val Acc: 82.30%
2026-01-14 13:01:47,541 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-01-14 13:01:47,542 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-01-14 13:01:47,545 - INFO - --------------------------------------------------
2026-01-14 13:01:47,546 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-01-14 13:01:55,853 - INFO - [Train] [33/90] | Loss: 0.3748 | Train Acc: 90.48%
2026-01-14 13:01:58,628 - INFO - [Valid] [33/90] | Loss: 0.5522 | Val Acc: 77.29%
2026-01-14 13:01:58,638 - INFO - [Metrics for 'abnormal'] | Precision: 0.7198 | Recall: 0.8344 | F1: 0.7729
2026-01-14 13:01:58,638 - INFO - [Metrics for 'normal'] | Precision: 0.8344 | Recall: 0.7198 | F1: 0.7729
2026-01-14 13:01:58,642 - INFO - --------------------------------------------------
2026-01-14 13:01:58,644 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-01-14 13:02:08,497 - INFO - [Train] [34/90] | Loss: 0.3413 | Train Acc: 92.41%
2026-01-14 13:02:11,063 - INFO - [Valid] [34/90] | Loss: 0.5469 | Val Acc: 80.53%
2026-01-14 13:02:11,074 - INFO - [Metrics for 'abnormal'] | Precision: 0.8527 | Recall: 0.7006 | F1: 0.7692
2026-01-14 13:02:11,075 - INFO - [Metrics for 'normal'] | Precision: 0.7762 | Recall: 0.8956 | F1: 0.8316
2026-01-14 13:02:11,080 - INFO - --------------------------------------------------
2026-01-14 13:02:11,083 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-01-14 13:02:20,804 - INFO - [Train] [35/90] | Loss: 0.3532 | Train Acc: 92.04%
2026-01-14 13:02:23,330 - INFO - [Valid] [35/90] | Loss: 0.5276 | Val Acc: 79.35%
2026-01-14 13:02:23,344 - INFO - [Metrics for 'abnormal'] | Precision: 0.7959 | Recall: 0.7452 | F1: 0.7697
2026-01-14 13:02:23,345 - INFO - [Metrics for 'normal'] | Precision: 0.7917 | Recall: 0.8352 | F1: 0.8128
2026-01-14 13:02:23,349 - INFO - --------------------------------------------------
2026-01-14 13:02:23,353 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-01-14 13:02:33,631 - INFO - [Train] [36/90] | Loss: 0.3436 | Train Acc: 92.86%
2026-01-14 13:02:35,889 - INFO - [Valid] [36/90] | Loss: 0.5503 | Val Acc: 79.65%
2026-01-14 13:02:35,913 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.7006 | F1: 0.7612
2026-01-14 13:02:35,913 - INFO - [Metrics for 'normal'] | Precision: 0.7729 | Recall: 0.8791 | F1: 0.8226
2026-01-14 13:02:35,917 - INFO - --------------------------------------------------
2026-01-14 13:02:35,919 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-01-14 13:02:44,222 - INFO - [Train] [37/90] | Loss: 0.3355 | Train Acc: 93.68%
2026-01-14 13:02:46,308 - INFO - [Valid] [37/90] | Loss: 0.5844 | Val Acc: 81.42%
2026-01-14 13:02:46,340 - INFO - [Metrics for 'abnormal'] | Precision: 0.8561 | Recall: 0.7197 | F1: 0.7820
2026-01-14 13:02:46,341 - INFO - [Metrics for 'normal'] | Precision: 0.7874 | Recall: 0.8956 | F1: 0.8380
2026-01-14 13:02:46,346 - INFO - --------------------------------------------------
2026-01-14 13:02:46,350 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-01-14 13:02:55,703 - INFO - [Train] [38/90] | Loss: 0.3215 | Train Acc: 94.35%
2026-01-14 13:02:57,590 - INFO - [Valid] [38/90] | Loss: 0.5878 | Val Acc: 81.12%
2026-01-14 13:02:57,602 - INFO - [Metrics for 'abnormal'] | Precision: 0.8496 | Recall: 0.7197 | F1: 0.7793
2026-01-14 13:02:57,603 - INFO - [Metrics for 'normal'] | Precision: 0.7864 | Recall: 0.8901 | F1: 0.8351
2026-01-14 13:02:57,607 - INFO - --------------------------------------------------
2026-01-14 13:02:57,610 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-01-14 13:03:08,121 - INFO - [Train] [39/90] | Loss: 0.3463 | Train Acc: 92.49%
2026-01-14 13:03:10,653 - INFO - [Valid] [39/90] | Loss: 0.7451 | Val Acc: 80.53%
2026-01-14 13:03:10,675 - INFO - [Metrics for 'abnormal'] | Precision: 0.8095 | Recall: 0.7580 | F1: 0.7829
2026-01-14 13:03:10,675 - INFO - [Metrics for 'normal'] | Precision: 0.8021 | Recall: 0.8462 | F1: 0.8235
2026-01-14 13:03:10,680 - INFO - --------------------------------------------------
2026-01-14 13:03:10,686 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-01-14 13:03:20,060 - INFO - [Train] [40/90] | Loss: 0.3528 | Train Acc: 91.44%
2026-01-14 13:03:22,149 - INFO - [Valid] [40/90] | Loss: 0.5588 | Val Acc: 79.65%
2026-01-14 13:03:22,188 - INFO - [Metrics for 'abnormal'] | Precision: 0.7973 | Recall: 0.7516 | F1: 0.7738
2026-01-14 13:03:22,191 - INFO - [Metrics for 'normal'] | Precision: 0.7958 | Recall: 0.8352 | F1: 0.8150
2026-01-14 13:03:22,196 - INFO - --------------------------------------------------
2026-01-14 13:03:22,200 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-01-14 13:03:31,153 - INFO - [Train] [41/90] | Loss: 0.3598 | Train Acc: 91.44%
2026-01-14 13:03:33,818 - INFO - [Valid] [41/90] | Loss: 0.5174 | Val Acc: 82.01%
2026-01-14 13:03:33,829 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 13:03:33,830 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 13:03:33,833 - INFO - --------------------------------------------------
2026-01-14 13:03:33,835 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-01-14 13:03:42,628 - INFO - [Train] [42/90] | Loss: 0.3469 | Train Acc: 92.11%
2026-01-14 13:03:45,627 - INFO - [Valid] [42/90] | Loss: 0.5421 | Val Acc: 79.94%
2026-01-14 13:03:45,639 - INFO - [Metrics for 'abnormal'] | Precision: 0.8938 | Recall: 0.6433 | F1: 0.7481
2026-01-14 13:03:45,640 - INFO - [Metrics for 'normal'] | Precision: 0.7522 | Recall: 0.9341 | F1: 0.8333
2026-01-14 13:03:45,644 - INFO - --------------------------------------------------
2026-01-14 13:03:45,647 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-01-14 13:03:53,788 - INFO - [Train] [43/90] | Loss: 0.3378 | Train Acc: 91.96%
2026-01-14 13:03:56,540 - INFO - [Valid] [43/90] | Loss: 0.5196 | Val Acc: 82.60%
2026-01-14 13:03:56,562 - INFO - [Metrics for 'abnormal'] | Precision: 0.8356 | Recall: 0.7771 | F1: 0.8053
2026-01-14 13:03:56,563 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8681 | F1: 0.8427
2026-01-14 13:03:56,568 - INFO - --------------------------------------------------
2026-01-14 13:03:56,571 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-01-14 13:04:04,335 - INFO - [Train] [44/90] | Loss: 0.3440 | Train Acc: 92.04%
2026-01-14 13:04:07,624 - INFO - [Valid] [44/90] | Loss: 0.5383 | Val Acc: 80.53%
2026-01-14 13:04:07,633 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-01-14 13:04:07,634 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-01-14 13:04:07,637 - INFO - --------------------------------------------------
2026-01-14 13:04:07,639 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-01-14 13:04:15,339 - INFO - [Train] [45/90] | Loss: 0.3137 | Train Acc: 93.60%
2026-01-14 13:04:18,260 - INFO - [Valid] [45/90] | Loss: 0.6248 | Val Acc: 80.83%
2026-01-14 13:04:18,273 - INFO - [Metrics for 'abnormal'] | Precision: 0.8151 | Recall: 0.7580 | F1: 0.7855
2026-01-14 13:04:18,274 - INFO - [Metrics for 'normal'] | Precision: 0.8031 | Recall: 0.8516 | F1: 0.8267
2026-01-14 13:04:18,278 - INFO - --------------------------------------------------
2026-01-14 13:04:18,281 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-01-14 13:04:27,911 - INFO - [Train] [46/90] | Loss: 0.3243 | Train Acc: 94.20%
2026-01-14 13:04:31,202 - INFO - [Valid] [46/90] | Loss: 0.5087 | Val Acc: 82.01%
2026-01-14 13:04:31,214 - INFO - [Metrics for 'abnormal'] | Precision: 0.8117 | Recall: 0.7962 | F1: 0.8039
2026-01-14 13:04:31,215 - INFO - [Metrics for 'normal'] | Precision: 0.8270 | Recall: 0.8407 | F1: 0.8338
2026-01-14 13:04:31,220 - INFO - --------------------------------------------------
2026-01-14 13:04:31,223 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-01-14 13:04:39,687 - INFO - [Train] [47/90] | Loss: 0.2962 | Train Acc: 95.09%
2026-01-14 13:04:42,399 - INFO - [Valid] [47/90] | Loss: 0.5130 | Val Acc: 82.60%
2026-01-14 13:04:42,447 - INFO - [Metrics for 'abnormal'] | Precision: 0.8182 | Recall: 0.8025 | F1: 0.8103
2026-01-14 13:04:42,449 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8462 | F1: 0.8392
2026-01-14 13:04:42,460 - INFO - --------------------------------------------------
2026-01-14 13:04:42,465 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-01-14 13:04:52,156 - INFO - [Train] [48/90] | Loss: 0.2991 | Train Acc: 94.87%
2026-01-14 13:04:54,707 - INFO - [Valid] [48/90] | Loss: 0.5070 | Val Acc: 81.42%
2026-01-14 13:04:54,719 - INFO - [Metrics for 'abnormal'] | Precision: 0.8052 | Recall: 0.7898 | F1: 0.7974
2026-01-14 13:04:54,720 - INFO - [Metrics for 'normal'] | Precision: 0.8216 | Recall: 0.8352 | F1: 0.8283
2026-01-14 13:04:54,725 - INFO - --------------------------------------------------
2026-01-14 13:04:54,728 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-01-14 13:05:04,525 - INFO - [Train] [49/90] | Loss: 0.2692 | Train Acc: 96.73%
2026-01-14 13:05:06,732 - INFO - [Valid] [49/90] | Loss: 0.5353 | Val Acc: 81.71%
2026-01-14 13:05:06,746 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8025 | F1: 0.8025
2026-01-14 13:05:06,747 - INFO - [Metrics for 'normal'] | Precision: 0.8297 | Recall: 0.8297 | F1: 0.8297
2026-01-14 13:05:06,752 - INFO - --------------------------------------------------
2026-01-14 13:05:06,756 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-01-14 13:05:16,255 - INFO - [Train] [50/90] | Loss: 0.2903 | Train Acc: 95.61%
2026-01-14 13:05:18,745 - INFO - [Valid] [50/90] | Loss: 0.5143 | Val Acc: 81.71%
2026-01-14 13:05:18,762 - INFO - [Metrics for 'abnormal'] | Precision: 0.7714 | Recall: 0.8599 | F1: 0.8133
2026-01-14 13:05:18,764 - INFO - [Metrics for 'normal'] | Precision: 0.8659 | Recall: 0.7802 | F1: 0.8208
2026-01-14 13:05:18,768 - INFO - --------------------------------------------------
2026-01-14 13:05:18,771 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-01-14 13:05:27,443 - INFO - [Train] [51/90] | Loss: 0.2817 | Train Acc: 96.35%
2026-01-14 13:05:29,715 - INFO - [Valid] [51/90] | Loss: 0.5242 | Val Acc: 80.53%
2026-01-14 13:05:29,732 - INFO - [Metrics for 'abnormal'] | Precision: 0.8421 | Recall: 0.7134 | F1: 0.7724
2026-01-14 13:05:29,736 - INFO - [Metrics for 'normal'] | Precision: 0.7816 | Recall: 0.8846 | F1: 0.8299
2026-01-14 13:05:29,741 - INFO - --------------------------------------------------
2026-01-14 13:05:29,753 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-01-14 13:05:38,707 - INFO - [Train] [52/90] | Loss: 0.2885 | Train Acc: 96.06%
2026-01-14 13:05:40,828 - INFO - [Valid] [52/90] | Loss: 0.5105 | Val Acc: 82.01%
2026-01-14 13:05:40,840 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-01-14 13:05:40,840 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-01-14 13:05:40,844 - INFO - --------------------------------------------------
2026-01-14 13:05:40,847 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-01-14 13:05:50,316 - INFO - [Train] [53/90] | Loss: 0.2669 | Train Acc: 96.88%
2026-01-14 13:05:52,881 - INFO - [Valid] [53/90] | Loss: 0.5273 | Val Acc: 83.48%
2026-01-14 13:05:52,913 - INFO - [Metrics for 'abnormal'] | Precision: 0.8389 | Recall: 0.7962 | F1: 0.8170
2026-01-14 13:05:52,913 - INFO - [Metrics for 'normal'] | Precision: 0.8316 | Recall: 0.8681 | F1: 0.8495
2026-01-14 13:05:52,920 - INFO - --------------------------------------------------
2026-01-14 13:05:52,926 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-01-14 13:06:02,439 - INFO - [Train] [54/90] | Loss: 0.2490 | Train Acc: 98.29%
2026-01-14 13:06:04,696 - INFO - [Valid] [54/90] | Loss: 0.5480 | Val Acc: 80.83%
2026-01-14 13:06:04,729 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8217 | F1: 0.7988
2026-01-14 13:06:04,735 - INFO - [Metrics for 'normal'] | Precision: 0.8382 | Recall: 0.7967 | F1: 0.8169
2026-01-14 13:06:04,739 - INFO - --------------------------------------------------
2026-01-14 13:06:04,742 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-01-14 13:06:12,823 - INFO - [Train] [55/90] | Loss: 0.2451 | Train Acc: 98.29%
2026-01-14 13:06:15,380 - INFO - [Valid] [55/90] | Loss: 0.5126 | Val Acc: 84.07%
2026-01-14 13:06:15,392 - INFO - [Metrics for 'abnormal'] | Precision: 0.8601 | Recall: 0.7834 | F1: 0.8200
2026-01-14 13:06:15,393 - INFO - [Metrics for 'normal'] | Precision: 0.8265 | Recall: 0.8901 | F1: 0.8571
2026-01-14 13:06:15,398 - INFO - --------------------------------------------------
2026-01-14 13:06:15,401 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-01-14 13:06:24,857 - INFO - [Train] [56/90] | Loss: 0.2567 | Train Acc: 97.62%
2026-01-14 13:06:27,613 - INFO - [Valid] [56/90] | Loss: 0.5026 | Val Acc: 82.01%
2026-01-14 13:06:27,627 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-01-14 13:06:27,627 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-01-14 13:06:27,631 - INFO - --------------------------------------------------
2026-01-14 13:06:27,634 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-01-14 13:06:37,126 - INFO - [Train] [57/90] | Loss: 0.2568 | Train Acc: 97.62%
2026-01-14 13:06:39,189 - INFO - [Valid] [57/90] | Loss: 0.6005 | Val Acc: 77.29%
2026-01-14 13:06:39,199 - INFO - [Metrics for 'abnormal'] | Precision: 0.7667 | Recall: 0.7325 | F1: 0.7492
2026-01-14 13:06:39,199 - INFO - [Metrics for 'normal'] | Precision: 0.7778 | Recall: 0.8077 | F1: 0.7925
2026-01-14 13:06:39,202 - INFO - --------------------------------------------------
2026-01-14 13:06:39,205 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-01-14 13:06:48,456 - INFO - [Train] [58/90] | Loss: 0.2940 | Train Acc: 95.31%
2026-01-14 13:06:50,705 - INFO - [Valid] [58/90] | Loss: 0.5325 | Val Acc: 78.47%
2026-01-14 13:06:50,718 - INFO - [Metrics for 'abnormal'] | Precision: 0.7530 | Recall: 0.7962 | F1: 0.7740
2026-01-14 13:06:50,718 - INFO - [Metrics for 'normal'] | Precision: 0.8150 | Recall: 0.7747 | F1: 0.7944
2026-01-14 13:06:50,723 - INFO - --------------------------------------------------
2026-01-14 13:06:50,726 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-01-14 13:06:59,587 - INFO - [Train] [59/90] | Loss: 0.2517 | Train Acc: 97.92%
2026-01-14 13:07:01,851 - INFO - [Valid] [59/90] | Loss: 0.6539 | Val Acc: 79.65%
2026-01-14 13:07:01,864 - INFO - [Metrics for 'abnormal'] | Precision: 0.8607 | Recall: 0.6688 | F1: 0.7527
2026-01-14 13:07:01,865 - INFO - [Metrics for 'normal'] | Precision: 0.7604 | Recall: 0.9066 | F1: 0.8271
2026-01-14 13:07:01,869 - INFO - --------------------------------------------------
2026-01-14 13:07:01,873 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-01-14 13:07:11,681 - INFO - [Train] [60/90] | Loss: 0.2680 | Train Acc: 97.40%
2026-01-14 13:07:15,634 - INFO - [Valid] [60/90] | Loss: 0.6050 | Val Acc: 79.94%
2026-01-14 13:07:15,645 - INFO - [Metrics for 'abnormal'] | Precision: 0.7665 | Recall: 0.8153 | F1: 0.7901
2026-01-14 13:07:15,645 - INFO - [Metrics for 'normal'] | Precision: 0.8314 | Recall: 0.7857 | F1: 0.8079
2026-01-14 13:07:15,650 - INFO - --------------------------------------------------
2026-01-14 13:07:15,654 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-01-14 13:07:25,153 - INFO - [Train] [61/90] | Loss: 0.2462 | Train Acc: 98.21%
2026-01-14 13:07:29,054 - INFO - [Valid] [61/90] | Loss: 0.6204 | Val Acc: 77.58%
2026-01-14 13:07:29,065 - INFO - [Metrics for 'abnormal'] | Precision: 0.7288 | Recall: 0.8217 | F1: 0.7725
2026-01-14 13:07:29,065 - INFO - [Metrics for 'normal'] | Precision: 0.8272 | Recall: 0.7363 | F1: 0.7791
2026-01-14 13:07:29,069 - INFO - --------------------------------------------------
2026-01-14 13:07:29,072 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-01-14 13:07:38,233 - INFO - [Train] [62/90] | Loss: 0.3051 | Train Acc: 94.20%
2026-01-14 13:07:41,421 - INFO - [Valid] [62/90] | Loss: 0.5497 | Val Acc: 81.42%
2026-01-14 13:07:41,435 - INFO - [Metrics for 'abnormal'] | Precision: 0.8264 | Recall: 0.7580 | F1: 0.7907
2026-01-14 13:07:41,436 - INFO - [Metrics for 'normal'] | Precision: 0.8051 | Recall: 0.8626 | F1: 0.8329
2026-01-14 13:07:41,441 - INFO - --------------------------------------------------
2026-01-14 13:07:41,444 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-01-14 13:07:50,906 - INFO - [Train] [63/90] | Loss: 0.2787 | Train Acc: 96.13%
2026-01-14 13:07:53,377 - INFO - [Valid] [63/90] | Loss: 0.5445 | Val Acc: 81.42%
2026-01-14 13:07:53,396 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-01-14 13:07:53,396 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-01-14 13:07:53,403 - INFO - --------------------------------------------------
2026-01-14 13:07:53,409 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-01-14 13:08:02,493 - INFO - [Train] [64/90] | Loss: 0.2765 | Train Acc: 95.83%
2026-01-14 13:08:05,497 - INFO - [Valid] [64/90] | Loss: 0.5841 | Val Acc: 81.12%
2026-01-14 13:08:05,518 - INFO - [Metrics for 'abnormal'] | Precision: 0.7853 | Recall: 0.8153 | F1: 0.8000
2026-01-14 13:08:05,518 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8077 | F1: 0.8212
2026-01-14 13:08:05,524 - INFO - --------------------------------------------------
2026-01-14 13:08:05,526 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-01-14 13:08:14,146 - INFO - [Train] [65/90] | Loss: 0.2559 | Train Acc: 97.25%
2026-01-14 13:08:17,478 - INFO - [Valid] [65/90] | Loss: 0.5604 | Val Acc: 82.30%
2026-01-14 13:08:17,501 - INFO - [Metrics for 'abnormal'] | Precision: 0.8212 | Recall: 0.7898 | F1: 0.8052
2026-01-14 13:08:17,505 - INFO - [Metrics for 'normal'] | Precision: 0.8245 | Recall: 0.8516 | F1: 0.8378
2026-01-14 13:08:17,512 - INFO - --------------------------------------------------
2026-01-14 13:08:17,518 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-01-14 13:08:25,948 - INFO - [Train] [66/90] | Loss: 0.2538 | Train Acc: 97.77%
2026-01-14 13:08:29,451 - INFO - [Valid] [66/90] | Loss: 0.5249 | Val Acc: 80.83%
2026-01-14 13:08:29,465 - INFO - [Metrics for 'abnormal'] | Precision: 0.7706 | Recall: 0.8344 | F1: 0.8012
2026-01-14 13:08:29,466 - INFO - [Metrics for 'normal'] | Precision: 0.8462 | Recall: 0.7857 | F1: 0.8148
2026-01-14 13:08:29,470 - INFO - --------------------------------------------------
2026-01-14 13:08:29,472 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-01-14 13:08:38,270 - INFO - [Train] [67/90] | Loss: 0.2363 | Train Acc: 98.51%
2026-01-14 13:08:41,941 - INFO - [Valid] [67/90] | Loss: 0.5482 | Val Acc: 80.83%
2026-01-14 13:08:41,954 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-01-14 13:08:41,956 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-01-14 13:08:41,960 - INFO - --------------------------------------------------
2026-01-14 13:08:41,963 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-01-14 13:08:51,849 - INFO - [Train] [68/90] | Loss: 0.2445 | Train Acc: 98.14%
2026-01-14 13:08:55,116 - INFO - [Valid] [68/90] | Loss: 0.5426 | Val Acc: 81.12%
2026-01-14 13:08:55,126 - INFO - [Metrics for 'abnormal'] | Precision: 0.8207 | Recall: 0.7580 | F1: 0.7881
2026-01-14 13:08:55,127 - INFO - [Metrics for 'normal'] | Precision: 0.8041 | Recall: 0.8571 | F1: 0.8298
2026-01-14 13:08:55,132 - INFO - --------------------------------------------------
2026-01-14 13:08:55,135 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-01-14 13:09:03,901 - INFO - [Train] [69/90] | Loss: 0.2568 | Train Acc: 97.32%
2026-01-14 13:09:07,008 - INFO - [Valid] [69/90] | Loss: 0.5180 | Val Acc: 83.78%
2026-01-14 13:09:07,040 - INFO - [Metrics for 'abnormal'] | Precision: 0.8400 | Recall: 0.8025 | F1: 0.8208
2026-01-14 13:09:07,040 - INFO - [Metrics for 'normal'] | Precision: 0.8360 | Recall: 0.8681 | F1: 0.8518
2026-01-14 13:09:07,050 - INFO - --------------------------------------------------
2026-01-14 13:09:07,056 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-01-14 13:09:15,519 - INFO - [Train] [70/90] | Loss: 0.2380 | Train Acc: 98.74%
2026-01-14 13:09:18,953 - INFO - [Valid] [70/90] | Loss: 0.5087 | Val Acc: 83.78%
2026-01-14 13:09:18,985 - INFO - [Metrics for 'abnormal'] | Precision: 0.8036 | Recall: 0.8599 | F1: 0.8308
2026-01-14 13:09:18,986 - INFO - [Metrics for 'normal'] | Precision: 0.8713 | Recall: 0.8187 | F1: 0.8442
2026-01-14 13:09:18,996 - INFO - --------------------------------------------------
2026-01-14 13:09:19,005 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-01-14 13:09:26,903 - INFO - [Train] [71/90] | Loss: 0.2363 | Train Acc: 98.59%
2026-01-14 13:09:30,890 - INFO - [Valid] [71/90] | Loss: 0.5095 | Val Acc: 82.01%
2026-01-14 13:09:30,902 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-01-14 13:09:30,903 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-01-14 13:09:30,907 - INFO - --------------------------------------------------
2026-01-14 13:09:30,910 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-01-14 13:09:39,475 - INFO - [Train] [72/90] | Loss: 0.2405 | Train Acc: 98.44%
2026-01-14 13:09:42,686 - INFO - [Valid] [72/90] | Loss: 0.5546 | Val Acc: 82.30%
2026-01-14 13:09:42,721 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-01-14 13:09:42,725 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-01-14 13:09:42,739 - INFO - --------------------------------------------------
2026-01-14 13:09:42,752 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-01-14 13:09:51,154 - INFO - [Train] [73/90] | Loss: 0.2345 | Train Acc: 98.74%
2026-01-14 13:09:54,535 - INFO - [Valid] [73/90] | Loss: 0.5172 | Val Acc: 82.60%
2026-01-14 13:09:54,546 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.8153 | F1: 0.8127
2026-01-14 13:09:54,547 - INFO - [Metrics for 'normal'] | Precision: 0.8398 | Recall: 0.8352 | F1: 0.8375
2026-01-14 13:09:54,551 - INFO - --------------------------------------------------
2026-01-14 13:09:54,553 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-01-14 13:10:03,114 - INFO - [Train] [74/90] | Loss: 0.2220 | Train Acc: 99.18%
2026-01-14 13:10:06,081 - INFO - [Valid] [74/90] | Loss: 0.5351 | Val Acc: 82.01%
2026-01-14 13:10:06,093 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-01-14 13:10:06,094 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-01-14 13:10:06,099 - INFO - --------------------------------------------------
2026-01-14 13:10:06,102 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-01-14 13:10:14,758 - INFO - [Train] [75/90] | Loss: 0.2337 | Train Acc: 98.59%
2026-01-14 13:10:17,469 - INFO - [Valid] [75/90] | Loss: 0.5475 | Val Acc: 82.01%
2026-01-14 13:10:17,482 - INFO - [Metrics for 'abnormal'] | Precision: 0.7759 | Recall: 0.8599 | F1: 0.8157
2026-01-14 13:10:17,483 - INFO - [Metrics for 'normal'] | Precision: 0.8667 | Recall: 0.7857 | F1: 0.8242
2026-01-14 13:10:17,487 - INFO - --------------------------------------------------
2026-01-14 13:10:17,490 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-01-14 13:10:27,309 - INFO - [Train] [76/90] | Loss: 0.2214 | Train Acc: 99.18%
2026-01-14 13:10:29,847 - INFO - [Valid] [76/90] | Loss: 0.5830 | Val Acc: 81.12%
2026-01-14 13:10:29,870 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-01-14 13:10:29,874 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-01-14 13:10:29,881 - INFO - --------------------------------------------------
2026-01-14 13:10:29,887 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-01-14 13:10:39,479 - INFO - [Train] [77/90] | Loss: 0.2240 | Train Acc: 99.18%
2026-01-14 13:10:41,968 - INFO - [Valid] [77/90] | Loss: 0.5533 | Val Acc: 81.71%
2026-01-14 13:10:41,992 - INFO - [Metrics for 'abnormal'] | Precision: 0.7914 | Recall: 0.8217 | F1: 0.8063
2026-01-14 13:10:41,992 - INFO - [Metrics for 'normal'] | Precision: 0.8409 | Recall: 0.8132 | F1: 0.8268
2026-01-14 13:10:42,003 - INFO - --------------------------------------------------
2026-01-14 13:10:42,006 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-01-14 13:10:51,172 - INFO - [Train] [78/90] | Loss: 0.2194 | Train Acc: 99.33%
2026-01-14 13:10:53,626 - INFO - [Valid] [78/90] | Loss: 0.5356 | Val Acc: 81.42%
2026-01-14 13:10:53,643 - INFO - [Metrics for 'abnormal'] | Precision: 0.7798 | Recall: 0.8344 | F1: 0.8062
2026-01-14 13:10:53,644 - INFO - [Metrics for 'normal'] | Precision: 0.8480 | Recall: 0.7967 | F1: 0.8215
2026-01-14 13:10:53,647 - INFO - --------------------------------------------------
2026-01-14 13:10:53,650 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-01-14 13:11:03,465 - INFO - [Train] [79/90] | Loss: 0.2171 | Train Acc: 99.55%
2026-01-14 13:11:05,547 - INFO - [Valid] [79/90] | Loss: 0.5564 | Val Acc: 81.42%
2026-01-14 13:11:05,558 - INFO - [Metrics for 'abnormal'] | Precision: 0.7937 | Recall: 0.8089 | F1: 0.8013
2026-01-14 13:11:05,559 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8187 | F1: 0.8255
2026-01-14 13:11:05,563 - INFO - --------------------------------------------------
2026-01-14 13:11:05,565 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-01-14 13:11:15,432 - INFO - [Train] [80/90] | Loss: 0.2215 | Train Acc: 99.18%
2026-01-14 13:11:17,299 - INFO - [Valid] [80/90] | Loss: 0.5797 | Val Acc: 82.01%
2026-01-14 13:11:17,311 - INFO - [Metrics for 'abnormal'] | Precision: 0.8077 | Recall: 0.8025 | F1: 0.8051
2026-01-14 13:11:17,311 - INFO - [Metrics for 'normal'] | Precision: 0.8306 | Recall: 0.8352 | F1: 0.8329
2026-01-14 13:11:17,316 - INFO - --------------------------------------------------
2026-01-14 13:11:17,319 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-01-14 13:11:27,606 - INFO - [Train] [81/90] | Loss: 0.2260 | Train Acc: 98.81%
2026-01-14 13:11:29,548 - INFO - [Valid] [81/90] | Loss: 0.5544 | Val Acc: 79.35%
2026-01-14 13:11:29,569 - INFO - [Metrics for 'abnormal'] | Precision: 0.7636 | Recall: 0.8025 | F1: 0.7826
2026-01-14 13:11:29,569 - INFO - [Metrics for 'normal'] | Precision: 0.8218 | Recall: 0.7857 | F1: 0.8034
2026-01-14 13:11:29,574 - INFO - --------------------------------------------------
2026-01-14 13:11:29,577 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-01-14 13:11:40,213 - INFO - [Train] [82/90] | Loss: 0.2153 | Train Acc: 99.55%
2026-01-14 13:11:41,992 - INFO - [Valid] [82/90] | Loss: 0.5828 | Val Acc: 80.53%
2026-01-14 13:11:42,002 - INFO - [Metrics for 'abnormal'] | Precision: 0.7935 | Recall: 0.7834 | F1: 0.7885
2026-01-14 13:11:42,002 - INFO - [Metrics for 'normal'] | Precision: 0.8152 | Recall: 0.8242 | F1: 0.8197
2026-01-14 13:11:42,006 - INFO - --------------------------------------------------
2026-01-14 13:11:42,009 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-01-14 13:11:52,063 - INFO - [Train] [83/90] | Loss: 0.2272 | Train Acc: 98.88%
2026-01-14 13:11:53,894 - INFO - [Valid] [83/90] | Loss: 0.5825 | Val Acc: 80.83%
2026-01-14 13:11:53,906 - INFO - [Metrics for 'abnormal'] | Precision: 0.7805 | Recall: 0.8153 | F1: 0.7975
2026-01-14 13:11:53,906 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8022 | F1: 0.8179
2026-01-14 13:11:53,911 - INFO - --------------------------------------------------
2026-01-14 13:11:53,914 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-01-14 13:12:02,684 - INFO - [Train] [84/90] | Loss: 0.2188 | Train Acc: 99.26%
2026-01-14 13:12:05,157 - INFO - [Valid] [84/90] | Loss: 0.5708 | Val Acc: 80.83%
2026-01-14 13:12:05,257 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-01-14 13:12:05,257 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-01-14 13:12:05,261 - INFO - --------------------------------------------------
2026-01-14 13:12:05,263 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-01-14 13:12:14,674 - INFO - [Train] [85/90] | Loss: 0.2155 | Train Acc: 99.55%
2026-01-14 13:12:16,980 - INFO - [Valid] [85/90] | Loss: 0.5501 | Val Acc: 81.12%
2026-01-14 13:12:16,993 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8280 | F1: 0.8025
2026-01-14 13:12:16,994 - INFO - [Metrics for 'normal'] | Precision: 0.8430 | Recall: 0.7967 | F1: 0.8192
2026-01-14 13:12:16,999 - INFO - --------------------------------------------------
2026-01-14 13:12:17,003 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-01-14 13:12:26,395 - INFO - [Train] [86/90] | Loss: 0.2207 | Train Acc: 99.11%
2026-01-14 13:12:29,520 - INFO - [Valid] [86/90] | Loss: 0.5481 | Val Acc: 82.60%
2026-01-14 13:12:29,532 - INFO - [Metrics for 'abnormal'] | Precision: 0.8311 | Recall: 0.7834 | F1: 0.8066
2026-01-14 13:12:29,532 - INFO - [Metrics for 'normal'] | Precision: 0.8220 | Recall: 0.8626 | F1: 0.8418
2026-01-14 13:12:29,536 - INFO - --------------------------------------------------
2026-01-14 13:12:29,539 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-01-14 13:12:39,425 - INFO - [Train] [87/90] | Loss: 0.2148 | Train Acc: 99.48%
2026-01-14 13:12:42,008 - INFO - [Valid] [87/90] | Loss: 0.5603 | Val Acc: 82.30%
2026-01-14 13:12:42,019 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-01-14 13:12:42,019 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-01-14 13:12:42,023 - INFO - --------------------------------------------------
2026-01-14 13:12:42,026 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-01-14 13:12:50,972 - INFO - [Train] [88/90] | Loss: 0.2144 | Train Acc: 99.26%
2026-01-14 13:12:53,244 - INFO - [Valid] [88/90] | Loss: 0.5714 | Val Acc: 81.12%
2026-01-14 13:12:53,257 - INFO - [Metrics for 'abnormal'] | Precision: 0.7853 | Recall: 0.8153 | F1: 0.8000
2026-01-14 13:12:53,257 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8077 | F1: 0.8212
2026-01-14 13:12:53,262 - INFO - --------------------------------------------------
2026-01-14 13:12:53,266 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-01-14 13:13:02,434 - INFO - [Train] [89/90] | Loss: 0.2146 | Train Acc: 99.55%
2026-01-14 13:13:04,133 - INFO - [Valid] [89/90] | Loss: 0.5806 | Val Acc: 81.42%
2026-01-14 13:13:04,144 - INFO - [Metrics for 'abnormal'] | Precision: 0.7937 | Recall: 0.8089 | F1: 0.8013
2026-01-14 13:13:04,144 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8187 | F1: 0.8255
2026-01-14 13:13:04,149 - INFO - --------------------------------------------------
2026-01-14 13:13:04,152 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-01-14 13:13:13,139 - INFO - [Train] [90/90] | Loss: 0.2096 | Train Acc: 99.85%
2026-01-14 13:13:15,280 - INFO - [Valid] [90/90] | Loss: 0.5480 | Val Acc: 80.83%
2026-01-14 13:13:15,299 - INFO - [Metrics for 'abnormal'] | Precision: 0.8067 | Recall: 0.7707 | F1: 0.7883
2026-01-14 13:13:15,301 - INFO - [Metrics for 'normal'] | Precision: 0.8095 | Recall: 0.8407 | F1: 0.8248
2026-01-14 13:13:15,307 - INFO - ==================================================
2026-01-14 13:13:15,308 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-01-14 13:13:15,309 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-01-14 13:13:15,309 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 13:13:15,503 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-01-14 13:13:15,505 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 13:13:15,506 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 13:13:15,509 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 13:13:25,826 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 13:13:25,999 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3233935546875)에 맞춰 변경되었습니다.
2026-01-14 13:13:26,000 - INFO - ==================================================
2026-01-14 13:13:26,098 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/best_model.pth'
2026-01-14 13:13:26,098 - INFO - ==================================================
2026-01-14 13:13:26,099 - INFO - Test 모드를 시작합니다.
2026-01-14 13:13:26,401 - INFO - 연산량 (MACs): 0.0913 GMACs per sample
2026-01-14 13:13:26,402 - INFO - 연산량 (FLOPs): 0.1826 GFLOPs per sample
2026-01-14 13:13:26,403 - INFO - ==================================================
2026-01-14 13:13:26,403 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-01-14 13:13:27,527 - INFO - 샘플 당 평균 Forward Pass 시간: 4.57ms (std: 1.74ms), FPS: 241.45 (std: 77.47) (1개 샘플 x 100회 반복)
2026-01-14 13:13:27,528 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 81.41 MB
2026-01-14 13:13:27,529 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-01-14 13:13:33,215 - INFO - [Test] Loss: 0.4218 | Test Acc: 80.24%
2026-01-14 13:13:33,230 - INFO - [Metrics for 'abnormal'] | Precision: 0.8462 | Recall: 0.7006 | F1: 0.7666
2026-01-14 13:13:33,230 - INFO - [Metrics for 'normal'] | Precision: 0.7751 | Recall: 0.8901 | F1: 0.8286
2026-01-14 13:13:34,232 - INFO - ==================================================
2026-01-14 13:13:34,232 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/confusion_matrix_20260114_123944.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/confusion_matrix_20260114_123944.pdf'
2026-01-14 13:13:34,232 - INFO - ==================================================
2026-01-14 13:13:34,232 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-01-14 13:13:41,721 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/model_fp32_20260114_123944.onnx'에 저장되었습니다. (크기: 4.40 MB)
2026-01-14 13:13:42,144 - INFO - [Model Load] ONNX 모델(FP32) 로드 메모리: 2556.67 MB (증가량: 6.89 MB)
2026-01-14 13:13:42,145 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-01-14 13:13:44,710 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 19.91ms (std: 16.70ms)
2026-01-14 13:13:44,710 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 77.06 FPS (std: 52.53) (1개 샘플 x 100회 반복)
2026-01-14 13:13:44,710 - INFO - [Inference Only] ONNX 런타임 추론 중 최대 CPU 메모리: 2560.67 MB (순수 증가량: 4.00 MB)
2026-01-14 13:13:44,710 - INFO - [Total Process] ONNX 모델(FP32) 전체 메모리 사용량: 2560.67 MB (전체 증가량: 10.89 MB)
2026-01-14 13:13:51,421 - INFO - [Test (ONNX)] | Test Acc (ONNX): 80.24%
2026-01-14 13:13:51,455 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8462 | Recall: 0.7006 | F1: 0.7666
2026-01-14 13:13:51,455 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.7751 | Recall: 0.8901 | F1: 0.8286
2026-01-14 13:13:52,028 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/val_acc.pdf'
2026-01-14 13:13:52,644 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/train_val_acc.pdf'
2026-01-14 13:13:53,362 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/F1_normal.pdf'
2026-01-14 13:13:54,240 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/val_loss.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/val_loss.pdf'
2026-01-14 13:13:54,884 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/learning_rate.pdf'
2026-01-14 13:14:01,299 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/compile.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260114_123944/graph_20260114_123944/compile.pdf'
