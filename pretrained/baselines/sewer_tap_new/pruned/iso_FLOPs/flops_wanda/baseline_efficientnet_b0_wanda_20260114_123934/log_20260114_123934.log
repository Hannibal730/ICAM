2026-01-14 12:39:34,879 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/log_20260114_123934.log'에 저장됩니다.
2026-01-14 12:39:34,884 - INFO - ==================================================
2026-01-14 12:39:34,884 - INFO - config.yaml:
2026-01-14 12:39:34,884 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_inference_dir: ./pretrained
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/cau/workspace/data/Sewer/Sewer-ML/valid
      train_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/cau/workspace/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/cau/workspace/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  patch_size: 56
  stride: 56
  cnn_feature_extractor:
    name: efficientnet_b0_feat2
  featured_patch_dim: 24
  emb_dim: 24
  num_heads: 2
  num_decoder_layers: 2
  num_decoder_patches: 1
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: true
  save_attention: true
  num_plot_attention: 5
baseline:
  model_name: efficientnet_b0
  use_wanda_pruning: true
  num_wanda_calib_samples: 1353
  pruning_flops_target: 0.1829

2026-01-14 12:39:34,885 - INFO - ==================================================
2026-01-14 12:39:35,079 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA RTX PRO 6000 Blackwell Server Edition)
2026-01-14 12:39:35,080 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-01-14 12:39:35,080 - INFO - '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-01-14 12:39:35,090 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-01-14 12:39:35,090 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-01-14 12:39:35,091 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-01-14 12:39:35,091 - INFO - Baseline 모델 'efficientnet_b0'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 12:39:35,384 - INFO - ==================================================
2026-01-14 12:39:35,384 - INFO - 모델 파라미터 수:
2026-01-14 12:39:35,385 - INFO -   - 총 파라미터: 4,010,110 개
2026-01-14 12:39:35,385 - INFO -   - 학습 가능한 파라미터: 4,010,110 개
2026-01-14 12:39:35,385 - INFO - ================================================================================
2026-01-14 12:39:35,385 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-01-14 12:39:35,385 - INFO - ================================================================================
2026-01-14 12:39:35,385 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:39:35,387 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-01-14 12:39:35,387 - INFO - ==================================================
2026-01-14 12:39:35,387 - INFO - train 모드를 시작합니다.
2026-01-14 12:39:35,387 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:39:35,387 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:39:35,388 - INFO - --------------------------------------------------
2026-01-14 12:39:35,390 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-01-14 12:39:41,395 - INFO - [Train] [1/10] | Loss: 0.5897 | Train Acc: 72.92%
2026-01-14 12:39:43,616 - INFO - [Valid] [1/10] | Loss: 1.0333 | Val Acc: 76.70%
2026-01-14 12:39:43,631 - INFO - [Metrics for 'abnormal'] | Precision: 0.8047 | Recall: 0.6561 | F1: 0.7228
2026-01-14 12:39:43,631 - INFO - [Metrics for 'normal'] | Precision: 0.7441 | Recall: 0.8626 | F1: 0.7990
2026-01-14 12:39:43,689 - INFO - [Best Model Saved] (val loss: 1.0333) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:39:43,689 - INFO - --------------------------------------------------
2026-01-14 12:39:43,691 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-01-14 12:39:48,734 - INFO - [Train] [2/10] | Loss: 0.5530 | Train Acc: 77.60%
2026-01-14 12:39:50,275 - INFO - [Valid] [2/10] | Loss: 0.5765 | Val Acc: 76.70%
2026-01-14 12:39:50,287 - INFO - [Metrics for 'abnormal'] | Precision: 0.8750 | Recall: 0.5796 | F1: 0.6973
2026-01-14 12:39:50,288 - INFO - [Metrics for 'normal'] | Precision: 0.7191 | Recall: 0.9286 | F1: 0.8106
2026-01-14 12:39:50,343 - INFO - [Best Model Saved] (val loss: 0.5765) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:39:50,344 - INFO - --------------------------------------------------
2026-01-14 12:39:50,345 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-01-14 12:39:55,325 - INFO - [Train] [3/10] | Loss: 0.5132 | Train Acc: 79.24%
2026-01-14 12:39:56,832 - INFO - [Valid] [3/10] | Loss: 0.5107 | Val Acc: 80.83%
2026-01-14 12:39:56,843 - INFO - [Metrics for 'abnormal'] | Precision: 0.8433 | Recall: 0.7197 | F1: 0.7766
2026-01-14 12:39:56,843 - INFO - [Metrics for 'normal'] | Precision: 0.7854 | Recall: 0.8846 | F1: 0.8320
2026-01-14 12:39:56,920 - INFO - [Best Model Saved] (val loss: 0.5107) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:39:56,921 - INFO - --------------------------------------------------
2026-01-14 12:39:56,924 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-01-14 12:40:03,281 - INFO - [Train] [4/10] | Loss: 0.4721 | Train Acc: 82.66%
2026-01-14 12:40:04,981 - INFO - [Valid] [4/10] | Loss: 0.5764 | Val Acc: 74.93%
2026-01-14 12:40:04,991 - INFO - [Metrics for 'abnormal'] | Precision: 0.6651 | Recall: 0.9236 | F1: 0.7733
2026-01-14 12:40:04,993 - INFO - [Metrics for 'normal'] | Precision: 0.9008 | Recall: 0.5989 | F1: 0.7195
2026-01-14 12:40:04,996 - INFO - --------------------------------------------------
2026-01-14 12:40:04,998 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-01-14 12:40:11,830 - INFO - [Train] [5/10] | Loss: 0.4559 | Train Acc: 83.71%
2026-01-14 12:40:14,490 - INFO - [Valid] [5/10] | Loss: 0.4587 | Val Acc: 83.78%
2026-01-14 12:40:14,500 - INFO - [Metrics for 'abnormal'] | Precision: 0.8228 | Recall: 0.8280 | F1: 0.8254
2026-01-14 12:40:14,500 - INFO - [Metrics for 'normal'] | Precision: 0.8508 | Recall: 0.8462 | F1: 0.8485
2026-01-14 12:40:14,575 - INFO - [Best Model Saved] (val loss: 0.4587) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:40:14,576 - INFO - --------------------------------------------------
2026-01-14 12:40:14,577 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-01-14 12:40:23,581 - INFO - [Train] [6/10] | Loss: 0.4273 | Train Acc: 86.09%
2026-01-14 12:40:25,669 - INFO - [Valid] [6/10] | Loss: 0.4498 | Val Acc: 84.96%
2026-01-14 12:40:25,680 - INFO - [Metrics for 'abnormal'] | Precision: 0.8397 | Recall: 0.8344 | F1: 0.8371
2026-01-14 12:40:25,680 - INFO - [Metrics for 'normal'] | Precision: 0.8579 | Recall: 0.8626 | F1: 0.8603
2026-01-14 12:40:25,746 - INFO - [Best Model Saved] (val loss: 0.4498) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:40:25,746 - INFO - --------------------------------------------------
2026-01-14 12:40:25,749 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-01-14 12:40:37,024 - INFO - [Train] [7/10] | Loss: 0.3927 | Train Acc: 89.21%
2026-01-14 12:40:38,787 - INFO - [Valid] [7/10] | Loss: 0.4580 | Val Acc: 84.37%
2026-01-14 12:40:38,802 - INFO - [Metrics for 'abnormal'] | Precision: 0.8171 | Recall: 0.8535 | F1: 0.8349
2026-01-14 12:40:38,803 - INFO - [Metrics for 'normal'] | Precision: 0.8686 | Recall: 0.8352 | F1: 0.8515
2026-01-14 12:40:38,809 - INFO - --------------------------------------------------
2026-01-14 12:40:38,813 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-01-14 12:40:51,707 - INFO - [Train] [8/10] | Loss: 0.3676 | Train Acc: 89.81%
2026-01-14 12:40:54,706 - INFO - [Valid] [8/10] | Loss: 0.4848 | Val Acc: 83.48%
2026-01-14 12:40:54,724 - INFO - [Metrics for 'abnormal'] | Precision: 0.7988 | Recall: 0.8599 | F1: 0.8282
2026-01-14 12:40:54,724 - INFO - [Metrics for 'normal'] | Precision: 0.8706 | Recall: 0.8132 | F1: 0.8409
2026-01-14 12:40:54,730 - INFO - --------------------------------------------------
2026-01-14 12:40:54,734 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-01-14 12:41:05,309 - INFO - [Train] [9/10] | Loss: 0.3309 | Train Acc: 92.71%
2026-01-14 12:41:07,679 - INFO - [Valid] [9/10] | Loss: 0.5280 | Val Acc: 82.30%
2026-01-14 12:41:07,706 - INFO - [Metrics for 'abnormal'] | Precision: 0.7803 | Recall: 0.8599 | F1: 0.8182
2026-01-14 12:41:07,706 - INFO - [Metrics for 'normal'] | Precision: 0.8675 | Recall: 0.7912 | F1: 0.8276
2026-01-14 12:41:07,713 - INFO - --------------------------------------------------
2026-01-14 12:41:07,717 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-01-14 12:41:17,856 - INFO - [Train] [10/10] | Loss: 0.3156 | Train Acc: 92.78%
2026-01-14 12:41:20,679 - INFO - [Valid] [10/10] | Loss: 0.4677 | Val Acc: 85.55%
2026-01-14 12:41:20,699 - INFO - [Metrics for 'abnormal'] | Precision: 0.8649 | Recall: 0.8153 | F1: 0.8393
2026-01-14 12:41:20,700 - INFO - [Metrics for 'normal'] | Precision: 0.8482 | Recall: 0.8901 | F1: 0.8686
2026-01-14 12:41:20,711 - INFO - ================================================================================
2026-01-14 12:41:20,712 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-01-14 12:41:20,712 - INFO - ================================================================================
2026-01-14 12:41:20,930 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'을(를) 불러왔습니다.
2026-01-14 12:41:20,931 - INFO - ================================================================================
2026-01-14 12:41:20,931 - INFO - 목표 FLOPs (0.1829 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-01-14 12:41:21,089 - INFO - 원본 모델 FLOPs: 0.8277 GFLOPs
2026-01-14 12:41:21,413 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:41:21,413 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:41:21,419 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:41:31,431 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:41:31,888 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-01-14 12:41:31,889 - INFO - ==================================================
2026-01-14 12:41:32,007 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.2459 GFLOPs (감소율: 70.30%)
2026-01-14 12:41:32,109 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:41:32,110 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:41:32,114 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:41:41,657 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:41:42,229 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-01-14 12:41:42,230 - INFO - ==================================================
2026-01-14 12:41:42,400 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.0817 GFLOPs (감소율: 90.13%)
2026-01-14 12:41:42,627 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:41:42,627 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:41:42,635 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:41:53,160 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:41:53,674 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.6187499999999999)에 맞춰 변경되었습니다.
2026-01-14 12:41:53,675 - INFO - ==================================================
2026-01-14 12:41:53,790 - INFO -   [탐색  3] 희소도: 0.6187 -> FLOPs: 0.1535 GFLOPs (감소율: 81.46%)
2026-01-14 12:41:53,853 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:41:53,853 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:41:53,856 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:03,590 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:04,782 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.556875)에 맞춰 변경되었습니다.
2026-01-14 12:42:04,783 - INFO - ==================================================
2026-01-14 12:42:04,897 - INFO -   [탐색  4] 희소도: 0.5569 -> FLOPs: 0.1960 GFLOPs (감소율: 76.32%)
2026-01-14 12:42:04,997 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:04,998 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:05,003 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:14,993 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:15,661 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5878125)에 맞춰 변경되었습니다.
2026-01-14 12:42:15,662 - INFO - ==================================================
2026-01-14 12:42:15,769 - INFO -   [탐색  5] 희소도: 0.5878 -> FLOPs: 0.1727 GFLOPs (감소율: 79.13%)
2026-01-14 12:42:15,832 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:15,833 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:15,837 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:23,527 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:24,103 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5723437499999999)에 맞춰 변경되었습니다.
2026-01-14 12:42:24,103 - INFO - ==================================================
2026-01-14 12:42:24,255 - INFO -   [탐색  6] 희소도: 0.5723 -> FLOPs: 0.1841 GFLOPs (감소율: 77.76%)
2026-01-14 12:42:24,336 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:24,337 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:24,341 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:33,351 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:33,972 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.580078125)에 맞춰 변경되었습니다.
2026-01-14 12:42:33,979 - INFO - ==================================================
2026-01-14 12:42:34,282 - INFO -   [탐색  7] 희소도: 0.5801 -> FLOPs: 0.1790 GFLOPs (감소율: 78.37%)
2026-01-14 12:42:34,436 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:34,436 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:34,444 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:44,363 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:45,527 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5762109375)에 맞춰 변경되었습니다.
2026-01-14 12:42:45,531 - INFO - ==================================================
2026-01-14 12:42:45,828 - INFO -   [탐색  8] 희소도: 0.5762 -> FLOPs: 0.1810 GFLOPs (감소율: 78.13%)
2026-01-14 12:42:46,051 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:46,055 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:46,059 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:42:54,503 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:42:55,075 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.57427734375)에 맞춰 변경되었습니다.
2026-01-14 12:42:55,077 - INFO - ==================================================
2026-01-14 12:42:55,225 - INFO -   [탐색  9] 희소도: 0.5743 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:42:55,316 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:42:55,316 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:42:55,321 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:05,133 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:05,536 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5733105468749999)에 맞춰 변경되었습니다.
2026-01-14 12:43:05,537 - INFO - ==================================================
2026-01-14 12:43:05,619 - INFO -   [탐색 10] 희소도: 0.5733 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:43:06,020 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:06,020 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:06,024 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:15,097 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:15,724 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737939453124999)에 맞춰 변경되었습니다.
2026-01-14 12:43:15,725 - INFO - ==================================================
2026-01-14 12:43:15,802 - INFO -   [탐색 11] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:43:15,898 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:15,899 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:15,903 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:24,909 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:25,320 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5735522460937499)에 맞춰 변경되었습니다.
2026-01-14 12:43:25,320 - INFO - ==================================================
2026-01-14 12:43:25,391 - INFO -   [탐색 12] 희소도: 0.5736 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:43:25,482 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:25,483 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:25,488 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:33,981 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:34,306 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5736730957031249)에 맞춰 변경되었습니다.
2026-01-14 12:43:34,307 - INFO - ==================================================
2026-01-14 12:43:34,389 - INFO -   [탐색 13] 희소도: 0.5737 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:43:34,466 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:34,467 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:34,472 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:42,942 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:43,374 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737335205078125)에 맞춰 변경되었습니다.
2026-01-14 12:43:43,378 - INFO - ==================================================
2026-01-14 12:43:43,509 - INFO -   [탐색 14] 희소도: 0.5737 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:43:43,594 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:43,595 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:43,598 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:43:53,270 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:43:53,588 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737637329101561)에 맞춰 변경되었습니다.
2026-01-14 12:43:53,588 - INFO - ==================================================
2026-01-14 12:43:53,672 - INFO -   [탐색 15] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:43:53,752 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:43:53,753 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:43:53,757 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:02,584 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:03,175 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737788391113281)에 맞춰 변경되었습니다.
2026-01-14 12:44:03,175 - INFO - ==================================================
2026-01-14 12:44:03,249 - INFO -   [탐색 16] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:44:03,332 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:03,333 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:03,337 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:11,592 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:12,408 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.573786392211914)에 맞춰 변경되었습니다.
2026-01-14 12:44:12,411 - INFO - ==================================================
2026-01-14 12:44:12,485 - INFO -   [탐색 17] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:44:12,562 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:12,563 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:12,567 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:21,078 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:21,675 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.573782615661621)에 맞춰 변경되었습니다.
2026-01-14 12:44:21,675 - INFO - ==================================================
2026-01-14 12:44:21,819 - INFO -   [탐색 18] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:44:21,897 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:21,897 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:21,901 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:30,285 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:30,626 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737845039367675)에 맞춰 변경되었습니다.
2026-01-14 12:44:30,627 - INFO - ==================================================
2026-01-14 12:44:30,710 - INFO -   [탐색 19] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:44:30,805 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:30,805 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:30,810 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:40,319 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:40,776 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737854480743407)에 맞춰 변경되었습니다.
2026-01-14 12:44:40,777 - INFO - ==================================================
2026-01-14 12:44:40,854 - INFO -   [탐색 20] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:44:40,933 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:40,934 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:40,938 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:49,230 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:49,540 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737849760055541)에 맞춰 변경되었습니다.
2026-01-14 12:44:49,541 - INFO - ==================================================
2026-01-14 12:44:49,615 - INFO -   [탐색 21] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:44:49,691 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:49,692 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:49,697 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:44:59,374 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:44:59,647 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847399711609)에 맞춰 변경되었습니다.
2026-01-14 12:44:59,647 - INFO - ==================================================
2026-01-14 12:44:59,713 - INFO -   [탐색 22] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:44:59,831 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:44:59,832 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:44:59,835 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:08,245 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:08,603 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737846219539642)에 맞춰 변경되었습니다.
2026-01-14 12:45:08,603 - INFO - ==================================================
2026-01-14 12:45:08,676 - INFO -   [탐색 23] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:45:08,752 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:08,752 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:08,757 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:18,128 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:18,737 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737846809625625)에 맞춰 변경되었습니다.
2026-01-14 12:45:18,737 - INFO - ==================================================
2026-01-14 12:45:18,809 - INFO -   [탐색 24] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:45:18,888 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:18,888 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:18,893 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:27,751 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:28,221 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847104668616)에 맞춰 변경되었습니다.
2026-01-14 12:45:28,222 - INFO - ==================================================
2026-01-14 12:45:28,334 - INFO -   [탐색 25] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:45:28,481 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:28,481 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:28,485 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:37,795 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:38,096 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847252190112)에 맞춰 변경되었습니다.
2026-01-14 12:45:38,097 - INFO - ==================================================
2026-01-14 12:45:38,160 - INFO -   [탐색 26] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:45:38,239 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:38,240 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:38,245 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:48,951 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:49,261 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847178429365)에 맞춰 변경되었습니다.
2026-01-14 12:45:49,263 - INFO - ==================================================
2026-01-14 12:45:49,336 - INFO -   [탐색 27] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:45:49,414 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:49,415 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:49,420 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:45:58,732 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:45:59,028 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847215309739)에 맞춰 변경되었습니다.
2026-01-14 12:45:59,028 - INFO - ==================================================
2026-01-14 12:45:59,107 - INFO -   [탐색 28] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:45:59,179 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:45:59,180 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:45:59,185 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:08,393 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:08,900 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847233749926)에 맞춰 변경되었습니다.
2026-01-14 12:46:08,900 - INFO - ==================================================
2026-01-14 12:46:08,971 - INFO -   [탐색 29] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:46:09,045 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:09,046 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:09,049 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:18,063 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:18,637 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847224529833)에 맞춰 변경되었습니다.
2026-01-14 12:46:18,637 - INFO - ==================================================
2026-01-14 12:46:18,715 - INFO -   [탐색 30] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:46:18,802 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:18,803 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:18,807 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:27,069 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:27,848 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847219919786)에 맞춰 변경되었습니다.
2026-01-14 12:46:27,849 - INFO - ==================================================
2026-01-14 12:46:27,912 - INFO -   [탐색 31] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:46:28,083 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:28,085 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:28,089 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:36,952 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:37,391 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.573784722222481)에 맞춰 변경되었습니다.
2026-01-14 12:46:37,392 - INFO - ==================================================
2026-01-14 12:46:37,518 - INFO -   [탐색 32] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:46:37,618 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:37,619 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:37,623 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:47,038 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:47,402 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847221072299)에 맞춰 변경되었습니다.
2026-01-14 12:46:47,403 - INFO - ==================================================
2026-01-14 12:46:47,477 - INFO -   [탐색 33] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:46:47,563 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:47,564 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:47,568 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:46:56,750 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:46:57,095 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847221648554)에 맞춰 변경되었습니다.
2026-01-14 12:46:57,096 - INFO - ==================================================
2026-01-14 12:46:57,156 - INFO -   [탐색 34] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:46:57,216 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:46:57,217 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:46:57,220 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:05,879 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:06,122 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847221936683)에 맞춰 변경되었습니다.
2026-01-14 12:47:06,122 - INFO - ==================================================
2026-01-14 12:47:06,190 - INFO -   [탐색 35] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:47:06,254 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:06,256 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:06,270 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:16,312 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:16,641 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222080746)에 맞춰 변경되었습니다.
2026-01-14 12:47:16,641 - INFO - ==================================================
2026-01-14 12:47:16,726 - INFO -   [탐색 36] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:47:16,806 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:16,806 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:16,811 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:26,162 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:26,556 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222152779)에 맞춰 변경되었습니다.
2026-01-14 12:47:26,557 - INFO - ==================================================
2026-01-14 12:47:26,642 - INFO -   [탐색 37] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:47:26,753 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:26,754 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:26,759 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:36,410 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:37,056 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222188794)에 맞춰 변경되었습니다.
2026-01-14 12:47:37,058 - INFO - ==================================================
2026-01-14 12:47:37,143 - INFO -   [탐색 38] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:47:37,248 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:37,248 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:37,253 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:46,774 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:47,293 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222206802)에 맞춰 변경되었습니다.
2026-01-14 12:47:47,294 - INFO - ==================================================
2026-01-14 12:47:47,368 - INFO -   [탐색 39] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:47:47,446 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:47,447 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:47,453 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:47:56,563 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:47:56,978 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222215806)에 맞춰 변경되었습니다.
2026-01-14 12:47:56,979 - INFO - ==================================================
2026-01-14 12:47:57,075 - INFO -   [탐색 40] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:47:57,165 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:47:57,166 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:47:57,169 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:05,944 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:06,300 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222220308)에 맞춰 변경되었습니다.
2026-01-14 12:48:06,301 - INFO - ==================================================
2026-01-14 12:48:06,416 - INFO -   [탐색 41] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:48:06,492 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:06,493 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:06,497 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:15,613 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:15,932 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222558)에 맞춰 변경되었습니다.
2026-01-14 12:48:15,933 - INFO - ==================================================
2026-01-14 12:48:16,021 - INFO -   [탐색 42] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:48:16,096 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:16,097 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:16,101 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:25,185 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:25,710 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222221433)에 맞춰 변경되었습니다.
2026-01-14 12:48:25,711 - INFO - ==================================================
2026-01-14 12:48:25,944 - INFO -   [탐색 43] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:48:26,043 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:26,044 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:26,048 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:34,774 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:35,823 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222221996)에 맞춰 변경되었습니다.
2026-01-14 12:48:35,824 - INFO - ==================================================
2026-01-14 12:48:36,024 - INFO -   [탐색 44] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:48:36,141 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:36,142 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:36,146 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:45,983 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:46,451 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222276)에 맞춰 변경되었습니다.
2026-01-14 12:48:46,452 - INFO - ==================================================
2026-01-14 12:48:46,527 - INFO -   [탐색 45] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:48:46,622 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:46,625 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:46,629 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:48:56,096 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:48:56,481 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222137)에 맞춰 변경되었습니다.
2026-01-14 12:48:56,482 - INFO - ==================================================
2026-01-14 12:48:56,556 - INFO -   [탐색 46] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:48:56,659 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:48:56,660 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:48:56,665 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:05,776 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:06,134 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222207)에 맞춰 변경되었습니다.
2026-01-14 12:49:06,135 - INFO - ==================================================
2026-01-14 12:49:06,213 - INFO -   [탐색 47] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:49:06,294 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:06,295 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:06,299 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:15,771 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:16,106 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222241)에 맞춰 변경되었습니다.
2026-01-14 12:49:16,107 - INFO - ==================================================
2026-01-14 12:49:16,179 - INFO -   [탐색 48] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:49:16,266 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:16,270 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:16,276 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:24,837 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:25,237 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:49:25,237 - INFO - ==================================================
2026-01-14 12:49:25,312 - INFO -   [탐색 49] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:49:25,426 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:25,427 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:25,431 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:34,442 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:34,736 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222214)에 맞춰 변경되었습니다.
2026-01-14 12:49:34,736 - INFO - ==================================================
2026-01-14 12:49:34,807 - INFO -   [탐색 50] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:49:34,873 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:34,873 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:34,877 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:43,454 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:44,089 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222219)에 맞춰 변경되었습니다.
2026-01-14 12:49:44,090 - INFO - ==================================================
2026-01-14 12:49:44,167 - INFO -   [탐색 51] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:49:44,250 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:44,251 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:44,255 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:49:53,223 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:49:53,665 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222221)에 맞춰 변경되었습니다.
2026-01-14 12:49:53,666 - INFO - ==================================================
2026-01-14 12:49:53,732 - INFO -   [탐색 52] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:49:53,810 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:49:53,811 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:49:53,815 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:02,587 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:02,911 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222222)에 맞춰 변경되었습니다.
2026-01-14 12:50:02,912 - INFO - ==================================================
2026-01-14 12:50:03,174 - INFO -   [탐색 53] 희소도: 0.5738 -> FLOPs: 0.1830 GFLOPs (감소율: 77.90%)
2026-01-14 12:50:03,377 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:03,378 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:03,383 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:12,120 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:12,531 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:50:12,532 - INFO - ==================================================
2026-01-14 12:50:12,621 - INFO -   [탐색 54] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:50:12,707 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:12,707 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:12,713 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:21,747 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:22,147 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:50:22,150 - INFO - ==================================================
2026-01-14 12:50:22,317 - INFO -   [탐색 55] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:50:22,457 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:22,458 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:22,470 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:31,534 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:31,886 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:50:31,887 - INFO - ==================================================
2026-01-14 12:50:31,969 - INFO -   [탐색 56] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:50:32,064 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:32,065 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:32,070 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:40,553 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:40,842 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:50:40,843 - INFO - ==================================================
2026-01-14 12:50:40,907 - INFO -   [탐색 57] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:50:40,978 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:40,979 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:40,982 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:49,631 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:50,454 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:50:50,454 - INFO - ==================================================
2026-01-14 12:50:50,528 - INFO -   [탐색 58] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:50:50,612 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:50,613 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:50,618 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:50:58,825 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:50:59,066 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:50:59,066 - INFO - ==================================================
2026-01-14 12:50:59,138 - INFO -   [탐색 59] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:50:59,226 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:50:59,227 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:50:59,230 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:09,048 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:09,259 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:51:09,260 - INFO - ==================================================
2026-01-14 12:51:09,373 - INFO -   [탐색 60] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:51:09,451 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:09,451 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:09,455 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:20,913 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:21,261 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:51:21,261 - INFO - ==================================================
2026-01-14 12:51:21,329 - INFO -   [탐색 61] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:51:21,396 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:21,397 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:21,400 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:31,479 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:31,822 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:51:31,823 - INFO - ==================================================
2026-01-14 12:51:31,902 - INFO -   [탐색 62] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:51:31,981 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:31,982 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:31,987 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:41,585 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:41,901 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:51:41,902 - INFO - ==================================================
2026-01-14 12:51:41,965 - INFO -   [탐색 63] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:51:42,026 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:42,026 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:42,029 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:51:51,586 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:51:51,924 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:51:51,925 - INFO - ==================================================
2026-01-14 12:51:51,999 - INFO -   [탐색 64] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:51:52,078 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:51:52,079 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:51:52,083 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:01,468 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:02,085 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:52:02,086 - INFO - ==================================================
2026-01-14 12:52:02,149 - INFO -   [탐색 65] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:52:02,209 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:02,209 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:02,212 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:11,285 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:11,599 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:52:11,600 - INFO - ==================================================
2026-01-14 12:52:11,684 - INFO -   [탐색 66] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:52:11,748 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:11,748 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:11,751 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:20,774 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:21,176 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:52:21,177 - INFO - ==================================================
2026-01-14 12:52:21,227 - INFO -   [탐색 67] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:52:21,305 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:21,306 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:21,311 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:30,272 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:30,584 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:52:30,585 - INFO - ==================================================
2026-01-14 12:52:30,661 - INFO -   [탐색 68] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:52:30,741 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:30,742 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:30,746 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:39,727 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:40,179 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:52:40,180 - INFO - ==================================================
2026-01-14 12:52:40,318 - INFO -   [탐색 69] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:52:40,445 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:40,445 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:40,450 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:48,902 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:49,432 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:52:49,433 - INFO - ==================================================
2026-01-14 12:52:49,534 - INFO -   [탐색 70] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:52:49,620 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:49,621 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:49,626 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:52:58,405 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:52:58,734 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:52:58,734 - INFO - ==================================================
2026-01-14 12:52:58,840 - INFO -   [탐색 71] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:52:58,909 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:52:58,910 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:52:58,914 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:06,873 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:08,379 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:53:08,380 - INFO - ==================================================
2026-01-14 12:53:08,479 - INFO -   [탐색 72] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:53:08,570 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:08,571 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:08,576 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:17,970 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:18,523 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:53:18,527 - INFO - ==================================================
2026-01-14 12:53:18,601 - INFO -   [탐색 73] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:53:18,696 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:18,696 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:18,699 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:27,980 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:28,352 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:53:28,353 - INFO - ==================================================
2026-01-14 12:53:28,427 - INFO -   [탐색 74] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:53:28,509 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:28,510 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:28,515 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:36,900 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:37,444 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:53:37,444 - INFO - ==================================================
2026-01-14 12:53:37,530 - INFO -   [탐색 75] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:53:37,642 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:37,643 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:37,647 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:46,847 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:47,257 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:53:47,258 - INFO - ==================================================
2026-01-14 12:53:47,356 - INFO -   [탐색 76] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:53:47,456 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:47,457 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:47,462 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:53:55,824 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:53:56,157 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:53:56,161 - INFO - ==================================================
2026-01-14 12:53:56,390 - INFO -   [탐색 77] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:53:56,473 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:53:56,473 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:53:56,477 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:04,049 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:04,672 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:54:04,673 - INFO - ==================================================
2026-01-14 12:54:04,851 - INFO -   [탐색 78] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:54:05,062 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:05,066 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:05,079 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:14,240 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:15,609 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:54:15,610 - INFO - ==================================================
2026-01-14 12:54:15,687 - INFO -   [탐색 79] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:54:15,812 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:15,813 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:15,816 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:24,703 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:25,165 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:54:25,167 - INFO - ==================================================
2026-01-14 12:54:25,241 - INFO -   [탐색 80] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:54:25,319 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:25,320 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:25,324 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:34,759 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:35,249 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:54:35,250 - INFO - ==================================================
2026-01-14 12:54:35,392 - INFO -   [탐색 81] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:54:35,501 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:35,502 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:35,506 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:43,776 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:44,577 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:54:44,582 - INFO - ==================================================
2026-01-14 12:54:44,775 - INFO -   [탐색 82] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:54:45,017 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:45,018 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:45,026 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:54:53,534 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:54:54,005 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:54:54,006 - INFO - ==================================================
2026-01-14 12:54:54,072 - INFO -   [탐색 83] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:54:54,378 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:54:54,378 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:54:54,383 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:02,452 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:02,795 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:55:02,796 - INFO - ==================================================
2026-01-14 12:55:02,868 - INFO -   [탐색 84] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:55:02,939 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:02,940 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:02,944 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:11,811 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:12,482 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:55:12,482 - INFO - ==================================================
2026-01-14 12:55:12,555 - INFO -   [탐색 85] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:55:12,627 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:12,628 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:12,632 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:22,060 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:22,483 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:55:22,484 - INFO - ==================================================
2026-01-14 12:55:22,592 - INFO -   [탐색 86] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:55:22,660 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:22,661 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:22,665 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:32,148 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:32,654 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:55:32,655 - INFO - ==================================================
2026-01-14 12:55:32,728 - INFO -   [탐색 87] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:55:32,808 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:32,808 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:32,812 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:42,266 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:42,600 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:55:42,602 - INFO - ==================================================
2026-01-14 12:55:42,681 - INFO -   [탐색 88] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:55:42,757 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:42,758 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:42,762 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:55:52,562 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:55:52,906 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:55:52,907 - INFO - ==================================================
2026-01-14 12:55:52,983 - INFO -   [탐색 89] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:55:53,068 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:55:53,069 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:55:53,073 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:02,533 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:03,096 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:56:03,097 - INFO - ==================================================
2026-01-14 12:56:03,183 - INFO -   [탐색 90] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:56:03,275 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:03,276 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:03,281 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:11,613 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:12,036 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:56:12,038 - INFO - ==================================================
2026-01-14 12:56:12,144 - INFO -   [탐색 91] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:56:12,223 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:12,224 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:12,229 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:21,255 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:22,280 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:56:22,281 - INFO - ==================================================
2026-01-14 12:56:22,354 - INFO -   [탐색 92] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:56:22,443 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:22,443 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:22,448 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:30,329 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:30,883 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:56:30,884 - INFO - ==================================================
2026-01-14 12:56:30,956 - INFO -   [탐색 93] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:56:31,041 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:31,042 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:31,047 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:39,711 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:40,443 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:56:40,444 - INFO - ==================================================
2026-01-14 12:56:40,595 - INFO -   [탐색 94] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:56:40,773 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:40,773 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:40,777 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:49,778 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:50,414 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:56:50,415 - INFO - ==================================================
2026-01-14 12:56:50,609 - INFO -   [탐색 95] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:56:50,711 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:50,712 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:56:50,717 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:56:58,705 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:56:59,610 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:56:59,618 - INFO - ==================================================
2026-01-14 12:56:59,848 - INFO -   [탐색 96] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:56:59,992 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:56:59,993 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:57:00,001 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:57:09,288 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:57:09,859 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:57:09,860 - INFO - ==================================================
2026-01-14 12:57:09,935 - INFO -   [탐색 97] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:57:10,016 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:57:10,017 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:57:10,022 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:57:18,517 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:57:18,989 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:57:18,990 - INFO - ==================================================
2026-01-14 12:57:19,069 - INFO -   [탐색 98] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:57:19,154 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:57:19,155 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:57:19,158 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:57:27,894 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:57:28,726 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:57:28,727 - INFO - ==================================================
2026-01-14 12:57:28,798 - INFO -   [탐색 99] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:57:28,886 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:57:28,887 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:57:28,891 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:57:37,749 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:57:38,102 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737847222222223)에 맞춰 변경되었습니다.
2026-01-14 12:57:38,103 - INFO - ==================================================
2026-01-14 12:57:38,179 - INFO -   [탐색 100] 희소도: 0.5738 -> FLOPs: 0.1829 GFLOPs (감소율: 77.91%)
2026-01-14 12:57:38,179 - INFO - 탐색 완료. 목표 FLOPs(0.1829)에 가장 근접한 최적 희소도는 0.5738 입니다.
2026-01-14 12:57:38,180 - INFO - ================================================================================
2026-01-14 12:57:38,186 - INFO - 계산된 Pruning 정보(희소도: 0.5738)를 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/pruning_info.yaml'에 저장했습니다.
2026-01-14 12:57:38,267 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-01-14 12:57:38,486 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 12:57:38,488 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 12:57:38,493 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 12:57:46,641 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 12:57:47,065 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737939453124999)에 맞춰 변경되었습니다.
2026-01-14 12:57:47,065 - INFO - ==================================================
2026-01-14 12:57:47,069 - INFO - ==================================================
2026-01-14 12:57:47,069 - INFO - 모델 파라미터 수:
2026-01-14 12:57:47,069 - INFO -   - 총 파라미터: 775,493 개
2026-01-14 12:57:47,070 - INFO -   - 학습 가능한 파라미터: 775,493 개
2026-01-14 12:57:47,142 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-01-14 12:57:47,284 - INFO - FLOPs가 0.8277 GFLOPs에서 0.1829 GFLOPs로 감소했습니다 (감소율: 77.91%).
2026-01-14 12:57:47,285 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-01-14 12:57:47,285 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-01-14 12:57:47,287 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-01-14 12:57:47,287 - INFO - ==================================================
2026-01-14 12:57:47,287 - INFO - train 모드를 시작합니다.
2026-01-14 12:57:47,287 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-01-14 12:57:47,287 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-01-14 12:57:47,287 - INFO - --------------------------------------------------
2026-01-14 12:57:47,289 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-01-14 12:57:57,613 - INFO - [Train] [11/90] | Loss: 0.5323 | Train Acc: 77.46%
2026-01-14 12:58:00,418 - INFO - [Valid] [11/90] | Loss: 0.5461 | Val Acc: 76.11%
2026-01-14 12:58:00,432 - INFO - [Metrics for 'abnormal'] | Precision: 0.7754 | Recall: 0.6815 | F1: 0.7254
2026-01-14 12:58:00,432 - INFO - [Metrics for 'normal'] | Precision: 0.7512 | Recall: 0.8297 | F1: 0.7885
2026-01-14 12:58:00,504 - INFO - [Best Model Saved] (val loss: 0.5461) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:58:00,505 - INFO - --------------------------------------------------
2026-01-14 12:58:00,508 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-01-14 12:58:10,228 - INFO - [Train] [12/90] | Loss: 0.5057 | Train Acc: 79.99%
2026-01-14 12:58:11,814 - INFO - [Valid] [12/90] | Loss: 0.5671 | Val Acc: 77.29%
2026-01-14 12:58:11,828 - INFO - [Metrics for 'abnormal'] | Precision: 0.8077 | Recall: 0.6688 | F1: 0.7317
2026-01-14 12:58:11,828 - INFO - [Metrics for 'normal'] | Precision: 0.7512 | Recall: 0.8626 | F1: 0.8031
2026-01-14 12:58:11,833 - INFO - --------------------------------------------------
2026-01-14 12:58:11,837 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-01-14 12:58:21,808 - INFO - [Train] [13/90] | Loss: 0.4907 | Train Acc: 80.36%
2026-01-14 12:58:24,411 - INFO - [Valid] [13/90] | Loss: 0.5503 | Val Acc: 75.81%
2026-01-14 12:58:24,423 - INFO - [Metrics for 'abnormal'] | Precision: 0.7005 | Recall: 0.8344 | F1: 0.7616
2026-01-14 12:58:24,424 - INFO - [Metrics for 'normal'] | Precision: 0.8289 | Recall: 0.6923 | F1: 0.7545
2026-01-14 12:58:24,429 - INFO - --------------------------------------------------
2026-01-14 12:58:24,433 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-01-14 12:58:33,517 - INFO - [Train] [14/90] | Loss: 0.4740 | Train Acc: 82.37%
2026-01-14 12:58:36,250 - INFO - [Valid] [14/90] | Loss: 0.5116 | Val Acc: 79.65%
2026-01-14 12:58:36,261 - INFO - [Metrics for 'abnormal'] | Precision: 0.7444 | Recall: 0.8535 | F1: 0.7953
2026-01-14 12:58:36,261 - INFO - [Metrics for 'normal'] | Precision: 0.8553 | Recall: 0.7473 | F1: 0.7977
2026-01-14 12:58:36,338 - INFO - [Best Model Saved] (val loss: 0.5116) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:58:36,338 - INFO - --------------------------------------------------
2026-01-14 12:58:36,341 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-01-14 12:58:46,237 - INFO - [Train] [15/90] | Loss: 0.4435 | Train Acc: 83.93%
2026-01-14 12:58:48,928 - INFO - [Valid] [15/90] | Loss: 0.5217 | Val Acc: 79.65%
2026-01-14 12:58:48,946 - INFO - [Metrics for 'abnormal'] | Precision: 0.7340 | Recall: 0.8790 | F1: 0.8000
2026-01-14 12:58:48,949 - INFO - [Metrics for 'normal'] | Precision: 0.8742 | Recall: 0.7253 | F1: 0.7928
2026-01-14 12:58:48,954 - INFO - --------------------------------------------------
2026-01-14 12:58:48,960 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-01-14 12:58:58,683 - INFO - [Train] [16/90] | Loss: 0.4421 | Train Acc: 84.90%
2026-01-14 12:59:01,334 - INFO - [Valid] [16/90] | Loss: 0.5193 | Val Acc: 79.94%
2026-01-14 12:59:01,346 - INFO - [Metrics for 'abnormal'] | Precision: 0.7665 | Recall: 0.8153 | F1: 0.7901
2026-01-14 12:59:01,347 - INFO - [Metrics for 'normal'] | Precision: 0.8314 | Recall: 0.7857 | F1: 0.8079
2026-01-14 12:59:01,353 - INFO - --------------------------------------------------
2026-01-14 12:59:01,356 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-01-14 12:59:09,800 - INFO - [Train] [17/90] | Loss: 0.4336 | Train Acc: 85.42%
2026-01-14 12:59:12,719 - INFO - [Valid] [17/90] | Loss: 0.4913 | Val Acc: 81.42%
2026-01-14 12:59:12,742 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-01-14 12:59:12,745 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-01-14 12:59:12,836 - INFO - [Best Model Saved] (val loss: 0.4913) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:59:12,836 - INFO - --------------------------------------------------
2026-01-14 12:59:12,838 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-01-14 12:59:22,275 - INFO - [Train] [18/90] | Loss: 0.4196 | Train Acc: 85.64%
2026-01-14 12:59:24,722 - INFO - [Valid] [18/90] | Loss: 0.5097 | Val Acc: 79.35%
2026-01-14 12:59:24,747 - INFO - [Metrics for 'abnormal'] | Precision: 0.7605 | Recall: 0.8089 | F1: 0.7840
2026-01-14 12:59:24,751 - INFO - [Metrics for 'normal'] | Precision: 0.8256 | Recall: 0.7802 | F1: 0.8023
2026-01-14 12:59:24,769 - INFO - --------------------------------------------------
2026-01-14 12:59:24,779 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-01-14 12:59:34,679 - INFO - [Train] [19/90] | Loss: 0.4120 | Train Acc: 86.38%
2026-01-14 12:59:37,018 - INFO - [Valid] [19/90] | Loss: 0.4886 | Val Acc: 79.65%
2026-01-14 12:59:37,040 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.7707 | F1: 0.7781
2026-01-14 12:59:37,043 - INFO - [Metrics for 'normal'] | Precision: 0.8054 | Recall: 0.8187 | F1: 0.8120
2026-01-14 12:59:37,158 - INFO - [Best Model Saved] (val loss: 0.4886) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:59:37,159 - INFO - --------------------------------------------------
2026-01-14 12:59:37,161 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-01-14 12:59:46,409 - INFO - [Train] [20/90] | Loss: 0.3937 | Train Acc: 88.24%
2026-01-14 12:59:48,658 - INFO - [Valid] [20/90] | Loss: 0.4819 | Val Acc: 84.07%
2026-01-14 12:59:48,673 - INFO - [Metrics for 'abnormal'] | Precision: 0.8084 | Recall: 0.8599 | F1: 0.8333
2026-01-14 12:59:48,675 - INFO - [Metrics for 'normal'] | Precision: 0.8721 | Recall: 0.8242 | F1: 0.8475
2026-01-14 12:59:48,750 - INFO - [Best Model Saved] (val loss: 0.4819) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 12:59:48,751 - INFO - --------------------------------------------------
2026-01-14 12:59:48,753 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-01-14 12:59:58,504 - INFO - [Train] [21/90] | Loss: 0.3943 | Train Acc: 87.50%
2026-01-14 13:00:01,728 - INFO - [Valid] [21/90] | Loss: 0.4642 | Val Acc: 84.66%
2026-01-14 13:00:01,745 - INFO - [Metrics for 'abnormal'] | Precision: 0.8302 | Recall: 0.8408 | F1: 0.8354
2026-01-14 13:00:01,747 - INFO - [Metrics for 'normal'] | Precision: 0.8611 | Recall: 0.8516 | F1: 0.8564
2026-01-14 13:00:02,095 - INFO - [Best Model Saved] (val loss: 0.4642) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 13:00:02,096 - INFO - --------------------------------------------------
2026-01-14 13:00:02,098 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-01-14 13:00:11,859 - INFO - [Train] [22/90] | Loss: 0.3863 | Train Acc: 88.84%
2026-01-14 13:00:14,127 - INFO - [Valid] [22/90] | Loss: 0.4924 | Val Acc: 81.42%
2026-01-14 13:00:14,145 - INFO - [Metrics for 'abnormal'] | Precision: 0.8264 | Recall: 0.7580 | F1: 0.7907
2026-01-14 13:00:14,149 - INFO - [Metrics for 'normal'] | Precision: 0.8051 | Recall: 0.8626 | F1: 0.8329
2026-01-14 13:00:14,152 - INFO - --------------------------------------------------
2026-01-14 13:00:14,158 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-01-14 13:00:22,863 - INFO - [Train] [23/90] | Loss: 0.3772 | Train Acc: 89.14%
2026-01-14 13:00:25,957 - INFO - [Valid] [23/90] | Loss: 0.4814 | Val Acc: 82.30%
2026-01-14 13:00:25,969 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8662 | F1: 0.8193
2026-01-14 13:00:25,969 - INFO - [Metrics for 'normal'] | Precision: 0.8720 | Recall: 0.7857 | F1: 0.8266
2026-01-14 13:00:25,973 - INFO - --------------------------------------------------
2026-01-14 13:00:25,976 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-01-14 13:00:35,823 - INFO - [Train] [24/90] | Loss: 0.3858 | Train Acc: 88.54%
2026-01-14 13:00:38,503 - INFO - [Valid] [24/90] | Loss: 0.4706 | Val Acc: 82.30%
2026-01-14 13:00:38,516 - INFO - [Metrics for 'abnormal'] | Precision: 0.7939 | Recall: 0.8344 | F1: 0.8137
2026-01-14 13:00:38,517 - INFO - [Metrics for 'normal'] | Precision: 0.8506 | Recall: 0.8132 | F1: 0.8315
2026-01-14 13:00:38,523 - INFO - --------------------------------------------------
2026-01-14 13:00:38,529 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-01-14 13:00:47,645 - INFO - [Train] [25/90] | Loss: 0.3645 | Train Acc: 90.55%
2026-01-14 13:00:50,674 - INFO - [Valid] [25/90] | Loss: 0.5480 | Val Acc: 82.60%
2026-01-14 13:00:50,690 - INFO - [Metrics for 'abnormal'] | Precision: 0.7988 | Recall: 0.8344 | F1: 0.8162
2026-01-14 13:00:50,691 - INFO - [Metrics for 'normal'] | Precision: 0.8514 | Recall: 0.8187 | F1: 0.8347
2026-01-14 13:00:50,702 - INFO - --------------------------------------------------
2026-01-14 13:00:50,704 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-01-14 13:00:59,747 - INFO - [Train] [26/90] | Loss: 0.3620 | Train Acc: 90.18%
2026-01-14 13:01:02,612 - INFO - [Valid] [26/90] | Loss: 0.4880 | Val Acc: 83.48%
2026-01-14 13:01:02,626 - INFO - [Metrics for 'abnormal'] | Precision: 0.7988 | Recall: 0.8599 | F1: 0.8282
2026-01-14 13:01:02,626 - INFO - [Metrics for 'normal'] | Precision: 0.8706 | Recall: 0.8132 | F1: 0.8409
2026-01-14 13:01:02,631 - INFO - --------------------------------------------------
2026-01-14 13:01:02,634 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-01-14 13:01:11,885 - INFO - [Train] [27/90] | Loss: 0.3454 | Train Acc: 91.44%
2026-01-14 13:01:14,545 - INFO - [Valid] [27/90] | Loss: 0.5000 | Val Acc: 82.30%
2026-01-14 13:01:14,556 - INFO - [Metrics for 'abnormal'] | Precision: 0.8819 | Recall: 0.7134 | F1: 0.7887
2026-01-14 13:01:14,557 - INFO - [Metrics for 'normal'] | Precision: 0.7877 | Recall: 0.9176 | F1: 0.8477
2026-01-14 13:01:14,562 - INFO - --------------------------------------------------
2026-01-14 13:01:14,565 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-01-14 13:01:24,219 - INFO - [Train] [28/90] | Loss: 0.3441 | Train Acc: 91.29%
2026-01-14 13:01:27,456 - INFO - [Valid] [28/90] | Loss: 0.4647 | Val Acc: 84.07%
2026-01-14 13:01:27,494 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.8535 | F1: 0.8323
2026-01-14 13:01:27,494 - INFO - [Metrics for 'normal'] | Precision: 0.8678 | Recall: 0.8297 | F1: 0.8483
2026-01-14 13:01:27,504 - INFO - --------------------------------------------------
2026-01-14 13:01:27,512 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-01-14 13:01:36,542 - INFO - [Train] [29/90] | Loss: 0.3396 | Train Acc: 92.19%
2026-01-14 13:01:39,449 - INFO - [Valid] [29/90] | Loss: 0.5099 | Val Acc: 80.83%
2026-01-14 13:01:39,460 - INFO - [Metrics for 'abnormal'] | Precision: 0.8108 | Recall: 0.7643 | F1: 0.7869
2026-01-14 13:01:39,461 - INFO - [Metrics for 'normal'] | Precision: 0.8063 | Recall: 0.8462 | F1: 0.8257
2026-01-14 13:01:39,465 - INFO - --------------------------------------------------
2026-01-14 13:01:39,468 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-01-14 13:01:48,446 - INFO - [Train] [30/90] | Loss: 0.3332 | Train Acc: 91.82%
2026-01-14 13:01:51,555 - INFO - [Valid] [30/90] | Loss: 0.4695 | Val Acc: 82.60%
2026-01-14 13:01:51,583 - INFO - [Metrics for 'abnormal'] | Precision: 0.8551 | Recall: 0.7516 | F1: 0.8000
2026-01-14 13:01:51,583 - INFO - [Metrics for 'normal'] | Precision: 0.8060 | Recall: 0.8901 | F1: 0.8460
2026-01-14 13:01:51,587 - INFO - --------------------------------------------------
2026-01-14 13:01:51,592 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-01-14 13:01:59,511 - INFO - [Train] [31/90] | Loss: 0.3267 | Train Acc: 92.56%
2026-01-14 13:02:03,004 - INFO - [Valid] [31/90] | Loss: 0.4726 | Val Acc: 82.60%
2026-01-14 13:02:03,018 - INFO - [Metrics for 'abnormal'] | Precision: 0.7849 | Recall: 0.8599 | F1: 0.8207
2026-01-14 13:02:03,019 - INFO - [Metrics for 'normal'] | Precision: 0.8683 | Recall: 0.7967 | F1: 0.8309
2026-01-14 13:02:03,024 - INFO - --------------------------------------------------
2026-01-14 13:02:03,028 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-01-14 13:02:11,165 - INFO - [Train] [32/90] | Loss: 0.3061 | Train Acc: 94.64%
2026-01-14 13:02:14,250 - INFO - [Valid] [32/90] | Loss: 0.5019 | Val Acc: 83.19%
2026-01-14 13:02:14,264 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.7962 | F1: 0.8143
2026-01-14 13:02:14,265 - INFO - [Metrics for 'normal'] | Precision: 0.8307 | Recall: 0.8626 | F1: 0.8464
2026-01-14 13:02:14,270 - INFO - --------------------------------------------------
2026-01-14 13:02:14,274 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-01-14 13:02:23,638 - INFO - [Train] [33/90] | Loss: 0.3099 | Train Acc: 93.45%
2026-01-14 13:02:26,700 - INFO - [Valid] [33/90] | Loss: 0.5046 | Val Acc: 83.48%
2026-01-14 13:02:26,713 - INFO - [Metrics for 'abnormal'] | Precision: 0.7886 | Recall: 0.8790 | F1: 0.8313
2026-01-14 13:02:26,718 - INFO - [Metrics for 'normal'] | Precision: 0.8841 | Recall: 0.7967 | F1: 0.8382
2026-01-14 13:02:26,727 - INFO - --------------------------------------------------
2026-01-14 13:02:26,731 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-01-14 13:02:35,247 - INFO - [Train] [34/90] | Loss: 0.3144 | Train Acc: 93.60%
2026-01-14 13:02:38,383 - INFO - [Valid] [34/90] | Loss: 0.4721 | Val Acc: 85.25%
2026-01-14 13:02:38,396 - INFO - [Metrics for 'abnormal'] | Precision: 0.8323 | Recall: 0.8535 | F1: 0.8428
2026-01-14 13:02:38,397 - INFO - [Metrics for 'normal'] | Precision: 0.8708 | Recall: 0.8516 | F1: 0.8611
2026-01-14 13:02:38,438 - INFO - --------------------------------------------------
2026-01-14 13:02:38,440 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-01-14 13:02:47,331 - INFO - [Train] [35/90] | Loss: 0.3165 | Train Acc: 94.27%
2026-01-14 13:02:50,427 - INFO - [Valid] [35/90] | Loss: 0.4839 | Val Acc: 83.78%
2026-01-14 13:02:50,442 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8662 | F1: 0.8318
2026-01-14 13:02:50,443 - INFO - [Metrics for 'normal'] | Precision: 0.8757 | Recall: 0.8132 | F1: 0.8433
2026-01-14 13:02:50,447 - INFO - --------------------------------------------------
2026-01-14 13:02:50,449 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-01-14 13:02:59,060 - INFO - [Train] [36/90] | Loss: 0.3047 | Train Acc: 95.16%
2026-01-14 13:03:02,022 - INFO - [Valid] [36/90] | Loss: 0.4710 | Val Acc: 83.48%
2026-01-14 13:03:02,034 - INFO - [Metrics for 'abnormal'] | Precision: 0.8024 | Recall: 0.8535 | F1: 0.8272
2026-01-14 13:03:02,035 - INFO - [Metrics for 'normal'] | Precision: 0.8663 | Recall: 0.8187 | F1: 0.8418
2026-01-14 13:03:02,040 - INFO - --------------------------------------------------
2026-01-14 13:03:02,043 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-01-14 13:03:10,642 - INFO - [Train] [37/90] | Loss: 0.2883 | Train Acc: 94.64%
2026-01-14 13:03:13,669 - INFO - [Valid] [37/90] | Loss: 0.4549 | Val Acc: 83.48%
2026-01-14 13:03:13,682 - INFO - [Metrics for 'abnormal'] | Precision: 0.8531 | Recall: 0.7771 | F1: 0.8133
2026-01-14 13:03:13,682 - INFO - [Metrics for 'normal'] | Precision: 0.8214 | Recall: 0.8846 | F1: 0.8519
2026-01-14 13:03:13,748 - INFO - [Best Model Saved] (val loss: 0.4549) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 13:03:13,749 - INFO - --------------------------------------------------
2026-01-14 13:03:13,751 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-01-14 13:03:22,761 - INFO - [Train] [38/90] | Loss: 0.2973 | Train Acc: 94.64%
2026-01-14 13:03:25,259 - INFO - [Valid] [38/90] | Loss: 0.4723 | Val Acc: 84.37%
2026-01-14 13:03:25,286 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.8280 | F1: 0.8307
2026-01-14 13:03:25,287 - INFO - [Metrics for 'normal'] | Precision: 0.8525 | Recall: 0.8571 | F1: 0.8548
2026-01-14 13:03:25,293 - INFO - --------------------------------------------------
2026-01-14 13:03:25,297 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-01-14 13:03:35,144 - INFO - [Train] [39/90] | Loss: 0.2925 | Train Acc: 94.94%
2026-01-14 13:03:37,084 - INFO - [Valid] [39/90] | Loss: 0.4833 | Val Acc: 84.96%
2026-01-14 13:03:37,099 - INFO - [Metrics for 'abnormal'] | Precision: 0.8354 | Recall: 0.8408 | F1: 0.8381
2026-01-14 13:03:37,100 - INFO - [Metrics for 'normal'] | Precision: 0.8619 | Recall: 0.8571 | F1: 0.8595
2026-01-14 13:03:37,105 - INFO - --------------------------------------------------
2026-01-14 13:03:37,108 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-01-14 13:03:46,820 - INFO - [Train] [40/90] | Loss: 0.2902 | Train Acc: 94.94%
2026-01-14 13:03:49,122 - INFO - [Valid] [40/90] | Loss: 0.4194 | Val Acc: 86.43%
2026-01-14 13:03:49,133 - INFO - [Metrics for 'abnormal'] | Precision: 0.8364 | Recall: 0.8790 | F1: 0.8571
2026-01-14 13:03:49,133 - INFO - [Metrics for 'normal'] | Precision: 0.8908 | Recall: 0.8516 | F1: 0.8708
2026-01-14 13:03:49,192 - INFO - [Best Model Saved] (val loss: 0.4194) -> 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 13:03:49,193 - INFO - --------------------------------------------------
2026-01-14 13:03:49,194 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-01-14 13:03:57,993 - INFO - [Train] [41/90] | Loss: 0.2867 | Train Acc: 95.76%
2026-01-14 13:04:00,452 - INFO - [Valid] [41/90] | Loss: 0.4771 | Val Acc: 84.96%
2026-01-14 13:04:00,476 - INFO - [Metrics for 'abnormal'] | Precision: 0.8118 | Recall: 0.8790 | F1: 0.8440
2026-01-14 13:04:00,480 - INFO - [Metrics for 'normal'] | Precision: 0.8876 | Recall: 0.8242 | F1: 0.8547
2026-01-14 13:04:00,484 - INFO - --------------------------------------------------
2026-01-14 13:04:00,490 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-01-14 13:04:09,463 - INFO - [Train] [42/90] | Loss: 0.2641 | Train Acc: 96.88%
2026-01-14 13:04:12,349 - INFO - [Valid] [42/90] | Loss: 0.4580 | Val Acc: 84.96%
2026-01-14 13:04:12,360 - INFO - [Metrics for 'abnormal'] | Precision: 0.8118 | Recall: 0.8790 | F1: 0.8440
2026-01-14 13:04:12,360 - INFO - [Metrics for 'normal'] | Precision: 0.8876 | Recall: 0.8242 | F1: 0.8547
2026-01-14 13:04:12,365 - INFO - --------------------------------------------------
2026-01-14 13:04:12,368 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-01-14 13:04:21,329 - INFO - [Train] [43/90] | Loss: 0.2708 | Train Acc: 96.21%
2026-01-14 13:04:24,459 - INFO - [Valid] [43/90] | Loss: 0.4788 | Val Acc: 86.43%
2026-01-14 13:04:24,470 - INFO - [Metrics for 'abnormal'] | Precision: 0.8581 | Recall: 0.8471 | F1: 0.8526
2026-01-14 13:04:24,471 - INFO - [Metrics for 'normal'] | Precision: 0.8696 | Recall: 0.8791 | F1: 0.8743
2026-01-14 13:04:24,475 - INFO - --------------------------------------------------
2026-01-14 13:04:24,478 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-01-14 13:04:33,454 - INFO - [Train] [44/90] | Loss: 0.2610 | Train Acc: 96.73%
2026-01-14 13:04:36,245 - INFO - [Valid] [44/90] | Loss: 0.5008 | Val Acc: 85.25%
2026-01-14 13:04:36,256 - INFO - [Metrics for 'abnormal'] | Precision: 0.8057 | Recall: 0.8981 | F1: 0.8494
2026-01-14 13:04:36,257 - INFO - [Metrics for 'normal'] | Precision: 0.9024 | Recall: 0.8132 | F1: 0.8555
2026-01-14 13:04:36,261 - INFO - --------------------------------------------------
2026-01-14 13:04:36,264 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-01-14 13:04:44,754 - INFO - [Train] [45/90] | Loss: 0.2493 | Train Acc: 97.40%
2026-01-14 13:04:47,500 - INFO - [Valid] [45/90] | Loss: 0.4828 | Val Acc: 85.25%
2026-01-14 13:04:47,517 - INFO - [Metrics for 'abnormal'] | Precision: 0.8741 | Recall: 0.7962 | F1: 0.8333
2026-01-14 13:04:47,520 - INFO - [Metrics for 'normal'] | Precision: 0.8367 | Recall: 0.9011 | F1: 0.8677
2026-01-14 13:04:47,525 - INFO - --------------------------------------------------
2026-01-14 13:04:47,528 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-01-14 13:04:56,521 - INFO - [Train] [46/90] | Loss: 0.2542 | Train Acc: 97.02%
2026-01-14 13:04:59,667 - INFO - [Valid] [46/90] | Loss: 0.5323 | Val Acc: 82.30%
2026-01-14 13:04:59,679 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-01-14 13:04:59,680 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-01-14 13:04:59,684 - INFO - --------------------------------------------------
2026-01-14 13:04:59,688 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-01-14 13:05:08,145 - INFO - [Train] [47/90] | Loss: 0.2719 | Train Acc: 96.13%
2026-01-14 13:05:11,198 - INFO - [Valid] [47/90] | Loss: 0.5032 | Val Acc: 85.55%
2026-01-14 13:05:11,211 - INFO - [Metrics for 'abnormal'] | Precision: 0.8068 | Recall: 0.9045 | F1: 0.8529
2026-01-14 13:05:11,211 - INFO - [Metrics for 'normal'] | Precision: 0.9080 | Recall: 0.8132 | F1: 0.8580
2026-01-14 13:05:11,216 - INFO - --------------------------------------------------
2026-01-14 13:05:11,218 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-01-14 13:05:20,402 - INFO - [Train] [48/90] | Loss: 0.2671 | Train Acc: 96.43%
2026-01-14 13:05:23,306 - INFO - [Valid] [48/90] | Loss: 0.4838 | Val Acc: 85.25%
2026-01-14 13:05:23,329 - INFO - [Metrics for 'abnormal'] | Precision: 0.8497 | Recall: 0.8280 | F1: 0.8387
2026-01-14 13:05:23,329 - INFO - [Metrics for 'normal'] | Precision: 0.8548 | Recall: 0.8736 | F1: 0.8641
2026-01-14 13:05:23,334 - INFO - --------------------------------------------------
2026-01-14 13:05:23,337 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-01-14 13:05:31,386 - INFO - [Train] [49/90] | Loss: 0.2415 | Train Acc: 97.99%
2026-01-14 13:05:35,258 - INFO - [Valid] [49/90] | Loss: 0.4572 | Val Acc: 86.14%
2026-01-14 13:05:35,273 - INFO - [Metrics for 'abnormal'] | Precision: 0.8716 | Recall: 0.8217 | F1: 0.8459
2026-01-14 13:05:35,274 - INFO - [Metrics for 'normal'] | Precision: 0.8534 | Recall: 0.8956 | F1: 0.8740
2026-01-14 13:05:35,279 - INFO - --------------------------------------------------
2026-01-14 13:05:35,282 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-01-14 13:05:45,170 - INFO - [Train] [50/90] | Loss: 0.2627 | Train Acc: 96.65%
2026-01-14 13:05:48,109 - INFO - [Valid] [50/90] | Loss: 0.4309 | Val Acc: 87.32%
2026-01-14 13:05:48,119 - INFO - [Metrics for 'abnormal'] | Precision: 0.8608 | Recall: 0.8662 | F1: 0.8635
2026-01-14 13:05:48,120 - INFO - [Metrics for 'normal'] | Precision: 0.8840 | Recall: 0.8791 | F1: 0.8815
2026-01-14 13:05:48,126 - INFO - --------------------------------------------------
2026-01-14 13:05:48,129 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-01-14 13:05:58,012 - INFO - [Train] [51/90] | Loss: 0.2431 | Train Acc: 97.99%
2026-01-14 13:06:00,417 - INFO - [Valid] [51/90] | Loss: 0.4563 | Val Acc: 86.43%
2026-01-14 13:06:00,433 - INFO - [Metrics for 'abnormal'] | Precision: 0.8535 | Recall: 0.8535 | F1: 0.8535
2026-01-14 13:06:00,437 - INFO - [Metrics for 'normal'] | Precision: 0.8736 | Recall: 0.8736 | F1: 0.8736
2026-01-14 13:06:00,448 - INFO - --------------------------------------------------
2026-01-14 13:06:00,453 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-01-14 13:06:10,431 - INFO - [Train] [52/90] | Loss: 0.2435 | Train Acc: 97.25%
2026-01-14 13:06:12,781 - INFO - [Valid] [52/90] | Loss: 0.4802 | Val Acc: 85.55%
2026-01-14 13:06:12,790 - INFO - [Metrics for 'abnormal'] | Precision: 0.8418 | Recall: 0.8471 | F1: 0.8444
2026-01-14 13:06:12,790 - INFO - [Metrics for 'normal'] | Precision: 0.8674 | Recall: 0.8626 | F1: 0.8650
2026-01-14 13:06:12,793 - INFO - --------------------------------------------------
2026-01-14 13:06:12,796 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-01-14 13:06:22,528 - INFO - [Train] [53/90] | Loss: 0.2524 | Train Acc: 97.17%
2026-01-14 13:06:24,928 - INFO - [Valid] [53/90] | Loss: 0.4705 | Val Acc: 84.07%
2026-01-14 13:06:24,940 - INFO - [Metrics for 'abnormal'] | Precision: 0.8652 | Recall: 0.7771 | F1: 0.8188
2026-01-14 13:06:24,941 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8956 | F1: 0.8579
2026-01-14 13:06:24,945 - INFO - --------------------------------------------------
2026-01-14 13:06:24,948 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-01-14 13:06:35,386 - INFO - [Train] [54/90] | Loss: 0.2320 | Train Acc: 98.44%
2026-01-14 13:06:38,138 - INFO - [Valid] [54/90] | Loss: 0.4595 | Val Acc: 87.32%
2026-01-14 13:06:38,149 - INFO - [Metrics for 'abnormal'] | Precision: 0.8562 | Recall: 0.8726 | F1: 0.8644
2026-01-14 13:06:38,149 - INFO - [Metrics for 'normal'] | Precision: 0.8883 | Recall: 0.8736 | F1: 0.8809
2026-01-14 13:06:38,153 - INFO - --------------------------------------------------
2026-01-14 13:06:38,155 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-01-14 13:06:46,812 - INFO - [Train] [55/90] | Loss: 0.2327 | Train Acc: 98.36%
2026-01-14 13:06:49,746 - INFO - [Valid] [55/90] | Loss: 0.4867 | Val Acc: 86.14%
2026-01-14 13:06:49,763 - INFO - [Metrics for 'abnormal'] | Precision: 0.8481 | Recall: 0.8535 | F1: 0.8508
2026-01-14 13:06:49,765 - INFO - [Metrics for 'normal'] | Precision: 0.8729 | Recall: 0.8681 | F1: 0.8705
2026-01-14 13:06:49,770 - INFO - --------------------------------------------------
2026-01-14 13:06:49,773 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-01-14 13:06:58,884 - INFO - [Train] [56/90] | Loss: 0.2342 | Train Acc: 97.99%
2026-01-14 13:07:01,700 - INFO - [Valid] [56/90] | Loss: 0.4743 | Val Acc: 87.32%
2026-01-14 13:07:01,711 - INFO - [Metrics for 'abnormal'] | Precision: 0.8434 | Recall: 0.8917 | F1: 0.8669
2026-01-14 13:07:01,712 - INFO - [Metrics for 'normal'] | Precision: 0.9017 | Recall: 0.8571 | F1: 0.8789
2026-01-14 13:07:01,716 - INFO - --------------------------------------------------
2026-01-14 13:07:01,720 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-01-14 13:07:11,898 - INFO - [Train] [57/90] | Loss: 0.2309 | Train Acc: 98.36%
2026-01-14 13:07:15,590 - INFO - [Valid] [57/90] | Loss: 0.4635 | Val Acc: 84.66%
2026-01-14 13:07:15,604 - INFO - [Metrics for 'abnormal'] | Precision: 0.8832 | Recall: 0.7707 | F1: 0.8231
2026-01-14 13:07:15,604 - INFO - [Metrics for 'normal'] | Precision: 0.8218 | Recall: 0.9121 | F1: 0.8646
2026-01-14 13:07:15,609 - INFO - --------------------------------------------------
2026-01-14 13:07:15,612 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-01-14 13:07:24,885 - INFO - [Train] [58/90] | Loss: 0.2171 | Train Acc: 99.03%
2026-01-14 13:07:27,856 - INFO - [Valid] [58/90] | Loss: 0.4656 | Val Acc: 84.96%
2026-01-14 13:07:27,875 - INFO - [Metrics for 'abnormal'] | Precision: 0.8272 | Recall: 0.8535 | F1: 0.8401
2026-01-14 13:07:27,880 - INFO - [Metrics for 'normal'] | Precision: 0.8701 | Recall: 0.8462 | F1: 0.8579
2026-01-14 13:07:27,883 - INFO - --------------------------------------------------
2026-01-14 13:07:27,888 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-01-14 13:07:37,036 - INFO - [Train] [59/90] | Loss: 0.2253 | Train Acc: 98.59%
2026-01-14 13:07:39,518 - INFO - [Valid] [59/90] | Loss: 0.5245 | Val Acc: 84.07%
2026-01-14 13:07:39,533 - INFO - [Metrics for 'abnormal'] | Precision: 0.8601 | Recall: 0.7834 | F1: 0.8200
2026-01-14 13:07:39,534 - INFO - [Metrics for 'normal'] | Precision: 0.8265 | Recall: 0.8901 | F1: 0.8571
2026-01-14 13:07:39,539 - INFO - --------------------------------------------------
2026-01-14 13:07:39,542 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-01-14 13:07:48,354 - INFO - [Train] [60/90] | Loss: 0.2284 | Train Acc: 98.51%
2026-01-14 13:07:51,088 - INFO - [Valid] [60/90] | Loss: 0.4657 | Val Acc: 86.14%
2026-01-14 13:07:51,125 - INFO - [Metrics for 'abnormal'] | Precision: 0.8313 | Recall: 0.8790 | F1: 0.8545
2026-01-14 13:07:51,125 - INFO - [Metrics for 'normal'] | Precision: 0.8902 | Recall: 0.8462 | F1: 0.8676
2026-01-14 13:07:51,133 - INFO - --------------------------------------------------
2026-01-14 13:07:51,137 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-01-14 13:07:59,538 - INFO - [Train] [61/90] | Loss: 0.2191 | Train Acc: 98.88%
2026-01-14 13:08:01,992 - INFO - [Valid] [61/90] | Loss: 0.4667 | Val Acc: 86.73%
2026-01-14 13:08:02,060 - INFO - [Metrics for 'abnormal'] | Precision: 0.8415 | Recall: 0.8790 | F1: 0.8598
2026-01-14 13:08:02,060 - INFO - [Metrics for 'normal'] | Precision: 0.8914 | Recall: 0.8571 | F1: 0.8739
2026-01-14 13:08:02,083 - INFO - --------------------------------------------------
2026-01-14 13:08:02,086 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-01-14 13:08:10,625 - INFO - [Train] [62/90] | Loss: 0.2277 | Train Acc: 98.59%
2026-01-14 13:08:13,968 - INFO - [Valid] [62/90] | Loss: 0.4859 | Val Acc: 84.37%
2026-01-14 13:08:13,979 - INFO - [Metrics for 'abnormal'] | Precision: 0.8171 | Recall: 0.8535 | F1: 0.8349
2026-01-14 13:08:13,980 - INFO - [Metrics for 'normal'] | Precision: 0.8686 | Recall: 0.8352 | F1: 0.8515
2026-01-14 13:08:13,985 - INFO - --------------------------------------------------
2026-01-14 13:08:13,988 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-01-14 13:08:22,102 - INFO - [Train] [63/90] | Loss: 0.2188 | Train Acc: 99.11%
2026-01-14 13:08:25,328 - INFO - [Valid] [63/90] | Loss: 0.4690 | Val Acc: 85.84%
2026-01-14 13:08:25,337 - INFO - [Metrics for 'abnormal'] | Precision: 0.8303 | Recall: 0.8726 | F1: 0.8509
2026-01-14 13:08:25,338 - INFO - [Metrics for 'normal'] | Precision: 0.8851 | Recall: 0.8462 | F1: 0.8652
2026-01-14 13:08:25,341 - INFO - --------------------------------------------------
2026-01-14 13:08:25,343 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-01-14 13:08:34,718 - INFO - [Train] [64/90] | Loss: 0.2170 | Train Acc: 99.26%
2026-01-14 13:08:37,249 - INFO - [Valid] [64/90] | Loss: 0.4489 | Val Acc: 86.73%
2026-01-14 13:08:37,277 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.8917 | F1: 0.8615
2026-01-14 13:08:37,277 - INFO - [Metrics for 'normal'] | Precision: 0.9006 | Recall: 0.8462 | F1: 0.8725
2026-01-14 13:08:37,284 - INFO - --------------------------------------------------
2026-01-14 13:08:37,289 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-01-14 13:08:46,656 - INFO - [Train] [65/90] | Loss: 0.2122 | Train Acc: 99.40%
2026-01-14 13:08:49,615 - INFO - [Valid] [65/90] | Loss: 0.4661 | Val Acc: 86.73%
2026-01-14 13:08:49,628 - INFO - [Metrics for 'abnormal'] | Precision: 0.8373 | Recall: 0.8854 | F1: 0.8607
2026-01-14 13:08:49,629 - INFO - [Metrics for 'normal'] | Precision: 0.8960 | Recall: 0.8516 | F1: 0.8732
2026-01-14 13:08:49,635 - INFO - --------------------------------------------------
2026-01-14 13:08:49,638 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-01-14 13:08:58,801 - INFO - [Train] [66/90] | Loss: 0.2106 | Train Acc: 99.63%
2026-01-14 13:09:01,846 - INFO - [Valid] [66/90] | Loss: 0.4668 | Val Acc: 85.55%
2026-01-14 13:09:01,858 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.8599 | F1: 0.8464
2026-01-14 13:09:01,859 - INFO - [Metrics for 'normal'] | Precision: 0.8757 | Recall: 0.8516 | F1: 0.8635
2026-01-14 13:09:01,863 - INFO - --------------------------------------------------
2026-01-14 13:09:01,867 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-01-14 13:09:11,203 - INFO - [Train] [67/90] | Loss: 0.2116 | Train Acc: 99.48%
2026-01-14 13:09:14,322 - INFO - [Valid] [67/90] | Loss: 0.4850 | Val Acc: 85.25%
2026-01-14 13:09:14,336 - INFO - [Metrics for 'abnormal'] | Precision: 0.8408 | Recall: 0.8408 | F1: 0.8408
2026-01-14 13:09:14,337 - INFO - [Metrics for 'normal'] | Precision: 0.8626 | Recall: 0.8626 | F1: 0.8626
2026-01-14 13:09:14,342 - INFO - --------------------------------------------------
2026-01-14 13:09:14,345 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-01-14 13:09:23,457 - INFO - [Train] [68/90] | Loss: 0.2152 | Train Acc: 99.26%
2026-01-14 13:09:25,967 - INFO - [Valid] [68/90] | Loss: 0.4510 | Val Acc: 86.73%
2026-01-14 13:09:25,980 - INFO - [Metrics for 'abnormal'] | Precision: 0.8457 | Recall: 0.8726 | F1: 0.8589
2026-01-14 13:09:25,981 - INFO - [Metrics for 'normal'] | Precision: 0.8870 | Recall: 0.8626 | F1: 0.8747
2026-01-14 13:09:25,986 - INFO - --------------------------------------------------
2026-01-14 13:09:25,989 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-01-14 13:09:35,598 - INFO - [Train] [69/90] | Loss: 0.2142 | Train Acc: 99.26%
2026-01-14 13:09:38,000 - INFO - [Valid] [69/90] | Loss: 0.4629 | Val Acc: 85.55%
2026-01-14 13:09:38,069 - INFO - [Metrics for 'abnormal'] | Precision: 0.8253 | Recall: 0.8726 | F1: 0.8483
2026-01-14 13:09:38,069 - INFO - [Metrics for 'normal'] | Precision: 0.8844 | Recall: 0.8407 | F1: 0.8620
2026-01-14 13:09:38,072 - INFO - --------------------------------------------------
2026-01-14 13:09:38,075 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-01-14 13:09:47,725 - INFO - [Train] [70/90] | Loss: 0.2059 | Train Acc: 99.78%
2026-01-14 13:09:50,529 - INFO - [Valid] [70/90] | Loss: 0.4659 | Val Acc: 87.32%
2026-01-14 13:09:50,539 - INFO - [Metrics for 'abnormal'] | Precision: 0.8608 | Recall: 0.8662 | F1: 0.8635
2026-01-14 13:09:50,540 - INFO - [Metrics for 'normal'] | Precision: 0.8840 | Recall: 0.8791 | F1: 0.8815
2026-01-14 13:09:50,544 - INFO - --------------------------------------------------
2026-01-14 13:09:50,547 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-01-14 13:10:00,681 - INFO - [Train] [71/90] | Loss: 0.2144 | Train Acc: 99.33%
2026-01-14 13:10:02,942 - INFO - [Valid] [71/90] | Loss: 0.4669 | Val Acc: 86.43%
2026-01-14 13:10:02,950 - INFO - [Metrics for 'abnormal'] | Precision: 0.8208 | Recall: 0.9045 | F1: 0.8606
2026-01-14 13:10:02,951 - INFO - [Metrics for 'normal'] | Precision: 0.9096 | Recall: 0.8297 | F1: 0.8678
2026-01-14 13:10:02,955 - INFO - --------------------------------------------------
2026-01-14 13:10:02,958 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-01-14 13:10:13,683 - INFO - [Train] [72/90] | Loss: 0.2129 | Train Acc: 99.40%
2026-01-14 13:10:15,743 - INFO - [Valid] [72/90] | Loss: 0.4721 | Val Acc: 86.43%
2026-01-14 13:10:15,757 - INFO - [Metrics for 'abnormal'] | Precision: 0.8447 | Recall: 0.8662 | F1: 0.8553
2026-01-14 13:10:15,758 - INFO - [Metrics for 'normal'] | Precision: 0.8820 | Recall: 0.8626 | F1: 0.8722
2026-01-14 13:10:15,765 - INFO - --------------------------------------------------
2026-01-14 13:10:15,768 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-01-14 13:10:25,769 - INFO - [Train] [73/90] | Loss: 0.2094 | Train Acc: 99.63%
2026-01-14 13:10:28,144 - INFO - [Valid] [73/90] | Loss: 0.4903 | Val Acc: 84.96%
2026-01-14 13:10:28,159 - INFO - [Metrics for 'abnormal'] | Precision: 0.8046 | Recall: 0.8917 | F1: 0.8459
2026-01-14 13:10:28,159 - INFO - [Metrics for 'normal'] | Precision: 0.8970 | Recall: 0.8132 | F1: 0.8530
2026-01-14 13:10:28,164 - INFO - --------------------------------------------------
2026-01-14 13:10:28,168 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-01-14 13:10:38,501 - INFO - [Train] [74/90] | Loss: 0.2124 | Train Acc: 99.40%
2026-01-14 13:10:40,656 - INFO - [Valid] [74/90] | Loss: 0.4552 | Val Acc: 86.73%
2026-01-14 13:10:40,664 - INFO - [Metrics for 'abnormal'] | Precision: 0.8457 | Recall: 0.8726 | F1: 0.8589
2026-01-14 13:10:40,664 - INFO - [Metrics for 'normal'] | Precision: 0.8870 | Recall: 0.8626 | F1: 0.8747
2026-01-14 13:10:40,667 - INFO - --------------------------------------------------
2026-01-14 13:10:40,669 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-01-14 13:10:49,946 - INFO - [Train] [75/90] | Loss: 0.2079 | Train Acc: 99.40%
2026-01-14 13:10:52,329 - INFO - [Valid] [75/90] | Loss: 0.4829 | Val Acc: 86.73%
2026-01-14 13:10:52,352 - INFO - [Metrics for 'abnormal'] | Precision: 0.8636 | Recall: 0.8471 | F1: 0.8553
2026-01-14 13:10:52,352 - INFO - [Metrics for 'normal'] | Precision: 0.8703 | Recall: 0.8846 | F1: 0.8774
2026-01-14 13:10:52,357 - INFO - --------------------------------------------------
2026-01-14 13:10:52,359 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-01-14 13:11:02,930 - INFO - [Train] [76/90] | Loss: 0.2109 | Train Acc: 99.26%
2026-01-14 13:11:05,330 - INFO - [Valid] [76/90] | Loss: 0.4800 | Val Acc: 86.14%
2026-01-14 13:11:05,340 - INFO - [Metrics for 'abnormal'] | Precision: 0.8354 | Recall: 0.8726 | F1: 0.8536
2026-01-14 13:11:05,341 - INFO - [Metrics for 'normal'] | Precision: 0.8857 | Recall: 0.8516 | F1: 0.8683
2026-01-14 13:11:05,344 - INFO - --------------------------------------------------
2026-01-14 13:11:05,347 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-01-14 13:11:15,611 - INFO - [Train] [77/90] | Loss: 0.2109 | Train Acc: 99.48%
2026-01-14 13:11:17,583 - INFO - [Valid] [77/90] | Loss: 0.4616 | Val Acc: 86.14%
2026-01-14 13:11:17,596 - INFO - [Metrics for 'abnormal'] | Precision: 0.8438 | Recall: 0.8599 | F1: 0.8517
2026-01-14 13:11:17,596 - INFO - [Metrics for 'normal'] | Precision: 0.8771 | Recall: 0.8626 | F1: 0.8698
2026-01-14 13:11:17,601 - INFO - --------------------------------------------------
2026-01-14 13:11:17,604 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-01-14 13:11:28,372 - INFO - [Train] [78/90] | Loss: 0.2137 | Train Acc: 99.33%
2026-01-14 13:11:30,585 - INFO - [Valid] [78/90] | Loss: 0.4592 | Val Acc: 87.02%
2026-01-14 13:11:30,603 - INFO - [Metrics for 'abnormal'] | Precision: 0.8645 | Recall: 0.8535 | F1: 0.8590
2026-01-14 13:11:30,607 - INFO - [Metrics for 'normal'] | Precision: 0.8750 | Recall: 0.8846 | F1: 0.8798
2026-01-14 13:11:30,614 - INFO - --------------------------------------------------
2026-01-14 13:11:30,620 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-01-14 13:11:40,816 - INFO - [Train] [79/90] | Loss: 0.2081 | Train Acc: 99.63%
2026-01-14 13:11:43,367 - INFO - [Valid] [79/90] | Loss: 0.4614 | Val Acc: 87.61%
2026-01-14 13:11:43,391 - INFO - [Metrics for 'abnormal'] | Precision: 0.8758 | Recall: 0.8535 | F1: 0.8645
2026-01-14 13:11:43,395 - INFO - [Metrics for 'normal'] | Precision: 0.8763 | Recall: 0.8956 | F1: 0.8859
2026-01-14 13:11:43,399 - INFO - --------------------------------------------------
2026-01-14 13:11:43,402 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-01-14 13:11:52,790 - INFO - [Train] [80/90] | Loss: 0.2044 | Train Acc: 99.85%
2026-01-14 13:11:55,198 - INFO - [Valid] [80/90] | Loss: 0.4655 | Val Acc: 87.61%
2026-01-14 13:11:55,218 - INFO - [Metrics for 'abnormal'] | Precision: 0.8808 | Recall: 0.8471 | F1: 0.8636
2026-01-14 13:11:55,222 - INFO - [Metrics for 'normal'] | Precision: 0.8723 | Recall: 0.9011 | F1: 0.8865
2026-01-14 13:11:55,226 - INFO - --------------------------------------------------
2026-01-14 13:11:55,232 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-01-14 13:12:05,406 - INFO - [Train] [81/90] | Loss: 0.2055 | Train Acc: 99.63%
2026-01-14 13:12:08,071 - INFO - [Valid] [81/90] | Loss: 0.4573 | Val Acc: 86.14%
2026-01-14 13:12:08,083 - INFO - [Metrics for 'abnormal'] | Precision: 0.8438 | Recall: 0.8599 | F1: 0.8517
2026-01-14 13:12:08,084 - INFO - [Metrics for 'normal'] | Precision: 0.8771 | Recall: 0.8626 | F1: 0.8698
2026-01-14 13:12:08,089 - INFO - --------------------------------------------------
2026-01-14 13:12:08,093 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-01-14 13:12:16,856 - INFO - [Train] [82/90] | Loss: 0.2111 | Train Acc: 99.40%
2026-01-14 13:12:19,485 - INFO - [Valid] [82/90] | Loss: 0.4801 | Val Acc: 86.43%
2026-01-14 13:12:19,494 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.9236 | F1: 0.8631
2026-01-14 13:12:19,495 - INFO - [Metrics for 'normal'] | Precision: 0.9250 | Recall: 0.8132 | F1: 0.8655
2026-01-14 13:12:19,498 - INFO - --------------------------------------------------
2026-01-14 13:12:19,502 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-01-14 13:12:29,340 - INFO - [Train] [83/90] | Loss: 0.2032 | Train Acc: 99.85%
2026-01-14 13:12:31,776 - INFO - [Valid] [83/90] | Loss: 0.4621 | Val Acc: 88.50%
2026-01-14 13:12:31,784 - INFO - [Metrics for 'abnormal'] | Precision: 0.8554 | Recall: 0.9045 | F1: 0.8793
2026-01-14 13:12:31,785 - INFO - [Metrics for 'normal'] | Precision: 0.9133 | Recall: 0.8681 | F1: 0.8901
2026-01-14 13:12:31,788 - INFO - --------------------------------------------------
2026-01-14 13:12:31,789 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-01-14 13:12:40,195 - INFO - [Train] [84/90] | Loss: 0.2041 | Train Acc: 99.70%
2026-01-14 13:12:42,917 - INFO - [Valid] [84/90] | Loss: 0.4512 | Val Acc: 87.02%
2026-01-14 13:12:42,932 - INFO - [Metrics for 'abnormal'] | Precision: 0.8553 | Recall: 0.8662 | F1: 0.8608
2026-01-14 13:12:42,936 - INFO - [Metrics for 'normal'] | Precision: 0.8833 | Recall: 0.8736 | F1: 0.8785
2026-01-14 13:12:42,943 - INFO - --------------------------------------------------
2026-01-14 13:12:42,946 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-01-14 13:12:52,033 - INFO - [Train] [85/90] | Loss: 0.2053 | Train Acc: 99.78%
2026-01-14 13:12:54,813 - INFO - [Valid] [85/90] | Loss: 0.4595 | Val Acc: 87.02%
2026-01-14 13:12:54,845 - INFO - [Metrics for 'abnormal'] | Precision: 0.8645 | Recall: 0.8535 | F1: 0.8590
2026-01-14 13:12:54,846 - INFO - [Metrics for 'normal'] | Precision: 0.8750 | Recall: 0.8846 | F1: 0.8798
2026-01-14 13:12:54,851 - INFO - --------------------------------------------------
2026-01-14 13:12:54,854 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-01-14 13:13:04,198 - INFO - [Train] [86/90] | Loss: 0.2090 | Train Acc: 99.55%
2026-01-14 13:13:06,639 - INFO - [Valid] [86/90] | Loss: 0.4650 | Val Acc: 87.02%
2026-01-14 13:13:06,650 - INFO - [Metrics for 'abnormal'] | Precision: 0.8466 | Recall: 0.8790 | F1: 0.8625
2026-01-14 13:13:06,651 - INFO - [Metrics for 'normal'] | Precision: 0.8920 | Recall: 0.8626 | F1: 0.8771
2026-01-14 13:13:06,655 - INFO - --------------------------------------------------
2026-01-14 13:13:06,658 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-01-14 13:13:16,186 - INFO - [Train] [87/90] | Loss: 0.2083 | Train Acc: 99.70%
2026-01-14 13:13:19,702 - INFO - [Valid] [87/90] | Loss: 0.4518 | Val Acc: 87.32%
2026-01-14 13:13:19,863 - INFO - [Metrics for 'abnormal'] | Precision: 0.8519 | Recall: 0.8790 | F1: 0.8652
2026-01-14 13:13:19,864 - INFO - [Metrics for 'normal'] | Precision: 0.8927 | Recall: 0.8681 | F1: 0.8802
2026-01-14 13:13:19,868 - INFO - --------------------------------------------------
2026-01-14 13:13:19,870 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-01-14 13:13:27,297 - INFO - [Train] [88/90] | Loss: 0.2041 | Train Acc: 99.78%
2026-01-14 13:13:30,489 - INFO - [Valid] [88/90] | Loss: 0.4583 | Val Acc: 87.61%
2026-01-14 13:13:30,501 - INFO - [Metrics for 'abnormal'] | Precision: 0.8710 | Recall: 0.8599 | F1: 0.8654
2026-01-14 13:13:30,502 - INFO - [Metrics for 'normal'] | Precision: 0.8804 | Recall: 0.8901 | F1: 0.8852
2026-01-14 13:13:30,507 - INFO - --------------------------------------------------
2026-01-14 13:13:30,510 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-01-14 13:13:38,784 - INFO - [Train] [89/90] | Loss: 0.2044 | Train Acc: 99.70%
2026-01-14 13:13:41,853 - INFO - [Valid] [89/90] | Loss: 0.4767 | Val Acc: 86.43%
2026-01-14 13:13:41,865 - INFO - [Metrics for 'abnormal'] | Precision: 0.8364 | Recall: 0.8790 | F1: 0.8571
2026-01-14 13:13:41,866 - INFO - [Metrics for 'normal'] | Precision: 0.8908 | Recall: 0.8516 | F1: 0.8708
2026-01-14 13:13:41,871 - INFO - --------------------------------------------------
2026-01-14 13:13:41,874 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-01-14 13:13:51,987 - INFO - [Train] [90/90] | Loss: 0.2089 | Train Acc: 99.48%
2026-01-14 13:13:54,174 - INFO - [Valid] [90/90] | Loss: 0.4464 | Val Acc: 87.02%
2026-01-14 13:13:54,185 - INFO - [Metrics for 'abnormal'] | Precision: 0.8553 | Recall: 0.8662 | F1: 0.8608
2026-01-14 13:13:54,186 - INFO - [Metrics for 'normal'] | Precision: 0.8833 | Recall: 0.8736 | F1: 0.8785
2026-01-14 13:13:54,191 - INFO - ==================================================
2026-01-14 13:13:54,191 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-01-14 13:13:54,192 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-01-14 13:13:54,192 - INFO - Baseline 모델 'efficientnet_b0'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-01-14 13:13:54,384 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-01-14 13:13:54,387 - INFO - Wanda Pruning을 시작합니다.
2026-01-14 13:13:54,388 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-01-14 13:13:54,392 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-01-14 13:14:01,278 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-01-14 13:14:01,638 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.5737939453124999)에 맞춰 변경되었습니다.
2026-01-14 13:14:01,639 - INFO - ==================================================
2026-01-14 13:14:01,779 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/best_model.pth'
2026-01-14 13:14:01,780 - INFO - ==================================================
2026-01-14 13:14:01,780 - INFO - Test 모드를 시작합니다.
2026-01-14 13:14:02,014 - INFO - 연산량 (MACs): 0.0914 GMACs per sample
2026-01-14 13:14:02,014 - INFO - 연산량 (FLOPs): 0.1829 GFLOPs per sample
2026-01-14 13:14:02,014 - INFO - ==================================================
2026-01-14 13:14:02,014 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-01-14 13:14:03,697 - INFO - 샘플 당 평균 Forward Pass 시간: 9.94ms (std: 3.05ms), FPS: 106.71 (std: 20.99) (1개 샘플 x 100회 반복)
2026-01-14 13:14:03,697 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 98.43 MB
2026-01-14 13:14:03,697 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-01-14 13:14:06,879 - INFO - [Test] Loss: 0.3331 | Test Acc: 86.43%
2026-01-14 13:14:06,896 - INFO - [Metrics for 'abnormal'] | Precision: 0.8364 | Recall: 0.8790 | F1: 0.8571
2026-01-14 13:14:06,896 - INFO - [Metrics for 'normal'] | Precision: 0.8908 | Recall: 0.8516 | F1: 0.8708
2026-01-14 13:14:07,547 - INFO - ==================================================
2026-01-14 13:14:07,548 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/confusion_matrix_20260114_123934.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/confusion_matrix_20260114_123934.pdf'
2026-01-14 13:14:07,548 - INFO - ==================================================
2026-01-14 13:14:07,548 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-01-14 13:14:19,038 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/model_fp32_20260114_123934.onnx'에 저장되었습니다. (크기: 2.99 MB)
2026-01-14 13:14:19,408 - INFO - [Model Load] ONNX 모델(FP32) 로드 메모리: 2535.00 MB (증가량: 8.01 MB)
2026-01-14 13:14:19,408 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-01-14 13:14:21,254 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 14.91ms (std: 10.05ms)
2026-01-14 13:14:21,254 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 87.02 FPS (std: 36.17) (1개 샘플 x 100회 반복)
2026-01-14 13:14:21,254 - INFO - [Inference Only] ONNX 런타임 추론 중 최대 CPU 메모리: 2542.51 MB (순수 증가량: 7.52 MB)
2026-01-14 13:14:21,255 - INFO - [Total Process] ONNX 모델(FP32) 전체 메모리 사용량: 2542.51 MB (전체 증가량: 15.52 MB)
2026-01-14 13:14:26,247 - INFO - [Test (ONNX)] | Test Acc (ONNX): 86.43%
2026-01-14 13:14:26,312 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8364 | Recall: 0.8790 | F1: 0.8571
2026-01-14 13:14:26,321 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8908 | Recall: 0.8516 | F1: 0.8708
2026-01-14 13:14:26,935 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/val_acc.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/val_acc.pdf'
2026-01-14 13:14:27,359 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/train_val_acc.pdf'
2026-01-14 13:14:27,752 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/F1_normal.pdf'
2026-01-14 13:14:28,329 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/val_loss.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/val_loss.pdf'
2026-01-14 13:14:28,780 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/learning_rate.pdf'
2026-01-14 13:14:33,386 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/compile.png' and 'log/Sewer-TAPNEW/baseline_efficientnet_b0_wanda_20260114_123934/graph_20260114_123934/compile.pdf'
