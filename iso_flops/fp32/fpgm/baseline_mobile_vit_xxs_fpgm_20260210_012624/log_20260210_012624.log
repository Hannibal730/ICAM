2026-02-10 01:26:24,795 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/log_20260210_012624.log'에 저장됩니다.
2026-02-10 01:26:24,798 - INFO - ==================================================
2026-02-10 01:26:24,798 - INFO - config.yaml:
2026-02-10 01:26:24,798 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: mobile_vit_xxs
  use_fpgm_pruning: true
  pruning_flops_target: 0.1816

2026-02-10 01:26:24,798 - INFO - ==================================================
2026-02-10 01:26:24,842 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:26:24,843 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:26:24,843 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:26:24,850 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:26:24,851 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:26:24,851 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:26:24,851 - INFO - Baseline 모델 'mobile_vit_xxs'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:26:25,082 - INFO - timm 모델(mobile_vit_xxs)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:26:25,098 - INFO - ==================================================
2026-02-10 01:26:25,098 - INFO - 모델 파라미터 수:
2026-02-10 01:26:25,098 - INFO -   - 총 파라미터: 951,666 개
2026-02-10 01:26:25,098 - INFO -   - 학습 가능한 파라미터: 951,666 개
2026-02-10 01:26:25,098 - INFO - ================================================================================
2026-02-10 01:26:25,098 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:26:25,098 - INFO - ================================================================================
2026-02-10 01:26:25,098 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:26:25,099 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:26:25,099 - INFO - ==================================================
2026-02-10 01:26:25,099 - INFO - train 모드를 시작합니다.
2026-02-10 01:26:25,100 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:26:25,100 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:26:25,100 - INFO - --------------------------------------------------
2026-02-10 01:26:25,101 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:26:33,124 - INFO - [Train] [1/10] | Loss: 0.5206 | Train Acc: 78.35%
2026-02-10 01:26:35,386 - INFO - [Valid] [1/10] | Loss: 0.5402 | Val Acc: 81.42%
2026-02-10 01:26:35,394 - INFO - [Metrics for 'abnormal'] | Precision: 0.8052 | Recall: 0.7898 | F1: 0.7974
2026-02-10 01:26:35,394 - INFO - [Metrics for 'normal'] | Precision: 0.8216 | Recall: 0.8352 | F1: 0.8283
2026-02-10 01:26:35,420 - INFO - [Best Model Saved] (val loss: 0.5402) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:26:35,420 - INFO - --------------------------------------------------
2026-02-10 01:26:35,422 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:26:42,603 - INFO - [Train] [2/10] | Loss: 0.4618 | Train Acc: 83.18%
2026-02-10 01:26:43,901 - INFO - [Valid] [2/10] | Loss: 0.5177 | Val Acc: 82.89%
2026-02-10 01:26:43,906 - INFO - [Metrics for 'abnormal'] | Precision: 0.8278 | Recall: 0.7962 | F1: 0.8117
2026-02-10 01:26:43,906 - INFO - [Metrics for 'normal'] | Precision: 0.8298 | Recall: 0.8571 | F1: 0.8432
2026-02-10 01:26:43,936 - INFO - [Best Model Saved] (val loss: 0.5177) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:26:43,936 - INFO - --------------------------------------------------
2026-02-10 01:26:43,938 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:26:49,229 - INFO - [Train] [3/10] | Loss: 0.4290 | Train Acc: 85.79%
2026-02-10 01:26:50,348 - INFO - [Valid] [3/10] | Loss: 0.5095 | Val Acc: 80.83%
2026-02-10 01:26:50,358 - INFO - [Metrics for 'abnormal'] | Precision: 0.7949 | Recall: 0.7898 | F1: 0.7923
2026-02-10 01:26:50,358 - INFO - [Metrics for 'normal'] | Precision: 0.8197 | Recall: 0.8242 | F1: 0.8219
2026-02-10 01:26:50,383 - INFO - [Best Model Saved] (val loss: 0.5095) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:26:50,384 - INFO - --------------------------------------------------
2026-02-10 01:26:50,385 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:26:55,869 - INFO - [Train] [4/10] | Loss: 0.3952 | Train Acc: 87.28%
2026-02-10 01:26:57,297 - INFO - [Valid] [4/10] | Loss: 0.5061 | Val Acc: 81.12%
2026-02-10 01:26:57,303 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.7707 | F1: 0.7908
2026-02-10 01:26:57,303 - INFO - [Metrics for 'normal'] | Precision: 0.8105 | Recall: 0.8462 | F1: 0.8280
2026-02-10 01:26:57,333 - INFO - [Best Model Saved] (val loss: 0.5061) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:26:57,334 - INFO - --------------------------------------------------
2026-02-10 01:26:57,335 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:27:03,632 - INFO - [Train] [5/10] | Loss: 0.3706 | Train Acc: 90.10%
2026-02-10 01:27:04,847 - INFO - [Valid] [5/10] | Loss: 0.5548 | Val Acc: 82.01%
2026-02-10 01:27:04,854 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 01:27:04,855 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 01:27:04,856 - INFO - --------------------------------------------------
2026-02-10 01:27:04,858 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:27:10,596 - INFO - [Train] [6/10] | Loss: 0.3580 | Train Acc: 90.55%
2026-02-10 01:27:11,737 - INFO - [Valid] [6/10] | Loss: 0.5013 | Val Acc: 81.12%
2026-02-10 01:27:11,742 - INFO - [Metrics for 'abnormal'] | Precision: 0.7962 | Recall: 0.7962 | F1: 0.7962
2026-02-10 01:27:11,742 - INFO - [Metrics for 'normal'] | Precision: 0.8242 | Recall: 0.8242 | F1: 0.8242
2026-02-10 01:27:11,786 - INFO - [Best Model Saved] (val loss: 0.5013) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:27:11,787 - INFO - --------------------------------------------------
2026-02-10 01:27:11,788 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:27:16,083 - INFO - [Train] [7/10] | Loss: 0.3034 | Train Acc: 94.27%
2026-02-10 01:27:17,120 - INFO - [Valid] [7/10] | Loss: 0.5032 | Val Acc: 81.12%
2026-02-10 01:27:17,126 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-02-10 01:27:17,126 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-02-10 01:27:17,128 - INFO - --------------------------------------------------
2026-02-10 01:27:17,129 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:27:21,323 - INFO - [Train] [8/10] | Loss: 0.2809 | Train Acc: 96.21%
2026-02-10 01:27:22,326 - INFO - [Valid] [8/10] | Loss: 0.5044 | Val Acc: 82.30%
2026-02-10 01:27:22,332 - INFO - [Metrics for 'abnormal'] | Precision: 0.7870 | Recall: 0.8471 | F1: 0.8160
2026-02-10 01:27:22,332 - INFO - [Metrics for 'normal'] | Precision: 0.8588 | Recall: 0.8022 | F1: 0.8295
2026-02-10 01:27:22,334 - INFO - --------------------------------------------------
2026-02-10 01:27:22,337 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:27:26,792 - INFO - [Train] [9/10] | Loss: 0.2637 | Train Acc: 97.25%
2026-02-10 01:27:27,866 - INFO - [Valid] [9/10] | Loss: 0.4928 | Val Acc: 83.19%
2026-02-10 01:27:27,870 - INFO - [Metrics for 'abnormal'] | Precision: 0.8125 | Recall: 0.8280 | F1: 0.8202
2026-02-10 01:27:27,870 - INFO - [Metrics for 'normal'] | Precision: 0.8492 | Recall: 0.8352 | F1: 0.8421
2026-02-10 01:27:27,892 - INFO - [Best Model Saved] (val loss: 0.4928) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:27:27,892 - INFO - --------------------------------------------------
2026-02-10 01:27:27,893 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:27:32,000 - INFO - [Train] [10/10] | Loss: 0.2529 | Train Acc: 98.14%
2026-02-10 01:27:33,046 - INFO - [Valid] [10/10] | Loss: 0.5059 | Val Acc: 84.66%
2026-02-10 01:27:33,051 - INFO - [Metrics for 'abnormal'] | Precision: 0.8344 | Recall: 0.8344 | F1: 0.8344
2026-02-10 01:27:33,051 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.8571 | F1: 0.8571
2026-02-10 01:27:33,054 - INFO - ================================================================================
2026-02-10 01:27:33,054 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:27:33,054 - INFO - ================================================================================
2026-02-10 01:27:33,102 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:27:33,102 - INFO - ================================================================================
2026-02-10 01:27:33,102 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:27:33,208 - INFO - 원본 모델 FLOPs: 0.5384 GFLOPs
2026-02-10 01:27:33,280 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:33,280 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:33,281 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:33,796 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:27:33,797 - INFO - ==================================================
2026-02-10 01:27:33,854 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.1747 GFLOPs (감소율: 67.56%)
2026-02-10 01:27:34,289 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:34,289 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,290 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,742 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.2475)에 맞춰 변경되었습니다.
2026-02-10 01:27:34,742 - INFO - ==================================================
2026-02-10 01:27:34,807 - INFO -   [탐색  2] 희소도: 0.2475 -> FLOPs: 0.3329 GFLOPs (감소율: 38.17%)
2026-02-10 01:27:34,843 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:34,844 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,844 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,432 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.37124999999999997)에 맞춰 변경되었습니다.
2026-02-10 01:27:35,432 - INFO - ==================================================
2026-02-10 01:27:35,536 - INFO -   [탐색  3] 희소도: 0.3712 -> FLOPs: 0.2479 GFLOPs (감소율: 53.96%)
2026-02-10 01:27:35,575 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:35,575 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,576 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,895 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.433125)에 맞춰 변경되었습니다.
2026-02-10 01:27:35,895 - INFO - ==================================================
2026-02-10 01:27:35,959 - INFO -   [탐색  4] 희소도: 0.4331 -> FLOPs: 0.2093 GFLOPs (감소율: 61.13%)
2026-02-10 01:27:35,997 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:35,997 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,998 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,307 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4640625)에 맞춰 변경되었습니다.
2026-02-10 01:27:36,307 - INFO - ==================================================
2026-02-10 01:27:36,380 - INFO -   [탐색  5] 희소도: 0.4641 -> FLOPs: 0.1880 GFLOPs (감소율: 65.08%)
2026-02-10 01:27:36,425 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:36,425 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,425 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:37,292 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47953124999999996)에 맞춰 변경되었습니다.
2026-02-10 01:27:37,292 - INFO - ==================================================
2026-02-10 01:27:37,358 - INFO -   [탐색  6] 희소도: 0.4795 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:37,398 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:37,398 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:37,398 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:37,873 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.471796875)에 맞춰 변경되었습니다.
2026-02-10 01:27:37,874 - INFO - ==================================================
2026-02-10 01:27:37,925 - INFO -   [탐색  7] 희소도: 0.4718 -> FLOPs: 0.1838 GFLOPs (감소율: 65.85%)
2026-02-10 01:27:37,960 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:37,960 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:37,961 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,408 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4756640625)에 맞춰 변경되었습니다.
2026-02-10 01:27:38,408 - INFO - ==================================================
2026-02-10 01:27:38,455 - INFO -   [탐색  8] 희소도: 0.4757 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-02-10 01:27:38,501 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:38,501 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,502 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,879 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47759765624999995)에 맞춰 변경되었습니다.
2026-02-10 01:27:38,879 - INFO - ==================================================
2026-02-10 01:27:38,921 - INFO -   [탐색  9] 희소도: 0.4776 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:38,948 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:38,948 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,948 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:39,397 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47856445312499996)에 맞춰 변경되었습니다.
2026-02-10 01:27:39,397 - INFO - ==================================================
2026-02-10 01:27:39,437 - INFO -   [탐색 10] 희소도: 0.4786 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:39,474 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:39,474 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:39,475 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,387 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47904785156249996)에 맞춰 변경되었습니다.
2026-02-10 01:27:40,387 - INFO - ==================================================
2026-02-10 01:27:40,431 - INFO -   [탐색 11] 희소도: 0.4790 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:40,469 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:40,469 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,470 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,890 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47928955078124996)에 맞춰 변경되었습니다.
2026-02-10 01:27:40,890 - INFO - ==================================================
2026-02-10 01:27:40,931 - INFO -   [탐색 12] 희소도: 0.4793 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:40,970 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:40,971 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,971 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,366 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916870117187493)에 맞춰 변경되었습니다.
2026-02-10 01:27:41,367 - INFO - ==================================================
2026-02-10 01:27:41,409 - INFO -   [탐색 13] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:41,449 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:41,450 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,451 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,766 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47910827636718745)에 맞춰 변경되었습니다.
2026-02-10 01:27:41,767 - INFO - ==================================================
2026-02-10 01:27:41,810 - INFO -   [탐색 14] 희소도: 0.4791 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:41,847 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:41,848 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,848 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,309 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791384887695312)에 맞춰 변경되었습니다.
2026-02-10 01:27:42,309 - INFO - ==================================================
2026-02-10 01:27:42,354 - INFO -   [탐색 15] 희소도: 0.4791 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:42,391 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:42,391 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,392 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,434 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791535949707031)에 맞춰 변경되었습니다.
2026-02-10 01:27:43,435 - INFO - ==================================================
2026-02-10 01:27:43,461 - INFO -   [탐색 16] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:43,484 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:43,484 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,485 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,898 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.479161148071289)에 맞춰 변경되었습니다.
2026-02-10 01:27:43,898 - INFO - ==================================================
2026-02-10 01:27:43,945 - INFO -   [탐색 17] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:43,997 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:43,998 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,999 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,320 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.479164924621582)에 맞춰 변경되었습니다.
2026-02-10 01:27:44,320 - INFO - ==================================================
2026-02-10 01:27:44,365 - INFO -   [탐색 18] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:44,400 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:44,400 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,400 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,724 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916681289672847)에 맞춰 변경되었습니다.
2026-02-10 01:27:44,724 - INFO - ==================================================
2026-02-10 01:27:44,768 - INFO -   [탐색 19] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:44,805 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:44,806 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,806 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:45,124 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916586875915523)에 맞춰 변경되었습니다.
2026-02-10 01:27:45,125 - INFO - ==================================================
2026-02-10 01:27:45,181 - INFO -   [탐색 20] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:45,226 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:45,226 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:45,227 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,191 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916634082794185)에 맞춰 변경되었습니다.
2026-02-10 01:27:46,192 - INFO - ==================================================
2026-02-10 01:27:46,237 - INFO -   [탐색 21] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:46,282 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:46,282 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,283 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,676 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916657686233516)에 맞춰 변경되었습니다.
2026-02-10 01:27:46,676 - INFO - ==================================================
2026-02-10 01:27:46,725 - INFO -   [탐색 22] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:46,764 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:46,764 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,765 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:47,233 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666948795318)에 맞춰 변경되었습니다.
2026-02-10 01:27:47,233 - INFO - ==================================================
2026-02-10 01:27:47,279 - INFO -   [탐색 23] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:47,318 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:47,319 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:47,319 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:47,750 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916663587093344)에 맞춰 변경되었습니다.
2026-02-10 01:27:47,750 - INFO - ==================================================
2026-02-10 01:27:47,795 - INFO -   [탐색 24] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:47,834 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:47,834 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:47,835 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:48,295 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666653752326)에 맞춰 변경되었습니다.
2026-02-10 01:27:48,295 - INFO - ==================================================
2026-02-10 01:27:48,331 - INFO -   [탐색 25] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:48,697 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:48,698 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:48,699 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:49,111 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916668012738217)에 맞춰 변경되었습니다.
2026-02-10 01:27:49,111 - INFO - ==================================================
2026-02-10 01:27:49,155 - INFO -   [탐색 26] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:49,195 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:49,196 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:49,196 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:49,520 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666727513074)에 맞춰 변경되었습니다.
2026-02-10 01:27:49,520 - INFO - ==================================================
2026-02-10 01:27:49,567 - INFO -   [탐색 27] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:49,611 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:49,612 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:49,612 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,005 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666906327)에 맞춰 변경되었습니다.
2026-02-10 01:27:50,005 - INFO - ==================================================
2026-02-10 01:27:50,047 - INFO -   [탐색 28] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:50,092 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:50,092 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,093 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,449 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666672192513)에 맞춰 변경되었습니다.
2026-02-10 01:27:50,449 - INFO - ==================================================
2026-02-10 01:27:50,496 - INFO -   [탐색 29] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:50,537 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:50,537 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,538 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:51,456 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666662972419)에 맞춰 변경되었습니다.
2026-02-10 01:27:51,457 - INFO - ==================================================
2026-02-10 01:27:51,511 - INFO -   [탐색 30] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:51,556 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:51,556 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:51,557 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:51,966 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666667582466)에 맞춰 변경되었습니다.
2026-02-10 01:27:51,966 - INFO - ==================================================
2026-02-10 01:27:52,006 - INFO -   [탐색 31] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:52,048 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:52,048 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:52,049 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:52,474 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666652774426)에 맞춰 변경되었습니다.
2026-02-10 01:27:52,474 - INFO - ==================================================
2026-02-10 01:27:52,524 - INFO -   [탐색 32] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:52,563 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:52,564 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:52,565 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:53,056 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666664299545)에 맞춰 변경되었습니다.
2026-02-10 01:27:53,056 - INFO - ==================================================
2026-02-10 01:27:53,099 - INFO -   [탐색 33] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:53,138 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:53,138 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:53,138 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:53,504 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.479166666700621)에 맞춰 변경되었습니다.
2026-02-10 01:27:53,504 - INFO - ==================================================
2026-02-10 01:27:53,552 - INFO -   [탐색 34] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:53,591 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:53,592 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:53,592 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:54,489 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666667180824)에 맞춰 변경되었습니다.
2026-02-10 01:27:54,489 - INFO - ==================================================
2026-02-10 01:27:54,535 - INFO -   [탐색 35] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:54,575 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:54,575 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:54,576 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,027 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666574018)에 맞춰 변경되었습니다.
2026-02-10 01:27:55,027 - INFO - ==================================================
2026-02-10 01:27:55,073 - INFO -   [탐색 36] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:55,113 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:55,113 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,114 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,484 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666460506)에 맞춰 변경되었습니다.
2026-02-10 01:27:55,485 - INFO - ==================================================
2026-02-10 01:27:55,531 - INFO -   [탐색 37] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:55,576 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:55,576 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,578 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,977 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666682066)에 맞춰 변경되었습니다.
2026-02-10 01:27:55,977 - INFO - ==================================================
2026-02-10 01:27:56,024 - INFO -   [탐색 38] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:56,067 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:56,068 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:56,068 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:56,426 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666640584)에 맞춰 변경되었습니다.
2026-02-10 01:27:56,426 - INFO - ==================================================
2026-02-10 01:27:56,474 - INFO -   [탐색 39] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:56,518 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:56,519 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:56,519 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:57,549 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666730623)에 맞춰 변경되었습니다.
2026-02-10 01:27:57,550 - INFO - ==================================================
2026-02-10 01:27:57,596 - INFO -   [탐색 40] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:57,641 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:57,641 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:57,642 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:58,099 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666685603)에 맞춰 변경되었습니다.
2026-02-10 01:27:58,099 - INFO - ==================================================
2026-02-10 01:27:58,148 - INFO -   [탐색 41] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:58,193 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:58,193 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:58,194 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:58,578 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666663094)에 맞춰 변경되었습니다.
2026-02-10 01:27:58,578 - INFO - ==================================================
2026-02-10 01:27:58,628 - INFO -   [탐색 42] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:27:58,674 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:58,674 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:58,675 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:59,148 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666674346)에 맞춰 변경되었습니다.
2026-02-10 01:27:59,149 - INFO - ==================================================
2026-02-10 01:27:59,197 - INFO -   [탐색 43] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:59,238 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:59,238 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:59,239 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:59,605 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666668717)에 맞춰 변경되었습니다.
2026-02-10 01:27:59,606 - INFO - ==================================================
2026-02-10 01:27:59,659 - INFO -   [탐색 44] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:27:59,700 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:59,700 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:59,701 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:00,754 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666591)에 맞춰 변경되었습니다.
2026-02-10 01:28:00,755 - INFO - ==================================================
2026-02-10 01:28:00,797 - INFO -   [탐색 45] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:00,839 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:00,839 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:00,840 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:01,333 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666731)에 맞춰 변경되었습니다.
2026-02-10 01:28:01,333 - INFO - ==================================================
2026-02-10 01:28:01,380 - INFO -   [탐색 46] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:28:01,423 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:01,423 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:01,423 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:01,820 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666661)에 맞춰 변경되었습니다.
2026-02-10 01:28:01,821 - INFO - ==================================================
2026-02-10 01:28:01,867 - INFO -   [탐색 47] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:01,911 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:01,911 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:01,912 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:02,302 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666696)에 맞춰 변경되었습니다.
2026-02-10 01:28:02,302 - INFO - ==================================================
2026-02-10 01:28:02,348 - INFO -   [탐색 48] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:28:02,392 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:02,392 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:02,392 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:02,771 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666785)에 맞춰 변경되었습니다.
2026-02-10 01:28:02,771 - INFO - ==================================================
2026-02-10 01:28:02,818 - INFO -   [탐색 49] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:28:02,856 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:02,856 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:02,856 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:03,785 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666696)에 맞춰 변경되었습니다.
2026-02-10 01:28:03,785 - INFO - ==================================================
2026-02-10 01:28:03,836 - INFO -   [탐색 50] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:28:03,882 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:03,882 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:03,883 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:04,299 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666665)에 맞춰 변경되었습니다.
2026-02-10 01:28:04,299 - INFO - ==================================================
2026-02-10 01:28:04,352 - INFO -   [탐색 51] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:04,395 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:04,395 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:04,396 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:04,761 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666674)에 맞춰 변경되었습니다.
2026-02-10 01:28:04,761 - INFO - ==================================================
2026-02-10 01:28:04,807 - INFO -   [탐색 52] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:28:04,847 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:04,847 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:04,848 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:05,232 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:05,233 - INFO - ==================================================
2026-02-10 01:28:05,279 - INFO -   [탐색 53] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:05,324 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:05,324 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:05,325 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:05,714 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666667)에 맞춰 변경되었습니다.
2026-02-10 01:28:05,715 - INFO - ==================================================
2026-02-10 01:28:06,310 - INFO -   [탐색 54] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:28:06,355 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:06,355 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:06,356 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:06,756 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:06,756 - INFO - ==================================================
2026-02-10 01:28:06,807 - INFO -   [탐색 55] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:06,849 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:06,849 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:06,850 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:07,298 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:07,299 - INFO - ==================================================
2026-02-10 01:28:07,343 - INFO -   [탐색 56] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:07,385 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:07,385 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:07,386 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:08,181 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:08,181 - INFO - ==================================================
2026-02-10 01:28:08,223 - INFO -   [탐색 57] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:08,266 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:08,267 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:08,267 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:08,643 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:08,644 - INFO - ==================================================
2026-02-10 01:28:08,688 - INFO -   [탐색 58] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:08,729 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:08,730 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:08,731 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:09,595 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:09,596 - INFO - ==================================================
2026-02-10 01:28:09,647 - INFO -   [탐색 59] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:09,688 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:09,688 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:09,689 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:10,123 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:10,124 - INFO - ==================================================
2026-02-10 01:28:10,170 - INFO -   [탐색 60] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:10,213 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:10,213 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:10,214 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:10,583 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:10,583 - INFO - ==================================================
2026-02-10 01:28:10,625 - INFO -   [탐색 61] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:10,667 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:10,668 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:10,668 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:11,040 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:11,041 - INFO - ==================================================
2026-02-10 01:28:11,093 - INFO -   [탐색 62] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:11,137 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:11,138 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:11,138 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:11,518 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:11,518 - INFO - ==================================================
2026-02-10 01:28:11,564 - INFO -   [탐색 63] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:11,608 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:11,609 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:11,610 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:12,551 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:12,552 - INFO - ==================================================
2026-02-10 01:28:12,601 - INFO -   [탐색 64] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:12,649 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:12,649 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:12,650 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:13,169 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:13,169 - INFO - ==================================================
2026-02-10 01:28:13,213 - INFO -   [탐색 65] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:13,254 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:13,254 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:13,255 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:13,704 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:13,705 - INFO - ==================================================
2026-02-10 01:28:13,748 - INFO -   [탐색 66] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:13,791 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:13,792 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:13,792 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:14,278 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:14,278 - INFO - ==================================================
2026-02-10 01:28:14,326 - INFO -   [탐색 67] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:14,376 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:14,376 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:14,377 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:14,880 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:14,880 - INFO - ==================================================
2026-02-10 01:28:14,922 - INFO -   [탐색 68] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:15,545 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:15,545 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:15,546 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:15,938 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:15,938 - INFO - ==================================================
2026-02-10 01:28:15,987 - INFO -   [탐색 69] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:16,029 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:16,029 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:16,030 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:16,376 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:16,377 - INFO - ==================================================
2026-02-10 01:28:16,423 - INFO -   [탐색 70] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:16,466 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:16,466 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:16,467 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:16,819 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:16,819 - INFO - ==================================================
2026-02-10 01:28:16,868 - INFO -   [탐색 71] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:16,910 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:16,910 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:16,911 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:17,278 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:17,279 - INFO - ==================================================
2026-02-10 01:28:17,330 - INFO -   [탐색 72] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:17,370 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:17,370 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:17,371 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:18,380 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:18,380 - INFO - ==================================================
2026-02-10 01:28:18,431 - INFO -   [탐색 73] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:18,473 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:18,474 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:18,475 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:18,886 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:18,886 - INFO - ==================================================
2026-02-10 01:28:18,935 - INFO -   [탐색 74] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:18,983 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:18,983 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:18,984 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:19,417 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:19,417 - INFO - ==================================================
2026-02-10 01:28:19,464 - INFO -   [탐색 75] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:19,507 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:19,507 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:19,508 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:19,940 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:19,940 - INFO - ==================================================
2026-02-10 01:28:19,986 - INFO -   [탐색 76] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:20,027 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:20,028 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:20,028 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:20,522 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:20,522 - INFO - ==================================================
2026-02-10 01:28:20,563 - INFO -   [탐색 77] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:20,604 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:20,605 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:20,605 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:21,726 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:21,727 - INFO - ==================================================
2026-02-10 01:28:21,773 - INFO -   [탐색 78] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:21,817 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:21,817 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:21,818 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:22,169 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:22,169 - INFO - ==================================================
2026-02-10 01:28:22,222 - INFO -   [탐색 79] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:22,267 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:22,267 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:22,268 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:22,660 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:22,660 - INFO - ==================================================
2026-02-10 01:28:22,707 - INFO -   [탐색 80] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:22,749 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:22,749 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:22,750 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:23,095 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:23,095 - INFO - ==================================================
2026-02-10 01:28:23,145 - INFO -   [탐색 81] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:23,188 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:23,188 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:23,189 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:23,537 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:23,537 - INFO - ==================================================
2026-02-10 01:28:23,586 - INFO -   [탐색 82] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:23,631 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:23,631 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:23,631 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:24,632 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:24,632 - INFO - ==================================================
2026-02-10 01:28:24,684 - INFO -   [탐색 83] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:24,728 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:24,728 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:24,729 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:25,163 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:25,164 - INFO - ==================================================
2026-02-10 01:28:25,214 - INFO -   [탐색 84] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:25,258 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:25,259 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:25,260 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:25,710 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:25,710 - INFO - ==================================================
2026-02-10 01:28:25,756 - INFO -   [탐색 85] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:25,800 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:25,800 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:25,801 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:26,219 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:26,220 - INFO - ==================================================
2026-02-10 01:28:26,265 - INFO -   [탐색 86] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:26,307 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:26,307 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:26,308 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:26,707 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:26,707 - INFO - ==================================================
2026-02-10 01:28:26,756 - INFO -   [탐색 87] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:27,364 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:27,364 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:27,365 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:27,826 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:27,826 - INFO - ==================================================
2026-02-10 01:28:27,877 - INFO -   [탐색 88] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:27,918 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:27,918 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:27,919 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:28,312 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:28,313 - INFO - ==================================================
2026-02-10 01:28:28,359 - INFO -   [탐색 89] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:28,402 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:28,402 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:28,403 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:28,907 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:28,907 - INFO - ==================================================
2026-02-10 01:28:28,944 - INFO -   [탐색 90] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:28,987 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:28,987 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:28,988 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:29,318 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:29,318 - INFO - ==================================================
2026-02-10 01:28:29,371 - INFO -   [탐색 91] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:29,416 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:29,416 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:29,417 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:30,421 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:30,421 - INFO - ==================================================
2026-02-10 01:28:30,472 - INFO -   [탐색 92] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:30,517 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:30,518 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:30,519 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:30,908 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:30,908 - INFO - ==================================================
2026-02-10 01:28:30,957 - INFO -   [탐색 93] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:31,000 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:31,000 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:31,001 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:31,362 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:31,362 - INFO - ==================================================
2026-02-10 01:28:31,412 - INFO -   [탐색 94] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:31,456 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:31,457 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:31,457 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:31,884 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:31,884 - INFO - ==================================================
2026-02-10 01:28:31,929 - INFO -   [탐색 95] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:31,969 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:31,970 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:31,970 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:32,374 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:32,374 - INFO - ==================================================
2026-02-10 01:28:32,419 - INFO -   [탐색 96] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:32,459 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:32,459 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:32,460 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:33,565 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:33,565 - INFO - ==================================================
2026-02-10 01:28:33,614 - INFO -   [탐색 97] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:33,654 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:33,655 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:33,655 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:34,094 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:34,094 - INFO - ==================================================
2026-02-10 01:28:34,138 - INFO -   [탐색 98] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:34,179 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:34,179 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:34,180 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:34,602 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:34,602 - INFO - ==================================================
2026-02-10 01:28:34,645 - INFO -   [탐색 99] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:34,686 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:34,686 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:34,687 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:35,099 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:28:35,100 - INFO - ==================================================
2026-02-10 01:28:35,146 - INFO -   [탐색 100] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:28:35,147 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.4786 입니다.
2026-02-10 01:28:35,147 - INFO - ================================================================================
2026-02-10 01:28:35,151 - INFO - 계산된 Pruning 정보(희소도: 0.4786)를 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:28:35,195 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:28:35,283 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:28:35,283 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:28:35,284 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:28:35,793 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47856445312499996)에 맞춰 변경되었습니다.
2026-02-10 01:28:35,794 - INFO - ==================================================
2026-02-10 01:28:35,796 - INFO - ==================================================
2026-02-10 01:28:35,796 - INFO - 모델 파라미터 수:
2026-02-10 01:28:35,796 - INFO -   - 총 파라미터: 320,501 개
2026-02-10 01:28:35,796 - INFO -   - 학습 가능한 파라미터: 320,501 개
2026-02-10 01:28:35,840 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:28:35,926 - INFO - FLOPs가 0.5384 GFLOPs에서 0.1824 GFLOPs로 감소했습니다 (감소율: 66.12%).
2026-02-10 01:28:35,927 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:28:35,927 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:28:35,928 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:28:35,928 - INFO - ==================================================
2026-02-10 01:28:35,928 - INFO - train 모드를 시작합니다.
2026-02-10 01:28:35,929 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:28:35,929 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:28:35,929 - INFO - --------------------------------------------------
2026-02-10 01:28:35,930 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:28:43,615 - INFO - [Train] [11/90] | Loss: 0.5098 | Train Acc: 78.94%
2026-02-10 01:28:45,264 - INFO - [Valid] [11/90] | Loss: 0.5086 | Val Acc: 80.83%
2026-02-10 01:28:45,271 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8217 | F1: 0.7988
2026-02-10 01:28:45,272 - INFO - [Metrics for 'normal'] | Precision: 0.8382 | Recall: 0.7967 | F1: 0.8169
2026-02-10 01:28:45,293 - INFO - [Best Model Saved] (val loss: 0.5086) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:28:45,293 - INFO - --------------------------------------------------
2026-02-10 01:28:45,295 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:28:52,422 - INFO - [Train] [12/90] | Loss: 0.4640 | Train Acc: 82.66%
2026-02-10 01:28:53,793 - INFO - [Valid] [12/90] | Loss: 0.5066 | Val Acc: 81.12%
2026-02-10 01:28:53,799 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-02-10 01:28:53,799 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-02-10 01:28:53,845 - INFO - [Best Model Saved] (val loss: 0.5066) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:28:53,846 - INFO - --------------------------------------------------
2026-02-10 01:28:53,847 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:29:01,189 - INFO - [Train] [13/90] | Loss: 0.4256 | Train Acc: 85.79%
2026-02-10 01:29:02,345 - INFO - [Valid] [13/90] | Loss: 0.5382 | Val Acc: 77.29%
2026-02-10 01:29:02,351 - INFO - [Metrics for 'abnormal'] | Precision: 0.7667 | Recall: 0.7325 | F1: 0.7492
2026-02-10 01:29:02,351 - INFO - [Metrics for 'normal'] | Precision: 0.7778 | Recall: 0.8077 | F1: 0.7925
2026-02-10 01:29:02,353 - INFO - --------------------------------------------------
2026-02-10 01:29:02,355 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:29:09,594 - INFO - [Train] [14/90] | Loss: 0.4183 | Train Acc: 86.68%
2026-02-10 01:29:11,181 - INFO - [Valid] [14/90] | Loss: 0.5087 | Val Acc: 81.42%
2026-02-10 01:29:11,186 - INFO - [Metrics for 'abnormal'] | Precision: 0.8176 | Recall: 0.7707 | F1: 0.7934
2026-02-10 01:29:11,187 - INFO - [Metrics for 'normal'] | Precision: 0.8115 | Recall: 0.8516 | F1: 0.8311
2026-02-10 01:29:11,188 - INFO - --------------------------------------------------
2026-02-10 01:29:11,190 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:29:18,343 - INFO - [Train] [15/90] | Loss: 0.3881 | Train Acc: 87.72%
2026-02-10 01:29:19,638 - INFO - [Valid] [15/90] | Loss: 0.5144 | Val Acc: 78.76%
2026-02-10 01:29:19,644 - INFO - [Metrics for 'abnormal'] | Precision: 0.7374 | Recall: 0.8408 | F1: 0.7857
2026-02-10 01:29:19,644 - INFO - [Metrics for 'normal'] | Precision: 0.8438 | Recall: 0.7418 | F1: 0.7895
2026-02-10 01:29:19,645 - INFO - --------------------------------------------------
2026-02-10 01:29:19,647 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:29:26,790 - INFO - [Train] [16/90] | Loss: 0.3701 | Train Acc: 89.81%
2026-02-10 01:29:28,063 - INFO - [Valid] [16/90] | Loss: 0.5365 | Val Acc: 81.42%
2026-02-10 01:29:28,068 - INFO - [Metrics for 'abnormal'] | Precision: 0.7866 | Recall: 0.8217 | F1: 0.8037
2026-02-10 01:29:28,068 - INFO - [Metrics for 'normal'] | Precision: 0.8400 | Recall: 0.8077 | F1: 0.8235
2026-02-10 01:29:28,070 - INFO - --------------------------------------------------
2026-02-10 01:29:28,072 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:29:34,624 - INFO - [Train] [17/90] | Loss: 0.3448 | Train Acc: 91.96%
2026-02-10 01:29:35,884 - INFO - [Valid] [17/90] | Loss: 0.5435 | Val Acc: 80.53%
2026-02-10 01:29:35,888 - INFO - [Metrics for 'abnormal'] | Precision: 0.8182 | Recall: 0.7452 | F1: 0.7800
2026-02-10 01:29:35,888 - INFO - [Metrics for 'normal'] | Precision: 0.7959 | Recall: 0.8571 | F1: 0.8254
2026-02-10 01:29:35,889 - INFO - --------------------------------------------------
2026-02-10 01:29:35,891 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:29:43,161 - INFO - [Train] [18/90] | Loss: 0.3390 | Train Acc: 92.04%
2026-02-10 01:29:44,135 - INFO - [Valid] [18/90] | Loss: 0.5147 | Val Acc: 80.83%
2026-02-10 01:29:44,144 - INFO - [Metrics for 'abnormal'] | Precision: 0.7805 | Recall: 0.8153 | F1: 0.7975
2026-02-10 01:29:44,144 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8022 | F1: 0.8179
2026-02-10 01:29:44,146 - INFO - --------------------------------------------------
2026-02-10 01:29:44,147 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:29:51,540 - INFO - [Train] [19/90] | Loss: 0.3198 | Train Acc: 93.60%
2026-02-10 01:29:52,964 - INFO - [Valid] [19/90] | Loss: 0.5170 | Val Acc: 80.24%
2026-02-10 01:29:52,969 - INFO - [Metrics for 'abnormal'] | Precision: 0.7419 | Recall: 0.8790 | F1: 0.8047
2026-02-10 01:29:52,970 - INFO - [Metrics for 'normal'] | Precision: 0.8758 | Recall: 0.7363 | F1: 0.8000
2026-02-10 01:29:52,971 - INFO - --------------------------------------------------
2026-02-10 01:29:52,973 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:30:00,195 - INFO - [Train] [20/90] | Loss: 0.3065 | Train Acc: 94.49%
2026-02-10 01:30:01,888 - INFO - [Valid] [20/90] | Loss: 0.4647 | Val Acc: 83.19%
2026-02-10 01:30:01,892 - INFO - [Metrics for 'abnormal'] | Precision: 0.8247 | Recall: 0.8089 | F1: 0.8167
2026-02-10 01:30:01,893 - INFO - [Metrics for 'normal'] | Precision: 0.8378 | Recall: 0.8516 | F1: 0.8447
2026-02-10 01:30:01,931 - INFO - [Best Model Saved] (val loss: 0.4647) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:30:01,931 - INFO - --------------------------------------------------
2026-02-10 01:30:01,933 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:30:09,035 - INFO - [Train] [21/90] | Loss: 0.3195 | Train Acc: 93.90%
2026-02-10 01:30:10,700 - INFO - [Valid] [21/90] | Loss: 0.5304 | Val Acc: 79.65%
2026-02-10 01:30:10,705 - INFO - [Metrics for 'abnormal'] | Precision: 0.7340 | Recall: 0.8790 | F1: 0.8000
2026-02-10 01:30:10,706 - INFO - [Metrics for 'normal'] | Precision: 0.8742 | Recall: 0.7253 | F1: 0.7928
2026-02-10 01:30:10,707 - INFO - --------------------------------------------------
2026-02-10 01:30:10,709 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:30:17,695 - INFO - [Train] [22/90] | Loss: 0.2983 | Train Acc: 94.72%
2026-02-10 01:30:18,849 - INFO - [Valid] [22/90] | Loss: 0.5258 | Val Acc: 82.89%
2026-02-10 01:30:18,854 - INFO - [Metrics for 'abnormal'] | Precision: 0.8462 | Recall: 0.7707 | F1: 0.8067
2026-02-10 01:30:18,854 - INFO - [Metrics for 'normal'] | Precision: 0.8163 | Recall: 0.8791 | F1: 0.8466
2026-02-10 01:30:18,856 - INFO - --------------------------------------------------
2026-02-10 01:30:18,858 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:30:25,930 - INFO - [Train] [23/90] | Loss: 0.2718 | Train Acc: 96.65%
2026-02-10 01:30:27,231 - INFO - [Valid] [23/90] | Loss: 0.5707 | Val Acc: 80.83%
2026-02-10 01:30:27,237 - INFO - [Metrics for 'abnormal'] | Precision: 0.8026 | Recall: 0.7771 | F1: 0.7896
2026-02-10 01:30:27,237 - INFO - [Metrics for 'normal'] | Precision: 0.8128 | Recall: 0.8352 | F1: 0.8238
2026-02-10 01:30:27,238 - INFO - --------------------------------------------------
2026-02-10 01:30:27,240 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:30:33,757 - INFO - [Train] [24/90] | Loss: 0.2673 | Train Acc: 96.80%
2026-02-10 01:30:35,232 - INFO - [Valid] [24/90] | Loss: 0.5671 | Val Acc: 78.47%
2026-02-10 01:30:35,237 - INFO - [Metrics for 'abnormal'] | Precision: 0.7763 | Recall: 0.7516 | F1: 0.7638
2026-02-10 01:30:35,238 - INFO - [Metrics for 'normal'] | Precision: 0.7914 | Recall: 0.8132 | F1: 0.8022
2026-02-10 01:30:35,240 - INFO - --------------------------------------------------
2026-02-10 01:30:35,241 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:30:42,608 - INFO - [Train] [25/90] | Loss: 0.2720 | Train Acc: 96.43%
2026-02-10 01:30:43,996 - INFO - [Valid] [25/90] | Loss: 0.5411 | Val Acc: 80.83%
2026-02-10 01:30:44,001 - INFO - [Metrics for 'abnormal'] | Precision: 0.7949 | Recall: 0.7898 | F1: 0.7923
2026-02-10 01:30:44,001 - INFO - [Metrics for 'normal'] | Precision: 0.8197 | Recall: 0.8242 | F1: 0.8219
2026-02-10 01:30:44,002 - INFO - --------------------------------------------------
2026-02-10 01:30:44,004 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:30:51,178 - INFO - [Train] [26/90] | Loss: 0.2779 | Train Acc: 95.61%
2026-02-10 01:30:52,525 - INFO - [Valid] [26/90] | Loss: 0.5129 | Val Acc: 81.12%
2026-02-10 01:30:52,531 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8280 | F1: 0.8025
2026-02-10 01:30:52,531 - INFO - [Metrics for 'normal'] | Precision: 0.8430 | Recall: 0.7967 | F1: 0.8192
2026-02-10 01:30:52,533 - INFO - --------------------------------------------------
2026-02-10 01:30:52,534 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 01:31:00,011 - INFO - [Train] [27/90] | Loss: 0.2856 | Train Acc: 95.76%
2026-02-10 01:31:01,539 - INFO - [Valid] [27/90] | Loss: 0.5370 | Val Acc: 80.83%
2026-02-10 01:31:01,543 - INFO - [Metrics for 'abnormal'] | Precision: 0.7738 | Recall: 0.8280 | F1: 0.8000
2026-02-10 01:31:01,544 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.7912 | F1: 0.8159
2026-02-10 01:31:01,545 - INFO - --------------------------------------------------
2026-02-10 01:31:01,547 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 01:31:08,955 - INFO - [Train] [28/90] | Loss: 0.2657 | Train Acc: 97.40%
2026-02-10 01:31:10,339 - INFO - [Valid] [28/90] | Loss: 0.5347 | Val Acc: 79.06%
2026-02-10 01:31:10,344 - INFO - [Metrics for 'abnormal'] | Precision: 0.7389 | Recall: 0.8471 | F1: 0.7893
2026-02-10 01:31:10,344 - INFO - [Metrics for 'normal'] | Precision: 0.8491 | Recall: 0.7418 | F1: 0.7918
2026-02-10 01:31:10,346 - INFO - --------------------------------------------------
2026-02-10 01:31:10,348 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 01:31:17,791 - INFO - [Train] [29/90] | Loss: 0.2516 | Train Acc: 97.99%
2026-02-10 01:31:19,330 - INFO - [Valid] [29/90] | Loss: 0.5268 | Val Acc: 82.60%
2026-02-10 01:31:19,337 - INFO - [Metrics for 'abnormal'] | Precision: 0.8182 | Recall: 0.8025 | F1: 0.8103
2026-02-10 01:31:19,337 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8462 | F1: 0.8392
2026-02-10 01:31:19,339 - INFO - --------------------------------------------------
2026-02-10 01:31:19,341 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 01:31:26,899 - INFO - [Train] [30/90] | Loss: 0.2512 | Train Acc: 97.84%
2026-02-10 01:31:28,298 - INFO - [Valid] [30/90] | Loss: 0.5116 | Val Acc: 82.89%
2026-02-10 01:31:28,303 - INFO - [Metrics for 'abnormal'] | Precision: 0.8153 | Recall: 0.8153 | F1: 0.8153
2026-02-10 01:31:28,304 - INFO - [Metrics for 'normal'] | Precision: 0.8407 | Recall: 0.8407 | F1: 0.8407
2026-02-10 01:31:28,305 - INFO - --------------------------------------------------
2026-02-10 01:31:28,307 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 01:31:34,756 - INFO - [Train] [31/90] | Loss: 0.2595 | Train Acc: 97.25%
2026-02-10 01:31:36,069 - INFO - [Valid] [31/90] | Loss: 0.5023 | Val Acc: 81.71%
2026-02-10 01:31:36,073 - INFO - [Metrics for 'abnormal'] | Precision: 0.7879 | Recall: 0.8280 | F1: 0.8075
2026-02-10 01:31:36,073 - INFO - [Metrics for 'normal'] | Precision: 0.8448 | Recall: 0.8077 | F1: 0.8258
2026-02-10 01:31:36,074 - INFO - --------------------------------------------------
2026-02-10 01:31:36,076 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 01:31:42,733 - INFO - [Train] [32/90] | Loss: 0.2724 | Train Acc: 96.43%
2026-02-10 01:31:44,008 - INFO - [Valid] [32/90] | Loss: 0.5046 | Val Acc: 82.30%
2026-02-10 01:31:44,012 - INFO - [Metrics for 'abnormal'] | Precision: 0.7870 | Recall: 0.8471 | F1: 0.8160
2026-02-10 01:31:44,012 - INFO - [Metrics for 'normal'] | Precision: 0.8588 | Recall: 0.8022 | F1: 0.8295
2026-02-10 01:31:44,014 - INFO - --------------------------------------------------
2026-02-10 01:31:44,015 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 01:31:50,590 - INFO - [Train] [33/90] | Loss: 0.2452 | Train Acc: 98.21%
2026-02-10 01:31:52,167 - INFO - [Valid] [33/90] | Loss: 0.5238 | Val Acc: 82.01%
2026-02-10 01:31:52,171 - INFO - [Metrics for 'abnormal'] | Precision: 0.8158 | Recall: 0.7898 | F1: 0.8026
2026-02-10 01:31:52,171 - INFO - [Metrics for 'normal'] | Precision: 0.8235 | Recall: 0.8462 | F1: 0.8347
2026-02-10 01:31:52,172 - INFO - --------------------------------------------------
2026-02-10 01:31:52,175 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 01:31:58,573 - INFO - [Train] [34/90] | Loss: 0.2615 | Train Acc: 97.02%
2026-02-10 01:32:00,064 - INFO - [Valid] [34/90] | Loss: 0.5155 | Val Acc: 81.71%
2026-02-10 01:32:00,070 - INFO - [Metrics for 'abnormal'] | Precision: 0.7950 | Recall: 0.8153 | F1: 0.8050
2026-02-10 01:32:00,070 - INFO - [Metrics for 'normal'] | Precision: 0.8371 | Recall: 0.8187 | F1: 0.8278
2026-02-10 01:32:00,072 - INFO - --------------------------------------------------
2026-02-10 01:32:00,074 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 01:32:06,601 - INFO - [Train] [35/90] | Loss: 0.2500 | Train Acc: 98.29%
2026-02-10 01:32:07,783 - INFO - [Valid] [35/90] | Loss: 0.5458 | Val Acc: 82.01%
2026-02-10 01:32:07,788 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-02-10 01:32:07,788 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-02-10 01:32:07,790 - INFO - --------------------------------------------------
2026-02-10 01:32:07,791 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 01:32:13,881 - INFO - [Train] [36/90] | Loss: 0.2508 | Train Acc: 97.77%
2026-02-10 01:32:14,774 - INFO - [Valid] [36/90] | Loss: 0.5615 | Val Acc: 81.71%
2026-02-10 01:32:14,780 - INFO - [Metrics for 'abnormal'] | Precision: 0.8188 | Recall: 0.7771 | F1: 0.7974
2026-02-10 01:32:14,780 - INFO - [Metrics for 'normal'] | Precision: 0.8158 | Recall: 0.8516 | F1: 0.8333
2026-02-10 01:32:14,782 - INFO - --------------------------------------------------
2026-02-10 01:32:14,787 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 01:32:21,382 - INFO - [Train] [37/90] | Loss: 0.2551 | Train Acc: 97.32%
2026-02-10 01:32:22,619 - INFO - [Valid] [37/90] | Loss: 0.4740 | Val Acc: 84.96%
2026-02-10 01:32:22,623 - INFO - [Metrics for 'abnormal'] | Precision: 0.8841 | Recall: 0.7771 | F1: 0.8271
2026-02-10 01:32:22,624 - INFO - [Metrics for 'normal'] | Precision: 0.8259 | Recall: 0.9121 | F1: 0.8668
2026-02-10 01:32:22,625 - INFO - --------------------------------------------------
2026-02-10 01:32:22,627 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 01:32:30,017 - INFO - [Train] [38/90] | Loss: 0.2694 | Train Acc: 96.13%
2026-02-10 01:32:31,570 - INFO - [Valid] [38/90] | Loss: 0.4994 | Val Acc: 83.19%
2026-02-10 01:32:31,575 - INFO - [Metrics for 'abnormal'] | Precision: 0.7941 | Recall: 0.8599 | F1: 0.8257
2026-02-10 01:32:31,575 - INFO - [Metrics for 'normal'] | Precision: 0.8698 | Recall: 0.8077 | F1: 0.8376
2026-02-10 01:32:31,577 - INFO - --------------------------------------------------
2026-02-10 01:32:31,579 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 01:32:39,148 - INFO - [Train] [39/90] | Loss: 0.2515 | Train Acc: 97.54%
2026-02-10 01:32:40,570 - INFO - [Valid] [39/90] | Loss: 0.5652 | Val Acc: 83.19%
2026-02-10 01:32:40,575 - INFO - [Metrics for 'abnormal'] | Precision: 0.8086 | Recall: 0.8344 | F1: 0.8213
2026-02-10 01:32:40,575 - INFO - [Metrics for 'normal'] | Precision: 0.8531 | Recall: 0.8297 | F1: 0.8412
2026-02-10 01:32:40,576 - INFO - --------------------------------------------------
2026-02-10 01:32:40,578 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 01:32:48,311 - INFO - [Train] [40/90] | Loss: 0.2412 | Train Acc: 98.29%
2026-02-10 01:32:50,012 - INFO - [Valid] [40/90] | Loss: 0.5390 | Val Acc: 79.94%
2026-02-10 01:32:50,022 - INFO - [Metrics for 'abnormal'] | Precision: 0.7908 | Recall: 0.7707 | F1: 0.7806
2026-02-10 01:32:50,022 - INFO - [Metrics for 'normal'] | Precision: 0.8065 | Recall: 0.8242 | F1: 0.8152
2026-02-10 01:32:50,024 - INFO - --------------------------------------------------
2026-02-10 01:32:50,026 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 01:32:57,467 - INFO - [Train] [41/90] | Loss: 0.2367 | Train Acc: 98.66%
2026-02-10 01:32:58,917 - INFO - [Valid] [41/90] | Loss: 0.5494 | Val Acc: 83.19%
2026-02-10 01:32:58,922 - INFO - [Metrics for 'abnormal'] | Precision: 0.8289 | Recall: 0.8025 | F1: 0.8155
2026-02-10 01:32:58,922 - INFO - [Metrics for 'normal'] | Precision: 0.8342 | Recall: 0.8571 | F1: 0.8455
2026-02-10 01:32:58,924 - INFO - --------------------------------------------------
2026-02-10 01:32:58,925 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 01:33:06,768 - INFO - [Train] [42/90] | Loss: 0.2407 | Train Acc: 98.07%
2026-02-10 01:33:08,165 - INFO - [Valid] [42/90] | Loss: 0.5383 | Val Acc: 83.19%
2026-02-10 01:33:08,170 - INFO - [Metrics for 'abnormal'] | Precision: 0.8205 | Recall: 0.8153 | F1: 0.8179
2026-02-10 01:33:08,170 - INFO - [Metrics for 'normal'] | Precision: 0.8415 | Recall: 0.8462 | F1: 0.8438
2026-02-10 01:33:08,172 - INFO - --------------------------------------------------
2026-02-10 01:33:08,173 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 01:33:15,183 - INFO - [Train] [43/90] | Loss: 0.2432 | Train Acc: 98.36%
2026-02-10 01:33:16,720 - INFO - [Valid] [43/90] | Loss: 0.5407 | Val Acc: 83.78%
2026-02-10 01:33:16,725 - INFO - [Metrics for 'abnormal'] | Precision: 0.8036 | Recall: 0.8599 | F1: 0.8308
2026-02-10 01:33:16,725 - INFO - [Metrics for 'normal'] | Precision: 0.8713 | Recall: 0.8187 | F1: 0.8442
2026-02-10 01:33:16,726 - INFO - --------------------------------------------------
2026-02-10 01:33:16,728 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 01:33:24,018 - INFO - [Train] [44/90] | Loss: 0.2339 | Train Acc: 98.51%
2026-02-10 01:33:25,498 - INFO - [Valid] [44/90] | Loss: 0.5441 | Val Acc: 84.07%
2026-02-10 01:33:25,502 - INFO - [Metrics for 'abnormal'] | Precision: 0.8239 | Recall: 0.8344 | F1: 0.8291
2026-02-10 01:33:25,502 - INFO - [Metrics for 'normal'] | Precision: 0.8556 | Recall: 0.8462 | F1: 0.8508
2026-02-10 01:33:25,504 - INFO - --------------------------------------------------
2026-02-10 01:33:25,505 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 01:33:32,903 - INFO - [Train] [45/90] | Loss: 0.2388 | Train Acc: 98.14%
2026-02-10 01:33:33,906 - INFO - [Valid] [45/90] | Loss: 0.5885 | Val Acc: 81.12%
2026-02-10 01:33:33,911 - INFO - [Metrics for 'abnormal'] | Precision: 0.8394 | Recall: 0.7325 | F1: 0.7823
2026-02-10 01:33:33,912 - INFO - [Metrics for 'normal'] | Precision: 0.7921 | Recall: 0.8791 | F1: 0.8333
2026-02-10 01:33:33,914 - INFO - --------------------------------------------------
2026-02-10 01:33:33,916 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 01:33:40,368 - INFO - [Train] [46/90] | Loss: 0.2590 | Train Acc: 97.32%
2026-02-10 01:33:41,389 - INFO - [Valid] [46/90] | Loss: 0.5018 | Val Acc: 82.30%
2026-02-10 01:33:41,394 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-02-10 01:33:41,394 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-02-10 01:33:41,397 - INFO - --------------------------------------------------
2026-02-10 01:33:41,399 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 01:33:46,550 - INFO - [Train] [47/90] | Loss: 0.2335 | Train Acc: 99.03%
2026-02-10 01:33:47,696 - INFO - [Valid] [47/90] | Loss: 0.5708 | Val Acc: 82.89%
2026-02-10 01:33:47,699 - INFO - [Metrics for 'abnormal'] | Precision: 0.7829 | Recall: 0.8726 | F1: 0.8253
2026-02-10 01:33:47,699 - INFO - [Metrics for 'normal'] | Precision: 0.8780 | Recall: 0.7912 | F1: 0.8324
2026-02-10 01:33:47,700 - INFO - --------------------------------------------------
2026-02-10 01:33:47,701 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 01:33:53,180 - INFO - [Train] [48/90] | Loss: 0.2382 | Train Acc: 98.29%
2026-02-10 01:33:54,370 - INFO - [Valid] [48/90] | Loss: 0.5354 | Val Acc: 82.01%
2026-02-10 01:33:54,374 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-02-10 01:33:54,374 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-02-10 01:33:54,375 - INFO - --------------------------------------------------
2026-02-10 01:33:54,377 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 01:33:59,255 - INFO - [Train] [49/90] | Loss: 0.2316 | Train Acc: 98.66%
2026-02-10 01:34:00,126 - INFO - [Valid] [49/90] | Loss: 0.5751 | Val Acc: 81.12%
2026-02-10 01:34:00,132 - INFO - [Metrics for 'abnormal'] | Precision: 0.8039 | Recall: 0.7834 | F1: 0.7935
2026-02-10 01:34:00,132 - INFO - [Metrics for 'normal'] | Precision: 0.8172 | Recall: 0.8352 | F1: 0.8261
2026-02-10 01:34:00,133 - INFO - --------------------------------------------------
2026-02-10 01:34:00,136 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 01:34:05,180 - INFO - [Train] [50/90] | Loss: 0.2281 | Train Acc: 98.66%
2026-02-10 01:34:06,097 - INFO - [Valid] [50/90] | Loss: 0.5700 | Val Acc: 82.89%
2026-02-10 01:34:06,102 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8408 | F1: 0.8199
2026-02-10 01:34:06,102 - INFO - [Metrics for 'normal'] | Precision: 0.8563 | Recall: 0.8187 | F1: 0.8371
2026-02-10 01:34:06,103 - INFO - --------------------------------------------------
2026-02-10 01:34:06,104 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 01:34:10,768 - INFO - [Train] [51/90] | Loss: 0.2361 | Train Acc: 98.44%
2026-02-10 01:34:11,745 - INFO - [Valid] [51/90] | Loss: 0.5745 | Val Acc: 83.19%
2026-02-10 01:34:11,750 - INFO - [Metrics for 'abnormal'] | Precision: 0.8205 | Recall: 0.8153 | F1: 0.8179
2026-02-10 01:34:11,750 - INFO - [Metrics for 'normal'] | Precision: 0.8415 | Recall: 0.8462 | F1: 0.8438
2026-02-10 01:34:11,752 - INFO - --------------------------------------------------
2026-02-10 01:34:11,754 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 01:34:16,444 - INFO - [Train] [52/90] | Loss: 0.2259 | Train Acc: 98.88%
2026-02-10 01:34:17,876 - INFO - [Valid] [52/90] | Loss: 0.6177 | Val Acc: 80.53%
2026-02-10 01:34:17,881 - INFO - [Metrics for 'abnormal'] | Precision: 0.7486 | Recall: 0.8726 | F1: 0.8059
2026-02-10 01:34:17,881 - INFO - [Metrics for 'normal'] | Precision: 0.8718 | Recall: 0.7473 | F1: 0.8047
2026-02-10 01:34:17,883 - INFO - --------------------------------------------------
2026-02-10 01:34:17,884 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 01:34:23,627 - INFO - [Train] [53/90] | Loss: 0.2360 | Train Acc: 98.36%
2026-02-10 01:34:25,115 - INFO - [Valid] [53/90] | Loss: 0.5449 | Val Acc: 83.48%
2026-02-10 01:34:25,123 - INFO - [Metrics for 'abnormal'] | Precision: 0.8531 | Recall: 0.7771 | F1: 0.8133
2026-02-10 01:34:25,123 - INFO - [Metrics for 'normal'] | Precision: 0.8214 | Recall: 0.8846 | F1: 0.8519
2026-02-10 01:34:25,125 - INFO - --------------------------------------------------
2026-02-10 01:34:25,126 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 01:34:31,424 - INFO - [Train] [54/90] | Loss: 0.2325 | Train Acc: 98.81%
2026-02-10 01:34:33,025 - INFO - [Valid] [54/90] | Loss: 0.4826 | Val Acc: 85.55%
2026-02-10 01:34:33,030 - INFO - [Metrics for 'abnormal'] | Precision: 0.8600 | Recall: 0.8217 | F1: 0.8404
2026-02-10 01:34:33,030 - INFO - [Metrics for 'normal'] | Precision: 0.8519 | Recall: 0.8846 | F1: 0.8679
2026-02-10 01:34:33,032 - INFO - --------------------------------------------------
2026-02-10 01:34:33,034 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 01:34:40,826 - INFO - [Train] [55/90] | Loss: 0.2268 | Train Acc: 98.96%
2026-02-10 01:34:42,503 - INFO - [Valid] [55/90] | Loss: 0.5313 | Val Acc: 82.89%
2026-02-10 01:34:42,507 - INFO - [Metrics for 'abnormal'] | Precision: 0.8037 | Recall: 0.8344 | F1: 0.8187
2026-02-10 01:34:42,508 - INFO - [Metrics for 'normal'] | Precision: 0.8523 | Recall: 0.8242 | F1: 0.8380
2026-02-10 01:34:42,509 - INFO - --------------------------------------------------
2026-02-10 01:34:42,511 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 01:34:49,582 - INFO - [Train] [56/90] | Loss: 0.2216 | Train Acc: 99.26%
2026-02-10 01:34:51,053 - INFO - [Valid] [56/90] | Loss: 0.5193 | Val Acc: 84.07%
2026-02-10 01:34:51,058 - INFO - [Metrics for 'abnormal'] | Precision: 0.8199 | Recall: 0.8408 | F1: 0.8302
2026-02-10 01:34:51,059 - INFO - [Metrics for 'normal'] | Precision: 0.8596 | Recall: 0.8407 | F1: 0.8500
2026-02-10 01:34:51,060 - INFO - --------------------------------------------------
2026-02-10 01:34:51,062 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 01:34:57,894 - INFO - [Train] [57/90] | Loss: 0.2167 | Train Acc: 99.63%
2026-02-10 01:34:59,364 - INFO - [Valid] [57/90] | Loss: 0.5524 | Val Acc: 83.78%
2026-02-10 01:34:59,372 - INFO - [Metrics for 'abnormal'] | Precision: 0.8036 | Recall: 0.8599 | F1: 0.8308
2026-02-10 01:34:59,372 - INFO - [Metrics for 'normal'] | Precision: 0.8713 | Recall: 0.8187 | F1: 0.8442
2026-02-10 01:34:59,376 - INFO - --------------------------------------------------
2026-02-10 01:34:59,378 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 01:35:06,550 - INFO - [Train] [58/90] | Loss: 0.2374 | Train Acc: 98.81%
2026-02-10 01:35:08,036 - INFO - [Valid] [58/90] | Loss: 0.5804 | Val Acc: 82.01%
2026-02-10 01:35:08,040 - INFO - [Metrics for 'abnormal'] | Precision: 0.8077 | Recall: 0.8025 | F1: 0.8051
2026-02-10 01:35:08,041 - INFO - [Metrics for 'normal'] | Precision: 0.8306 | Recall: 0.8352 | F1: 0.8329
2026-02-10 01:35:08,042 - INFO - --------------------------------------------------
2026-02-10 01:35:08,043 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 01:35:15,300 - INFO - [Train] [59/90] | Loss: 0.2275 | Train Acc: 98.74%
2026-02-10 01:35:16,861 - INFO - [Valid] [59/90] | Loss: 0.5466 | Val Acc: 84.07%
2026-02-10 01:35:16,866 - INFO - [Metrics for 'abnormal'] | Precision: 0.8084 | Recall: 0.8599 | F1: 0.8333
2026-02-10 01:35:16,866 - INFO - [Metrics for 'normal'] | Precision: 0.8721 | Recall: 0.8242 | F1: 0.8475
2026-02-10 01:35:16,867 - INFO - --------------------------------------------------
2026-02-10 01:35:16,869 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 01:35:24,245 - INFO - [Train] [60/90] | Loss: 0.2144 | Train Acc: 99.70%
2026-02-10 01:35:25,929 - INFO - [Valid] [60/90] | Loss: 0.5344 | Val Acc: 84.07%
2026-02-10 01:35:25,935 - INFO - [Metrics for 'abnormal'] | Precision: 0.8160 | Recall: 0.8471 | F1: 0.8313
2026-02-10 01:35:25,935 - INFO - [Metrics for 'normal'] | Precision: 0.8636 | Recall: 0.8352 | F1: 0.8492
2026-02-10 01:35:25,936 - INFO - --------------------------------------------------
2026-02-10 01:35:25,938 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 01:35:33,493 - INFO - [Train] [61/90] | Loss: 0.2113 | Train Acc: 99.63%
2026-02-10 01:35:34,923 - INFO - [Valid] [61/90] | Loss: 0.5201 | Val Acc: 84.66%
2026-02-10 01:35:34,928 - INFO - [Metrics for 'abnormal'] | Precision: 0.8107 | Recall: 0.8726 | F1: 0.8405
2026-02-10 01:35:34,928 - INFO - [Metrics for 'normal'] | Precision: 0.8824 | Recall: 0.8242 | F1: 0.8523
2026-02-10 01:35:34,929 - INFO - --------------------------------------------------
2026-02-10 01:35:34,931 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 01:35:42,162 - INFO - [Train] [62/90] | Loss: 0.2100 | Train Acc: 99.78%
2026-02-10 01:35:43,729 - INFO - [Valid] [62/90] | Loss: 0.5179 | Val Acc: 84.96%
2026-02-10 01:35:43,734 - INFO - [Metrics for 'abnormal'] | Precision: 0.8313 | Recall: 0.8471 | F1: 0.8391
2026-02-10 01:35:43,734 - INFO - [Metrics for 'normal'] | Precision: 0.8659 | Recall: 0.8516 | F1: 0.8587
2026-02-10 01:35:43,736 - INFO - --------------------------------------------------
2026-02-10 01:35:43,738 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 01:35:50,738 - INFO - [Train] [63/90] | Loss: 0.2104 | Train Acc: 99.78%
2026-02-10 01:35:52,083 - INFO - [Valid] [63/90] | Loss: 0.5783 | Val Acc: 82.89%
2026-02-10 01:35:52,088 - INFO - [Metrics for 'abnormal'] | Precision: 0.7829 | Recall: 0.8726 | F1: 0.8253
2026-02-10 01:35:52,089 - INFO - [Metrics for 'normal'] | Precision: 0.8780 | Recall: 0.7912 | F1: 0.8324
2026-02-10 01:35:52,090 - INFO - --------------------------------------------------
2026-02-10 01:35:52,092 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 01:35:59,445 - INFO - [Train] [64/90] | Loss: 0.2203 | Train Acc: 99.33%
2026-02-10 01:36:01,018 - INFO - [Valid] [64/90] | Loss: 0.5215 | Val Acc: 84.07%
2026-02-10 01:36:01,023 - INFO - [Metrics for 'abnormal'] | Precision: 0.8047 | Recall: 0.8662 | F1: 0.8344
2026-02-10 01:36:01,023 - INFO - [Metrics for 'normal'] | Precision: 0.8765 | Recall: 0.8187 | F1: 0.8466
2026-02-10 01:36:01,024 - INFO - --------------------------------------------------
2026-02-10 01:36:01,029 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 01:36:07,968 - INFO - [Train] [65/90] | Loss: 0.2061 | Train Acc: 99.85%
2026-02-10 01:36:09,414 - INFO - [Valid] [65/90] | Loss: 0.5504 | Val Acc: 84.07%
2026-02-10 01:36:09,419 - INFO - [Metrics for 'abnormal'] | Precision: 0.8084 | Recall: 0.8599 | F1: 0.8333
2026-02-10 01:36:09,420 - INFO - [Metrics for 'normal'] | Precision: 0.8721 | Recall: 0.8242 | F1: 0.8475
2026-02-10 01:36:09,422 - INFO - --------------------------------------------------
2026-02-10 01:36:09,424 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 01:36:16,030 - INFO - [Train] [66/90] | Loss: 0.2087 | Train Acc: 99.78%
2026-02-10 01:36:17,347 - INFO - [Valid] [66/90] | Loss: 0.5349 | Val Acc: 84.66%
2026-02-10 01:36:17,352 - INFO - [Metrics for 'abnormal'] | Precision: 0.8107 | Recall: 0.8726 | F1: 0.8405
2026-02-10 01:36:17,352 - INFO - [Metrics for 'normal'] | Precision: 0.8824 | Recall: 0.8242 | F1: 0.8523
2026-02-10 01:36:17,354 - INFO - --------------------------------------------------
2026-02-10 01:36:17,357 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 01:36:23,245 - INFO - [Train] [67/90] | Loss: 0.2113 | Train Acc: 99.63%
2026-02-10 01:36:24,531 - INFO - [Valid] [67/90] | Loss: 0.5360 | Val Acc: 82.01%
2026-02-10 01:36:24,541 - INFO - [Metrics for 'abnormal'] | Precision: 0.8200 | Recall: 0.7834 | F1: 0.8013
2026-02-10 01:36:24,541 - INFO - [Metrics for 'normal'] | Precision: 0.8201 | Recall: 0.8516 | F1: 0.8356
2026-02-10 01:36:24,543 - INFO - --------------------------------------------------
2026-02-10 01:36:24,544 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 01:36:30,540 - INFO - [Train] [68/90] | Loss: 0.2102 | Train Acc: 99.48%
2026-02-10 01:36:31,896 - INFO - [Valid] [68/90] | Loss: 0.5232 | Val Acc: 82.89%
2026-02-10 01:36:31,901 - INFO - [Metrics for 'abnormal'] | Precision: 0.8322 | Recall: 0.7898 | F1: 0.8105
2026-02-10 01:36:31,902 - INFO - [Metrics for 'normal'] | Precision: 0.8263 | Recall: 0.8626 | F1: 0.8441
2026-02-10 01:36:31,903 - INFO - --------------------------------------------------
2026-02-10 01:36:31,905 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 01:36:37,819 - INFO - [Train] [69/90] | Loss: 0.2163 | Train Acc: 99.26%
2026-02-10 01:36:39,299 - INFO - [Valid] [69/90] | Loss: 0.5538 | Val Acc: 83.78%
2026-02-10 01:36:39,305 - INFO - [Metrics for 'abnormal'] | Precision: 0.8072 | Recall: 0.8535 | F1: 0.8297
2026-02-10 01:36:39,306 - INFO - [Metrics for 'normal'] | Precision: 0.8671 | Recall: 0.8242 | F1: 0.8451
2026-02-10 01:36:39,308 - INFO - --------------------------------------------------
2026-02-10 01:36:39,313 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 01:36:46,630 - INFO - [Train] [70/90] | Loss: 0.2155 | Train Acc: 99.48%
2026-02-10 01:36:48,163 - INFO - [Valid] [70/90] | Loss: 0.5202 | Val Acc: 83.78%
2026-02-10 01:36:48,168 - INFO - [Metrics for 'abnormal'] | Precision: 0.7931 | Recall: 0.8790 | F1: 0.8338
2026-02-10 01:36:48,168 - INFO - [Metrics for 'normal'] | Precision: 0.8848 | Recall: 0.8022 | F1: 0.8415
2026-02-10 01:36:48,169 - INFO - --------------------------------------------------
2026-02-10 01:36:48,171 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 01:36:55,455 - INFO - [Train] [71/90] | Loss: 0.2090 | Train Acc: 99.63%
2026-02-10 01:36:57,121 - INFO - [Valid] [71/90] | Loss: 0.5343 | Val Acc: 84.37%
2026-02-10 01:36:57,126 - INFO - [Metrics for 'abnormal'] | Precision: 0.8291 | Recall: 0.8344 | F1: 0.8317
2026-02-10 01:36:57,126 - INFO - [Metrics for 'normal'] | Precision: 0.8564 | Recall: 0.8516 | F1: 0.8540
2026-02-10 01:36:57,133 - INFO - --------------------------------------------------
2026-02-10 01:36:57,135 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 01:37:04,330 - INFO - [Train] [72/90] | Loss: 0.2101 | Train Acc: 99.70%
2026-02-10 01:37:05,838 - INFO - [Valid] [72/90] | Loss: 0.5332 | Val Acc: 83.48%
2026-02-10 01:37:05,842 - INFO - [Metrics for 'abnormal'] | Precision: 0.7886 | Recall: 0.8790 | F1: 0.8313
2026-02-10 01:37:05,843 - INFO - [Metrics for 'normal'] | Precision: 0.8841 | Recall: 0.7967 | F1: 0.8382
2026-02-10 01:37:05,844 - INFO - --------------------------------------------------
2026-02-10 01:37:05,846 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 01:37:13,479 - INFO - [Train] [73/90] | Loss: 0.2062 | Train Acc: 99.78%
2026-02-10 01:37:15,044 - INFO - [Valid] [73/90] | Loss: 0.5253 | Val Acc: 85.55%
2026-02-10 01:37:15,050 - INFO - [Metrics for 'abnormal'] | Precision: 0.8375 | Recall: 0.8535 | F1: 0.8454
2026-02-10 01:37:15,050 - INFO - [Metrics for 'normal'] | Precision: 0.8715 | Recall: 0.8571 | F1: 0.8643
2026-02-10 01:37:15,052 - INFO - --------------------------------------------------
2026-02-10 01:37:15,054 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 01:37:22,344 - INFO - [Train] [74/90] | Loss: 0.2060 | Train Acc: 99.78%
2026-02-10 01:37:23,773 - INFO - [Valid] [74/90] | Loss: 0.5356 | Val Acc: 84.96%
2026-02-10 01:37:23,778 - INFO - [Metrics for 'abnormal'] | Precision: 0.8272 | Recall: 0.8535 | F1: 0.8401
2026-02-10 01:37:23,778 - INFO - [Metrics for 'normal'] | Precision: 0.8701 | Recall: 0.8462 | F1: 0.8579
2026-02-10 01:37:23,780 - INFO - --------------------------------------------------
2026-02-10 01:37:23,782 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 01:37:31,395 - INFO - [Train] [75/90] | Loss: 0.2093 | Train Acc: 99.63%
2026-02-10 01:37:32,881 - INFO - [Valid] [75/90] | Loss: 0.5369 | Val Acc: 82.30%
2026-02-10 01:37:32,887 - INFO - [Metrics for 'abnormal'] | Precision: 0.7740 | Recall: 0.8726 | F1: 0.8204
2026-02-10 01:37:32,887 - INFO - [Metrics for 'normal'] | Precision: 0.8765 | Recall: 0.7802 | F1: 0.8256
2026-02-10 01:37:32,889 - INFO - --------------------------------------------------
2026-02-10 01:37:32,891 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 01:37:39,175 - INFO - [Train] [76/90] | Loss: 0.2110 | Train Acc: 99.63%
2026-02-10 01:37:40,504 - INFO - [Valid] [76/90] | Loss: 0.5053 | Val Acc: 84.37%
2026-02-10 01:37:40,509 - INFO - [Metrics for 'abnormal'] | Precision: 0.8210 | Recall: 0.8471 | F1: 0.8339
2026-02-10 01:37:40,509 - INFO - [Metrics for 'normal'] | Precision: 0.8644 | Recall: 0.8407 | F1: 0.8524
2026-02-10 01:37:40,511 - INFO - --------------------------------------------------
2026-02-10 01:37:40,513 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 01:37:47,059 - INFO - [Train] [77/90] | Loss: 0.2165 | Train Acc: 99.18%
2026-02-10 01:37:48,469 - INFO - [Valid] [77/90] | Loss: 0.5051 | Val Acc: 84.96%
2026-02-10 01:37:48,473 - INFO - [Metrics for 'abnormal'] | Precision: 0.8272 | Recall: 0.8535 | F1: 0.8401
2026-02-10 01:37:48,473 - INFO - [Metrics for 'normal'] | Precision: 0.8701 | Recall: 0.8462 | F1: 0.8579
2026-02-10 01:37:48,474 - INFO - --------------------------------------------------
2026-02-10 01:37:48,476 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 01:37:54,830 - INFO - [Train] [78/90] | Loss: 0.2068 | Train Acc: 99.85%
2026-02-10 01:37:56,108 - INFO - [Valid] [78/90] | Loss: 0.5479 | Val Acc: 83.78%
2026-02-10 01:37:56,116 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8662 | F1: 0.8318
2026-02-10 01:37:56,116 - INFO - [Metrics for 'normal'] | Precision: 0.8757 | Recall: 0.8132 | F1: 0.8433
2026-02-10 01:37:56,117 - INFO - --------------------------------------------------
2026-02-10 01:37:56,119 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 01:38:02,227 - INFO - [Train] [79/90] | Loss: 0.2020 | Train Acc: 99.93%
2026-02-10 01:38:03,548 - INFO - [Valid] [79/90] | Loss: 0.5479 | Val Acc: 82.89%
2026-02-10 01:38:03,555 - INFO - [Metrics for 'abnormal'] | Precision: 0.8037 | Recall: 0.8344 | F1: 0.8187
2026-02-10 01:38:03,559 - INFO - [Metrics for 'normal'] | Precision: 0.8523 | Recall: 0.8242 | F1: 0.8380
2026-02-10 01:38:03,561 - INFO - --------------------------------------------------
2026-02-10 01:38:03,562 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 01:38:10,347 - INFO - [Train] [80/90] | Loss: 0.2037 | Train Acc: 99.93%
2026-02-10 01:38:11,829 - INFO - [Valid] [80/90] | Loss: 0.5735 | Val Acc: 82.60%
2026-02-10 01:38:11,834 - INFO - [Metrics for 'abnormal'] | Precision: 0.7753 | Recall: 0.8790 | F1: 0.8239
2026-02-10 01:38:11,834 - INFO - [Metrics for 'normal'] | Precision: 0.8820 | Recall: 0.7802 | F1: 0.8280
2026-02-10 01:38:11,836 - INFO - --------------------------------------------------
2026-02-10 01:38:11,838 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 01:38:17,550 - INFO - [Train] [81/90] | Loss: 0.2037 | Train Acc: 99.93%
2026-02-10 01:38:18,600 - INFO - [Valid] [81/90] | Loss: 0.5615 | Val Acc: 83.48%
2026-02-10 01:38:18,605 - INFO - [Metrics for 'abnormal'] | Precision: 0.7953 | Recall: 0.8662 | F1: 0.8293
2026-02-10 01:38:18,605 - INFO - [Metrics for 'normal'] | Precision: 0.8750 | Recall: 0.8077 | F1: 0.8400
2026-02-10 01:38:18,606 - INFO - --------------------------------------------------
2026-02-10 01:38:18,607 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 01:38:25,138 - INFO - [Train] [82/90] | Loss: 0.2031 | Train Acc: 99.93%
2026-02-10 01:38:26,605 - INFO - [Valid] [82/90] | Loss: 0.5670 | Val Acc: 82.89%
2026-02-10 01:38:26,609 - INFO - [Metrics for 'abnormal'] | Precision: 0.7929 | Recall: 0.8535 | F1: 0.8221
2026-02-10 01:38:26,609 - INFO - [Metrics for 'normal'] | Precision: 0.8647 | Recall: 0.8077 | F1: 0.8352
2026-02-10 01:38:26,611 - INFO - --------------------------------------------------
2026-02-10 01:38:26,613 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 01:38:33,348 - INFO - [Train] [83/90] | Loss: 0.2047 | Train Acc: 99.85%
2026-02-10 01:38:34,822 - INFO - [Valid] [83/90] | Loss: 0.5643 | Val Acc: 83.19%
2026-02-10 01:38:34,827 - INFO - [Metrics for 'abnormal'] | Precision: 0.7976 | Recall: 0.8535 | F1: 0.8246
2026-02-10 01:38:34,828 - INFO - [Metrics for 'normal'] | Precision: 0.8655 | Recall: 0.8132 | F1: 0.8385
2026-02-10 01:38:34,829 - INFO - --------------------------------------------------
2026-02-10 01:38:34,831 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 01:38:41,841 - INFO - [Train] [84/90] | Loss: 0.2021 | Train Acc: 100.00%
2026-02-10 01:38:43,502 - INFO - [Valid] [84/90] | Loss: 0.5568 | Val Acc: 83.19%
2026-02-10 01:38:43,506 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8471 | F1: 0.8235
2026-02-10 01:38:43,507 - INFO - [Metrics for 'normal'] | Precision: 0.8613 | Recall: 0.8187 | F1: 0.8394
2026-02-10 01:38:43,508 - INFO - --------------------------------------------------
2026-02-10 01:38:43,510 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 01:38:50,847 - INFO - [Train] [85/90] | Loss: 0.2024 | Train Acc: 99.93%
2026-02-10 01:38:52,246 - INFO - [Valid] [85/90] | Loss: 0.5592 | Val Acc: 84.07%
2026-02-10 01:38:52,251 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8726 | F1: 0.8354
2026-02-10 01:38:52,252 - INFO - [Metrics for 'normal'] | Precision: 0.8810 | Recall: 0.8132 | F1: 0.8457
2026-02-10 01:38:52,253 - INFO - --------------------------------------------------
2026-02-10 01:38:52,255 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 01:38:59,693 - INFO - [Train] [86/90] | Loss: 0.2018 | Train Acc: 99.85%
2026-02-10 01:39:01,094 - INFO - [Valid] [86/90] | Loss: 0.5626 | Val Acc: 84.66%
2026-02-10 01:39:01,102 - INFO - [Metrics for 'abnormal'] | Precision: 0.8144 | Recall: 0.8662 | F1: 0.8395
2026-02-10 01:39:01,102 - INFO - [Metrics for 'normal'] | Precision: 0.8779 | Recall: 0.8297 | F1: 0.8531
2026-02-10 01:39:01,106 - INFO - --------------------------------------------------
2026-02-10 01:39:01,108 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 01:39:08,264 - INFO - [Train] [87/90] | Loss: 0.2006 | Train Acc: 100.00%
2026-02-10 01:39:09,821 - INFO - [Valid] [87/90] | Loss: 0.5529 | Val Acc: 83.78%
2026-02-10 01:39:09,827 - INFO - [Metrics for 'abnormal'] | Precision: 0.8036 | Recall: 0.8599 | F1: 0.8308
2026-02-10 01:39:09,827 - INFO - [Metrics for 'normal'] | Precision: 0.8713 | Recall: 0.8187 | F1: 0.8442
2026-02-10 01:39:09,829 - INFO - --------------------------------------------------
2026-02-10 01:39:09,830 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 01:39:16,773 - INFO - [Train] [88/90] | Loss: 0.2035 | Train Acc: 99.85%
2026-02-10 01:39:18,274 - INFO - [Valid] [88/90] | Loss: 0.5614 | Val Acc: 83.78%
2026-02-10 01:39:18,278 - INFO - [Metrics for 'abnormal'] | Precision: 0.8072 | Recall: 0.8535 | F1: 0.8297
2026-02-10 01:39:18,279 - INFO - [Metrics for 'normal'] | Precision: 0.8671 | Recall: 0.8242 | F1: 0.8451
2026-02-10 01:39:18,282 - INFO - --------------------------------------------------
2026-02-10 01:39:18,283 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 01:39:25,310 - INFO - [Train] [89/90] | Loss: 0.2034 | Train Acc: 99.78%
2026-02-10 01:39:26,416 - INFO - [Valid] [89/90] | Loss: 0.5428 | Val Acc: 83.78%
2026-02-10 01:39:26,421 - INFO - [Metrics for 'abnormal'] | Precision: 0.8072 | Recall: 0.8535 | F1: 0.8297
2026-02-10 01:39:26,421 - INFO - [Metrics for 'normal'] | Precision: 0.8671 | Recall: 0.8242 | F1: 0.8451
2026-02-10 01:39:26,423 - INFO - --------------------------------------------------
2026-02-10 01:39:26,429 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 01:39:33,538 - INFO - [Train] [90/90] | Loss: 0.2142 | Train Acc: 99.33%
2026-02-10 01:39:35,064 - INFO - [Valid] [90/90] | Loss: 0.5386 | Val Acc: 84.37%
2026-02-10 01:39:35,069 - INFO - [Metrics for 'abnormal'] | Precision: 0.8250 | Recall: 0.8408 | F1: 0.8328
2026-02-10 01:39:35,069 - INFO - [Metrics for 'normal'] | Precision: 0.8603 | Recall: 0.8462 | F1: 0.8532
2026-02-10 01:39:35,072 - INFO - ==================================================
2026-02-10 01:39:35,072 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 01:39:35,072 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 01:39:35,072 - INFO - Baseline 모델 'mobile_vit_xxs'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:39:35,126 - INFO - timm 모델(mobile_vit_xxs)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:39:35,149 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 01:39:35,150 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:39:35,150 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:35,151 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:35,510 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47856445312499996)에 맞춰 변경되었습니다.
2026-02-10 01:39:35,510 - INFO - ==================================================
2026-02-10 01:39:35,550 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/best_model.pth'
2026-02-10 01:39:35,551 - INFO - ==================================================
2026-02-10 01:39:35,551 - INFO - Test 모드를 시작합니다.
2026-02-10 01:39:35,727 - INFO - 연산량 (MACs): 0.0912 GMACs per sample
2026-02-10 01:39:35,728 - INFO - 연산량 (FLOPs): 0.1824 GFLOPs per sample
2026-02-10 01:39:35,728 - INFO - ==================================================
2026-02-10 01:39:35,728 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 01:39:37,334 - INFO - 샘플 당 평균 Forward Pass 시간: 6.54ms (std: 0.97ms), FPS: 156.18 (std: 23.81) (1개 샘플 x 100회 반복)
2026-02-10 01:39:37,335 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 44.36 MB
2026-02-10 01:39:37,335 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 01:39:39,950 - INFO - [Test] Loss: 0.3923 | Test Acc: 83.19%
2026-02-10 01:39:39,959 - INFO - [Metrics for 'abnormal'] | Precision: 0.8247 | Recall: 0.8089 | F1: 0.8167
2026-02-10 01:39:39,959 - INFO - [Metrics for 'normal'] | Precision: 0.8378 | Recall: 0.8516 | F1: 0.8447
2026-02-10 01:39:40,278 - INFO - ==================================================
2026-02-10 01:39:40,278 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/confusion_matrix_20260210_012624.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/confusion_matrix_20260210_012624.pdf'
2026-02-10 01:39:40,278 - INFO - ==================================================
2026-02-10 01:39:40,278 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 01:39:44,656 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/model_fp32_20260210_012624.onnx'에 저장되었습니다. (크기: 1.40 MB)
2026-02-10 01:39:45,376 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 5.44 MB
2026-02-10 01:39:45,376 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 01:39:49,363 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 32.81ms (std: 19.31ms)
2026-02-10 01:39:49,364 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 42.03 FPS (std: 23.60) (1개 샘플 x 100회 반복)
2026-02-10 01:39:49,364 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 12.56 MB
2026-02-10 01:39:49,364 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 23.82 MB
2026-02-10 01:40:06,430 - INFO - [Test (ONNX)] | Test Acc (ONNX): 83.19%
2026-02-10 01:40:06,463 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8247 | Recall: 0.8089 | F1: 0.8167
2026-02-10 01:40:06,468 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8378 | Recall: 0.8516 | F1: 0.8447
2026-02-10 01:40:06,760 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/val_acc.pdf'
2026-02-10 01:40:07,013 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/train_val_acc.pdf'
2026-02-10 01:40:07,221 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/F1_normal.pdf'
2026-02-10 01:40:07,480 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/val_loss.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/val_loss.pdf'
2026-02-10 01:40:07,682 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/learning_rate.pdf'
2026-02-10 01:40:10,031 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/compile.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_fpgm_20260210_012624/graph_20260210_012624/compile.pdf'
