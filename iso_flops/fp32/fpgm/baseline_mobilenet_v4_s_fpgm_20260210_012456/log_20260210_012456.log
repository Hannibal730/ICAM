2026-02-10 01:24:56,784 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/log_20260210_012456.log'에 저장됩니다.
2026-02-10 01:24:56,786 - INFO - ==================================================
2026-02-10 01:24:56,787 - INFO - config.yaml:
2026-02-10 01:24:56,787 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: mobilenet_v4_s
  use_fpgm_pruning: true
  pruning_flops_target: 0.1816

2026-02-10 01:24:56,787 - INFO - ==================================================
2026-02-10 01:24:56,815 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:24:56,815 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:24:56,815 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:24:56,819 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:24:56,819 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:24:56,819 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:24:56,819 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:24:57,000 - INFO - ==================================================
2026-02-10 01:24:57,001 - INFO - 모델 파라미터 수:
2026-02-10 01:24:57,001 - INFO -   - 총 파라미터: 2,495,586 개
2026-02-10 01:24:57,001 - INFO -   - 학습 가능한 파라미터: 2,495,586 개
2026-02-10 01:24:57,001 - INFO - ================================================================================
2026-02-10 01:24:57,001 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:24:57,001 - INFO - ================================================================================
2026-02-10 01:24:57,001 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:24:57,001 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:24:57,001 - INFO - ==================================================
2026-02-10 01:24:57,001 - INFO - train 모드를 시작합니다.
2026-02-10 01:24:57,001 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:24:57,001 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:24:57,001 - INFO - --------------------------------------------------
2026-02-10 01:24:57,002 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:24:59,703 - INFO - [Train] [1/10] | Loss: 2.7433 | Train Acc: 64.51%
2026-02-10 01:25:00,768 - INFO - [Valid] [1/10] | Loss: 0.7415 | Val Acc: 71.68%
2026-02-10 01:25:00,772 - INFO - [Metrics for 'abnormal'] | Precision: 0.7440 | Recall: 0.5924 | F1: 0.6596
2026-02-10 01:25:00,773 - INFO - [Metrics for 'normal'] | Precision: 0.7009 | Recall: 0.8242 | F1: 0.7576
2026-02-10 01:25:00,789 - INFO - [Best Model Saved] (val loss: 0.7415) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:25:00,789 - INFO - --------------------------------------------------
2026-02-10 01:25:00,790 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:25:02,715 - INFO - [Train] [2/10] | Loss: 0.7367 | Train Acc: 69.20%
2026-02-10 01:25:03,320 - INFO - [Valid] [2/10] | Loss: 1.4109 | Val Acc: 61.65%
2026-02-10 01:25:03,324 - INFO - [Metrics for 'abnormal'] | Precision: 0.6262 | Recall: 0.4268 | F1: 0.5076
2026-02-10 01:25:03,324 - INFO - [Metrics for 'normal'] | Precision: 0.6121 | Recall: 0.7802 | F1: 0.6860
2026-02-10 01:25:03,325 - INFO - --------------------------------------------------
2026-02-10 01:25:03,326 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:25:05,290 - INFO - [Train] [3/10] | Loss: 0.6696 | Train Acc: 71.28%
2026-02-10 01:25:05,830 - INFO - [Valid] [3/10] | Loss: 0.8142 | Val Acc: 70.50%
2026-02-10 01:25:05,833 - INFO - [Metrics for 'abnormal'] | Precision: 0.6256 | Recall: 0.9045 | F1: 0.7396
2026-02-10 01:25:05,833 - INFO - [Metrics for 'normal'] | Precision: 0.8661 | Recall: 0.5330 | F1: 0.6599
2026-02-10 01:25:05,834 - INFO - --------------------------------------------------
2026-02-10 01:25:05,835 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:25:08,107 - INFO - [Train] [4/10] | Loss: 0.5946 | Train Acc: 75.37%
2026-02-10 01:25:08,684 - INFO - [Valid] [4/10] | Loss: 0.7038 | Val Acc: 70.21%
2026-02-10 01:25:08,687 - INFO - [Metrics for 'abnormal'] | Precision: 0.6842 | Recall: 0.6624 | F1: 0.6731
2026-02-10 01:25:08,696 - INFO - [Metrics for 'normal'] | Precision: 0.7166 | Recall: 0.7363 | F1: 0.7263
2026-02-10 01:25:08,713 - INFO - [Best Model Saved] (val loss: 0.7038) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:25:08,713 - INFO - --------------------------------------------------
2026-02-10 01:25:08,714 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:25:11,457 - INFO - [Train] [5/10] | Loss: 0.5756 | Train Acc: 76.41%
2026-02-10 01:25:12,442 - INFO - [Valid] [5/10] | Loss: 0.7351 | Val Acc: 77.29%
2026-02-10 01:25:12,447 - INFO - [Metrics for 'abnormal'] | Precision: 0.7151 | Recall: 0.8471 | F1: 0.7755
2026-02-10 01:25:12,447 - INFO - [Metrics for 'normal'] | Precision: 0.8431 | Recall: 0.7088 | F1: 0.7701
2026-02-10 01:25:12,449 - INFO - --------------------------------------------------
2026-02-10 01:25:12,450 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:25:14,852 - INFO - [Train] [6/10] | Loss: 0.5417 | Train Acc: 78.79%
2026-02-10 01:25:15,596 - INFO - [Valid] [6/10] | Loss: 0.6396 | Val Acc: 72.86%
2026-02-10 01:25:15,601 - INFO - [Metrics for 'abnormal'] | Precision: 0.6540 | Recall: 0.8790 | F1: 0.7500
2026-02-10 01:25:15,601 - INFO - [Metrics for 'normal'] | Precision: 0.8516 | Recall: 0.5989 | F1: 0.7032
2026-02-10 01:25:15,630 - INFO - [Best Model Saved] (val loss: 0.6396) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:25:15,630 - INFO - --------------------------------------------------
2026-02-10 01:25:15,631 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:25:17,845 - INFO - [Train] [7/10] | Loss: 0.5055 | Train Acc: 80.36%
2026-02-10 01:25:18,284 - INFO - [Valid] [7/10] | Loss: 0.5698 | Val Acc: 80.24%
2026-02-10 01:25:18,288 - INFO - [Metrics for 'abnormal'] | Precision: 0.8169 | Recall: 0.7389 | F1: 0.7759
2026-02-10 01:25:18,288 - INFO - [Metrics for 'normal'] | Precision: 0.7919 | Recall: 0.8571 | F1: 0.8232
2026-02-10 01:25:18,316 - INFO - [Best Model Saved] (val loss: 0.5698) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:25:18,316 - INFO - --------------------------------------------------
2026-02-10 01:25:18,317 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:25:20,570 - INFO - [Train] [8/10] | Loss: 0.4788 | Train Acc: 81.92%
2026-02-10 01:25:21,110 - INFO - [Valid] [8/10] | Loss: 0.6045 | Val Acc: 74.04%
2026-02-10 01:25:21,113 - INFO - [Metrics for 'abnormal'] | Precision: 0.6885 | Recall: 0.8025 | F1: 0.7412
2026-02-10 01:25:21,114 - INFO - [Metrics for 'normal'] | Precision: 0.8013 | Recall: 0.6868 | F1: 0.7396
2026-02-10 01:25:21,115 - INFO - --------------------------------------------------
2026-02-10 01:25:21,115 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:25:22,999 - INFO - [Train] [9/10] | Loss: 0.4472 | Train Acc: 83.63%
2026-02-10 01:25:23,651 - INFO - [Valid] [9/10] | Loss: 0.5168 | Val Acc: 80.24%
2026-02-10 01:25:23,655 - INFO - [Metrics for 'abnormal'] | Precision: 0.8309 | Recall: 0.7197 | F1: 0.7713
2026-02-10 01:25:23,656 - INFO - [Metrics for 'normal'] | Precision: 0.7833 | Recall: 0.8736 | F1: 0.8260
2026-02-10 01:25:23,686 - INFO - [Best Model Saved] (val loss: 0.5168) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:25:23,686 - INFO - --------------------------------------------------
2026-02-10 01:25:23,687 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:25:25,618 - INFO - [Train] [10/10] | Loss: 0.4424 | Train Acc: 84.30%
2026-02-10 01:25:26,148 - INFO - [Valid] [10/10] | Loss: 0.5335 | Val Acc: 79.06%
2026-02-10 01:25:26,151 - INFO - [Metrics for 'abnormal'] | Precision: 0.7867 | Recall: 0.7516 | F1: 0.7687
2026-02-10 01:25:26,151 - INFO - [Metrics for 'normal'] | Precision: 0.7937 | Recall: 0.8242 | F1: 0.8086
2026-02-10 01:25:26,152 - INFO - ================================================================================
2026-02-10 01:25:26,152 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:25:26,152 - INFO - ================================================================================
2026-02-10 01:25:26,173 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:25:26,174 - INFO - ================================================================================
2026-02-10 01:25:26,174 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:25:26,202 - INFO - 원본 모델 FLOPs: 0.3853 GFLOPs
2026-02-10 01:25:26,232 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:26,233 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:26,327 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:25:26,327 - INFO - ==================================================
2026-02-10 01:25:26,360 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.1092 GFLOPs (감소율: 71.66%)
2026-02-10 01:25:26,385 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:26,385 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:26,450 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.2475)에 맞춰 변경되었습니다.
2026-02-10 01:25:26,450 - INFO - ==================================================
2026-02-10 01:25:26,491 - INFO -   [탐색  2] 희소도: 0.2475 -> FLOPs: 0.2263 GFLOPs (감소율: 41.26%)
2026-02-10 01:25:26,517 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:26,517 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:26,621 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.37124999999999997)에 맞춰 변경되었습니다.
2026-02-10 01:25:26,621 - INFO - ==================================================
2026-02-10 01:25:26,657 - INFO -   [탐색  3] 희소도: 0.3712 -> FLOPs: 0.1626 GFLOPs (감소율: 57.81%)
2026-02-10 01:25:26,683 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:26,684 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:26,779 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.30937499999999996)에 맞춰 변경되었습니다.
2026-02-10 01:25:26,779 - INFO - ==================================================
2026-02-10 01:25:26,806 - INFO -   [탐색  4] 희소도: 0.3094 -> FLOPs: 0.1932 GFLOPs (감소율: 49.86%)
2026-02-10 01:25:27,120 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:27,120 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:27,223 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.34031249999999996)에 맞춰 변경되었습니다.
2026-02-10 01:25:27,223 - INFO - ==================================================
2026-02-10 01:25:27,250 - INFO -   [탐색  5] 희소도: 0.3403 -> FLOPs: 0.1775 GFLOPs (감소율: 53.92%)
2026-02-10 01:25:27,266 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:27,266 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:27,370 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32484375)에 맞춰 변경되었습니다.
2026-02-10 01:25:27,371 - INFO - ==================================================
2026-02-10 01:25:27,396 - INFO -   [탐색  6] 희소도: 0.3248 -> FLOPs: 0.1825 GFLOPs (감소율: 52.64%)
2026-02-10 01:25:27,412 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:27,412 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:27,515 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.33257812499999995)에 맞춰 변경되었습니다.
2026-02-10 01:25:27,515 - INFO - ==================================================
2026-02-10 01:25:27,542 - INFO -   [탐색  7] 희소도: 0.3326 -> FLOPs: 0.1806 GFLOPs (감소율: 53.13%)
2026-02-10 01:25:27,561 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:27,561 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:27,678 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32871093749999997)에 맞춰 변경되었습니다.
2026-02-10 01:25:27,678 - INFO - ==================================================
2026-02-10 01:25:27,700 - INFO -   [탐색  8] 희소도: 0.3287 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:27,715 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:27,716 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:27,889 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32677734375)에 맞춰 변경되었습니다.
2026-02-10 01:25:27,889 - INFO - ==================================================
2026-02-10 01:25:27,906 - INFO -   [탐색  9] 희소도: 0.3268 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:27,919 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:27,919 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:28,044 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.327744140625)에 맞춰 변경되었습니다.
2026-02-10 01:25:28,045 - INFO - ==================================================
2026-02-10 01:25:28,064 - INFO -   [탐색 10] 희소도: 0.3277 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:28,080 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:28,080 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:28,124 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3282275390625)에 맞춰 변경되었습니다.
2026-02-10 01:25:28,124 - INFO - ==================================================
2026-02-10 01:25:28,142 - INFO -   [탐색 11] 희소도: 0.3282 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:28,159 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:28,159 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:28,203 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32798583984375)에 맞춰 변경되었습니다.
2026-02-10 01:25:28,203 - INFO - ==================================================
2026-02-10 01:25:28,223 - INFO -   [탐색 12] 희소도: 0.3280 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:28,239 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:28,239 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:28,334 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32810668945312504)에 맞춰 변경되었습니다.
2026-02-10 01:25:28,334 - INFO - ==================================================
2026-02-10 01:25:28,348 - INFO -   [탐색 13] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:28,363 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:28,363 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:28,403 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32816711425781253)에 맞춰 변경되었습니다.
2026-02-10 01:25:28,403 - INFO - ==================================================
2026-02-10 01:25:28,418 - INFO -   [탐색 14] 희소도: 0.3282 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:28,433 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:28,434 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:28,485 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281369018554688)에 맞춰 변경되었습니다.
2026-02-10 01:25:28,485 - INFO - ==================================================
2026-02-10 01:25:28,510 - INFO -   [탐색 15] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:28,534 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:28,534 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:28,599 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281217956542969)에 맞춰 변경되었습니다.
2026-02-10 01:25:28,599 - INFO - ==================================================
2026-02-10 01:25:28,625 - INFO -   [탐색 16] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:28,649 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:28,649 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:29,154 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812934875488287)에 맞춰 변경되었습니다.
2026-02-10 01:25:29,154 - INFO - ==================================================
2026-02-10 01:25:29,175 - INFO -   [탐색 17] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:29,193 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:29,193 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:29,300 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281255722045899)에 맞춰 변경되었습니다.
2026-02-10 01:25:29,300 - INFO - ==================================================
2026-02-10 01:25:29,318 - INFO -   [탐색 18] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:29,335 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:29,335 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:29,436 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812368392944335)에 맞춰 변경되었습니다.
2026-02-10 01:25:29,437 - INFO - ==================================================
2026-02-10 01:25:29,457 - INFO -   [탐색 19] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:29,475 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:29,475 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:29,576 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281246280670166)에 맞춰 변경되었습니다.
2026-02-10 01:25:29,576 - INFO - ==================================================
2026-02-10 01:25:29,591 - INFO -   [탐색 20] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:29,609 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:29,609 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:29,722 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281251001358032)에 맞춰 변경되었습니다.
2026-02-10 01:25:29,722 - INFO - ==================================================
2026-02-10 01:25:29,739 - INFO -   [탐색 21] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:29,756 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:29,756 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:29,863 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281248641014099)에 맞춰 변경되었습니다.
2026-02-10 01:25:29,864 - INFO - ==================================================
2026-02-10 01:25:29,879 - INFO -   [탐색 22] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:29,897 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:29,897 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:30,083 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249821186065)에 맞춰 변경되었습니다.
2026-02-10 01:25:30,083 - INFO - ==================================================
2026-02-10 01:25:30,096 - INFO -   [탐색 23] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:30,110 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:30,110 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:30,232 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812504112720486)에 맞춰 변경되었습니다.
2026-02-10 01:25:30,233 - INFO - ==================================================
2026-02-10 01:25:30,258 - INFO -   [탐색 24] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:30,286 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:30,286 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:30,354 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250116229057)에 맞춰 변경되었습니다.
2026-02-10 01:25:30,354 - INFO - ==================================================
2026-02-10 01:25:30,381 - INFO -   [탐색 25] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:30,410 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:30,410 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:30,506 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499687075614)에 맞춰 변경되었습니다.
2026-02-10 01:25:30,506 - INFO - ==================================================
2026-02-10 01:25:30,521 - INFO -   [탐색 26] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:30,537 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:30,537 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:30,577 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250042468309)에 맞춰 변경되었습니다.
2026-02-10 01:25:30,577 - INFO - ==================================================
2026-02-10 01:25:30,592 - INFO -   [탐색 27] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:30,900 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:30,900 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:31,003 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250005587935)에 맞춰 변경되었습니다.
2026-02-10 01:25:31,003 - INFO - ==================================================
2026-02-10 01:25:31,024 - INFO -   [탐색 28] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:31,043 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:31,043 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:31,160 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499871477485)에 맞춰 변경되었습니다.
2026-02-10 01:25:31,160 - INFO - ==================================================
2026-02-10 01:25:31,177 - INFO -   [탐색 29] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:31,194 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:31,194 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:31,307 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249996367842)에 맞춰 변경되었습니다.
2026-02-10 01:25:31,307 - INFO - ==================================================
2026-02-10 01:25:31,329 - INFO -   [탐색 30] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:31,357 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:31,357 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:31,471 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000977889)에 맞춰 변경되었습니다.
2026-02-10 01:25:31,472 - INFO - ==================================================
2026-02-10 01:25:31,495 - INFO -   [탐색 31] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:31,517 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:31,517 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:31,621 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499986728655)에 맞춰 변경되었습니다.
2026-02-10 01:25:31,621 - INFO - ==================================================
2026-02-10 01:25:31,638 - INFO -   [탐색 32] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:31,657 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:31,657 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:31,770 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499998253775)에 맞춰 변경되었습니다.
2026-02-10 01:25:31,770 - INFO - ==================================================
2026-02-10 01:25:31,787 - INFO -   [탐색 33] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:31,806 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:31,807 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:31,931 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000401633)에 맞춰 변경되었습니다.
2026-02-10 01:25:31,931 - INFO - ==================================================
2026-02-10 01:25:31,954 - INFO -   [탐색 34] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:31,977 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:31,977 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:32,129 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500001135053)에 맞춰 변경되었습니다.
2026-02-10 01:25:32,129 - INFO - ==================================================
2026-02-10 01:25:32,145 - INFO -   [탐색 35] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:32,161 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:32,161 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:32,364 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999969441)에 맞춰 변경되었습니다.
2026-02-10 01:25:32,365 - INFO - ==================================================
2026-02-10 01:25:32,380 - INFO -   [탐색 36] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:32,397 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:32,397 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:32,438 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000414735)에 맞춰 변경되었습니다.
2026-02-10 01:25:32,439 - INFO - ==================================================
2026-02-10 01:25:32,455 - INFO -   [탐색 37] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:32,473 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:32,473 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:32,517 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000054573)에 맞춰 변경되었습니다.
2026-02-10 01:25:32,517 - INFO - ==================================================
2026-02-10 01:25:32,534 - INFO -   [탐색 38] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:32,771 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:32,771 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:32,832 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999987449)에 맞춰 변경되었습니다.
2026-02-10 01:25:32,832 - INFO - ==================================================
2026-02-10 01:25:32,858 - INFO -   [탐색 39] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:32,884 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:32,884 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:32,950 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999996453)에 맞춰 변경되었습니다.
2026-02-10 01:25:32,950 - INFO - ==================================================
2026-02-10 01:25:32,974 - INFO -   [탐색 40] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:33,003 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:33,003 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:33,056 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000955)에 맞춰 변경되었습니다.
2026-02-10 01:25:33,057 - INFO - ==================================================
2026-02-10 01:25:33,081 - INFO -   [탐색 41] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:33,100 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:33,100 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:33,200 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999998704)에 맞춰 변경되었습니다.
2026-02-10 01:25:33,200 - INFO - ==================================================
2026-02-10 01:25:33,219 - INFO -   [탐색 42] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:33,241 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:33,241 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:33,354 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999999829)에 맞춰 변경되었습니다.
2026-02-10 01:25:33,355 - INFO - ==================================================
2026-02-10 01:25:33,376 - INFO -   [탐색 43] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:33,400 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:33,401 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:33,511 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000392)에 맞춰 변경되었습니다.
2026-02-10 01:25:33,512 - INFO - ==================================================
2026-02-10 01:25:33,536 - INFO -   [탐색 44] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:33,555 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:33,555 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:33,676 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000001105)에 맞춰 변경되었습니다.
2026-02-10 01:25:33,677 - INFO - ==================================================
2026-02-10 01:25:33,701 - INFO -   [탐색 45] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:33,727 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:33,727 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:33,844 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.328124999999997)에 맞춰 변경되었습니다.
2026-02-10 01:25:33,844 - INFO - ==================================================
2026-02-10 01:25:33,862 - INFO -   [탐색 46] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:33,885 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:33,885 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:33,978 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.328125000000004)에 맞춰 변경되었습니다.
2026-02-10 01:25:33,979 - INFO - ==================================================
2026-02-10 01:25:33,996 - INFO -   [탐색 47] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:34,013 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:34,013 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:34,133 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000005)에 맞춰 변경되었습니다.
2026-02-10 01:25:34,134 - INFO - ==================================================
2026-02-10 01:25:34,150 - INFO -   [탐색 48] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:34,167 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:34,167 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:34,326 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999999988)에 맞춰 변경되었습니다.
2026-02-10 01:25:34,326 - INFO - ==================================================
2026-02-10 01:25:34,340 - INFO -   [탐색 49] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:34,354 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:34,354 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:34,597 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499999999967)에 맞춰 변경되었습니다.
2026-02-10 01:25:34,597 - INFO - ==================================================
2026-02-10 01:25:34,623 - INFO -   [탐색 50] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:34,650 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:34,650 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:34,721 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:34,721 - INFO - ==================================================
2026-02-10 01:25:34,745 - INFO -   [탐색 51] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:34,764 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:34,764 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:34,834 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999999999)에 맞춰 변경되었습니다.
2026-02-10 01:25:34,834 - INFO - ==================================================
2026-02-10 01:25:34,849 - INFO -   [탐색 52] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:34,864 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:34,864 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:34,904 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.328125)에 맞춰 변경되었습니다.
2026-02-10 01:25:34,904 - INFO - ==================================================
2026-02-10 01:25:34,917 - INFO -   [탐색 53] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:34,931 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:34,931 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:34,989 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000000006)에 맞춰 변경되었습니다.
2026-02-10 01:25:34,989 - INFO - ==================================================
2026-02-10 01:25:35,014 - INFO -   [탐색 54] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:25:35,039 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:35,039 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:35,105 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:35,105 - INFO - ==================================================
2026-02-10 01:25:35,130 - INFO -   [탐색 55] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:35,159 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:35,160 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:35,271 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:35,271 - INFO - ==================================================
2026-02-10 01:25:35,297 - INFO -   [탐색 56] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:35,323 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:35,323 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:35,446 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:35,446 - INFO - ==================================================
2026-02-10 01:25:35,464 - INFO -   [탐색 57] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:35,483 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:35,484 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:35,578 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:35,578 - INFO - ==================================================
2026-02-10 01:25:35,596 - INFO -   [탐색 58] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:35,613 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:35,613 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:35,713 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:35,713 - INFO - ==================================================
2026-02-10 01:25:35,730 - INFO -   [탐색 59] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:35,747 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:35,747 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:35,855 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:35,855 - INFO - ==================================================
2026-02-10 01:25:35,879 - INFO -   [탐색 60] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:35,905 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:35,905 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:36,027 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:36,027 - INFO - ==================================================
2026-02-10 01:25:36,047 - INFO -   [탐색 61] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:36,064 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:36,064 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:36,488 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:36,488 - INFO - ==================================================
2026-02-10 01:25:36,501 - INFO -   [탐색 62] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:36,515 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:36,516 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:36,695 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:36,695 - INFO - ==================================================
2026-02-10 01:25:36,721 - INFO -   [탐색 63] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:36,747 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:36,748 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:36,816 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:36,816 - INFO - ==================================================
2026-02-10 01:25:36,843 - INFO -   [탐색 64] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:36,872 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:36,872 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:36,963 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:36,963 - INFO - ==================================================
2026-02-10 01:25:36,978 - INFO -   [탐색 65] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:36,994 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:36,994 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:37,033 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:37,033 - INFO - ==================================================
2026-02-10 01:25:37,048 - INFO -   [탐색 66] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:37,063 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:37,063 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:37,116 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:37,116 - INFO - ==================================================
2026-02-10 01:25:37,132 - INFO -   [탐색 67] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:37,149 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:37,149 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:37,191 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:37,191 - INFO - ==================================================
2026-02-10 01:25:37,207 - INFO -   [탐색 68] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:37,225 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:37,225 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:37,267 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:37,267 - INFO - ==================================================
2026-02-10 01:25:37,283 - INFO -   [탐색 69] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:37,302 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:37,302 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:37,353 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:37,354 - INFO - ==================================================
2026-02-10 01:25:37,372 - INFO -   [탐색 70] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:37,400 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:37,401 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:37,522 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:37,523 - INFO - ==================================================
2026-02-10 01:25:37,549 - INFO -   [탐색 71] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:37,577 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:37,577 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:37,696 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:37,696 - INFO - ==================================================
2026-02-10 01:25:37,720 - INFO -   [탐색 72] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:37,747 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:37,747 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:37,859 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:37,859 - INFO - ==================================================
2026-02-10 01:25:37,876 - INFO -   [탐색 73] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:37,895 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:37,895 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:37,992 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:37,992 - INFO - ==================================================
2026-02-10 01:25:38,008 - INFO -   [탐색 74] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:38,026 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:38,026 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:38,406 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:38,407 - INFO - ==================================================
2026-02-10 01:25:38,428 - INFO -   [탐색 75] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:38,447 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:38,448 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:38,559 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:38,559 - INFO - ==================================================
2026-02-10 01:25:38,576 - INFO -   [탐색 76] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:38,592 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:38,592 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:38,827 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:38,827 - INFO - ==================================================
2026-02-10 01:25:38,840 - INFO -   [탐색 77] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:38,853 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:38,853 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:38,897 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:38,897 - INFO - ==================================================
2026-02-10 01:25:38,916 - INFO -   [탐색 78] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:38,934 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:38,934 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:38,978 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:38,978 - INFO - ==================================================
2026-02-10 01:25:38,997 - INFO -   [탐색 79] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:39,014 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:39,015 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:39,068 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:39,068 - INFO - ==================================================
2026-02-10 01:25:39,086 - INFO -   [탐색 80] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:39,110 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:39,110 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:39,181 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:39,181 - INFO - ==================================================
2026-02-10 01:25:39,196 - INFO -   [탐색 81] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:39,211 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:39,211 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:39,250 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:39,250 - INFO - ==================================================
2026-02-10 01:25:39,265 - INFO -   [탐색 82] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:39,279 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:39,279 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:39,319 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:39,319 - INFO - ==================================================
2026-02-10 01:25:39,337 - INFO -   [탐색 83] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:39,356 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:39,357 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:39,399 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:39,399 - INFO - ==================================================
2026-02-10 01:25:39,416 - INFO -   [탐색 84] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:39,433 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:39,433 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:39,475 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:39,475 - INFO - ==================================================
2026-02-10 01:25:39,492 - INFO -   [탐색 85] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:39,862 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:39,862 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:39,958 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:39,959 - INFO - ==================================================
2026-02-10 01:25:39,977 - INFO -   [탐색 86] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:39,999 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:39,999 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:40,092 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:40,092 - INFO - ==================================================
2026-02-10 01:25:40,109 - INFO -   [탐색 87] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:40,125 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:40,125 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:40,228 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:40,228 - INFO - ==================================================
2026-02-10 01:25:40,245 - INFO -   [탐색 88] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:40,265 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:40,265 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:40,364 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:40,364 - INFO - ==================================================
2026-02-10 01:25:40,381 - INFO -   [탐색 89] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:40,399 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:40,399 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:40,508 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:40,508 - INFO - ==================================================
2026-02-10 01:25:40,525 - INFO -   [탐색 90] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:40,546 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:40,546 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:40,653 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:40,653 - INFO - ==================================================
2026-02-10 01:25:40,670 - INFO -   [탐색 91] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:40,687 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:40,687 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:40,834 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:40,834 - INFO - ==================================================
2026-02-10 01:25:40,848 - INFO -   [탐색 92] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:40,862 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:40,862 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:41,055 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:41,055 - INFO - ==================================================
2026-02-10 01:25:41,083 - INFO -   [탐색 93] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:41,110 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:41,110 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:41,175 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:41,175 - INFO - ==================================================
2026-02-10 01:25:41,202 - INFO -   [탐색 94] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:41,230 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:41,230 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:41,339 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:41,339 - INFO - ==================================================
2026-02-10 01:25:41,353 - INFO -   [탐색 95] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:41,368 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:41,369 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:41,406 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:41,406 - INFO - ==================================================
2026-02-10 01:25:41,421 - INFO -   [탐색 96] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:41,711 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:41,711 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:41,829 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:41,829 - INFO - ==================================================
2026-02-10 01:25:41,855 - INFO -   [탐색 97] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:41,881 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:41,881 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:41,990 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:41,990 - INFO - ==================================================
2026-02-10 01:25:42,013 - INFO -   [탐색 98] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:42,037 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:42,037 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:42,154 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:42,154 - INFO - ==================================================
2026-02-10 01:25:42,172 - INFO -   [탐색 99] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:42,194 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:42,194 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:42,298 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:25:42,299 - INFO - ==================================================
2026-02-10 01:25:42,319 - INFO -   [탐색 100] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:25:42,320 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.3277 입니다.
2026-02-10 01:25:42,320 - INFO - ================================================================================
2026-02-10 01:25:42,323 - INFO - 계산된 Pruning 정보(희소도: 0.3277)를 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:25:42,344 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:25:42,384 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:25:42,384 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:25:42,524 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.327744140625)에 맞춰 변경되었습니다.
2026-02-10 01:25:42,524 - INFO - ==================================================
2026-02-10 01:25:42,525 - INFO - ==================================================
2026-02-10 01:25:42,525 - INFO - 모델 파라미터 수:
2026-02-10 01:25:42,525 - INFO -   - 총 파라미터: 1,146,105 개
2026-02-10 01:25:42,525 - INFO -   - 학습 가능한 파라미터: 1,146,105 개
2026-02-10 01:25:42,542 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:25:42,576 - INFO - FLOPs가 0.3853 GFLOPs에서 0.1822 GFLOPs로 감소했습니다 (감소율: 52.70%).
2026-02-10 01:25:42,576 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:25:42,576 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:25:42,577 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:25:42,577 - INFO - ==================================================
2026-02-10 01:25:42,577 - INFO - train 모드를 시작합니다.
2026-02-10 01:25:42,577 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:25:42,577 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:25:42,577 - INFO - --------------------------------------------------
2026-02-10 01:25:42,578 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:25:44,428 - INFO - [Train] [11/90] | Loss: 0.8500 | Train Acc: 64.51%
2026-02-10 01:25:45,004 - INFO - [Valid] [11/90] | Loss: 0.7058 | Val Acc: 68.73%
2026-02-10 01:25:45,008 - INFO - [Metrics for 'abnormal'] | Precision: 0.7931 | Recall: 0.4395 | F1: 0.5656
2026-02-10 01:25:45,008 - INFO - [Metrics for 'normal'] | Precision: 0.6508 | Recall: 0.9011 | F1: 0.7558
2026-02-10 01:25:45,029 - INFO - [Best Model Saved] (val loss: 0.7058) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:25:45,030 - INFO - --------------------------------------------------
2026-02-10 01:25:45,031 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:25:47,239 - INFO - [Train] [12/90] | Loss: 0.6540 | Train Acc: 73.88%
2026-02-10 01:25:47,676 - INFO - [Valid] [12/90] | Loss: 0.6144 | Val Acc: 74.04%
2026-02-10 01:25:47,680 - INFO - [Metrics for 'abnormal'] | Precision: 0.7285 | Recall: 0.7006 | F1: 0.7143
2026-02-10 01:25:47,680 - INFO - [Metrics for 'normal'] | Precision: 0.7500 | Recall: 0.7747 | F1: 0.7622
2026-02-10 01:25:47,712 - INFO - [Best Model Saved] (val loss: 0.6144) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:25:47,712 - INFO - --------------------------------------------------
2026-02-10 01:25:47,713 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:25:50,481 - INFO - [Train] [13/90] | Loss: 0.5120 | Train Acc: 79.76%
2026-02-10 01:25:51,208 - INFO - [Valid] [13/90] | Loss: 0.5831 | Val Acc: 73.45%
2026-02-10 01:25:51,213 - INFO - [Metrics for 'abnormal'] | Precision: 0.6736 | Recall: 0.8280 | F1: 0.7429
2026-02-10 01:25:51,214 - INFO - [Metrics for 'normal'] | Precision: 0.8151 | Recall: 0.6538 | F1: 0.7256
2026-02-10 01:25:51,240 - INFO - [Best Model Saved] (val loss: 0.5831) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:25:51,240 - INFO - --------------------------------------------------
2026-02-10 01:25:51,242 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:25:54,012 - INFO - [Train] [14/90] | Loss: 0.5503 | Train Acc: 77.68%
2026-02-10 01:25:54,962 - INFO - [Valid] [14/90] | Loss: 0.6495 | Val Acc: 72.86%
2026-02-10 01:25:54,967 - INFO - [Metrics for 'abnormal'] | Precision: 0.7686 | Recall: 0.5924 | F1: 0.6691
2026-02-10 01:25:54,967 - INFO - [Metrics for 'normal'] | Precision: 0.7064 | Recall: 0.8462 | F1: 0.7700
2026-02-10 01:25:54,969 - INFO - --------------------------------------------------
2026-02-10 01:25:54,970 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:25:58,027 - INFO - [Train] [15/90] | Loss: 0.5463 | Train Acc: 78.94%
2026-02-10 01:25:58,759 - INFO - [Valid] [15/90] | Loss: 0.8521 | Val Acc: 62.83%
2026-02-10 01:25:58,764 - INFO - [Metrics for 'abnormal'] | Precision: 0.5756 | Recall: 0.7516 | F1: 0.6519
2026-02-10 01:25:58,765 - INFO - [Metrics for 'normal'] | Precision: 0.7090 | Recall: 0.5220 | F1: 0.6013
2026-02-10 01:25:58,766 - INFO - --------------------------------------------------
2026-02-10 01:25:58,768 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:26:02,381 - INFO - [Train] [16/90] | Loss: 0.5166 | Train Acc: 79.84%
2026-02-10 01:26:03,009 - INFO - [Valid] [16/90] | Loss: 0.6896 | Val Acc: 70.50%
2026-02-10 01:26:03,014 - INFO - [Metrics for 'abnormal'] | Precision: 0.8065 | Recall: 0.4777 | F1: 0.6000
2026-02-10 01:26:03,014 - INFO - [Metrics for 'normal'] | Precision: 0.6667 | Recall: 0.9011 | F1: 0.7664
2026-02-10 01:26:03,016 - INFO - --------------------------------------------------
2026-02-10 01:26:03,017 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:26:07,385 - INFO - [Train] [17/90] | Loss: 0.5087 | Train Acc: 80.65%
2026-02-10 01:26:08,425 - INFO - [Valid] [17/90] | Loss: 0.5471 | Val Acc: 75.81%
2026-02-10 01:26:08,430 - INFO - [Metrics for 'abnormal'] | Precision: 0.8788 | Recall: 0.5541 | F1: 0.6797
2026-02-10 01:26:08,430 - INFO - [Metrics for 'normal'] | Precision: 0.7083 | Recall: 0.9341 | F1: 0.8057
2026-02-10 01:26:08,457 - INFO - [Best Model Saved] (val loss: 0.5471) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:26:08,458 - INFO - --------------------------------------------------
2026-02-10 01:26:08,459 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:26:12,545 - INFO - [Train] [18/90] | Loss: 0.4991 | Train Acc: 81.32%
2026-02-10 01:26:13,663 - INFO - [Valid] [18/90] | Loss: 0.5173 | Val Acc: 78.17%
2026-02-10 01:26:13,668 - INFO - [Metrics for 'abnormal'] | Precision: 0.7748 | Recall: 0.7452 | F1: 0.7597
2026-02-10 01:26:13,668 - INFO - [Metrics for 'normal'] | Precision: 0.7872 | Recall: 0.8132 | F1: 0.8000
2026-02-10 01:26:13,700 - INFO - [Best Model Saved] (val loss: 0.5173) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:26:13,700 - INFO - --------------------------------------------------
2026-02-10 01:26:13,701 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:26:17,893 - INFO - [Train] [19/90] | Loss: 0.4828 | Train Acc: 81.55%
2026-02-10 01:26:19,080 - INFO - [Valid] [19/90] | Loss: 0.5771 | Val Acc: 75.52%
2026-02-10 01:26:19,091 - INFO - [Metrics for 'abnormal'] | Precision: 0.7467 | Recall: 0.7134 | F1: 0.7296
2026-02-10 01:26:19,091 - INFO - [Metrics for 'normal'] | Precision: 0.7619 | Recall: 0.7912 | F1: 0.7763
2026-02-10 01:26:19,093 - INFO - --------------------------------------------------
2026-02-10 01:26:19,094 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:26:23,551 - INFO - [Train] [20/90] | Loss: 0.4708 | Train Acc: 84.15%
2026-02-10 01:26:24,593 - INFO - [Valid] [20/90] | Loss: 1.1182 | Val Acc: 65.49%
2026-02-10 01:26:24,598 - INFO - [Metrics for 'abnormal'] | Precision: 0.6190 | Recall: 0.6624 | F1: 0.6400
2026-02-10 01:26:24,598 - INFO - [Metrics for 'normal'] | Precision: 0.6901 | Recall: 0.6484 | F1: 0.6686
2026-02-10 01:26:24,599 - INFO - --------------------------------------------------
2026-02-10 01:26:24,601 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:26:30,504 - INFO - [Train] [21/90] | Loss: 0.4827 | Train Acc: 81.85%
2026-02-10 01:26:32,044 - INFO - [Valid] [21/90] | Loss: 0.5417 | Val Acc: 79.94%
2026-02-10 01:26:32,047 - INFO - [Metrics for 'abnormal'] | Precision: 0.7908 | Recall: 0.7707 | F1: 0.7806
2026-02-10 01:26:32,048 - INFO - [Metrics for 'normal'] | Precision: 0.8065 | Recall: 0.8242 | F1: 0.8152
2026-02-10 01:26:32,049 - INFO - --------------------------------------------------
2026-02-10 01:26:32,050 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:26:36,825 - INFO - [Train] [22/90] | Loss: 0.4689 | Train Acc: 84.60%
2026-02-10 01:26:38,429 - INFO - [Valid] [22/90] | Loss: 0.8954 | Val Acc: 64.90%
2026-02-10 01:26:38,434 - INFO - [Metrics for 'abnormal'] | Precision: 0.5941 | Recall: 0.7643 | F1: 0.6685
2026-02-10 01:26:38,434 - INFO - [Metrics for 'normal'] | Precision: 0.7299 | Recall: 0.5495 | F1: 0.6270
2026-02-10 01:26:38,435 - INFO - --------------------------------------------------
2026-02-10 01:26:38,437 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:26:43,598 - INFO - [Train] [23/90] | Loss: 0.4407 | Train Acc: 85.19%
2026-02-10 01:26:44,773 - INFO - [Valid] [23/90] | Loss: 0.5016 | Val Acc: 79.65%
2026-02-10 01:26:44,777 - INFO - [Metrics for 'abnormal'] | Precision: 0.8235 | Recall: 0.7134 | F1: 0.7645
2026-02-10 01:26:44,778 - INFO - [Metrics for 'normal'] | Precision: 0.7783 | Recall: 0.8681 | F1: 0.8208
2026-02-10 01:26:44,806 - INFO - [Best Model Saved] (val loss: 0.5016) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:26:44,806 - INFO - --------------------------------------------------
2026-02-10 01:26:44,807 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:26:48,786 - INFO - [Train] [24/90] | Loss: 0.4608 | Train Acc: 83.56%
2026-02-10 01:26:49,949 - INFO - [Valid] [24/90] | Loss: 0.5906 | Val Acc: 82.89%
2026-02-10 01:26:49,960 - INFO - [Metrics for 'abnormal'] | Precision: 0.8367 | Recall: 0.7834 | F1: 0.8092
2026-02-10 01:26:49,960 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.8681 | F1: 0.8449
2026-02-10 01:26:49,962 - INFO - --------------------------------------------------
2026-02-10 01:26:49,963 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:26:54,940 - INFO - [Train] [25/90] | Loss: 0.4290 | Train Acc: 86.09%
2026-02-10 01:26:55,786 - INFO - [Valid] [25/90] | Loss: 0.4813 | Val Acc: 82.60%
2026-02-10 01:26:55,791 - INFO - [Metrics for 'abnormal'] | Precision: 0.8311 | Recall: 0.7834 | F1: 0.8066
2026-02-10 01:26:55,791 - INFO - [Metrics for 'normal'] | Precision: 0.8220 | Recall: 0.8626 | F1: 0.8418
2026-02-10 01:26:55,823 - INFO - [Best Model Saved] (val loss: 0.4813) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:26:55,823 - INFO - --------------------------------------------------
2026-02-10 01:26:55,824 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:27:00,655 - INFO - [Train] [26/90] | Loss: 0.4091 | Train Acc: 86.46%
2026-02-10 01:27:01,814 - INFO - [Valid] [26/90] | Loss: 0.5752 | Val Acc: 75.81%
2026-02-10 01:27:01,820 - INFO - [Metrics for 'abnormal'] | Precision: 0.8713 | Recall: 0.5605 | F1: 0.6822
2026-02-10 01:27:01,820 - INFO - [Metrics for 'normal'] | Precision: 0.7101 | Recall: 0.9286 | F1: 0.8048
2026-02-10 01:27:01,822 - INFO - --------------------------------------------------
2026-02-10 01:27:01,823 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 01:27:06,158 - INFO - [Train] [27/90] | Loss: 0.4087 | Train Acc: 87.87%
2026-02-10 01:27:07,371 - INFO - [Valid] [27/90] | Loss: 0.5320 | Val Acc: 79.94%
2026-02-10 01:27:07,380 - INFO - [Metrics for 'abnormal'] | Precision: 0.8938 | Recall: 0.6433 | F1: 0.7481
2026-02-10 01:27:07,381 - INFO - [Metrics for 'normal'] | Precision: 0.7522 | Recall: 0.9341 | F1: 0.8333
2026-02-10 01:27:07,382 - INFO - --------------------------------------------------
2026-02-10 01:27:07,384 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 01:27:11,321 - INFO - [Train] [28/90] | Loss: 0.3922 | Train Acc: 88.39%
2026-02-10 01:27:12,463 - INFO - [Valid] [28/90] | Loss: 0.6711 | Val Acc: 71.68%
2026-02-10 01:27:12,468 - INFO - [Metrics for 'abnormal'] | Precision: 0.6649 | Recall: 0.7834 | F1: 0.7193
2026-02-10 01:27:12,468 - INFO - [Metrics for 'normal'] | Precision: 0.7792 | Recall: 0.6593 | F1: 0.7143
2026-02-10 01:27:12,470 - INFO - --------------------------------------------------
2026-02-10 01:27:12,471 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 01:27:15,604 - INFO - [Train] [29/90] | Loss: 0.4277 | Train Acc: 86.83%
2026-02-10 01:27:16,244 - INFO - [Valid] [29/90] | Loss: 0.5446 | Val Acc: 81.42%
2026-02-10 01:27:16,248 - INFO - [Metrics for 'abnormal'] | Precision: 0.8456 | Recall: 0.7325 | F1: 0.7850
2026-02-10 01:27:16,248 - INFO - [Metrics for 'normal'] | Precision: 0.7931 | Recall: 0.8846 | F1: 0.8364
2026-02-10 01:27:16,250 - INFO - --------------------------------------------------
2026-02-10 01:27:16,251 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 01:27:19,964 - INFO - [Train] [30/90] | Loss: 0.3860 | Train Acc: 89.14%
2026-02-10 01:27:20,683 - INFO - [Valid] [30/90] | Loss: 0.5624 | Val Acc: 79.35%
2026-02-10 01:27:20,686 - INFO - [Metrics for 'abnormal'] | Precision: 0.7514 | Recall: 0.8280 | F1: 0.7879
2026-02-10 01:27:20,686 - INFO - [Metrics for 'normal'] | Precision: 0.8373 | Recall: 0.7637 | F1: 0.7989
2026-02-10 01:27:20,687 - INFO - --------------------------------------------------
2026-02-10 01:27:20,687 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 01:27:24,249 - INFO - [Train] [31/90] | Loss: 0.3680 | Train Acc: 90.03%
2026-02-10 01:27:25,027 - INFO - [Valid] [31/90] | Loss: 0.5059 | Val Acc: 79.65%
2026-02-10 01:27:25,033 - INFO - [Metrics for 'abnormal'] | Precision: 0.7716 | Recall: 0.7962 | F1: 0.7837
2026-02-10 01:27:25,033 - INFO - [Metrics for 'normal'] | Precision: 0.8192 | Recall: 0.7967 | F1: 0.8078
2026-02-10 01:27:25,034 - INFO - --------------------------------------------------
2026-02-10 01:27:25,035 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 01:27:27,895 - INFO - [Train] [32/90] | Loss: 0.3778 | Train Acc: 91.00%
2026-02-10 01:27:28,984 - INFO - [Valid] [32/90] | Loss: 0.5807 | Val Acc: 77.88%
2026-02-10 01:27:28,988 - INFO - [Metrics for 'abnormal'] | Precision: 0.6881 | Recall: 0.9554 | F1: 0.8000
2026-02-10 01:27:28,988 - INFO - [Metrics for 'normal'] | Precision: 0.9421 | Recall: 0.6264 | F1: 0.7525
2026-02-10 01:27:28,990 - INFO - --------------------------------------------------
2026-02-10 01:27:28,992 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 01:27:31,898 - INFO - [Train] [33/90] | Loss: 0.3489 | Train Acc: 91.59%
2026-02-10 01:27:32,987 - INFO - [Valid] [33/90] | Loss: 0.4874 | Val Acc: 82.01%
2026-02-10 01:27:32,990 - INFO - [Metrics for 'abnormal'] | Precision: 0.8380 | Recall: 0.7580 | F1: 0.7960
2026-02-10 01:27:32,990 - INFO - [Metrics for 'normal'] | Precision: 0.8071 | Recall: 0.8736 | F1: 0.8391
2026-02-10 01:27:32,991 - INFO - --------------------------------------------------
2026-02-10 01:27:32,991 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 01:27:35,224 - INFO - [Train] [34/90] | Loss: 0.3252 | Train Acc: 93.45%
2026-02-10 01:27:35,745 - INFO - [Valid] [34/90] | Loss: 0.4753 | Val Acc: 84.07%
2026-02-10 01:27:35,751 - INFO - [Metrics for 'abnormal'] | Precision: 0.8456 | Recall: 0.8025 | F1: 0.8235
2026-02-10 01:27:35,752 - INFO - [Metrics for 'normal'] | Precision: 0.8368 | Recall: 0.8736 | F1: 0.8548
2026-02-10 01:27:35,785 - INFO - [Best Model Saved] (val loss: 0.4753) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:27:35,785 - INFO - --------------------------------------------------
2026-02-10 01:27:35,786 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 01:27:38,217 - INFO - [Train] [35/90] | Loss: 0.3193 | Train Acc: 92.93%
2026-02-10 01:27:38,789 - INFO - [Valid] [35/90] | Loss: 0.4873 | Val Acc: 80.53%
2026-02-10 01:27:38,792 - INFO - [Metrics for 'abnormal'] | Precision: 0.8273 | Recall: 0.7325 | F1: 0.7770
2026-02-10 01:27:38,792 - INFO - [Metrics for 'normal'] | Precision: 0.7900 | Recall: 0.8681 | F1: 0.8272
2026-02-10 01:27:38,793 - INFO - --------------------------------------------------
2026-02-10 01:27:38,793 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 01:27:40,841 - INFO - [Train] [36/90] | Loss: 0.3267 | Train Acc: 93.38%
2026-02-10 01:27:41,598 - INFO - [Valid] [36/90] | Loss: 0.5581 | Val Acc: 81.12%
2026-02-10 01:27:41,602 - INFO - [Metrics for 'abnormal'] | Precision: 0.8444 | Recall: 0.7261 | F1: 0.7808
2026-02-10 01:27:41,602 - INFO - [Metrics for 'normal'] | Precision: 0.7892 | Recall: 0.8846 | F1: 0.8342
2026-02-10 01:27:41,604 - INFO - --------------------------------------------------
2026-02-10 01:27:41,605 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 01:27:43,715 - INFO - [Train] [37/90] | Loss: 0.3255 | Train Acc: 93.23%
2026-02-10 01:27:44,840 - INFO - [Valid] [37/90] | Loss: 0.5025 | Val Acc: 81.71%
2026-02-10 01:27:44,845 - INFO - [Metrics for 'abnormal'] | Precision: 0.8369 | Recall: 0.7516 | F1: 0.7919
2026-02-10 01:27:44,845 - INFO - [Metrics for 'normal'] | Precision: 0.8030 | Recall: 0.8736 | F1: 0.8368
2026-02-10 01:27:44,847 - INFO - --------------------------------------------------
2026-02-10 01:27:44,848 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 01:27:48,465 - INFO - [Train] [38/90] | Loss: 0.3194 | Train Acc: 93.30%
2026-02-10 01:27:49,163 - INFO - [Valid] [38/90] | Loss: 0.4918 | Val Acc: 84.07%
2026-02-10 01:27:49,168 - INFO - [Metrics for 'abnormal'] | Precision: 0.8503 | Recall: 0.7962 | F1: 0.8224
2026-02-10 01:27:49,168 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8791 | F1: 0.8556
2026-02-10 01:27:49,174 - INFO - --------------------------------------------------
2026-02-10 01:27:49,175 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 01:27:53,030 - INFO - [Train] [39/90] | Loss: 0.2937 | Train Acc: 95.24%
2026-02-10 01:27:54,172 - INFO - [Valid] [39/90] | Loss: 0.5199 | Val Acc: 83.78%
2026-02-10 01:27:54,177 - INFO - [Metrics for 'abnormal'] | Precision: 0.8400 | Recall: 0.8025 | F1: 0.8208
2026-02-10 01:27:54,184 - INFO - [Metrics for 'normal'] | Precision: 0.8360 | Recall: 0.8681 | F1: 0.8518
2026-02-10 01:27:54,186 - INFO - --------------------------------------------------
2026-02-10 01:27:54,187 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 01:27:57,926 - INFO - [Train] [40/90] | Loss: 0.2916 | Train Acc: 95.76%
2026-02-10 01:27:59,146 - INFO - [Valid] [40/90] | Loss: 0.5589 | Val Acc: 79.35%
2026-02-10 01:27:59,154 - INFO - [Metrics for 'abnormal'] | Precision: 0.8085 | Recall: 0.7261 | F1: 0.7651
2026-02-10 01:27:59,154 - INFO - [Metrics for 'normal'] | Precision: 0.7828 | Recall: 0.8516 | F1: 0.8158
2026-02-10 01:27:59,156 - INFO - --------------------------------------------------
2026-02-10 01:27:59,157 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 01:28:03,368 - INFO - [Train] [41/90] | Loss: 0.3122 | Train Acc: 93.90%
2026-02-10 01:28:04,653 - INFO - [Valid] [41/90] | Loss: 0.5275 | Val Acc: 82.60%
2026-02-10 01:28:04,658 - INFO - [Metrics for 'abnormal'] | Precision: 0.8603 | Recall: 0.7452 | F1: 0.7986
2026-02-10 01:28:04,658 - INFO - [Metrics for 'normal'] | Precision: 0.8030 | Recall: 0.8956 | F1: 0.8468
2026-02-10 01:28:04,659 - INFO - --------------------------------------------------
2026-02-10 01:28:04,661 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 01:28:09,048 - INFO - [Train] [42/90] | Loss: 0.3175 | Train Acc: 94.35%
2026-02-10 01:28:10,324 - INFO - [Valid] [42/90] | Loss: 0.5357 | Val Acc: 82.01%
2026-02-10 01:28:10,329 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.7643 | F1: 0.7973
2026-02-10 01:28:10,329 - INFO - [Metrics for 'normal'] | Precision: 0.8103 | Recall: 0.8681 | F1: 0.8382
2026-02-10 01:28:10,334 - INFO - --------------------------------------------------
2026-02-10 01:28:10,336 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 01:28:15,301 - INFO - [Train] [43/90] | Loss: 0.2994 | Train Acc: 94.87%
2026-02-10 01:28:16,770 - INFO - [Valid] [43/90] | Loss: 0.5431 | Val Acc: 82.60%
2026-02-10 01:28:16,775 - INFO - [Metrics for 'abnormal'] | Precision: 0.8224 | Recall: 0.7962 | F1: 0.8091
2026-02-10 01:28:16,776 - INFO - [Metrics for 'normal'] | Precision: 0.8289 | Recall: 0.8516 | F1: 0.8401
2026-02-10 01:28:16,778 - INFO - --------------------------------------------------
2026-02-10 01:28:16,780 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 01:28:21,620 - INFO - [Train] [44/90] | Loss: 0.2836 | Train Acc: 96.50%
2026-02-10 01:28:22,770 - INFO - [Valid] [44/90] | Loss: 0.5607 | Val Acc: 83.78%
2026-02-10 01:28:22,775 - INFO - [Metrics for 'abnormal'] | Precision: 0.8592 | Recall: 0.7771 | F1: 0.8161
2026-02-10 01:28:22,776 - INFO - [Metrics for 'normal'] | Precision: 0.8223 | Recall: 0.8901 | F1: 0.8549
2026-02-10 01:28:22,777 - INFO - --------------------------------------------------
2026-02-10 01:28:22,778 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 01:28:27,564 - INFO - [Train] [45/90] | Loss: 0.2738 | Train Acc: 96.95%
2026-02-10 01:28:28,705 - INFO - [Valid] [45/90] | Loss: 0.5406 | Val Acc: 81.71%
2026-02-10 01:28:28,710 - INFO - [Metrics for 'abnormal'] | Precision: 0.8322 | Recall: 0.7580 | F1: 0.7933
2026-02-10 01:28:28,710 - INFO - [Metrics for 'normal'] | Precision: 0.8061 | Recall: 0.8681 | F1: 0.8360
2026-02-10 01:28:28,711 - INFO - --------------------------------------------------
2026-02-10 01:28:28,713 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 01:28:33,613 - INFO - [Train] [46/90] | Loss: 0.2840 | Train Acc: 96.13%
2026-02-10 01:28:34,683 - INFO - [Valid] [46/90] | Loss: 0.5478 | Val Acc: 82.89%
2026-02-10 01:28:34,688 - INFO - [Metrics for 'abnormal'] | Precision: 0.7861 | Recall: 0.8662 | F1: 0.8242
2026-02-10 01:28:34,688 - INFO - [Metrics for 'normal'] | Precision: 0.8735 | Recall: 0.7967 | F1: 0.8333
2026-02-10 01:28:34,690 - INFO - --------------------------------------------------
2026-02-10 01:28:34,695 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 01:28:39,798 - INFO - [Train] [47/90] | Loss: 0.2694 | Train Acc: 97.25%
2026-02-10 01:28:41,416 - INFO - [Valid] [47/90] | Loss: 0.5099 | Val Acc: 84.37%
2026-02-10 01:28:41,422 - INFO - [Metrics for 'abnormal'] | Precision: 0.8514 | Recall: 0.8025 | F1: 0.8262
2026-02-10 01:28:41,423 - INFO - [Metrics for 'normal'] | Precision: 0.8377 | Recall: 0.8791 | F1: 0.8579
2026-02-10 01:28:41,425 - INFO - --------------------------------------------------
2026-02-10 01:28:41,427 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 01:28:46,084 - INFO - [Train] [48/90] | Loss: 0.2665 | Train Acc: 97.10%
2026-02-10 01:28:47,763 - INFO - [Valid] [48/90] | Loss: 0.5255 | Val Acc: 80.53%
2026-02-10 01:28:47,767 - INFO - [Metrics for 'abnormal'] | Precision: 0.7898 | Recall: 0.7898 | F1: 0.7898
2026-02-10 01:28:47,767 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8187 | F1: 0.8187
2026-02-10 01:28:47,768 - INFO - --------------------------------------------------
2026-02-10 01:28:47,770 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 01:28:52,995 - INFO - [Train] [49/90] | Loss: 0.2618 | Train Acc: 97.84%
2026-02-10 01:28:54,414 - INFO - [Valid] [49/90] | Loss: 0.5307 | Val Acc: 82.89%
2026-02-10 01:28:54,419 - INFO - [Metrics for 'abnormal'] | Precision: 0.8278 | Recall: 0.7962 | F1: 0.8117
2026-02-10 01:28:54,419 - INFO - [Metrics for 'normal'] | Precision: 0.8298 | Recall: 0.8571 | F1: 0.8432
2026-02-10 01:28:54,420 - INFO - --------------------------------------------------
2026-02-10 01:28:54,426 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 01:29:00,059 - INFO - [Train] [50/90] | Loss: 0.2567 | Train Acc: 97.77%
2026-02-10 01:29:01,338 - INFO - [Valid] [50/90] | Loss: 0.5545 | Val Acc: 83.19%
2026-02-10 01:29:01,342 - INFO - [Metrics for 'abnormal'] | Precision: 0.8472 | Recall: 0.7771 | F1: 0.8106
2026-02-10 01:29:01,343 - INFO - [Metrics for 'normal'] | Precision: 0.8205 | Recall: 0.8791 | F1: 0.8488
2026-02-10 01:29:01,344 - INFO - --------------------------------------------------
2026-02-10 01:29:01,346 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 01:29:07,280 - INFO - [Train] [51/90] | Loss: 0.2602 | Train Acc: 97.32%
2026-02-10 01:29:08,561 - INFO - [Valid] [51/90] | Loss: 0.5450 | Val Acc: 81.42%
2026-02-10 01:29:08,566 - INFO - [Metrics for 'abnormal'] | Precision: 0.7582 | Recall: 0.8790 | F1: 0.8142
2026-02-10 01:29:08,566 - INFO - [Metrics for 'normal'] | Precision: 0.8790 | Recall: 0.7582 | F1: 0.8142
2026-02-10 01:29:08,568 - INFO - --------------------------------------------------
2026-02-10 01:29:08,569 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 01:29:14,619 - INFO - [Train] [52/90] | Loss: 0.2606 | Train Acc: 97.92%
2026-02-10 01:29:16,097 - INFO - [Valid] [52/90] | Loss: 0.5476 | Val Acc: 80.53%
2026-02-10 01:29:16,107 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-02-10 01:29:16,107 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-02-10 01:29:16,109 - INFO - --------------------------------------------------
2026-02-10 01:29:16,110 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 01:29:22,288 - INFO - [Train] [53/90] | Loss: 0.2497 | Train Acc: 98.29%
2026-02-10 01:29:23,808 - INFO - [Valid] [53/90] | Loss: 0.5405 | Val Acc: 81.42%
2026-02-10 01:29:23,813 - INFO - [Metrics for 'abnormal'] | Precision: 0.7640 | Recall: 0.8662 | F1: 0.8119
2026-02-10 01:29:23,813 - INFO - [Metrics for 'normal'] | Precision: 0.8696 | Recall: 0.7692 | F1: 0.8163
2026-02-10 01:29:23,815 - INFO - --------------------------------------------------
2026-02-10 01:29:23,816 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 01:29:29,082 - INFO - [Train] [54/90] | Loss: 0.2532 | Train Acc: 98.07%
2026-02-10 01:29:30,471 - INFO - [Valid] [54/90] | Loss: 0.5230 | Val Acc: 82.30%
2026-02-10 01:29:30,477 - INFO - [Metrics for 'abnormal'] | Precision: 0.7904 | Recall: 0.8408 | F1: 0.8148
2026-02-10 01:29:30,477 - INFO - [Metrics for 'normal'] | Precision: 0.8547 | Recall: 0.8077 | F1: 0.8305
2026-02-10 01:29:30,479 - INFO - --------------------------------------------------
2026-02-10 01:29:30,481 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 01:29:36,531 - INFO - [Train] [55/90] | Loss: 0.2515 | Train Acc: 97.99%
2026-02-10 01:29:38,014 - INFO - [Valid] [55/90] | Loss: 0.5435 | Val Acc: 81.71%
2026-02-10 01:29:38,022 - INFO - [Metrics for 'abnormal'] | Precision: 0.8231 | Recall: 0.7707 | F1: 0.7961
2026-02-10 01:29:38,023 - INFO - [Metrics for 'normal'] | Precision: 0.8125 | Recall: 0.8571 | F1: 0.8342
2026-02-10 01:29:38,024 - INFO - --------------------------------------------------
2026-02-10 01:29:38,026 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 01:29:43,891 - INFO - [Train] [56/90] | Loss: 0.2578 | Train Acc: 97.69%
2026-02-10 01:29:44,819 - INFO - [Valid] [56/90] | Loss: 0.6531 | Val Acc: 76.99%
2026-02-10 01:29:44,824 - INFO - [Metrics for 'abnormal'] | Precision: 0.7005 | Recall: 0.8790 | F1: 0.7797
2026-02-10 01:29:44,825 - INFO - [Metrics for 'normal'] | Precision: 0.8662 | Recall: 0.6758 | F1: 0.7593
2026-02-10 01:29:44,827 - INFO - --------------------------------------------------
2026-02-10 01:29:44,828 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 01:29:50,994 - INFO - [Train] [57/90] | Loss: 0.2763 | Train Acc: 96.43%
2026-02-10 01:29:51,801 - INFO - [Valid] [57/90] | Loss: 0.5961 | Val Acc: 81.12%
2026-02-10 01:29:51,808 - INFO - [Metrics for 'abnormal'] | Precision: 0.7291 | Recall: 0.9427 | F1: 0.8222
2026-02-10 01:29:51,808 - INFO - [Metrics for 'normal'] | Precision: 0.9338 | Recall: 0.6978 | F1: 0.7987
2026-02-10 01:29:51,810 - INFO - --------------------------------------------------
2026-02-10 01:29:51,811 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 01:29:57,794 - INFO - [Train] [58/90] | Loss: 0.2589 | Train Acc: 98.14%
2026-02-10 01:29:59,128 - INFO - [Valid] [58/90] | Loss: 0.5292 | Val Acc: 81.42%
2026-02-10 01:29:59,133 - INFO - [Metrics for 'abnormal'] | Precision: 0.8357 | Recall: 0.7452 | F1: 0.7879
2026-02-10 01:29:59,133 - INFO - [Metrics for 'normal'] | Precision: 0.7990 | Recall: 0.8736 | F1: 0.8346
2026-02-10 01:29:59,135 - INFO - --------------------------------------------------
2026-02-10 01:29:59,136 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 01:30:04,449 - INFO - [Train] [59/90] | Loss: 0.2597 | Train Acc: 97.54%
2026-02-10 01:30:05,942 - INFO - [Valid] [59/90] | Loss: 0.5365 | Val Acc: 80.53%
2026-02-10 01:30:05,947 - INFO - [Metrics for 'abnormal'] | Precision: 0.8421 | Recall: 0.7134 | F1: 0.7724
2026-02-10 01:30:05,947 - INFO - [Metrics for 'normal'] | Precision: 0.7816 | Recall: 0.8846 | F1: 0.8299
2026-02-10 01:30:05,949 - INFO - --------------------------------------------------
2026-02-10 01:30:05,950 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 01:30:11,672 - INFO - [Train] [60/90] | Loss: 0.2463 | Train Acc: 98.14%
2026-02-10 01:30:13,144 - INFO - [Valid] [60/90] | Loss: 0.4949 | Val Acc: 83.78%
2026-02-10 01:30:13,149 - INFO - [Metrics for 'abnormal'] | Precision: 0.8312 | Recall: 0.8153 | F1: 0.8232
2026-02-10 01:30:13,149 - INFO - [Metrics for 'normal'] | Precision: 0.8432 | Recall: 0.8571 | F1: 0.8501
2026-02-10 01:30:13,150 - INFO - --------------------------------------------------
2026-02-10 01:30:13,152 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 01:30:18,660 - INFO - [Train] [61/90] | Loss: 0.2528 | Train Acc: 98.36%
2026-02-10 01:30:20,161 - INFO - [Valid] [61/90] | Loss: 0.5163 | Val Acc: 82.60%
2026-02-10 01:30:20,170 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-02-10 01:30:20,171 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-02-10 01:30:20,172 - INFO - --------------------------------------------------
2026-02-10 01:30:20,173 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 01:30:25,831 - INFO - [Train] [62/90] | Loss: 0.2376 | Train Acc: 98.96%
2026-02-10 01:30:27,170 - INFO - [Valid] [62/90] | Loss: 0.4822 | Val Acc: 82.30%
2026-02-10 01:30:27,175 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-02-10 01:30:27,175 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-02-10 01:30:27,177 - INFO - --------------------------------------------------
2026-02-10 01:30:27,178 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 01:30:33,675 - INFO - [Train] [63/90] | Loss: 0.2309 | Train Acc: 98.96%
2026-02-10 01:30:35,178 - INFO - [Valid] [63/90] | Loss: 0.5022 | Val Acc: 84.07%
2026-02-10 01:30:35,189 - INFO - [Metrics for 'abnormal'] | Precision: 0.8652 | Recall: 0.7771 | F1: 0.8188
2026-02-10 01:30:35,189 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8956 | F1: 0.8579
2026-02-10 01:30:35,190 - INFO - --------------------------------------------------
2026-02-10 01:30:35,192 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 01:30:40,942 - INFO - [Train] [64/90] | Loss: 0.2329 | Train Acc: 98.96%
2026-02-10 01:30:42,422 - INFO - [Valid] [64/90] | Loss: 0.5891 | Val Acc: 82.01%
2026-02-10 01:30:42,429 - INFO - [Metrics for 'abnormal'] | Precision: 0.8243 | Recall: 0.7771 | F1: 0.8000
2026-02-10 01:30:42,429 - INFO - [Metrics for 'normal'] | Precision: 0.8168 | Recall: 0.8571 | F1: 0.8365
2026-02-10 01:30:42,431 - INFO - --------------------------------------------------
2026-02-10 01:30:42,432 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 01:30:47,866 - INFO - [Train] [65/90] | Loss: 0.2342 | Train Acc: 98.96%
2026-02-10 01:30:49,287 - INFO - [Valid] [65/90] | Loss: 0.5215 | Val Acc: 82.89%
2026-02-10 01:30:49,291 - INFO - [Metrics for 'abnormal'] | Precision: 0.8235 | Recall: 0.8025 | F1: 0.8129
2026-02-10 01:30:49,291 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8516 | F1: 0.8424
2026-02-10 01:30:49,293 - INFO - --------------------------------------------------
2026-02-10 01:30:49,294 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 01:30:54,603 - INFO - [Train] [66/90] | Loss: 0.2405 | Train Acc: 98.29%
2026-02-10 01:30:56,234 - INFO - [Valid] [66/90] | Loss: 0.5279 | Val Acc: 82.60%
2026-02-10 01:30:56,242 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.8153 | F1: 0.8127
2026-02-10 01:30:56,242 - INFO - [Metrics for 'normal'] | Precision: 0.8398 | Recall: 0.8352 | F1: 0.8375
2026-02-10 01:30:56,245 - INFO - --------------------------------------------------
2026-02-10 01:30:56,246 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 01:31:01,546 - INFO - [Train] [67/90] | Loss: 0.2462 | Train Acc: 98.29%
2026-02-10 01:31:02,995 - INFO - [Valid] [67/90] | Loss: 0.5350 | Val Acc: 82.60%
2026-02-10 01:31:03,000 - INFO - [Metrics for 'abnormal'] | Precision: 0.8141 | Recall: 0.8089 | F1: 0.8115
2026-02-10 01:31:03,001 - INFO - [Metrics for 'normal'] | Precision: 0.8361 | Recall: 0.8407 | F1: 0.8384
2026-02-10 01:31:03,002 - INFO - --------------------------------------------------
2026-02-10 01:31:03,004 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 01:31:08,578 - INFO - [Train] [68/90] | Loss: 0.2449 | Train Acc: 98.21%
2026-02-10 01:31:09,538 - INFO - [Valid] [68/90] | Loss: 0.5354 | Val Acc: 80.83%
2026-02-10 01:31:09,543 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-02-10 01:31:09,543 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-02-10 01:31:09,545 - INFO - --------------------------------------------------
2026-02-10 01:31:09,546 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 01:31:15,537 - INFO - [Train] [69/90] | Loss: 0.2387 | Train Acc: 98.59%
2026-02-10 01:31:17,030 - INFO - [Valid] [69/90] | Loss: 0.5393 | Val Acc: 81.42%
2026-02-10 01:31:17,035 - INFO - [Metrics for 'abnormal'] | Precision: 0.8264 | Recall: 0.7580 | F1: 0.7907
2026-02-10 01:31:17,035 - INFO - [Metrics for 'normal'] | Precision: 0.8051 | Recall: 0.8626 | F1: 0.8329
2026-02-10 01:31:17,036 - INFO - --------------------------------------------------
2026-02-10 01:31:17,037 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 01:31:21,542 - INFO - [Train] [70/90] | Loss: 0.2441 | Train Acc: 98.14%
2026-02-10 01:31:23,060 - INFO - [Valid] [70/90] | Loss: 0.6887 | Val Acc: 81.12%
2026-02-10 01:31:23,064 - INFO - [Metrics for 'abnormal'] | Precision: 0.7627 | Recall: 0.8599 | F1: 0.8084
2026-02-10 01:31:23,064 - INFO - [Metrics for 'normal'] | Precision: 0.8642 | Recall: 0.7692 | F1: 0.8140
2026-02-10 01:31:23,066 - INFO - --------------------------------------------------
2026-02-10 01:31:23,067 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 01:31:28,111 - INFO - [Train] [71/90] | Loss: 0.2436 | Train Acc: 98.44%
2026-02-10 01:31:29,480 - INFO - [Valid] [71/90] | Loss: 0.5382 | Val Acc: 80.83%
2026-02-10 01:31:29,485 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-02-10 01:31:29,486 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-02-10 01:31:29,487 - INFO - --------------------------------------------------
2026-02-10 01:31:29,489 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 01:31:34,532 - INFO - [Train] [72/90] | Loss: 0.2337 | Train Acc: 98.96%
2026-02-10 01:31:35,915 - INFO - [Valid] [72/90] | Loss: 0.5768 | Val Acc: 81.12%
2026-02-10 01:31:35,920 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-02-10 01:31:35,920 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-02-10 01:31:35,922 - INFO - --------------------------------------------------
2026-02-10 01:31:35,928 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 01:31:41,133 - INFO - [Train] [73/90] | Loss: 0.2277 | Train Acc: 99.33%
2026-02-10 01:31:42,612 - INFO - [Valid] [73/90] | Loss: 0.5837 | Val Acc: 80.83%
2026-02-10 01:31:42,618 - INFO - [Metrics for 'abnormal'] | Precision: 0.7674 | Recall: 0.8408 | F1: 0.8024
2026-02-10 01:31:42,618 - INFO - [Metrics for 'normal'] | Precision: 0.8503 | Recall: 0.7802 | F1: 0.8138
2026-02-10 01:31:42,620 - INFO - --------------------------------------------------
2026-02-10 01:31:42,621 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 01:31:48,330 - INFO - [Train] [74/90] | Loss: 0.2348 | Train Acc: 98.96%
2026-02-10 01:31:49,833 - INFO - [Valid] [74/90] | Loss: 0.5835 | Val Acc: 81.71%
2026-02-10 01:31:49,838 - INFO - [Metrics for 'abnormal'] | Precision: 0.7746 | Recall: 0.8535 | F1: 0.8121
2026-02-10 01:31:49,838 - INFO - [Metrics for 'normal'] | Precision: 0.8614 | Recall: 0.7857 | F1: 0.8218
2026-02-10 01:31:49,839 - INFO - --------------------------------------------------
2026-02-10 01:31:49,841 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 01:31:55,377 - INFO - [Train] [75/90] | Loss: 0.2301 | Train Acc: 99.33%
2026-02-10 01:31:56,757 - INFO - [Valid] [75/90] | Loss: 0.5921 | Val Acc: 81.42%
2026-02-10 01:31:56,761 - INFO - [Metrics for 'abnormal'] | Precision: 0.8013 | Recall: 0.7962 | F1: 0.7987
2026-02-10 01:31:56,765 - INFO - [Metrics for 'normal'] | Precision: 0.8251 | Recall: 0.8297 | F1: 0.8274
2026-02-10 01:31:56,768 - INFO - --------------------------------------------------
2026-02-10 01:31:56,769 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 01:32:01,780 - INFO - [Train] [76/90] | Loss: 0.2245 | Train Acc: 99.48%
2026-02-10 01:32:03,094 - INFO - [Valid] [76/90] | Loss: 0.5686 | Val Acc: 80.53%
2026-02-10 01:32:03,099 - INFO - [Metrics for 'abnormal'] | Precision: 0.7692 | Recall: 0.8280 | F1: 0.7975
2026-02-10 01:32:03,099 - INFO - [Metrics for 'normal'] | Precision: 0.8412 | Recall: 0.7857 | F1: 0.8125
2026-02-10 01:32:03,101 - INFO - --------------------------------------------------
2026-02-10 01:32:03,103 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 01:32:07,961 - INFO - [Train] [77/90] | Loss: 0.2277 | Train Acc: 99.40%
2026-02-10 01:32:09,227 - INFO - [Valid] [77/90] | Loss: 0.5971 | Val Acc: 80.83%
2026-02-10 01:32:09,232 - INFO - [Metrics for 'abnormal'] | Precision: 0.8108 | Recall: 0.7643 | F1: 0.7869
2026-02-10 01:32:09,233 - INFO - [Metrics for 'normal'] | Precision: 0.8063 | Recall: 0.8462 | F1: 0.8257
2026-02-10 01:32:09,235 - INFO - --------------------------------------------------
2026-02-10 01:32:09,237 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 01:32:14,105 - INFO - [Train] [78/90] | Loss: 0.2298 | Train Acc: 99.33%
2026-02-10 01:32:15,228 - INFO - [Valid] [78/90] | Loss: 0.5808 | Val Acc: 80.83%
2026-02-10 01:32:15,232 - INFO - [Metrics for 'abnormal'] | Precision: 0.8108 | Recall: 0.7643 | F1: 0.7869
2026-02-10 01:32:15,232 - INFO - [Metrics for 'normal'] | Precision: 0.8063 | Recall: 0.8462 | F1: 0.8257
2026-02-10 01:32:15,234 - INFO - --------------------------------------------------
2026-02-10 01:32:15,237 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 01:32:20,616 - INFO - [Train] [79/90] | Loss: 0.2176 | Train Acc: 99.70%
2026-02-10 01:32:22,055 - INFO - [Valid] [79/90] | Loss: 0.5769 | Val Acc: 80.53%
2026-02-10 01:32:22,060 - INFO - [Metrics for 'abnormal'] | Precision: 0.8095 | Recall: 0.7580 | F1: 0.7829
2026-02-10 01:32:22,061 - INFO - [Metrics for 'normal'] | Precision: 0.8021 | Recall: 0.8462 | F1: 0.8235
2026-02-10 01:32:22,062 - INFO - --------------------------------------------------
2026-02-10 01:32:22,063 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 01:32:28,095 - INFO - [Train] [80/90] | Loss: 0.2184 | Train Acc: 99.70%
2026-02-10 01:32:29,767 - INFO - [Valid] [80/90] | Loss: 0.5773 | Val Acc: 81.12%
2026-02-10 01:32:29,773 - INFO - [Metrics for 'abnormal'] | Precision: 0.8345 | Recall: 0.7389 | F1: 0.7838
2026-02-10 01:32:29,773 - INFO - [Metrics for 'normal'] | Precision: 0.7950 | Recall: 0.8736 | F1: 0.8325
2026-02-10 01:32:29,775 - INFO - --------------------------------------------------
2026-02-10 01:32:29,777 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 01:32:35,857 - INFO - [Train] [81/90] | Loss: 0.2179 | Train Acc: 99.48%
2026-02-10 01:32:37,390 - INFO - [Valid] [81/90] | Loss: 0.5441 | Val Acc: 82.60%
2026-02-10 01:32:37,409 - INFO - [Metrics for 'abnormal'] | Precision: 0.7952 | Recall: 0.8408 | F1: 0.8173
2026-02-10 01:32:37,410 - INFO - [Metrics for 'normal'] | Precision: 0.8555 | Recall: 0.8132 | F1: 0.8338
2026-02-10 01:32:37,411 - INFO - --------------------------------------------------
2026-02-10 01:32:37,413 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 01:32:42,938 - INFO - [Train] [82/90] | Loss: 0.2226 | Train Acc: 99.26%
2026-02-10 01:32:44,424 - INFO - [Valid] [82/90] | Loss: 0.5476 | Val Acc: 82.30%
2026-02-10 01:32:44,429 - INFO - [Metrics for 'abnormal'] | Precision: 0.7904 | Recall: 0.8408 | F1: 0.8148
2026-02-10 01:32:44,429 - INFO - [Metrics for 'normal'] | Precision: 0.8547 | Recall: 0.8077 | F1: 0.8305
2026-02-10 01:32:44,431 - INFO - --------------------------------------------------
2026-02-10 01:32:44,432 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 01:32:49,716 - INFO - [Train] [83/90] | Loss: 0.2184 | Train Acc: 99.33%
2026-02-10 01:32:51,240 - INFO - [Valid] [83/90] | Loss: 0.5368 | Val Acc: 81.42%
2026-02-10 01:32:51,245 - INFO - [Metrics for 'abnormal'] | Precision: 0.7975 | Recall: 0.8025 | F1: 0.8000
2026-02-10 01:32:51,246 - INFO - [Metrics for 'normal'] | Precision: 0.8287 | Recall: 0.8242 | F1: 0.8264
2026-02-10 01:32:51,247 - INFO - --------------------------------------------------
2026-02-10 01:32:51,249 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 01:32:57,069 - INFO - [Train] [84/90] | Loss: 0.2189 | Train Acc: 99.55%
2026-02-10 01:32:58,481 - INFO - [Valid] [84/90] | Loss: 0.5692 | Val Acc: 82.01%
2026-02-10 01:32:58,487 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-02-10 01:32:58,487 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-02-10 01:32:58,488 - INFO - --------------------------------------------------
2026-02-10 01:32:58,494 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 01:33:03,940 - INFO - [Train] [85/90] | Loss: 0.2222 | Train Acc: 99.11%
2026-02-10 01:33:05,569 - INFO - [Valid] [85/90] | Loss: 0.5608 | Val Acc: 81.71%
2026-02-10 01:33:05,574 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.8089 | F1: 0.8038
2026-02-10 01:33:05,581 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8242 | F1: 0.8287
2026-02-10 01:33:05,584 - INFO - --------------------------------------------------
2026-02-10 01:33:05,585 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 01:33:11,577 - INFO - [Train] [86/90] | Loss: 0.2200 | Train Acc: 99.33%
2026-02-10 01:33:13,152 - INFO - [Valid] [86/90] | Loss: 0.5423 | Val Acc: 82.60%
2026-02-10 01:33:13,157 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.8153 | F1: 0.8127
2026-02-10 01:33:13,157 - INFO - [Metrics for 'normal'] | Precision: 0.8398 | Recall: 0.8352 | F1: 0.8375
2026-02-10 01:33:13,163 - INFO - --------------------------------------------------
2026-02-10 01:33:13,164 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 01:33:18,908 - INFO - [Train] [87/90] | Loss: 0.2174 | Train Acc: 99.33%
2026-02-10 01:33:20,419 - INFO - [Valid] [87/90] | Loss: 0.5605 | Val Acc: 82.01%
2026-02-10 01:33:20,429 - INFO - [Metrics for 'abnormal'] | Precision: 0.8077 | Recall: 0.8025 | F1: 0.8051
2026-02-10 01:33:20,429 - INFO - [Metrics for 'normal'] | Precision: 0.8306 | Recall: 0.8352 | F1: 0.8329
2026-02-10 01:33:20,430 - INFO - --------------------------------------------------
2026-02-10 01:33:20,432 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 01:33:26,150 - INFO - [Train] [88/90] | Loss: 0.2179 | Train Acc: 99.33%
2026-02-10 01:33:27,535 - INFO - [Valid] [88/90] | Loss: 0.5193 | Val Acc: 82.60%
2026-02-10 01:33:27,540 - INFO - [Metrics for 'abnormal'] | Precision: 0.8141 | Recall: 0.8089 | F1: 0.8115
2026-02-10 01:33:27,544 - INFO - [Metrics for 'normal'] | Precision: 0.8361 | Recall: 0.8407 | F1: 0.8384
2026-02-10 01:33:27,546 - INFO - --------------------------------------------------
2026-02-10 01:33:27,548 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 01:33:32,987 - INFO - [Train] [89/90] | Loss: 0.2109 | Train Acc: 99.63%
2026-02-10 01:33:33,958 - INFO - [Valid] [89/90] | Loss: 0.5328 | Val Acc: 82.30%
2026-02-10 01:33:33,963 - INFO - [Metrics for 'abnormal'] | Precision: 0.8050 | Recall: 0.8153 | F1: 0.8101
2026-02-10 01:33:33,964 - INFO - [Metrics for 'normal'] | Precision: 0.8389 | Recall: 0.8297 | F1: 0.8343
2026-02-10 01:33:33,965 - INFO - --------------------------------------------------
2026-02-10 01:33:33,967 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 01:33:39,159 - INFO - [Train] [90/90] | Loss: 0.2175 | Train Acc: 99.48%
2026-02-10 01:33:39,994 - INFO - [Valid] [90/90] | Loss: 0.5623 | Val Acc: 82.60%
2026-02-10 01:33:40,005 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8280 | F1: 0.8150
2026-02-10 01:33:40,005 - INFO - [Metrics for 'normal'] | Precision: 0.8475 | Recall: 0.8242 | F1: 0.8357
2026-02-10 01:33:40,007 - INFO - ==================================================
2026-02-10 01:33:40,008 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 01:33:40,008 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 01:33:40,008 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:33:40,166 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 01:33:40,167 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:33:40,167 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:33:40,307 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.327744140625)에 맞춰 변경되었습니다.
2026-02-10 01:33:40,308 - INFO - ==================================================
2026-02-10 01:33:40,416 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/best_model.pth'
2026-02-10 01:33:40,416 - INFO - ==================================================
2026-02-10 01:33:40,416 - INFO - Test 모드를 시작합니다.
2026-02-10 01:33:40,586 - INFO - 연산량 (MACs): 0.0911 GMACs per sample
2026-02-10 01:33:40,586 - INFO - 연산량 (FLOPs): 0.1822 GFLOPs per sample
2026-02-10 01:33:40,586 - INFO - ==================================================
2026-02-10 01:33:40,587 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 01:33:41,750 - INFO - 샘플 당 평균 Forward Pass 시간: 2.50ms (std: 0.62ms), FPS: 425.70 (std: 123.53) (1개 샘플 x 100회 반복)
2026-02-10 01:33:41,750 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 80.59 MB
2026-02-10 01:33:41,750 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 01:33:43,973 - INFO - [Test] Loss: 0.3800 | Test Acc: 84.07%
2026-02-10 01:33:43,981 - INFO - [Metrics for 'abnormal'] | Precision: 0.8456 | Recall: 0.8025 | F1: 0.8235
2026-02-10 01:33:43,982 - INFO - [Metrics for 'normal'] | Precision: 0.8368 | Recall: 0.8736 | F1: 0.8548
2026-02-10 01:33:44,283 - INFO - ==================================================
2026-02-10 01:33:44,283 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/confusion_matrix_20260210_012456.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/confusion_matrix_20260210_012456.pdf'
2026-02-10 01:33:44,283 - INFO - ==================================================
2026-02-10 01:33:44,283 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 01:33:47,344 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/model_fp32_20260210_012456.onnx'에 저장되었습니다. (크기: 4.36 MB)
2026-02-10 01:33:47,846 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 20.25 MB
2026-02-10 01:33:47,846 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 01:33:50,058 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 16.42ms (std: 11.30ms)
2026-02-10 01:33:50,059 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 97.69 FPS (std: 78.06) (1개 샘플 x 100회 반복)
2026-02-10 01:33:50,059 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 4.88 MB
2026-02-10 01:33:50,059 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 19.79 MB
2026-02-10 01:33:54,463 - INFO - [Test (ONNX)] | Test Acc (ONNX): 84.07%
2026-02-10 01:33:54,473 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8456 | Recall: 0.8025 | F1: 0.8235
2026-02-10 01:33:54,474 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8368 | Recall: 0.8736 | F1: 0.8548
2026-02-10 01:33:54,743 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/val_acc.pdf'
2026-02-10 01:33:54,976 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/train_val_acc.pdf'
2026-02-10 01:33:55,168 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/F1_normal.pdf'
2026-02-10 01:33:55,365 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/val_loss.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/val_loss.pdf'
2026-02-10 01:33:55,565 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/learning_rate.pdf'
2026-02-10 01:33:57,897 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/compile.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_fpgm_20260210_012456/graph_20260210_012456/compile.pdf'
