2026-02-10 01:26:03,035 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/log_20260210_012603.log'에 저장됩니다.
2026-02-10 01:26:03,039 - INFO - ==================================================
2026-02-10 01:26:03,039 - INFO - config.yaml:
2026-02-10 01:26:03,039 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: deit_tiny
  use_fpgm_pruning: true
  pruning_flops_target: 0.1816

2026-02-10 01:26:03,039 - INFO - ==================================================
2026-02-10 01:26:03,089 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:26:03,090 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:26:03,090 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:26:03,095 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:26:03,096 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:26:03,096 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:26:03,096 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:26:05,834 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:26:05,835 - INFO - ==================================================
2026-02-10 01:26:05,835 - INFO - 모델 파라미터 수:
2026-02-10 01:26:05,835 - INFO -   - 총 파라미터: 5,524,802 개
2026-02-10 01:26:05,835 - INFO -   - 학습 가능한 파라미터: 5,524,802 개
2026-02-10 01:26:05,835 - INFO - ================================================================================
2026-02-10 01:26:05,835 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:26:05,835 - INFO - ================================================================================
2026-02-10 01:26:05,835 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:26:05,836 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:26:05,836 - INFO - ==================================================
2026-02-10 01:26:05,836 - INFO - train 모드를 시작합니다.
2026-02-10 01:26:05,836 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:26:05,836 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:26:05,836 - INFO - --------------------------------------------------
2026-02-10 01:26:05,837 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:26:11,548 - INFO - [Train] [1/10] | Loss: 0.7051 | Train Acc: 60.49%
2026-02-10 01:26:13,236 - INFO - [Valid] [1/10] | Loss: 0.6724 | Val Acc: 59.88%
2026-02-10 01:26:13,251 - INFO - [Metrics for 'abnormal'] | Precision: 0.5488 | Recall: 0.7516 | F1: 0.6344
2026-02-10 01:26:13,252 - INFO - [Metrics for 'normal'] | Precision: 0.6855 | Recall: 0.4670 | F1: 0.5556
2026-02-10 01:26:13,298 - INFO - [Best Model Saved] (val loss: 0.6724) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:26:13,298 - INFO - --------------------------------------------------
2026-02-10 01:26:13,300 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:26:18,116 - INFO - [Train] [2/10] | Loss: 0.6464 | Train Acc: 64.43%
2026-02-10 01:26:19,508 - INFO - [Valid] [2/10] | Loss: 0.6629 | Val Acc: 61.65%
2026-02-10 01:26:19,513 - INFO - [Metrics for 'abnormal'] | Precision: 0.7547 | Recall: 0.2548 | F1: 0.3810
2026-02-10 01:26:19,513 - INFO - [Metrics for 'normal'] | Precision: 0.5909 | Recall: 0.9286 | F1: 0.7222
2026-02-10 01:26:19,602 - INFO - [Best Model Saved] (val loss: 0.6629) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:26:19,603 - INFO - --------------------------------------------------
2026-02-10 01:26:19,604 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:26:24,641 - INFO - [Train] [3/10] | Loss: 0.5956 | Train Acc: 68.90%
2026-02-10 01:26:25,997 - INFO - [Valid] [3/10] | Loss: 0.5660 | Val Acc: 74.93%
2026-02-10 01:26:26,002 - INFO - [Metrics for 'abnormal'] | Precision: 0.7727 | Recall: 0.6497 | F1: 0.7059
2026-02-10 01:26:26,002 - INFO - [Metrics for 'normal'] | Precision: 0.7343 | Recall: 0.8352 | F1: 0.7815
2026-02-10 01:26:26,060 - INFO - [Best Model Saved] (val loss: 0.5660) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:26:26,060 - INFO - --------------------------------------------------
2026-02-10 01:26:26,061 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:26:32,511 - INFO - [Train] [4/10] | Loss: 0.5630 | Train Acc: 75.82%
2026-02-10 01:26:33,870 - INFO - [Valid] [4/10] | Loss: 0.5505 | Val Acc: 74.04%
2026-02-10 01:26:33,875 - INFO - [Metrics for 'abnormal'] | Precision: 0.7117 | Recall: 0.7389 | F1: 0.7250
2026-02-10 01:26:33,875 - INFO - [Metrics for 'normal'] | Precision: 0.7670 | Recall: 0.7418 | F1: 0.7542
2026-02-10 01:26:33,953 - INFO - [Best Model Saved] (val loss: 0.5505) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:26:33,953 - INFO - --------------------------------------------------
2026-02-10 01:26:33,954 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:26:40,461 - INFO - [Train] [5/10] | Loss: 0.5155 | Train Acc: 78.94%
2026-02-10 01:26:41,771 - INFO - [Valid] [5/10] | Loss: 0.5579 | Val Acc: 74.63%
2026-02-10 01:26:41,776 - INFO - [Metrics for 'abnormal'] | Precision: 0.7126 | Recall: 0.7580 | F1: 0.7346
2026-02-10 01:26:41,776 - INFO - [Metrics for 'normal'] | Precision: 0.7791 | Recall: 0.7363 | F1: 0.7571
2026-02-10 01:26:41,778 - INFO - --------------------------------------------------
2026-02-10 01:26:41,779 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:26:47,123 - INFO - [Train] [6/10] | Loss: 0.5177 | Train Acc: 78.50%
2026-02-10 01:26:48,443 - INFO - [Valid] [6/10] | Loss: 0.5393 | Val Acc: 74.63%
2026-02-10 01:26:48,446 - INFO - [Metrics for 'abnormal'] | Precision: 0.6940 | Recall: 0.8089 | F1: 0.7471
2026-02-10 01:26:48,446 - INFO - [Metrics for 'normal'] | Precision: 0.8077 | Recall: 0.6923 | F1: 0.7456
2026-02-10 01:26:48,483 - INFO - [Best Model Saved] (val loss: 0.5393) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:26:48,484 - INFO - --------------------------------------------------
2026-02-10 01:26:48,484 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:26:53,130 - INFO - [Train] [7/10] | Loss: 0.4901 | Train Acc: 81.32%
2026-02-10 01:26:54,629 - INFO - [Valid] [7/10] | Loss: 0.5521 | Val Acc: 76.99%
2026-02-10 01:26:54,633 - INFO - [Metrics for 'abnormal'] | Precision: 0.8264 | Recall: 0.6369 | F1: 0.7194
2026-02-10 01:26:54,634 - INFO - [Metrics for 'normal'] | Precision: 0.7385 | Recall: 0.8846 | F1: 0.8050
2026-02-10 01:26:54,635 - INFO - --------------------------------------------------
2026-02-10 01:26:54,636 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:26:58,443 - INFO - [Train] [8/10] | Loss: 0.4861 | Train Acc: 81.70%
2026-02-10 01:26:59,715 - INFO - [Valid] [8/10] | Loss: 0.5293 | Val Acc: 77.29%
2026-02-10 01:26:59,720 - INFO - [Metrics for 'abnormal'] | Precision: 0.8175 | Recall: 0.6561 | F1: 0.7279
2026-02-10 01:26:59,720 - INFO - [Metrics for 'normal'] | Precision: 0.7465 | Recall: 0.8736 | F1: 0.8051
2026-02-10 01:26:59,785 - INFO - [Best Model Saved] (val loss: 0.5293) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:26:59,785 - INFO - --------------------------------------------------
2026-02-10 01:26:59,786 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:27:04,313 - INFO - [Train] [9/10] | Loss: 0.4830 | Train Acc: 80.88%
2026-02-10 01:27:05,599 - INFO - [Valid] [9/10] | Loss: 0.5372 | Val Acc: 76.40%
2026-02-10 01:27:05,603 - INFO - [Metrics for 'abnormal'] | Precision: 0.7655 | Recall: 0.7070 | F1: 0.7351
2026-02-10 01:27:05,604 - INFO - [Metrics for 'normal'] | Precision: 0.7629 | Recall: 0.8132 | F1: 0.7872
2026-02-10 01:27:05,605 - INFO - --------------------------------------------------
2026-02-10 01:27:05,606 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:27:10,659 - INFO - [Train] [10/10] | Loss: 0.4752 | Train Acc: 82.29%
2026-02-10 01:27:12,055 - INFO - [Valid] [10/10] | Loss: 0.5302 | Val Acc: 76.70%
2026-02-10 01:27:12,060 - INFO - [Metrics for 'abnormal'] | Precision: 0.7378 | Recall: 0.7707 | F1: 0.7539
2026-02-10 01:27:12,065 - INFO - [Metrics for 'normal'] | Precision: 0.7943 | Recall: 0.7637 | F1: 0.7787
2026-02-10 01:27:12,069 - INFO - ================================================================================
2026-02-10 01:27:12,069 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:27:12,069 - INFO - ================================================================================
2026-02-10 01:27:12,113 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:27:12,113 - INFO - ================================================================================
2026-02-10 01:27:12,113 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:27:12,142 - INFO - 원본 모델 FLOPs: 2.1493 GFLOPs
2026-02-10 01:27:12,198 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:12,199 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:12,200 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:12,565 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:27:12,566 - INFO - ==================================================
2026-02-10 01:27:12,590 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.7288 GFLOPs (감소율: 66.09%)
2026-02-10 01:27:12,621 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:12,622 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:12,622 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:13,350 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-02-10 01:27:13,350 - INFO - ==================================================
2026-02-10 01:27:13,379 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.2840 GFLOPs (감소율: 86.79%)
2026-02-10 01:27:13,408 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:13,408 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:13,408 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:13,713 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-02-10 01:27:13,714 - INFO - ==================================================
2026-02-10 01:27:13,737 - INFO -   [탐색  3] 희소도: 0.8662 -> FLOPs: 0.1224 GFLOPs (감소율: 94.30%)
2026-02-10 01:27:13,766 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:13,767 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:13,767 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:14,086 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.804375)에 맞춰 변경되었습니다.
2026-02-10 01:27:14,087 - INFO - ==================================================
2026-02-10 01:27:14,110 - INFO -   [탐색  4] 희소도: 0.8044 -> FLOPs: 0.1980 GFLOPs (감소율: 90.79%)
2026-02-10 01:27:14,139 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:14,140 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:14,140 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:14,593 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8353124999999999)에 맞춰 변경되었습니다.
2026-02-10 01:27:14,594 - INFO - ==================================================
2026-02-10 01:27:14,618 - INFO -   [탐색  5] 희소도: 0.8353 -> FLOPs: 0.1588 GFLOPs (감소율: 92.61%)
2026-02-10 01:27:14,648 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:14,648 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:14,648 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:15,014 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81984375)에 맞춰 변경되었습니다.
2026-02-10 01:27:15,014 - INFO - ==================================================
2026-02-10 01:27:15,039 - INFO -   [탐색  6] 희소도: 0.8198 -> FLOPs: 0.1781 GFLOPs (감소율: 91.72%)
2026-02-10 01:27:15,071 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:15,071 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:15,071 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:15,817 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8121093749999999)에 맞춰 변경되었습니다.
2026-02-10 01:27:15,818 - INFO - ==================================================
2026-02-10 01:27:15,844 - INFO -   [탐색  7] 희소도: 0.8121 -> FLOPs: 0.1906 GFLOPs (감소율: 91.13%)
2026-02-10 01:27:15,876 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:15,877 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:15,877 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:16,133 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8159765625)에 맞춰 변경되었습니다.
2026-02-10 01:27:16,133 - INFO - ==================================================
2026-02-10 01:27:16,156 - INFO -   [탐색  8] 희소도: 0.8160 -> FLOPs: 0.1843 GFLOPs (감소율: 91.43%)
2026-02-10 01:27:16,183 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:16,184 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:16,184 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:16,424 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81791015625)에 맞춰 변경되었습니다.
2026-02-10 01:27:16,425 - INFO - ==================================================
2026-02-10 01:27:16,451 - INFO -   [탐색  9] 희소도: 0.8179 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:16,476 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:16,476 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:16,477 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:16,717 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-02-10 01:27:16,717 - INFO - ==================================================
2026-02-10 01:27:16,738 - INFO -   [탐색 10] 희소도: 0.8169 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:16,764 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:16,764 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:16,764 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:17,047 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8174267578125)에 맞춰 변경되었습니다.
2026-02-10 01:27:17,048 - INFO - ==================================================
2026-02-10 01:27:17,069 - INFO -   [탐색 11] 희소도: 0.8174 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:17,094 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:17,094 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:17,094 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:17,862 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81766845703125)에 맞춰 변경되었습니다.
2026-02-10 01:27:17,863 - INFO - ==================================================
2026-02-10 01:27:17,887 - INFO -   [탐색 12] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:17,913 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:17,913 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:17,914 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:18,217 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.817789306640625)에 맞춰 변경되었습니다.
2026-02-10 01:27:18,217 - INFO - ==================================================
2026-02-10 01:27:18,238 - INFO -   [탐색 13] 희소도: 0.8178 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:18,267 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:18,267 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:18,268 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:18,577 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177288818359375)에 맞춰 변경되었습니다.
2026-02-10 01:27:18,578 - INFO - ==================================================
2026-02-10 01:27:18,599 - INFO -   [탐색 14] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:18,627 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:18,627 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:18,628 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:18,948 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8176986694335937)에 맞춰 변경되었습니다.
2026-02-10 01:27:18,948 - INFO - ==================================================
2026-02-10 01:27:18,974 - INFO -   [탐색 15] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:19,002 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:19,003 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:19,003 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:19,334 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177137756347657)에 맞춰 변경되었습니다.
2026-02-10 01:27:19,335 - INFO - ==================================================
2026-02-10 01:27:19,365 - INFO -   [탐색 16] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:19,392 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:19,393 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:19,393 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:20,181 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177062225341797)에 맞춰 변경되었습니다.
2026-02-10 01:27:20,181 - INFO - ==================================================
2026-02-10 01:27:20,203 - INFO -   [탐색 17] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:20,231 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:20,231 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:20,232 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:20,507 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177099990844727)에 맞춰 변경되었습니다.
2026-02-10 01:27:20,507 - INFO - ==================================================
2026-02-10 01:27:20,532 - INFO -   [탐색 18] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:20,558 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:20,558 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:20,559 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:20,893 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177081108093263)에 맞춰 변경되었습니다.
2026-02-10 01:27:20,894 - INFO - ==================================================
2026-02-10 01:27:20,913 - INFO -   [탐색 19] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:20,935 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:20,936 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:20,936 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:21,201 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177090549468995)에 맞춰 변경되었습니다.
2026-02-10 01:27:21,201 - INFO - ==================================================
2026-02-10 01:27:21,223 - INFO -   [탐색 20] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:21,250 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:21,250 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:21,251 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:21,495 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177085828781129)에 맞춰 변경되었습니다.
2026-02-10 01:27:21,496 - INFO - ==================================================
2026-02-10 01:27:21,519 - INFO -   [탐색 21] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:21,547 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:21,547 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:21,547 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:22,403 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083468437196)에 맞춰 변경되었습니다.
2026-02-10 01:27:22,403 - INFO - ==================================================
2026-02-10 01:27:22,427 - INFO -   [탐색 22] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:22,456 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:22,456 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:22,457 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:22,709 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082288265229)에 맞춰 변경되었습니다.
2026-02-10 01:27:22,709 - INFO - ==================================================
2026-02-10 01:27:22,731 - INFO -   [탐색 23] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:22,756 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:22,756 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:22,757 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:23,017 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082878351213)에 맞춰 변경되었습니다.
2026-02-10 01:27:23,018 - INFO - ==================================================
2026-02-10 01:27:23,039 - INFO -   [탐색 24] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:23,065 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:23,065 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:23,066 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:23,323 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083173394204)에 맞춰 변경되었습니다.
2026-02-10 01:27:23,324 - INFO - ==================================================
2026-02-10 01:27:23,347 - INFO -   [탐색 25] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:23,373 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:23,373 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:23,374 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:23,660 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833209157)에 맞춰 변경되었습니다.
2026-02-10 01:27:23,660 - INFO - ==================================================
2026-02-10 01:27:23,683 - INFO -   [탐색 26] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:23,710 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:23,710 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:23,710 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:24,469 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083394676448)에 맞춰 변경되었습니다.
2026-02-10 01:27:24,470 - INFO - ==================================================
2026-02-10 01:27:24,492 - INFO -   [탐색 27] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:24,518 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:24,518 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:24,519 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:24,848 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083357796073)에 맞춰 변경되었습니다.
2026-02-10 01:27:24,848 - INFO - ==================================================
2026-02-10 01:27:24,869 - INFO -   [탐색 28] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:24,891 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:24,891 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:24,891 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:25,231 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083339355886)에 맞춰 변경되었습니다.
2026-02-10 01:27:25,231 - INFO - ==================================================
2026-02-10 01:27:25,251 - INFO -   [탐색 29] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:25,278 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:25,278 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:25,279 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:25,574 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083330135793)에 맞춰 변경되었습니다.
2026-02-10 01:27:25,574 - INFO - ==================================================
2026-02-10 01:27:25,596 - INFO -   [탐색 30] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:25,621 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:25,621 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:25,622 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:25,901 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083334745839)에 맞춰 변경되었습니다.
2026-02-10 01:27:25,901 - INFO - ==================================================
2026-02-10 01:27:25,925 - INFO -   [탐색 31] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:25,952 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:25,952 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:25,953 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:26,703 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083332440815)에 맞춰 변경되었습니다.
2026-02-10 01:27:26,703 - INFO - ==================================================
2026-02-10 01:27:26,731 - INFO -   [탐색 32] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:26,755 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:26,756 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:26,756 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:27,009 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333593327)에 맞춰 변경되었습니다.
2026-02-10 01:27:27,009 - INFO - ==================================================
2026-02-10 01:27:27,037 - INFO -   [탐색 33] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:27,065 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:27,065 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:27,065 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:27,318 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333017071)에 맞춰 변경되었습니다.
2026-02-10 01:27:27,318 - INFO - ==================================================
2026-02-10 01:27:27,341 - INFO -   [탐색 34] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:27,369 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:27,369 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:27,369 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:27,714 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833333052)에 맞춰 변경되었습니다.
2026-02-10 01:27:27,715 - INFO - ==================================================
2026-02-10 01:27:27,735 - INFO -   [탐색 35] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:27,756 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:27,756 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:27,756 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:28,039 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333449263)에 맞춰 변경되었습니다.
2026-02-10 01:27:28,039 - INFO - ==================================================
2026-02-10 01:27:28,061 - INFO -   [탐색 36] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:28,089 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:28,089 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:28,090 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:29,040 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333377231)에 맞춰 변경되었습니다.
2026-02-10 01:27:29,040 - INFO - ==================================================
2026-02-10 01:27:29,064 - INFO -   [탐색 37] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:29,092 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:29,092 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:29,093 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:29,368 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333341215)에 맞춰 변경되었습니다.
2026-02-10 01:27:29,369 - INFO - ==================================================
2026-02-10 01:27:29,394 - INFO -   [탐색 38] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:29,423 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:29,423 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:29,423 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:29,711 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333323207)에 맞춰 변경되었습니다.
2026-02-10 01:27:29,711 - INFO - ==================================================
2026-02-10 01:27:29,739 - INFO -   [탐색 39] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:29,771 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:29,772 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:29,772 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:30,157 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332211)에 맞춰 변경되었습니다.
2026-02-10 01:27:30,157 - INFO - ==================================================
2026-02-10 01:27:30,182 - INFO -   [탐색 40] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:30,212 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:30,212 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:30,213 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:30,650 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333336713)에 맞춰 변경되었습니다.
2026-02-10 01:27:30,651 - INFO - ==================================================
2026-02-10 01:27:30,675 - INFO -   [탐색 41] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:30,701 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:30,701 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:30,701 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:31,504 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333334463)에 맞춰 변경되었습니다.
2026-02-10 01:27:31,504 - INFO - ==================================================
2026-02-10 01:27:31,526 - INFO -   [탐색 42] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:31,552 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:31,552 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:31,552 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:32,034 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333337)에 맞춰 변경되었습니다.
2026-02-10 01:27:32,034 - INFO - ==================================================
2026-02-10 01:27:32,056 - INFO -   [탐색 43] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:32,085 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:32,085 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:32,085 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:32,331 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332774)에 맞춰 변경되었습니다.
2026-02-10 01:27:32,332 - INFO - ==================================================
2026-02-10 01:27:32,356 - INFO -   [탐색 44] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:32,383 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:32,383 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:32,384 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:32,639 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333055)에 맞춰 변경되었습니다.
2026-02-10 01:27:32,639 - INFO - ==================================================
2026-02-10 01:27:32,663 - INFO -   [탐색 45] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:32,689 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:32,689 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:32,690 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:32,955 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333196)에 맞춰 변경되었습니다.
2026-02-10 01:27:32,956 - INFO - ==================================================
2026-02-10 01:27:32,972 - INFO -   [탐색 46] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:32,990 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:32,991 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:32,991 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:33,762 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333266)에 맞춰 변경되었습니다.
2026-02-10 01:27:33,763 - INFO - ==================================================
2026-02-10 01:27:33,785 - INFO -   [탐색 47] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:33,810 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:33,810 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:33,811 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,094 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333302)에 맞춰 변경되었습니다.
2026-02-10 01:27:34,094 - INFO - ==================================================
2026-02-10 01:27:34,115 - INFO -   [탐색 48] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:34,140 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:34,141 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,141 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,407 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333319)에 맞춰 변경되었습니다.
2026-02-10 01:27:34,408 - INFO - ==================================================
2026-02-10 01:27:34,430 - INFO -   [탐색 49] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:34,455 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:34,456 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,456 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,749 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333328)에 맞춰 변경되었습니다.
2026-02-10 01:27:34,749 - INFO - ==================================================
2026-02-10 01:27:34,770 - INFO -   [탐색 50] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:34,796 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:34,796 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,797 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,271 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:35,272 - INFO - ==================================================
2026-02-10 01:27:35,289 - INFO -   [탐색 51] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:35,308 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:35,308 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,308 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,928 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333335)에 맞춰 변경되었습니다.
2026-02-10 01:27:35,929 - INFO - ==================================================
2026-02-10 01:27:35,949 - INFO -   [탐색 52] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:35,975 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:35,976 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,976 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,213 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333334)에 맞춰 변경되었습니다.
2026-02-10 01:27:36,214 - INFO - ==================================================
2026-02-10 01:27:36,235 - INFO -   [탐색 53] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:27:36,260 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:36,260 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,261 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,505 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:36,505 - INFO - ==================================================
2026-02-10 01:27:36,529 - INFO -   [탐색 54] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:36,555 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:36,556 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,556 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,805 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:36,805 - INFO - ==================================================
2026-02-10 01:27:36,828 - INFO -   [탐색 55] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:36,853 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:36,853 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,854 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:37,092 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:37,093 - INFO - ==================================================
2026-02-10 01:27:37,118 - INFO -   [탐색 56] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:37,142 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:37,142 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:37,142 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:37,920 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:37,920 - INFO - ==================================================
2026-02-10 01:27:37,943 - INFO -   [탐색 57] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:37,964 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:37,964 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:37,964 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,311 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:38,312 - INFO - ==================================================
2026-02-10 01:27:38,334 - INFO -   [탐색 58] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:38,359 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:38,359 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,359 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,597 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:38,598 - INFO - ==================================================
2026-02-10 01:27:38,614 - INFO -   [탐색 59] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:38,630 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:38,630 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,630 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,860 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:38,860 - INFO - ==================================================
2026-02-10 01:27:38,883 - INFO -   [탐색 60] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:38,910 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:38,910 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,910 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:39,188 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:39,189 - INFO - ==================================================
2026-02-10 01:27:39,209 - INFO -   [탐색 61] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:39,231 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:39,231 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:39,232 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:39,878 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:39,878 - INFO - ==================================================
2026-02-10 01:27:39,903 - INFO -   [탐색 62] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:39,930 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:39,930 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:39,931 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,216 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:40,217 - INFO - ==================================================
2026-02-10 01:27:40,238 - INFO -   [탐색 63] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:40,262 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:40,262 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,263 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,532 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:40,532 - INFO - ==================================================
2026-02-10 01:27:40,555 - INFO -   [탐색 64] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:40,579 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:40,579 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,579 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,843 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:40,843 - INFO - ==================================================
2026-02-10 01:27:40,864 - INFO -   [탐색 65] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:40,887 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:40,887 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,888 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,131 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:41,132 - INFO - ==================================================
2026-02-10 01:27:41,156 - INFO -   [탐색 66] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:41,184 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:41,184 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,185 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,819 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:41,819 - INFO - ==================================================
2026-02-10 01:27:41,841 - INFO -   [탐색 67] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:41,867 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:41,867 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,868 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,127 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:42,128 - INFO - ==================================================
2026-02-10 01:27:42,149 - INFO -   [탐색 68] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:42,174 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:42,174 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,174 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,466 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:42,466 - INFO - ==================================================
2026-02-10 01:27:42,489 - INFO -   [탐색 69] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:42,514 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:42,514 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,515 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,768 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:42,768 - INFO - ==================================================
2026-02-10 01:27:42,789 - INFO -   [탐색 70] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:42,813 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:42,814 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,814 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,098 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:43,098 - INFO - ==================================================
2026-02-10 01:27:43,118 - INFO -   [탐색 71] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:43,142 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:43,142 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,143 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,835 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:43,835 - INFO - ==================================================
2026-02-10 01:27:43,859 - INFO -   [탐색 72] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:43,882 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:43,882 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,883 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,140 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:44,141 - INFO - ==================================================
2026-02-10 01:27:44,162 - INFO -   [탐색 73] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:44,188 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:44,188 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,189 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,453 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:44,453 - INFO - ==================================================
2026-02-10 01:27:44,475 - INFO -   [탐색 74] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:44,503 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:44,503 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,504 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,729 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:44,729 - INFO - ==================================================
2026-02-10 01:27:44,753 - INFO -   [탐색 75] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:44,777 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:44,778 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,778 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:44,998 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:44,998 - INFO - ==================================================
2026-02-10 01:27:45,019 - INFO -   [탐색 76] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:45,045 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:45,045 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:45,046 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:45,819 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:45,819 - INFO - ==================================================
2026-02-10 01:27:45,843 - INFO -   [탐색 77] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:45,869 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:45,870 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:45,870 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,148 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:46,148 - INFO - ==================================================
2026-02-10 01:27:46,171 - INFO -   [탐색 78] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:46,196 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:46,196 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,196 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,458 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:46,458 - INFO - ==================================================
2026-02-10 01:27:46,479 - INFO -   [탐색 79] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:46,504 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:46,504 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,505 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,752 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:46,752 - INFO - ==================================================
2026-02-10 01:27:46,774 - INFO -   [탐색 80] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:46,802 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:46,802 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:46,802 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:47,084 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:47,085 - INFO - ==================================================
2026-02-10 01:27:47,106 - INFO -   [탐색 81] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:47,133 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:47,133 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:47,134 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:47,916 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:47,916 - INFO - ==================================================
2026-02-10 01:27:47,941 - INFO -   [탐색 82] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:47,969 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:47,969 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:47,970 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:48,278 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:48,279 - INFO - ==================================================
2026-02-10 01:27:48,297 - INFO -   [탐색 83] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:48,323 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:48,323 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:48,323 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:48,573 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:48,573 - INFO - ==================================================
2026-02-10 01:27:48,595 - INFO -   [탐색 84] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:48,621 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:48,621 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:48,622 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:48,904 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:48,904 - INFO - ==================================================
2026-02-10 01:27:48,925 - INFO -   [탐색 85] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:48,950 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:48,950 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:48,951 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:49,176 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:49,176 - INFO - ==================================================
2026-02-10 01:27:49,198 - INFO -   [탐색 86] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:49,221 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:49,221 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:49,221 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:49,989 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:49,989 - INFO - ==================================================
2026-02-10 01:27:50,011 - INFO -   [탐색 87] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:50,034 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:50,035 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,035 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,293 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:50,293 - INFO - ==================================================
2026-02-10 01:27:50,315 - INFO -   [탐색 88] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:50,340 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:50,340 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,340 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,591 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:50,592 - INFO - ==================================================
2026-02-10 01:27:50,614 - INFO -   [탐색 89] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:50,640 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:50,640 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,640 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,885 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:50,885 - INFO - ==================================================
2026-02-10 01:27:50,911 - INFO -   [탐색 90] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:50,939 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:50,940 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:50,940 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:51,223 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:51,223 - INFO - ==================================================
2026-02-10 01:27:51,246 - INFO -   [탐색 91] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:51,272 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:51,272 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:51,273 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:52,102 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:52,102 - INFO - ==================================================
2026-02-10 01:27:52,127 - INFO -   [탐색 92] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:52,154 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:52,154 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:52,155 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:52,435 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:52,435 - INFO - ==================================================
2026-02-10 01:27:52,461 - INFO -   [탐색 93] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:52,489 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:52,489 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:52,490 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:52,861 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:52,862 - INFO - ==================================================
2026-02-10 01:27:52,885 - INFO -   [탐색 94] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:52,910 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:52,910 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:52,910 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:53,172 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:53,172 - INFO - ==================================================
2026-02-10 01:27:53,196 - INFO -   [탐색 95] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:53,223 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:53,223 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:53,223 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:53,464 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:53,464 - INFO - ==================================================
2026-02-10 01:27:53,485 - INFO -   [탐색 96] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:53,515 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:53,515 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:53,516 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:54,302 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:54,303 - INFO - ==================================================
2026-02-10 01:27:54,323 - INFO -   [탐색 97] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:54,347 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:54,348 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:54,348 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:54,651 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:54,651 - INFO - ==================================================
2026-02-10 01:27:54,674 - INFO -   [탐색 98] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:54,701 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:54,702 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:54,702 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,009 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:55,009 - INFO - ==================================================
2026-02-10 01:27:55,032 - INFO -   [탐색 99] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:55,059 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:55,060 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,060 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,311 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:27:55,311 - INFO - ==================================================
2026-02-10 01:27:55,336 - INFO -   [탐색 100] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:27:55,336 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.8169 입니다.
2026-02-10 01:27:55,336 - INFO - ================================================================================
2026-02-10 01:27:55,339 - INFO - 계산된 Pruning 정보(희소도: 0.8169)를 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:27:55,360 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:27:55,405 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:55,405 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,406 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:27:55,719 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-02-10 01:27:55,719 - INFO - ==================================================
2026-02-10 01:27:55,721 - INFO - ==================================================
2026-02-10 01:27:55,721 - INFO - 모델 파라미터 수:
2026-02-10 01:27:55,721 - INFO -   - 총 파라미터: 485,259 개
2026-02-10 01:27:55,721 - INFO -   - 학습 가능한 파라미터: 485,259 개
2026-02-10 01:27:55,740 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:27:55,783 - INFO - FLOPs가 2.1493 GFLOPs에서 0.1840 GFLOPs로 감소했습니다 (감소율: 91.44%).
2026-02-10 01:27:55,784 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:27:55,784 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:27:55,784 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:27:55,785 - INFO - ==================================================
2026-02-10 01:27:55,785 - INFO - train 모드를 시작합니다.
2026-02-10 01:27:55,785 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:27:55,785 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:27:55,785 - INFO - --------------------------------------------------
2026-02-10 01:27:55,785 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:28:01,644 - INFO - [Train] [11/90] | Loss: 0.5461 | Train Acc: 76.93%
2026-02-10 01:28:02,859 - INFO - [Valid] [11/90] | Loss: 0.5574 | Val Acc: 75.22%
2026-02-10 01:28:02,864 - INFO - [Metrics for 'abnormal'] | Precision: 0.7874 | Recall: 0.6369 | F1: 0.7042
2026-02-10 01:28:02,864 - INFO - [Metrics for 'normal'] | Precision: 0.7311 | Recall: 0.8516 | F1: 0.7868
2026-02-10 01:28:02,887 - INFO - [Best Model Saved] (val loss: 0.5574) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:28:02,887 - INFO - --------------------------------------------------
2026-02-10 01:28:02,889 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:28:08,203 - INFO - [Train] [12/90] | Loss: 0.5002 | Train Acc: 81.03%
2026-02-10 01:28:09,352 - INFO - [Valid] [12/90] | Loss: 0.5770 | Val Acc: 74.34%
2026-02-10 01:28:09,357 - INFO - [Metrics for 'abnormal'] | Precision: 0.7692 | Recall: 0.6369 | F1: 0.6969
2026-02-10 01:28:09,364 - INFO - [Metrics for 'normal'] | Precision: 0.7273 | Recall: 0.8352 | F1: 0.7775
2026-02-10 01:28:09,366 - INFO - --------------------------------------------------
2026-02-10 01:28:09,368 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:28:14,562 - INFO - [Train] [13/90] | Loss: 0.5031 | Train Acc: 79.99%
2026-02-10 01:28:15,501 - INFO - [Valid] [13/90] | Loss: 0.5449 | Val Acc: 76.40%
2026-02-10 01:28:15,506 - INFO - [Metrics for 'abnormal'] | Precision: 0.7852 | Recall: 0.6752 | F1: 0.7260
2026-02-10 01:28:15,507 - INFO - [Metrics for 'normal'] | Precision: 0.7500 | Recall: 0.8407 | F1: 0.7927
2026-02-10 01:28:15,532 - INFO - [Best Model Saved] (val loss: 0.5449) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:28:15,532 - INFO - --------------------------------------------------
2026-02-10 01:28:15,533 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:28:20,901 - INFO - [Train] [14/90] | Loss: 0.4895 | Train Acc: 81.18%
2026-02-10 01:28:21,877 - INFO - [Valid] [14/90] | Loss: 0.5538 | Val Acc: 73.75%
2026-02-10 01:28:21,882 - INFO - [Metrics for 'abnormal'] | Precision: 0.6771 | Recall: 0.8280 | F1: 0.7450
2026-02-10 01:28:21,882 - INFO - [Metrics for 'normal'] | Precision: 0.8163 | Recall: 0.6593 | F1: 0.7295
2026-02-10 01:28:21,884 - INFO - --------------------------------------------------
2026-02-10 01:28:21,885 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:28:27,727 - INFO - [Train] [15/90] | Loss: 0.4841 | Train Acc: 81.77%
2026-02-10 01:28:28,880 - INFO - [Valid] [15/90] | Loss: 0.5525 | Val Acc: 75.22%
2026-02-10 01:28:28,884 - INFO - [Metrics for 'abnormal'] | Precision: 0.7417 | Recall: 0.7134 | F1: 0.7273
2026-02-10 01:28:28,884 - INFO - [Metrics for 'normal'] | Precision: 0.7606 | Recall: 0.7857 | F1: 0.7730
2026-02-10 01:28:28,886 - INFO - --------------------------------------------------
2026-02-10 01:28:28,887 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:28:34,454 - INFO - [Train] [16/90] | Loss: 0.4858 | Train Acc: 80.95%
2026-02-10 01:28:35,651 - INFO - [Valid] [16/90] | Loss: 0.5479 | Val Acc: 76.11%
2026-02-10 01:28:35,656 - INFO - [Metrics for 'abnormal'] | Precision: 0.7111 | Recall: 0.8153 | F1: 0.7596
2026-02-10 01:28:35,656 - INFO - [Metrics for 'normal'] | Precision: 0.8176 | Recall: 0.7143 | F1: 0.7625
2026-02-10 01:28:35,658 - INFO - --------------------------------------------------
2026-02-10 01:28:35,659 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:28:42,133 - INFO - [Train] [17/90] | Loss: 0.4763 | Train Acc: 81.99%
2026-02-10 01:28:43,516 - INFO - [Valid] [17/90] | Loss: 0.5780 | Val Acc: 74.93%
2026-02-10 01:28:43,520 - INFO - [Metrics for 'abnormal'] | Precision: 0.6856 | Recall: 0.8471 | F1: 0.7578
2026-02-10 01:28:43,521 - INFO - [Metrics for 'normal'] | Precision: 0.8345 | Recall: 0.6648 | F1: 0.7401
2026-02-10 01:28:43,522 - INFO - --------------------------------------------------
2026-02-10 01:28:43,523 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:28:50,239 - INFO - [Train] [18/90] | Loss: 0.4762 | Train Acc: 81.99%
2026-02-10 01:28:51,696 - INFO - [Valid] [18/90] | Loss: 0.5367 | Val Acc: 77.29%
2026-02-10 01:28:51,701 - INFO - [Metrics for 'abnormal'] | Precision: 0.7667 | Recall: 0.7325 | F1: 0.7492
2026-02-10 01:28:51,701 - INFO - [Metrics for 'normal'] | Precision: 0.7778 | Recall: 0.8077 | F1: 0.7925
2026-02-10 01:28:51,721 - INFO - [Best Model Saved] (val loss: 0.5367) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:28:51,722 - INFO - --------------------------------------------------
2026-02-10 01:28:51,724 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:28:58,192 - INFO - [Train] [19/90] | Loss: 0.4658 | Train Acc: 82.96%
2026-02-10 01:28:59,718 - INFO - [Valid] [19/90] | Loss: 0.5398 | Val Acc: 74.93%
2026-02-10 01:28:59,723 - INFO - [Metrics for 'abnormal'] | Precision: 0.7169 | Recall: 0.7580 | F1: 0.7368
2026-02-10 01:28:59,724 - INFO - [Metrics for 'normal'] | Precision: 0.7803 | Recall: 0.7418 | F1: 0.7606
2026-02-10 01:28:59,725 - INFO - --------------------------------------------------
2026-02-10 01:28:59,726 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:29:06,005 - INFO - [Train] [20/90] | Loss: 0.4691 | Train Acc: 83.04%
2026-02-10 01:29:07,597 - INFO - [Valid] [20/90] | Loss: 0.5355 | Val Acc: 76.11%
2026-02-10 01:29:07,606 - INFO - [Metrics for 'abnormal'] | Precision: 0.7111 | Recall: 0.8153 | F1: 0.7596
2026-02-10 01:29:07,606 - INFO - [Metrics for 'normal'] | Precision: 0.8176 | Recall: 0.7143 | F1: 0.7625
2026-02-10 01:29:07,622 - INFO - [Best Model Saved] (val loss: 0.5355) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:29:07,622 - INFO - --------------------------------------------------
2026-02-10 01:29:07,624 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:29:13,311 - INFO - [Train] [21/90] | Loss: 0.4652 | Train Acc: 82.44%
2026-02-10 01:29:14,783 - INFO - [Valid] [21/90] | Loss: 0.5357 | Val Acc: 76.40%
2026-02-10 01:29:14,789 - INFO - [Metrics for 'abnormal'] | Precision: 0.7278 | Recall: 0.7834 | F1: 0.7546
2026-02-10 01:29:14,789 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.7473 | F1: 0.7727
2026-02-10 01:29:14,791 - INFO - --------------------------------------------------
2026-02-10 01:29:14,796 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:29:20,510 - INFO - [Train] [22/90] | Loss: 0.4621 | Train Acc: 83.48%
2026-02-10 01:29:22,092 - INFO - [Valid] [22/90] | Loss: 0.5166 | Val Acc: 77.58%
2026-02-10 01:29:22,097 - INFO - [Metrics for 'abnormal'] | Precision: 0.7718 | Recall: 0.7325 | F1: 0.7516
2026-02-10 01:29:22,097 - INFO - [Metrics for 'normal'] | Precision: 0.7789 | Recall: 0.8132 | F1: 0.7957
2026-02-10 01:29:22,116 - INFO - [Best Model Saved] (val loss: 0.5166) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:29:22,116 - INFO - --------------------------------------------------
2026-02-10 01:29:22,118 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:29:28,326 - INFO - [Train] [23/90] | Loss: 0.4662 | Train Acc: 81.99%
2026-02-10 01:29:29,801 - INFO - [Valid] [23/90] | Loss: 0.5196 | Val Acc: 77.58%
2026-02-10 01:29:29,807 - INFO - [Metrics for 'abnormal'] | Precision: 0.7580 | Recall: 0.7580 | F1: 0.7580
2026-02-10 01:29:29,807 - INFO - [Metrics for 'normal'] | Precision: 0.7912 | Recall: 0.7912 | F1: 0.7912
2026-02-10 01:29:29,808 - INFO - --------------------------------------------------
2026-02-10 01:29:29,810 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:29:36,245 - INFO - [Train] [24/90] | Loss: 0.4519 | Train Acc: 82.74%
2026-02-10 01:29:37,700 - INFO - [Valid] [24/90] | Loss: 0.5086 | Val Acc: 76.99%
2026-02-10 01:29:37,705 - INFO - [Metrics for 'abnormal'] | Precision: 0.7926 | Recall: 0.6815 | F1: 0.7329
2026-02-10 01:29:37,705 - INFO - [Metrics for 'normal'] | Precision: 0.7549 | Recall: 0.8462 | F1: 0.7979
2026-02-10 01:29:37,721 - INFO - [Best Model Saved] (val loss: 0.5086) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:29:37,721 - INFO - --------------------------------------------------
2026-02-10 01:29:37,722 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:29:44,129 - INFO - [Train] [25/90] | Loss: 0.4582 | Train Acc: 82.51%
2026-02-10 01:29:45,276 - INFO - [Valid] [25/90] | Loss: 0.5063 | Val Acc: 78.47%
2026-02-10 01:29:45,284 - INFO - [Metrics for 'abnormal'] | Precision: 0.7763 | Recall: 0.7516 | F1: 0.7638
2026-02-10 01:29:45,284 - INFO - [Metrics for 'normal'] | Precision: 0.7914 | Recall: 0.8132 | F1: 0.8022
2026-02-10 01:29:45,303 - INFO - [Best Model Saved] (val loss: 0.5063) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:29:45,303 - INFO - --------------------------------------------------
2026-02-10 01:29:45,304 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:29:51,510 - INFO - [Train] [26/90] | Loss: 0.4484 | Train Acc: 84.00%
2026-02-10 01:29:52,850 - INFO - [Valid] [26/90] | Loss: 0.5396 | Val Acc: 74.93%
2026-02-10 01:29:52,855 - INFO - [Metrics for 'abnormal'] | Precision: 0.7118 | Recall: 0.7707 | F1: 0.7401
2026-02-10 01:29:52,855 - INFO - [Metrics for 'normal'] | Precision: 0.7870 | Recall: 0.7308 | F1: 0.7578
2026-02-10 01:29:52,857 - INFO - --------------------------------------------------
2026-02-10 01:29:52,858 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 01:29:59,504 - INFO - [Train] [27/90] | Loss: 0.4588 | Train Acc: 82.07%
2026-02-10 01:30:00,886 - INFO - [Valid] [27/90] | Loss: 0.5269 | Val Acc: 77.29%
2026-02-10 01:30:00,891 - INFO - [Metrics for 'abnormal'] | Precision: 0.7151 | Recall: 0.8471 | F1: 0.7755
2026-02-10 01:30:00,891 - INFO - [Metrics for 'normal'] | Precision: 0.8431 | Recall: 0.7088 | F1: 0.7701
2026-02-10 01:30:00,893 - INFO - --------------------------------------------------
2026-02-10 01:30:00,894 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 01:30:07,226 - INFO - [Train] [28/90] | Loss: 0.4366 | Train Acc: 84.60%
2026-02-10 01:30:08,688 - INFO - [Valid] [28/90] | Loss: 0.5266 | Val Acc: 78.76%
2026-02-10 01:30:08,693 - INFO - [Metrics for 'abnormal'] | Precision: 0.7297 | Recall: 0.8599 | F1: 0.7895
2026-02-10 01:30:08,693 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.7253 | F1: 0.7857
2026-02-10 01:30:08,695 - INFO - --------------------------------------------------
2026-02-10 01:30:08,697 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 01:30:14,603 - INFO - [Train] [29/90] | Loss: 0.4556 | Train Acc: 82.81%
2026-02-10 01:30:16,370 - INFO - [Valid] [29/90] | Loss: 0.5035 | Val Acc: 76.70%
2026-02-10 01:30:16,380 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.6879 | F1: 0.7322
2026-02-10 01:30:16,380 - INFO - [Metrics for 'normal'] | Precision: 0.7562 | Recall: 0.8352 | F1: 0.7937
2026-02-10 01:30:16,400 - INFO - [Best Model Saved] (val loss: 0.5035) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:30:16,401 - INFO - --------------------------------------------------
2026-02-10 01:30:16,402 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 01:30:22,385 - INFO - [Train] [30/90] | Loss: 0.4491 | Train Acc: 83.56%
2026-02-10 01:30:23,984 - INFO - [Valid] [30/90] | Loss: 0.4888 | Val Acc: 79.35%
2026-02-10 01:30:23,989 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.7771 | F1: 0.7771
2026-02-10 01:30:23,989 - INFO - [Metrics for 'normal'] | Precision: 0.8077 | Recall: 0.8077 | F1: 0.8077
2026-02-10 01:30:24,020 - INFO - [Best Model Saved] (val loss: 0.4888) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:30:24,021 - INFO - --------------------------------------------------
2026-02-10 01:30:24,022 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 01:30:30,022 - INFO - [Train] [31/90] | Loss: 0.4260 | Train Acc: 84.90%
2026-02-10 01:30:31,818 - INFO - [Valid] [31/90] | Loss: 0.4961 | Val Acc: 76.99%
2026-02-10 01:30:31,823 - INFO - [Metrics for 'abnormal'] | Precision: 0.7801 | Recall: 0.7006 | F1: 0.7383
2026-02-10 01:30:31,823 - INFO - [Metrics for 'normal'] | Precision: 0.7626 | Recall: 0.8297 | F1: 0.7947
2026-02-10 01:30:31,824 - INFO - --------------------------------------------------
2026-02-10 01:30:31,825 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 01:30:38,042 - INFO - [Train] [32/90] | Loss: 0.4407 | Train Acc: 84.15%
2026-02-10 01:30:39,633 - INFO - [Valid] [32/90] | Loss: 0.4964 | Val Acc: 79.35%
2026-02-10 01:30:39,638 - INFO - [Metrics for 'abnormal'] | Precision: 0.7959 | Recall: 0.7452 | F1: 0.7697
2026-02-10 01:30:39,638 - INFO - [Metrics for 'normal'] | Precision: 0.7917 | Recall: 0.8352 | F1: 0.8128
2026-02-10 01:30:39,640 - INFO - --------------------------------------------------
2026-02-10 01:30:39,641 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 01:30:45,758 - INFO - [Train] [33/90] | Loss: 0.4429 | Train Acc: 84.08%
2026-02-10 01:30:47,311 - INFO - [Valid] [33/90] | Loss: 0.4943 | Val Acc: 80.53%
2026-02-10 01:30:47,318 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-02-10 01:30:47,318 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-02-10 01:30:47,320 - INFO - --------------------------------------------------
2026-02-10 01:30:47,321 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 01:30:53,234 - INFO - [Train] [34/90] | Loss: 0.4267 | Train Acc: 84.45%
2026-02-10 01:30:54,837 - INFO - [Valid] [34/90] | Loss: 0.4933 | Val Acc: 79.65%
2026-02-10 01:30:54,847 - INFO - [Metrics for 'abnormal'] | Precision: 0.7529 | Recall: 0.8344 | F1: 0.7915
2026-02-10 01:30:54,847 - INFO - [Metrics for 'normal'] | Precision: 0.8424 | Recall: 0.7637 | F1: 0.8012
2026-02-10 01:30:54,849 - INFO - --------------------------------------------------
2026-02-10 01:30:54,850 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 01:31:01,003 - INFO - [Train] [35/90] | Loss: 0.4325 | Train Acc: 84.97%
2026-02-10 01:31:02,498 - INFO - [Valid] [35/90] | Loss: 0.4906 | Val Acc: 80.53%
2026-02-10 01:31:02,504 - INFO - [Metrics for 'abnormal'] | Precision: 0.7725 | Recall: 0.8217 | F1: 0.7963
2026-02-10 01:31:02,504 - INFO - [Metrics for 'normal'] | Precision: 0.8372 | Recall: 0.7912 | F1: 0.8136
2026-02-10 01:31:02,506 - INFO - --------------------------------------------------
2026-02-10 01:31:02,509 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 01:31:09,097 - INFO - [Train] [36/90] | Loss: 0.4232 | Train Acc: 85.27%
2026-02-10 01:31:10,540 - INFO - [Valid] [36/90] | Loss: 0.4968 | Val Acc: 78.47%
2026-02-10 01:31:10,545 - INFO - [Metrics for 'abnormal'] | Precision: 0.7658 | Recall: 0.7707 | F1: 0.7683
2026-02-10 01:31:10,546 - INFO - [Metrics for 'normal'] | Precision: 0.8011 | Recall: 0.7967 | F1: 0.7989
2026-02-10 01:31:10,547 - INFO - --------------------------------------------------
2026-02-10 01:31:10,548 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 01:31:17,578 - INFO - [Train] [37/90] | Loss: 0.4210 | Train Acc: 86.09%
2026-02-10 01:31:19,120 - INFO - [Valid] [37/90] | Loss: 0.4853 | Val Acc: 78.47%
2026-02-10 01:31:19,125 - INFO - [Metrics for 'abnormal'] | Precision: 0.7360 | Recall: 0.8344 | F1: 0.7821
2026-02-10 01:31:19,126 - INFO - [Metrics for 'normal'] | Precision: 0.8385 | Recall: 0.7418 | F1: 0.7872
2026-02-10 01:31:19,143 - INFO - [Best Model Saved] (val loss: 0.4853) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:31:19,143 - INFO - --------------------------------------------------
2026-02-10 01:31:19,145 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 01:31:25,691 - INFO - [Train] [38/90] | Loss: 0.4209 | Train Acc: 85.34%
2026-02-10 01:31:26,771 - INFO - [Valid] [38/90] | Loss: 0.4830 | Val Acc: 80.24%
2026-02-10 01:31:26,776 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8025 | F1: 0.7900
2026-02-10 01:31:26,776 - INFO - [Metrics for 'normal'] | Precision: 0.8249 | Recall: 0.8022 | F1: 0.8134
2026-02-10 01:31:26,792 - INFO - [Best Model Saved] (val loss: 0.4830) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:31:26,793 - INFO - --------------------------------------------------
2026-02-10 01:31:26,794 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 01:31:32,573 - INFO - [Train] [39/90] | Loss: 0.4175 | Train Acc: 85.79%
2026-02-10 01:31:33,848 - INFO - [Valid] [39/90] | Loss: 0.4920 | Val Acc: 81.71%
2026-02-10 01:31:33,853 - INFO - [Metrics for 'abnormal'] | Precision: 0.7684 | Recall: 0.8662 | F1: 0.8144
2026-02-10 01:31:33,854 - INFO - [Metrics for 'normal'] | Precision: 0.8704 | Recall: 0.7747 | F1: 0.8198
2026-02-10 01:31:33,856 - INFO - --------------------------------------------------
2026-02-10 01:31:33,857 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 01:31:39,111 - INFO - [Train] [40/90] | Loss: 0.4039 | Train Acc: 86.38%
2026-02-10 01:31:40,671 - INFO - [Valid] [40/90] | Loss: 0.4669 | Val Acc: 80.53%
2026-02-10 01:31:40,675 - INFO - [Metrics for 'abnormal'] | Precision: 0.7661 | Recall: 0.8344 | F1: 0.7988
2026-02-10 01:31:40,675 - INFO - [Metrics for 'normal'] | Precision: 0.8452 | Recall: 0.7802 | F1: 0.8114
2026-02-10 01:31:40,689 - INFO - [Best Model Saved] (val loss: 0.4669) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:31:40,689 - INFO - --------------------------------------------------
2026-02-10 01:31:40,690 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 01:31:46,309 - INFO - [Train] [41/90] | Loss: 0.3936 | Train Acc: 87.35%
2026-02-10 01:31:47,794 - INFO - [Valid] [41/90] | Loss: 0.4837 | Val Acc: 80.24%
2026-02-10 01:31:47,803 - INFO - [Metrics for 'abnormal'] | Precision: 0.7744 | Recall: 0.8089 | F1: 0.7913
2026-02-10 01:31:47,803 - INFO - [Metrics for 'normal'] | Precision: 0.8286 | Recall: 0.7967 | F1: 0.8123
2026-02-10 01:31:47,804 - INFO - --------------------------------------------------
2026-02-10 01:31:47,805 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 01:31:53,522 - INFO - [Train] [42/90] | Loss: 0.4177 | Train Acc: 86.68%
2026-02-10 01:31:55,010 - INFO - [Valid] [42/90] | Loss: 0.4867 | Val Acc: 78.76%
2026-02-10 01:31:55,017 - INFO - [Metrics for 'abnormal'] | Precision: 0.7972 | Recall: 0.7261 | F1: 0.7600
2026-02-10 01:31:55,017 - INFO - [Metrics for 'normal'] | Precision: 0.7806 | Recall: 0.8407 | F1: 0.8095
2026-02-10 01:31:55,018 - INFO - --------------------------------------------------
2026-02-10 01:31:55,019 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 01:32:00,825 - INFO - [Train] [43/90] | Loss: 0.4058 | Train Acc: 86.61%
2026-02-10 01:32:02,468 - INFO - [Valid] [43/90] | Loss: 0.4816 | Val Acc: 79.94%
2026-02-10 01:32:02,474 - INFO - [Metrics for 'abnormal'] | Precision: 0.7486 | Recall: 0.8535 | F1: 0.7976
2026-02-10 01:32:02,474 - INFO - [Metrics for 'normal'] | Precision: 0.8562 | Recall: 0.7527 | F1: 0.8012
2026-02-10 01:32:02,475 - INFO - --------------------------------------------------
2026-02-10 01:32:02,477 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 01:32:08,038 - INFO - [Train] [44/90] | Loss: 0.3996 | Train Acc: 87.28%
2026-02-10 01:32:09,288 - INFO - [Valid] [44/90] | Loss: 0.5197 | Val Acc: 78.17%
2026-02-10 01:32:09,298 - INFO - [Metrics for 'abnormal'] | Precision: 0.7024 | Recall: 0.9172 | F1: 0.7956
2026-02-10 01:32:09,299 - INFO - [Metrics for 'normal'] | Precision: 0.9030 | Recall: 0.6648 | F1: 0.7658
2026-02-10 01:32:09,300 - INFO - --------------------------------------------------
2026-02-10 01:32:09,301 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 01:32:14,367 - INFO - [Train] [45/90] | Loss: 0.3939 | Train Acc: 86.98%
2026-02-10 01:32:15,600 - INFO - [Valid] [45/90] | Loss: 0.5134 | Val Acc: 79.94%
2026-02-10 01:32:15,607 - INFO - [Metrics for 'abnormal'] | Precision: 0.7282 | Recall: 0.9045 | F1: 0.8068
2026-02-10 01:32:15,607 - INFO - [Metrics for 'normal'] | Precision: 0.8958 | Recall: 0.7088 | F1: 0.7914
2026-02-10 01:32:15,609 - INFO - --------------------------------------------------
2026-02-10 01:32:15,610 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 01:32:22,300 - INFO - [Train] [46/90] | Loss: 0.3861 | Train Acc: 88.17%
2026-02-10 01:32:23,668 - INFO - [Valid] [46/90] | Loss: 0.4777 | Val Acc: 80.53%
2026-02-10 01:32:23,673 - INFO - [Metrics for 'abnormal'] | Precision: 0.7542 | Recall: 0.8599 | F1: 0.8036
2026-02-10 01:32:23,674 - INFO - [Metrics for 'normal'] | Precision: 0.8625 | Recall: 0.7582 | F1: 0.8070
2026-02-10 01:32:23,675 - INFO - --------------------------------------------------
2026-02-10 01:32:23,676 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 01:32:29,887 - INFO - [Train] [47/90] | Loss: 0.3911 | Train Acc: 88.10%
2026-02-10 01:32:31,304 - INFO - [Valid] [47/90] | Loss: 0.5228 | Val Acc: 78.47%
2026-02-10 01:32:31,311 - INFO - [Metrics for 'abnormal'] | Precision: 0.7658 | Recall: 0.7707 | F1: 0.7683
2026-02-10 01:32:31,311 - INFO - [Metrics for 'normal'] | Precision: 0.8011 | Recall: 0.7967 | F1: 0.7989
2026-02-10 01:32:31,313 - INFO - --------------------------------------------------
2026-02-10 01:32:31,315 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 01:32:38,033 - INFO - [Train] [48/90] | Loss: 0.4048 | Train Acc: 86.98%
2026-02-10 01:32:39,160 - INFO - [Valid] [48/90] | Loss: 0.5031 | Val Acc: 79.35%
2026-02-10 01:32:39,166 - INFO - [Metrics for 'abnormal'] | Precision: 0.7514 | Recall: 0.8280 | F1: 0.7879
2026-02-10 01:32:39,166 - INFO - [Metrics for 'normal'] | Precision: 0.8373 | Recall: 0.7637 | F1: 0.7989
2026-02-10 01:32:39,168 - INFO - --------------------------------------------------
2026-02-10 01:32:39,171 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 01:32:45,780 - INFO - [Train] [49/90] | Loss: 0.3886 | Train Acc: 87.72%
2026-02-10 01:32:47,316 - INFO - [Valid] [49/90] | Loss: 0.4995 | Val Acc: 79.65%
2026-02-10 01:32:47,321 - INFO - [Metrics for 'abnormal'] | Precision: 0.7973 | Recall: 0.7516 | F1: 0.7738
2026-02-10 01:32:47,321 - INFO - [Metrics for 'normal'] | Precision: 0.7958 | Recall: 0.8352 | F1: 0.8150
2026-02-10 01:32:47,323 - INFO - --------------------------------------------------
2026-02-10 01:32:47,324 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 01:32:53,829 - INFO - [Train] [50/90] | Loss: 0.3753 | Train Acc: 88.84%
2026-02-10 01:32:55,317 - INFO - [Valid] [50/90] | Loss: 0.4769 | Val Acc: 80.24%
2026-02-10 01:32:55,327 - INFO - [Metrics for 'abnormal'] | Precision: 0.7586 | Recall: 0.8408 | F1: 0.7976
2026-02-10 01:32:55,327 - INFO - [Metrics for 'normal'] | Precision: 0.8485 | Recall: 0.7692 | F1: 0.8069
2026-02-10 01:32:55,329 - INFO - --------------------------------------------------
2026-02-10 01:32:55,330 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 01:33:01,680 - INFO - [Train] [51/90] | Loss: 0.3717 | Train Acc: 88.84%
2026-02-10 01:33:03,362 - INFO - [Valid] [51/90] | Loss: 0.5051 | Val Acc: 79.94%
2026-02-10 01:33:03,372 - INFO - [Metrics for 'abnormal'] | Precision: 0.7486 | Recall: 0.8535 | F1: 0.7976
2026-02-10 01:33:03,372 - INFO - [Metrics for 'normal'] | Precision: 0.8562 | Recall: 0.7527 | F1: 0.8012
2026-02-10 01:33:03,373 - INFO - --------------------------------------------------
2026-02-10 01:33:03,374 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 01:33:09,341 - INFO - [Train] [52/90] | Loss: 0.3790 | Train Acc: 88.91%
2026-02-10 01:33:11,088 - INFO - [Valid] [52/90] | Loss: 0.4702 | Val Acc: 82.01%
2026-02-10 01:33:11,094 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-02-10 01:33:11,095 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-02-10 01:33:11,100 - INFO - --------------------------------------------------
2026-02-10 01:33:11,101 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 01:33:17,160 - INFO - [Train] [53/90] | Loss: 0.3723 | Train Acc: 88.62%
2026-02-10 01:33:18,777 - INFO - [Valid] [53/90] | Loss: 0.4721 | Val Acc: 81.12%
2026-02-10 01:33:18,783 - INFO - [Metrics for 'abnormal'] | Precision: 0.7719 | Recall: 0.8408 | F1: 0.8049
2026-02-10 01:33:18,783 - INFO - [Metrics for 'normal'] | Precision: 0.8512 | Recall: 0.7857 | F1: 0.8171
2026-02-10 01:33:18,784 - INFO - --------------------------------------------------
2026-02-10 01:33:18,786 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 01:33:25,581 - INFO - [Train] [54/90] | Loss: 0.3764 | Train Acc: 88.24%
2026-02-10 01:33:27,123 - INFO - [Valid] [54/90] | Loss: 0.4942 | Val Acc: 80.83%
2026-02-10 01:33:27,133 - INFO - [Metrics for 'abnormal'] | Precision: 0.7473 | Recall: 0.8854 | F1: 0.8105
2026-02-10 01:33:27,133 - INFO - [Metrics for 'normal'] | Precision: 0.8824 | Recall: 0.7418 | F1: 0.8060
2026-02-10 01:33:27,135 - INFO - --------------------------------------------------
2026-02-10 01:33:27,136 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 01:33:33,274 - INFO - [Train] [55/90] | Loss: 0.3666 | Train Acc: 88.91%
2026-02-10 01:33:34,451 - INFO - [Valid] [55/90] | Loss: 0.4715 | Val Acc: 81.42%
2026-02-10 01:33:34,457 - INFO - [Metrics for 'abnormal'] | Precision: 0.7733 | Recall: 0.8471 | F1: 0.8085
2026-02-10 01:33:34,457 - INFO - [Metrics for 'normal'] | Precision: 0.8563 | Recall: 0.7857 | F1: 0.8195
2026-02-10 01:33:34,459 - INFO - --------------------------------------------------
2026-02-10 01:33:34,460 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 01:33:39,683 - INFO - [Train] [56/90] | Loss: 0.3688 | Train Acc: 88.99%
2026-02-10 01:33:40,384 - INFO - [Valid] [56/90] | Loss: 0.4760 | Val Acc: 80.24%
2026-02-10 01:33:40,389 - INFO - [Metrics for 'abnormal'] | Precision: 0.7616 | Recall: 0.8344 | F1: 0.7964
2026-02-10 01:33:40,390 - INFO - [Metrics for 'normal'] | Precision: 0.8443 | Recall: 0.7747 | F1: 0.8080
2026-02-10 01:33:40,392 - INFO - --------------------------------------------------
2026-02-10 01:33:40,393 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 01:33:44,708 - INFO - [Train] [57/90] | Loss: 0.3591 | Train Acc: 90.10%
2026-02-10 01:33:45,646 - INFO - [Valid] [57/90] | Loss: 0.4995 | Val Acc: 81.42%
2026-02-10 01:33:45,650 - INFO - [Metrics for 'abnormal'] | Precision: 0.7527 | Recall: 0.8917 | F1: 0.8163
2026-02-10 01:33:45,650 - INFO - [Metrics for 'normal'] | Precision: 0.8889 | Recall: 0.7473 | F1: 0.8119
2026-02-10 01:33:45,651 - INFO - --------------------------------------------------
2026-02-10 01:33:45,652 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 01:33:49,412 - INFO - [Train] [58/90] | Loss: 0.3585 | Train Acc: 89.96%
2026-02-10 01:33:50,635 - INFO - [Valid] [58/90] | Loss: 0.4805 | Val Acc: 79.35%
2026-02-10 01:33:50,641 - INFO - [Metrics for 'abnormal'] | Precision: 0.7605 | Recall: 0.8089 | F1: 0.7840
2026-02-10 01:33:50,641 - INFO - [Metrics for 'normal'] | Precision: 0.8256 | Recall: 0.7802 | F1: 0.8023
2026-02-10 01:33:50,643 - INFO - --------------------------------------------------
2026-02-10 01:33:50,644 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 01:33:55,090 - INFO - [Train] [59/90] | Loss: 0.3494 | Train Acc: 90.03%
2026-02-10 01:33:56,140 - INFO - [Valid] [59/90] | Loss: 0.4828 | Val Acc: 80.24%
2026-02-10 01:33:56,146 - INFO - [Metrics for 'abnormal'] | Precision: 0.7419 | Recall: 0.8790 | F1: 0.8047
2026-02-10 01:33:56,146 - INFO - [Metrics for 'normal'] | Precision: 0.8758 | Recall: 0.7363 | F1: 0.8000
2026-02-10 01:33:56,148 - INFO - --------------------------------------------------
2026-02-10 01:33:56,149 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 01:33:59,959 - INFO - [Train] [60/90] | Loss: 0.3579 | Train Acc: 89.96%
2026-02-10 01:34:01,098 - INFO - [Valid] [60/90] | Loss: 0.4809 | Val Acc: 79.94%
2026-02-10 01:34:01,104 - INFO - [Metrics for 'abnormal'] | Precision: 0.7486 | Recall: 0.8535 | F1: 0.7976
2026-02-10 01:34:01,104 - INFO - [Metrics for 'normal'] | Precision: 0.8562 | Recall: 0.7527 | F1: 0.8012
2026-02-10 01:34:01,106 - INFO - --------------------------------------------------
2026-02-10 01:34:01,107 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 01:34:05,055 - INFO - [Train] [61/90] | Loss: 0.3608 | Train Acc: 89.58%
2026-02-10 01:34:05,971 - INFO - [Valid] [61/90] | Loss: 0.4906 | Val Acc: 79.94%
2026-02-10 01:34:05,974 - INFO - [Metrics for 'abnormal'] | Precision: 0.7633 | Recall: 0.8217 | F1: 0.7914
2026-02-10 01:34:05,974 - INFO - [Metrics for 'normal'] | Precision: 0.8353 | Recall: 0.7802 | F1: 0.8068
2026-02-10 01:34:05,975 - INFO - --------------------------------------------------
2026-02-10 01:34:05,975 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 01:34:09,369 - INFO - [Train] [62/90] | Loss: 0.3534 | Train Acc: 90.10%
2026-02-10 01:34:10,217 - INFO - [Valid] [62/90] | Loss: 0.5190 | Val Acc: 80.24%
2026-02-10 01:34:10,220 - INFO - [Metrics for 'abnormal'] | Precision: 0.7557 | Recall: 0.8471 | F1: 0.7988
2026-02-10 01:34:10,220 - INFO - [Metrics for 'normal'] | Precision: 0.8528 | Recall: 0.7637 | F1: 0.8058
2026-02-10 01:34:10,221 - INFO - --------------------------------------------------
2026-02-10 01:34:10,221 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 01:34:13,907 - INFO - [Train] [63/90] | Loss: 0.3378 | Train Acc: 90.85%
2026-02-10 01:34:14,883 - INFO - [Valid] [63/90] | Loss: 0.4939 | Val Acc: 81.12%
2026-02-10 01:34:14,887 - INFO - [Metrics for 'abnormal'] | Precision: 0.7598 | Recall: 0.8662 | F1: 0.8095
2026-02-10 01:34:14,887 - INFO - [Metrics for 'normal'] | Precision: 0.8688 | Recall: 0.7637 | F1: 0.8129
2026-02-10 01:34:14,889 - INFO - --------------------------------------------------
2026-02-10 01:34:14,890 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 01:34:18,837 - INFO - [Train] [64/90] | Loss: 0.3397 | Train Acc: 91.67%
2026-02-10 01:34:20,159 - INFO - [Valid] [64/90] | Loss: 0.4968 | Val Acc: 80.53%
2026-02-10 01:34:20,164 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.8471 | F1: 0.8012
2026-02-10 01:34:20,164 - INFO - [Metrics for 'normal'] | Precision: 0.8537 | Recall: 0.7692 | F1: 0.8092
2026-02-10 01:34:20,170 - INFO - --------------------------------------------------
2026-02-10 01:34:20,171 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 01:34:24,240 - INFO - [Train] [65/90] | Loss: 0.3368 | Train Acc: 91.37%
2026-02-10 01:34:25,482 - INFO - [Valid] [65/90] | Loss: 0.5064 | Val Acc: 79.65%
2026-02-10 01:34:25,488 - INFO - [Metrics for 'abnormal'] | Precision: 0.7366 | Recall: 0.8726 | F1: 0.7988
2026-02-10 01:34:25,489 - INFO - [Metrics for 'normal'] | Precision: 0.8693 | Recall: 0.7308 | F1: 0.7940
2026-02-10 01:34:25,490 - INFO - --------------------------------------------------
2026-02-10 01:34:25,491 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 01:34:30,844 - INFO - [Train] [66/90] | Loss: 0.3356 | Train Acc: 91.67%
2026-02-10 01:34:31,948 - INFO - [Valid] [66/90] | Loss: 0.4958 | Val Acc: 80.24%
2026-02-10 01:34:31,953 - INFO - [Metrics for 'abnormal'] | Precision: 0.7744 | Recall: 0.8089 | F1: 0.7913
2026-02-10 01:34:31,953 - INFO - [Metrics for 'normal'] | Precision: 0.8286 | Recall: 0.7967 | F1: 0.8123
2026-02-10 01:34:31,954 - INFO - --------------------------------------------------
2026-02-10 01:34:31,955 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 01:34:38,938 - INFO - [Train] [67/90] | Loss: 0.3381 | Train Acc: 90.92%
2026-02-10 01:34:40,342 - INFO - [Valid] [67/90] | Loss: 0.4996 | Val Acc: 80.53%
2026-02-10 01:34:40,347 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.8471 | F1: 0.8012
2026-02-10 01:34:40,348 - INFO - [Metrics for 'normal'] | Precision: 0.8537 | Recall: 0.7692 | F1: 0.8092
2026-02-10 01:34:40,349 - INFO - --------------------------------------------------
2026-02-10 01:34:40,351 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 01:34:46,627 - INFO - [Train] [68/90] | Loss: 0.3362 | Train Acc: 91.74%
2026-02-10 01:34:48,146 - INFO - [Valid] [68/90] | Loss: 0.4868 | Val Acc: 79.65%
2026-02-10 01:34:48,151 - INFO - [Metrics for 'abnormal'] | Precision: 0.7683 | Recall: 0.8025 | F1: 0.7850
2026-02-10 01:34:48,152 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.7912 | F1: 0.8067
2026-02-10 01:34:48,153 - INFO - --------------------------------------------------
2026-02-10 01:34:48,154 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 01:34:54,531 - INFO - [Train] [69/90] | Loss: 0.3295 | Train Acc: 92.49%
2026-02-10 01:34:56,114 - INFO - [Valid] [69/90] | Loss: 0.4995 | Val Acc: 80.53%
2026-02-10 01:34:56,122 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.8471 | F1: 0.8012
2026-02-10 01:34:56,123 - INFO - [Metrics for 'normal'] | Precision: 0.8537 | Recall: 0.7692 | F1: 0.8092
2026-02-10 01:34:56,124 - INFO - --------------------------------------------------
2026-02-10 01:34:56,126 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 01:35:02,543 - INFO - [Train] [70/90] | Loss: 0.3238 | Train Acc: 92.04%
2026-02-10 01:35:04,065 - INFO - [Valid] [70/90] | Loss: 0.5031 | Val Acc: 81.42%
2026-02-10 01:35:04,074 - INFO - [Metrics for 'abnormal'] | Precision: 0.7640 | Recall: 0.8662 | F1: 0.8119
2026-02-10 01:35:04,074 - INFO - [Metrics for 'normal'] | Precision: 0.8696 | Recall: 0.7692 | F1: 0.8163
2026-02-10 01:35:04,075 - INFO - --------------------------------------------------
2026-02-10 01:35:04,077 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 01:35:10,204 - INFO - [Train] [71/90] | Loss: 0.3279 | Train Acc: 91.74%
2026-02-10 01:35:11,834 - INFO - [Valid] [71/90] | Loss: 0.5037 | Val Acc: 81.12%
2026-02-10 01:35:11,839 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-02-10 01:35:11,839 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-02-10 01:35:11,841 - INFO - --------------------------------------------------
2026-02-10 01:35:11,842 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 01:35:18,064 - INFO - [Train] [72/90] | Loss: 0.3263 | Train Acc: 92.04%
2026-02-10 01:35:19,464 - INFO - [Valid] [72/90] | Loss: 0.5104 | Val Acc: 80.24%
2026-02-10 01:35:19,469 - INFO - [Metrics for 'abnormal'] | Precision: 0.7368 | Recall: 0.8917 | F1: 0.8069
2026-02-10 01:35:19,469 - INFO - [Metrics for 'normal'] | Precision: 0.8859 | Recall: 0.7253 | F1: 0.7976
2026-02-10 01:35:19,471 - INFO - --------------------------------------------------
2026-02-10 01:35:19,472 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 01:35:25,795 - INFO - [Train] [73/90] | Loss: 0.3227 | Train Acc: 92.63%
2026-02-10 01:35:26,948 - INFO - [Valid] [73/90] | Loss: 0.5145 | Val Acc: 79.65%
2026-02-10 01:35:26,952 - INFO - [Metrics for 'abnormal'] | Precision: 0.7418 | Recall: 0.8599 | F1: 0.7965
2026-02-10 01:35:26,953 - INFO - [Metrics for 'normal'] | Precision: 0.8599 | Recall: 0.7418 | F1: 0.7965
2026-02-10 01:35:26,954 - INFO - --------------------------------------------------
2026-02-10 01:35:26,955 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 01:35:33,556 - INFO - [Train] [74/90] | Loss: 0.3241 | Train Acc: 91.89%
2026-02-10 01:35:35,046 - INFO - [Valid] [74/90] | Loss: 0.5135 | Val Acc: 79.65%
2026-02-10 01:35:35,052 - INFO - [Metrics for 'abnormal'] | Precision: 0.7558 | Recall: 0.8280 | F1: 0.7903
2026-02-10 01:35:35,052 - INFO - [Metrics for 'normal'] | Precision: 0.8383 | Recall: 0.7692 | F1: 0.8023
2026-02-10 01:35:35,053 - INFO - --------------------------------------------------
2026-02-10 01:35:35,055 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 01:35:41,573 - INFO - [Train] [75/90] | Loss: 0.3089 | Train Acc: 92.86%
2026-02-10 01:35:42,865 - INFO - [Valid] [75/90] | Loss: 0.5130 | Val Acc: 79.94%
2026-02-10 01:35:42,870 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8089 | F1: 0.7888
2026-02-10 01:35:42,870 - INFO - [Metrics for 'normal'] | Precision: 0.8276 | Recall: 0.7912 | F1: 0.8090
2026-02-10 01:35:42,872 - INFO - --------------------------------------------------
2026-02-10 01:35:42,873 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 01:35:49,825 - INFO - [Train] [76/90] | Loss: 0.3144 | Train Acc: 92.86%
2026-02-10 01:35:51,278 - INFO - [Valid] [76/90] | Loss: 0.5015 | Val Acc: 80.83%
2026-02-10 01:35:51,283 - INFO - [Metrics for 'abnormal'] | Precision: 0.7674 | Recall: 0.8408 | F1: 0.8024
2026-02-10 01:35:51,283 - INFO - [Metrics for 'normal'] | Precision: 0.8503 | Recall: 0.7802 | F1: 0.8138
2026-02-10 01:35:51,285 - INFO - --------------------------------------------------
2026-02-10 01:35:51,286 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 01:35:58,035 - INFO - [Train] [77/90] | Loss: 0.3045 | Train Acc: 93.60%
2026-02-10 01:35:59,430 - INFO - [Valid] [77/90] | Loss: 0.5153 | Val Acc: 78.76%
2026-02-10 01:35:59,436 - INFO - [Metrics for 'abnormal'] | Precision: 0.7515 | Recall: 0.8089 | F1: 0.7791
2026-02-10 01:35:59,437 - INFO - [Metrics for 'normal'] | Precision: 0.8235 | Recall: 0.7692 | F1: 0.7955
2026-02-10 01:35:59,438 - INFO - --------------------------------------------------
2026-02-10 01:35:59,440 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 01:36:06,105 - INFO - [Train] [78/90] | Loss: 0.2977 | Train Acc: 94.20%
2026-02-10 01:36:07,542 - INFO - [Valid] [78/90] | Loss: 0.5048 | Val Acc: 82.01%
2026-02-10 01:36:07,546 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8726 | F1: 0.8179
2026-02-10 01:36:07,547 - INFO - [Metrics for 'normal'] | Precision: 0.8758 | Recall: 0.7747 | F1: 0.8222
2026-02-10 01:36:07,548 - INFO - --------------------------------------------------
2026-02-10 01:36:07,549 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 01:36:12,819 - INFO - [Train] [79/90] | Loss: 0.3032 | Train Acc: 93.82%
2026-02-10 01:36:14,240 - INFO - [Valid] [79/90] | Loss: 0.5131 | Val Acc: 80.83%
2026-02-10 01:36:14,248 - INFO - [Metrics for 'abnormal'] | Precision: 0.7527 | Recall: 0.8726 | F1: 0.8083
2026-02-10 01:36:14,249 - INFO - [Metrics for 'normal'] | Precision: 0.8726 | Recall: 0.7527 | F1: 0.8083
2026-02-10 01:36:14,250 - INFO - --------------------------------------------------
2026-02-10 01:36:14,251 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 01:36:19,696 - INFO - [Train] [80/90] | Loss: 0.2947 | Train Acc: 94.12%
2026-02-10 01:36:21,015 - INFO - [Valid] [80/90] | Loss: 0.5184 | Val Acc: 79.06%
2026-02-10 01:36:21,019 - INFO - [Metrics for 'abnormal'] | Precision: 0.7500 | Recall: 0.8217 | F1: 0.7842
2026-02-10 01:36:21,019 - INFO - [Metrics for 'normal'] | Precision: 0.8323 | Recall: 0.7637 | F1: 0.7966
2026-02-10 01:36:21,021 - INFO - --------------------------------------------------
2026-02-10 01:36:21,022 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 01:36:25,874 - INFO - [Train] [81/90] | Loss: 0.3003 | Train Acc: 94.05%
2026-02-10 01:36:27,226 - INFO - [Valid] [81/90] | Loss: 0.5315 | Val Acc: 79.06%
2026-02-10 01:36:27,231 - INFO - [Metrics for 'abnormal'] | Precision: 0.7560 | Recall: 0.8089 | F1: 0.7815
2026-02-10 01:36:27,231 - INFO - [Metrics for 'normal'] | Precision: 0.8246 | Recall: 0.7747 | F1: 0.7989
2026-02-10 01:36:27,233 - INFO - --------------------------------------------------
2026-02-10 01:36:27,234 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 01:36:32,455 - INFO - [Train] [82/90] | Loss: 0.2936 | Train Acc: 94.49%
2026-02-10 01:36:33,882 - INFO - [Valid] [82/90] | Loss: 0.5304 | Val Acc: 81.12%
2026-02-10 01:36:33,887 - INFO - [Metrics for 'abnormal'] | Precision: 0.7541 | Recall: 0.8790 | F1: 0.8118
2026-02-10 01:36:33,887 - INFO - [Metrics for 'normal'] | Precision: 0.8782 | Recall: 0.7527 | F1: 0.8107
2026-02-10 01:36:33,888 - INFO - --------------------------------------------------
2026-02-10 01:36:33,890 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 01:36:39,744 - INFO - [Train] [83/90] | Loss: 0.2968 | Train Acc: 94.79%
2026-02-10 01:36:41,371 - INFO - [Valid] [83/90] | Loss: 0.5426 | Val Acc: 80.53%
2026-02-10 01:36:41,376 - INFO - [Metrics for 'abnormal'] | Precision: 0.7514 | Recall: 0.8662 | F1: 0.8047
2026-02-10 01:36:41,376 - INFO - [Metrics for 'normal'] | Precision: 0.8671 | Recall: 0.7527 | F1: 0.8059
2026-02-10 01:36:41,377 - INFO - --------------------------------------------------
2026-02-10 01:36:41,378 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 01:36:48,024 - INFO - [Train] [84/90] | Loss: 0.2918 | Train Acc: 94.49%
2026-02-10 01:36:49,409 - INFO - [Valid] [84/90] | Loss: 0.5091 | Val Acc: 82.60%
2026-02-10 01:36:49,414 - INFO - [Metrics for 'abnormal'] | Precision: 0.7816 | Recall: 0.8662 | F1: 0.8218
2026-02-10 01:36:49,421 - INFO - [Metrics for 'normal'] | Precision: 0.8727 | Recall: 0.7912 | F1: 0.8300
2026-02-10 01:36:49,425 - INFO - --------------------------------------------------
2026-02-10 01:36:49,426 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 01:36:55,531 - INFO - [Train] [85/90] | Loss: 0.2970 | Train Acc: 93.68%
2026-02-10 01:36:56,993 - INFO - [Valid] [85/90] | Loss: 0.5292 | Val Acc: 79.65%
2026-02-10 01:36:56,998 - INFO - [Metrics for 'abnormal'] | Precision: 0.7558 | Recall: 0.8280 | F1: 0.7903
2026-02-10 01:36:56,998 - INFO - [Metrics for 'normal'] | Precision: 0.8383 | Recall: 0.7692 | F1: 0.8023
2026-02-10 01:36:56,999 - INFO - --------------------------------------------------
2026-02-10 01:36:57,000 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 01:37:02,849 - INFO - [Train] [86/90] | Loss: 0.2844 | Train Acc: 94.64%
2026-02-10 01:37:04,364 - INFO - [Valid] [86/90] | Loss: 0.5451 | Val Acc: 79.65%
2026-02-10 01:37:04,370 - INFO - [Metrics for 'abnormal'] | Precision: 0.7651 | Recall: 0.8089 | F1: 0.7864
2026-02-10 01:37:04,370 - INFO - [Metrics for 'normal'] | Precision: 0.8266 | Recall: 0.7857 | F1: 0.8056
2026-02-10 01:37:04,372 - INFO - --------------------------------------------------
2026-02-10 01:37:04,373 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 01:37:10,877 - INFO - [Train] [87/90] | Loss: 0.2887 | Train Acc: 94.79%
2026-02-10 01:37:12,462 - INFO - [Valid] [87/90] | Loss: 0.5383 | Val Acc: 79.94%
2026-02-10 01:37:12,466 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8089 | F1: 0.7888
2026-02-10 01:37:12,467 - INFO - [Metrics for 'normal'] | Precision: 0.8276 | Recall: 0.7912 | F1: 0.8090
2026-02-10 01:37:12,468 - INFO - --------------------------------------------------
2026-02-10 01:37:12,469 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 01:37:17,908 - INFO - [Train] [88/90] | Loss: 0.2890 | Train Acc: 94.57%
2026-02-10 01:37:19,441 - INFO - [Valid] [88/90] | Loss: 0.5503 | Val Acc: 79.65%
2026-02-10 01:37:19,446 - INFO - [Metrics for 'abnormal'] | Precision: 0.7683 | Recall: 0.8025 | F1: 0.7850
2026-02-10 01:37:19,446 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.7912 | F1: 0.8067
2026-02-10 01:37:19,448 - INFO - --------------------------------------------------
2026-02-10 01:37:19,449 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 01:37:25,500 - INFO - [Train] [89/90] | Loss: 0.2827 | Train Acc: 95.46%
2026-02-10 01:37:27,220 - INFO - [Valid] [89/90] | Loss: 0.5473 | Val Acc: 78.76%
2026-02-10 01:37:27,224 - INFO - [Metrics for 'abnormal'] | Precision: 0.7515 | Recall: 0.8089 | F1: 0.7791
2026-02-10 01:37:27,225 - INFO - [Metrics for 'normal'] | Precision: 0.8235 | Recall: 0.7692 | F1: 0.7955
2026-02-10 01:37:27,227 - INFO - --------------------------------------------------
2026-02-10 01:37:27,229 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 01:37:32,995 - INFO - [Train] [90/90] | Loss: 0.2806 | Train Acc: 95.24%
2026-02-10 01:37:34,506 - INFO - [Valid] [90/90] | Loss: 0.5522 | Val Acc: 78.47%
2026-02-10 01:37:34,514 - INFO - [Metrics for 'abnormal'] | Precision: 0.7471 | Recall: 0.8089 | F1: 0.7768
2026-02-10 01:37:34,514 - INFO - [Metrics for 'normal'] | Precision: 0.8225 | Recall: 0.7637 | F1: 0.7920
2026-02-10 01:37:34,517 - INFO - ==================================================
2026-02-10 01:37:34,517 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 01:37:34,517 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 01:37:34,517 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:37:37,602 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:37:37,603 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 01:37:37,604 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:37:37,608 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:37,608 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:37:38,506 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-02-10 01:37:38,506 - INFO - ==================================================
2026-02-10 01:37:38,543 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/best_model.pth'
2026-02-10 01:37:38,544 - INFO - ==================================================
2026-02-10 01:37:38,544 - INFO - Test 모드를 시작합니다.
2026-02-10 01:37:38,707 - INFO - 연산량 (MACs): 0.0920 GMACs per sample
2026-02-10 01:37:38,707 - INFO - 연산량 (FLOPs): 0.1840 GFLOPs per sample
2026-02-10 01:37:38,711 - INFO - ==================================================
2026-02-10 01:37:38,711 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 01:37:40,098 - INFO - 샘플 당 평균 Forward Pass 시간: 4.59ms (std: 1.04ms), FPS: 228.38 (std: 50.73) (1개 샘플 x 100회 반복)
2026-02-10 01:37:40,098 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 112.93 MB
2026-02-10 01:37:40,099 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 01:37:42,476 - INFO - [Test] Loss: 0.4003 | Test Acc: 80.53%
2026-02-10 01:37:42,482 - INFO - [Metrics for 'abnormal'] | Precision: 0.7661 | Recall: 0.8344 | F1: 0.7988
2026-02-10 01:37:42,483 - INFO - [Metrics for 'normal'] | Precision: 0.8452 | Recall: 0.7802 | F1: 0.8114
2026-02-10 01:37:42,779 - INFO - ==================================================
2026-02-10 01:37:42,779 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/confusion_matrix_20260210_012603.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/confusion_matrix_20260210_012603.pdf'
2026-02-10 01:37:42,779 - INFO - ==================================================
2026-02-10 01:37:42,779 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 01:37:45,508 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/model_fp32_20260210_012603.onnx'에 저장되었습니다. (크기: 1.96 MB)
2026-02-10 01:37:46,144 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 13.88 MB
2026-02-10 01:37:46,144 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 01:37:50,117 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 33.30ms (std: 17.29ms)
2026-02-10 01:37:50,117 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 42.64 FPS (std: 32.71) (1개 샘플 x 100회 반복)
2026-02-10 01:37:50,117 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 4.12 MB
2026-02-10 01:37:50,117 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 15.31 MB
2026-02-10 01:38:05,898 - INFO - [Test (ONNX)] | Test Acc (ONNX): 80.53%
2026-02-10 01:38:05,922 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.7661 | Recall: 0.8344 | F1: 0.7988
2026-02-10 01:38:05,926 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8452 | Recall: 0.7802 | F1: 0.8114
2026-02-10 01:38:06,227 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/val_acc.pdf'
2026-02-10 01:38:06,468 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/train_val_acc.pdf'
2026-02-10 01:38:06,667 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/F1_normal.pdf'
2026-02-10 01:38:06,885 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/val_loss.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/val_loss.pdf'
2026-02-10 01:38:07,099 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/learning_rate.pdf'
2026-02-10 01:38:09,573 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/compile.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_fpgm_20260210_012603/graph_20260210_012603/compile.pdf'
