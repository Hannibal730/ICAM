2026-02-10 01:25:48,913 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/log_20260210_012548.log'에 저장됩니다.
2026-02-10 01:25:48,916 - INFO - ==================================================
2026-02-10 01:25:48,916 - INFO - config.yaml:
2026-02-10 01:25:48,916 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: xie2019
  use_fpgm_pruning: true
  pruning_flops_target: 0.1816

2026-02-10 01:25:48,916 - INFO - ==================================================
2026-02-10 01:25:48,958 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:25:48,960 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:25:48,960 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:25:48,966 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:25:48,966 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:25:48,967 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:25:48,967 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:25:49,195 - INFO - ==================================================
2026-02-10 01:25:49,195 - INFO - 모델 파라미터 수:
2026-02-10 01:25:49,195 - INFO -   - 총 파라미터: 9,160,194 개
2026-02-10 01:25:49,195 - INFO -   - 학습 가능한 파라미터: 9,160,194 개
2026-02-10 01:25:49,195 - INFO - ================================================================================
2026-02-10 01:25:49,195 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:25:49,195 - INFO - ================================================================================
2026-02-10 01:25:49,195 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:25:49,195 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:25:49,196 - INFO - ==================================================
2026-02-10 01:25:49,196 - INFO - train 모드를 시작합니다.
2026-02-10 01:25:49,196 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:25:49,196 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:25:49,196 - INFO - --------------------------------------------------
2026-02-10 01:25:49,196 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:25:52,600 - INFO - [Train] [1/10] | Loss: 0.5857 | Train Acc: 74.18%
2026-02-10 01:25:54,021 - INFO - [Valid] [1/10] | Loss: 0.6176 | Val Acc: 72.27%
2026-02-10 01:25:54,027 - INFO - [Metrics for 'abnormal'] | Precision: 0.8987 | Recall: 0.4522 | F1: 0.6017
2026-02-10 01:25:54,028 - INFO - [Metrics for 'normal'] | Precision: 0.6692 | Recall: 0.9560 | F1: 0.7873
2026-02-10 01:25:54,086 - INFO - [Best Model Saved] (val loss: 0.6176) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:25:54,086 - INFO - --------------------------------------------------
2026-02-10 01:25:54,087 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:25:57,249 - INFO - [Train] [2/10] | Loss: 0.5502 | Train Acc: 78.12%
2026-02-10 01:25:58,032 - INFO - [Valid] [2/10] | Loss: 0.5861 | Val Acc: 73.45%
2026-02-10 01:25:58,035 - INFO - [Metrics for 'abnormal'] | Precision: 0.6831 | Recall: 0.7962 | F1: 0.7353
2026-02-10 01:25:58,036 - INFO - [Metrics for 'normal'] | Precision: 0.7949 | Recall: 0.6813 | F1: 0.7337
2026-02-10 01:25:58,092 - INFO - [Best Model Saved] (val loss: 0.5861) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:25:58,092 - INFO - --------------------------------------------------
2026-02-10 01:25:58,092 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:26:01,676 - INFO - [Train] [3/10] | Loss: 0.5161 | Train Acc: 80.28%
2026-02-10 01:26:02,409 - INFO - [Valid] [3/10] | Loss: 0.5593 | Val Acc: 74.04%
2026-02-10 01:26:02,414 - INFO - [Metrics for 'abnormal'] | Precision: 0.6825 | Recall: 0.8217 | F1: 0.7457
2026-02-10 01:26:02,414 - INFO - [Metrics for 'normal'] | Precision: 0.8133 | Recall: 0.6703 | F1: 0.7349
2026-02-10 01:26:02,487 - INFO - [Best Model Saved] (val loss: 0.5593) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:26:02,487 - INFO - --------------------------------------------------
2026-02-10 01:26:02,488 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:26:05,797 - INFO - [Train] [4/10] | Loss: 0.4958 | Train Acc: 81.47%
2026-02-10 01:26:06,999 - INFO - [Valid] [4/10] | Loss: 0.5640 | Val Acc: 76.40%
2026-02-10 01:26:07,004 - INFO - [Metrics for 'abnormal'] | Precision: 0.8407 | Recall: 0.6051 | F1: 0.7037
2026-02-10 01:26:07,005 - INFO - [Metrics for 'normal'] | Precision: 0.7257 | Recall: 0.9011 | F1: 0.8039
2026-02-10 01:26:07,006 - INFO - --------------------------------------------------
2026-02-10 01:26:07,007 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:26:11,023 - INFO - [Train] [5/10] | Loss: 0.4877 | Train Acc: 81.62%
2026-02-10 01:26:11,934 - INFO - [Valid] [5/10] | Loss: 0.5246 | Val Acc: 77.88%
2026-02-10 01:26:11,939 - INFO - [Metrics for 'abnormal'] | Precision: 0.7228 | Recall: 0.8471 | F1: 0.7801
2026-02-10 01:26:11,939 - INFO - [Metrics for 'normal'] | Precision: 0.8452 | Recall: 0.7198 | F1: 0.7774
2026-02-10 01:26:12,051 - INFO - [Best Model Saved] (val loss: 0.5246) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:26:12,052 - INFO - --------------------------------------------------
2026-02-10 01:26:12,052 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:26:16,506 - INFO - [Train] [6/10] | Loss: 0.4681 | Train Acc: 84.38%
2026-02-10 01:26:17,645 - INFO - [Valid] [6/10] | Loss: 0.5177 | Val Acc: 79.06%
2026-02-10 01:26:17,650 - INFO - [Metrics for 'abnormal'] | Precision: 0.7590 | Recall: 0.8025 | F1: 0.7802
2026-02-10 01:26:17,650 - INFO - [Metrics for 'normal'] | Precision: 0.8208 | Recall: 0.7802 | F1: 0.8000
2026-02-10 01:26:17,718 - INFO - [Best Model Saved] (val loss: 0.5177) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:26:17,718 - INFO - --------------------------------------------------
2026-02-10 01:26:17,718 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:26:22,022 - INFO - [Train] [7/10] | Loss: 0.4578 | Train Acc: 84.52%
2026-02-10 01:26:23,244 - INFO - [Valid] [7/10] | Loss: 0.5117 | Val Acc: 78.76%
2026-02-10 01:26:23,248 - INFO - [Metrics for 'abnormal'] | Precision: 0.7673 | Recall: 0.7771 | F1: 0.7722
2026-02-10 01:26:23,249 - INFO - [Metrics for 'normal'] | Precision: 0.8056 | Recall: 0.7967 | F1: 0.8011
2026-02-10 01:26:23,311 - INFO - [Best Model Saved] (val loss: 0.5117) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:26:23,311 - INFO - --------------------------------------------------
2026-02-10 01:26:23,311 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:26:28,161 - INFO - [Train] [8/10] | Loss: 0.4386 | Train Acc: 84.75%
2026-02-10 01:26:29,614 - INFO - [Valid] [8/10] | Loss: 0.5165 | Val Acc: 79.94%
2026-02-10 01:26:29,618 - INFO - [Metrics for 'abnormal'] | Precision: 0.7665 | Recall: 0.8153 | F1: 0.7901
2026-02-10 01:26:29,619 - INFO - [Metrics for 'normal'] | Precision: 0.8314 | Recall: 0.7857 | F1: 0.8079
2026-02-10 01:26:29,625 - INFO - --------------------------------------------------
2026-02-10 01:26:29,625 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:26:33,922 - INFO - [Train] [9/10] | Loss: 0.4260 | Train Acc: 85.57%
2026-02-10 01:26:35,560 - INFO - [Valid] [9/10] | Loss: 0.5181 | Val Acc: 80.53%
2026-02-10 01:26:35,564 - INFO - [Metrics for 'abnormal'] | Precision: 0.7862 | Recall: 0.7962 | F1: 0.7911
2026-02-10 01:26:35,564 - INFO - [Metrics for 'normal'] | Precision: 0.8222 | Recall: 0.8132 | F1: 0.8177
2026-02-10 01:26:35,570 - INFO - --------------------------------------------------
2026-02-10 01:26:35,571 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:26:41,434 - INFO - [Train] [10/10] | Loss: 0.4261 | Train Acc: 85.79%
2026-02-10 01:26:42,608 - INFO - [Valid] [10/10] | Loss: 0.5195 | Val Acc: 80.83%
2026-02-10 01:26:42,613 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-02-10 01:26:42,613 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-02-10 01:26:42,615 - INFO - ================================================================================
2026-02-10 01:26:42,615 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:26:42,615 - INFO - ================================================================================
2026-02-10 01:26:42,649 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:26:42,649 - INFO - ================================================================================
2026-02-10 01:26:42,649 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:26:42,667 - INFO - 원본 모델 FLOPs: 2.8696 GFLOPs
2026-02-10 01:26:42,672 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:42,673 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:43,339 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:26:43,340 - INFO - ==================================================
2026-02-10 01:26:43,347 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 1.3003 GFLOPs (감소율: 54.69%)
2026-02-10 01:26:43,350 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:43,350 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:43,916 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-02-10 01:26:43,916 - INFO - ==================================================
2026-02-10 01:26:43,923 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.6166 GFLOPs (감소율: 78.51%)
2026-02-10 01:26:43,926 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:43,926 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:44,495 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-02-10 01:26:44,495 - INFO - ==================================================
2026-02-10 01:26:44,501 - INFO -   [탐색  3] 희소도: 0.8662 -> FLOPs: 0.3005 GFLOPs (감소율: 89.53%)
2026-02-10 01:26:44,503 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:44,503 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:45,046 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.928125)에 맞춰 변경되었습니다.
2026-02-10 01:26:45,046 - INFO - ==================================================
2026-02-10 01:26:45,052 - INFO -   [탐색  4] 희소도: 0.9281 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:45,055 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:45,055 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:46,008 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8971875)에 맞춰 변경되었습니다.
2026-02-10 01:26:46,009 - INFO - ==================================================
2026-02-10 01:26:46,015 - INFO -   [탐색  5] 희소도: 0.8972 -> FLOPs: 0.2238 GFLOPs (감소율: 92.20%)
2026-02-10 01:26:46,018 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:46,018 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:46,583 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.91265625)에 맞춰 변경되었습니다.
2026-02-10 01:26:46,583 - INFO - ==================================================
2026-02-10 01:26:46,590 - INFO -   [탐색  6] 희소도: 0.9127 -> FLOPs: 0.1858 GFLOPs (감소율: 93.52%)
2026-02-10 01:26:46,592 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:46,593 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:47,132 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.920390625)에 맞춰 변경되었습니다.
2026-02-10 01:26:47,132 - INFO - ==================================================
2026-02-10 01:26:47,149 - INFO -   [탐색  7] 희소도: 0.9204 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:26:47,152 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:47,153 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:47,730 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9242578125)에 맞춰 변경되었습니다.
2026-02-10 01:26:47,731 - INFO - ==================================================
2026-02-10 01:26:47,735 - INFO -   [탐색  8] 희소도: 0.9243 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:47,737 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:47,737 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:48,359 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.92232421875)에 맞춰 변경되었습니다.
2026-02-10 01:26:48,359 - INFO - ==================================================
2026-02-10 01:26:48,362 - INFO -   [탐색  9] 희소도: 0.9223 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:48,364 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:48,364 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:48,848 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-02-10 01:26:48,849 - INFO - ==================================================
2026-02-10 01:26:48,851 - INFO -   [탐색 10] 희소도: 0.9214 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:26:48,855 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:48,856 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:49,904 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218408203125)에 맞춰 변경되었습니다.
2026-02-10 01:26:49,904 - INFO - ==================================================
2026-02-10 01:26:49,907 - INFO -   [탐색 11] 희소도: 0.9218 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:26:49,910 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:49,911 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:50,470 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9220825195312501)에 맞춰 변경되었습니다.
2026-02-10 01:26:50,470 - INFO - ==================================================
2026-02-10 01:26:50,473 - INFO -   [탐색 12] 희소도: 0.9221 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:50,476 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:50,476 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:51,035 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9219616699218751)에 맞춰 변경되었습니다.
2026-02-10 01:26:51,036 - INFO - ==================================================
2026-02-10 01:26:51,039 - INFO -   [탐색 13] 희소도: 0.9220 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:51,041 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:51,042 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:51,611 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9219012451171875)에 맞춰 변경되었습니다.
2026-02-10 01:26:51,611 - INFO - ==================================================
2026-02-10 01:26:51,613 - INFO -   [탐색 14] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:51,616 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:51,616 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:52,186 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218710327148438)에 맞춰 변경되었습니다.
2026-02-10 01:26:52,186 - INFO - ==================================================
2026-02-10 01:26:52,190 - INFO -   [탐색 15] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:26:52,194 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:52,194 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:52,800 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218861389160157)에 맞춰 변경되었습니다.
2026-02-10 01:26:52,801 - INFO - ==================================================
2026-02-10 01:26:52,804 - INFO -   [탐색 16] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:52,807 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:52,807 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:53,396 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218785858154297)에 맞춰 변경되었습니다.
2026-02-10 01:26:53,396 - INFO - ==================================================
2026-02-10 01:26:53,399 - INFO -   [탐색 17] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:53,402 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:53,402 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:54,388 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218748092651368)에 맞춰 변경되었습니다.
2026-02-10 01:26:54,388 - INFO - ==================================================
2026-02-10 01:26:54,390 - INFO -   [탐색 18] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:26:54,393 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:54,393 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:54,945 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218766975402832)에 맞춰 변경되었습니다.
2026-02-10 01:26:54,945 - INFO - ==================================================
2026-02-10 01:26:54,950 - INFO -   [탐색 19] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:54,952 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:54,952 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:55,572 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.92187575340271)에 맞춰 변경되었습니다.
2026-02-10 01:26:55,572 - INFO - ==================================================
2026-02-10 01:26:55,575 - INFO -   [탐색 20] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:55,581 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:55,581 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:56,159 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218752813339234)에 맞춰 변경되었습니다.
2026-02-10 01:26:56,159 - INFO - ==================================================
2026-02-10 01:26:56,162 - INFO -   [탐색 21] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:56,165 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:56,165 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:56,749 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750452995301)에 맞춰 변경되었습니다.
2026-02-10 01:26:56,750 - INFO - ==================================================
2026-02-10 01:26:56,754 - INFO -   [탐색 22] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:56,756 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:56,756 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:57,308 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749272823334)에 맞춰 변경되었습니다.
2026-02-10 01:26:57,308 - INFO - ==================================================
2026-02-10 01:26:57,311 - INFO -   [탐색 23] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:26:57,313 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:57,313 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:57,883 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749862909318)에 맞춰 변경되었습니다.
2026-02-10 01:26:57,883 - INFO - ==================================================
2026-02-10 01:26:57,886 - INFO -   [탐색 24] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:26:57,889 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:57,889 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:58,914 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750157952309)에 맞춰 변경되었습니다.
2026-02-10 01:26:58,914 - INFO - ==================================================
2026-02-10 01:26:58,917 - INFO -   [탐색 25] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:58,920 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:58,920 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:26:59,489 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750010430814)에 맞춰 변경되었습니다.
2026-02-10 01:26:59,489 - INFO - ==================================================
2026-02-10 01:26:59,492 - INFO -   [탐색 26] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:26:59,495 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:26:59,495 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:00,044 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749936670065)에 맞춰 변경되었습니다.
2026-02-10 01:27:00,044 - INFO - ==================================================
2026-02-10 01:27:00,047 - INFO -   [탐색 27] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:00,049 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:00,050 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:00,601 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921874997355044)에 맞춰 변경되었습니다.
2026-02-10 01:27:00,601 - INFO - ==================================================
2026-02-10 01:27:00,604 - INFO -   [탐색 28] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:00,608 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:00,608 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:01,185 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749991990627)에 맞춰 변경되었습니다.
2026-02-10 01:27:01,185 - INFO - ==================================================
2026-02-10 01:27:01,188 - INFO -   [탐색 29] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:01,191 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:01,192 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:01,850 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875000121072)에 맞춰 변경되었습니다.
2026-02-10 01:27:01,850 - INFO - ==================================================
2026-02-10 01:27:01,853 - INFO -   [탐색 30] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:01,856 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:01,856 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:02,838 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749996600674)에 맞춰 변경되었습니다.
2026-02-10 01:27:02,839 - INFO - ==================================================
2026-02-10 01:27:02,842 - INFO -   [탐색 31] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:02,845 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:02,845 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:03,437 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749998905698)에 맞춰 변경되었습니다.
2026-02-10 01:27:03,437 - INFO - ==================================================
2026-02-10 01:27:03,440 - INFO -   [탐색 32] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:03,444 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:03,444 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:04,030 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000058209)에 맞춰 변경되었습니다.
2026-02-10 01:27:04,030 - INFO - ==================================================
2026-02-10 01:27:04,033 - INFO -   [탐색 33] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:04,037 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:04,037 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:04,652 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999481953)에 맞춰 변경되었습니다.
2026-02-10 01:27:04,653 - INFO - ==================================================
2026-02-10 01:27:04,656 - INFO -   [탐색 34] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:04,658 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:04,658 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:05,247 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999770082)에 맞춰 변경되었습니다.
2026-02-10 01:27:05,247 - INFO - ==================================================
2026-02-10 01:27:05,251 - INFO -   [탐색 35] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:05,254 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:05,254 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:05,833 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999914145)에 맞춰 변경되었습니다.
2026-02-10 01:27:05,833 - INFO - ==================================================
2026-02-10 01:27:05,836 - INFO -   [탐색 36] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:05,838 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:05,838 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:06,410 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999986178)에 맞춰 변경되었습니다.
2026-02-10 01:27:06,411 - INFO - ==================================================
2026-02-10 01:27:06,415 - INFO -   [탐색 37] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:06,418 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:06,418 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:07,431 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000022193)에 맞춰 변경되었습니다.
2026-02-10 01:27:07,431 - INFO - ==================================================
2026-02-10 01:27:07,434 - INFO -   [탐색 38] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:07,436 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:07,436 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:08,017 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000004186)에 맞춰 변경되었습니다.
2026-02-10 01:27:08,017 - INFO - ==================================================
2026-02-10 01:27:08,020 - INFO -   [탐색 39] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:08,023 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:08,023 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:08,646 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999995182)에 맞춰 변경되었습니다.
2026-02-10 01:27:08,646 - INFO - ==================================================
2026-02-10 01:27:08,649 - INFO -   [탐색 40] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:08,651 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:08,652 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:09,223 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999999684)에 맞춰 변경되었습니다.
2026-02-10 01:27:09,224 - INFO - ==================================================
2026-02-10 01:27:09,228 - INFO -   [탐색 41] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:09,232 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:09,232 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:09,805 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000001934)에 맞춰 변경되었습니다.
2026-02-10 01:27:09,805 - INFO - ==================================================
2026-02-10 01:27:09,809 - INFO -   [탐색 42] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:09,812 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:09,812 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:10,392 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000808)에 맞춰 변경되었습니다.
2026-02-10 01:27:10,393 - INFO - ==================================================
2026-02-10 01:27:10,396 - INFO -   [탐색 43] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:10,398 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:10,398 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:11,383 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000246)에 맞춰 변경되었습니다.
2026-02-10 01:27:11,383 - INFO - ==================================================
2026-02-10 01:27:11,386 - INFO -   [탐색 44] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:11,389 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:11,389 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:11,972 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999999964)에 맞춰 변경되었습니다.
2026-02-10 01:27:11,972 - INFO - ==================================================
2026-02-10 01:27:11,974 - INFO -   [탐색 45] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:11,978 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:11,978 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:12,535 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000105)에 맞춰 변경되었습니다.
2026-02-10 01:27:12,535 - INFO - ==================================================
2026-02-10 01:27:12,538 - INFO -   [탐색 46] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:12,540 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:12,541 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:13,110 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000036)에 맞춰 변경되었습니다.
2026-02-10 01:27:13,110 - INFO - ==================================================
2026-02-10 01:27:13,114 - INFO -   [탐색 47] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:13,116 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:13,116 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:13,668 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:13,668 - INFO - ==================================================
2026-02-10 01:27:13,671 - INFO -   [탐색 48] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:13,674 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:13,675 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:14,226 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000018)에 맞춰 변경되었습니다.
2026-02-10 01:27:14,227 - INFO - ==================================================
2026-02-10 01:27:14,230 - INFO -   [탐색 49] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:14,232 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:14,232 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:14,777 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000009)에 맞춰 변경되었습니다.
2026-02-10 01:27:14,777 - INFO - ==================================================
2026-02-10 01:27:14,780 - INFO -   [탐색 50] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:14,783 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:14,783 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:15,603 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000004)에 맞춰 변경되었습니다.
2026-02-10 01:27:15,604 - INFO - ==================================================
2026-02-10 01:27:15,605 - INFO -   [탐색 51] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:15,607 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:15,607 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:16,136 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000002)에 맞춰 변경되었습니다.
2026-02-10 01:27:16,137 - INFO - ==================================================
2026-02-10 01:27:16,139 - INFO -   [탐색 52] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:16,142 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:16,142 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:16,671 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000001)에 맞춰 변경되었습니다.
2026-02-10 01:27:16,672 - INFO - ==================================================
2026-02-10 01:27:16,675 - INFO -   [탐색 53] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:27:16,677 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:16,677 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:17,190 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:17,190 - INFO - ==================================================
2026-02-10 01:27:17,193 - INFO -   [탐색 54] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:17,196 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:17,196 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:17,736 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:17,736 - INFO - ==================================================
2026-02-10 01:27:17,739 - INFO -   [탐색 55] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:17,741 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:17,741 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:18,268 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:18,269 - INFO - ==================================================
2026-02-10 01:27:18,272 - INFO -   [탐색 56] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:18,274 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:18,274 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:19,174 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:19,174 - INFO - ==================================================
2026-02-10 01:27:19,178 - INFO -   [탐색 57] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:19,180 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:19,180 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:19,717 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:19,717 - INFO - ==================================================
2026-02-10 01:27:19,721 - INFO -   [탐색 58] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:19,723 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:19,724 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:20,144 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:20,144 - INFO - ==================================================
2026-02-10 01:27:20,148 - INFO -   [탐색 59] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:20,151 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:20,151 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:20,702 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:20,703 - INFO - ==================================================
2026-02-10 01:27:20,706 - INFO -   [탐색 60] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:20,708 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:20,708 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:21,203 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:21,203 - INFO - ==================================================
2026-02-10 01:27:21,206 - INFO -   [탐색 61] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:21,208 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:21,209 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:21,739 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:21,739 - INFO - ==================================================
2026-02-10 01:27:21,743 - INFO -   [탐색 62] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:21,746 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:21,746 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:22,327 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:22,331 - INFO - ==================================================
2026-02-10 01:27:22,334 - INFO -   [탐색 63] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:22,337 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:22,337 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:23,238 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:23,238 - INFO - ==================================================
2026-02-10 01:27:23,241 - INFO -   [탐색 64] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:23,243 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:23,243 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:23,776 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:23,776 - INFO - ==================================================
2026-02-10 01:27:23,779 - INFO -   [탐색 65] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:23,781 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:23,781 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:24,317 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:24,317 - INFO - ==================================================
2026-02-10 01:27:24,320 - INFO -   [탐색 66] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:24,322 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:24,322 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:24,877 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:24,877 - INFO - ==================================================
2026-02-10 01:27:24,880 - INFO -   [탐색 67] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:24,882 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:24,882 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:25,340 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:25,340 - INFO - ==================================================
2026-02-10 01:27:25,342 - INFO -   [탐색 68] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:25,344 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:25,344 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:25,868 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:25,869 - INFO - ==================================================
2026-02-10 01:27:25,871 - INFO -   [탐색 69] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:25,873 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:25,874 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:26,758 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:26,758 - INFO - ==================================================
2026-02-10 01:27:26,761 - INFO -   [탐색 70] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:26,764 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:26,764 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:27,310 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:27,310 - INFO - ==================================================
2026-02-10 01:27:27,314 - INFO -   [탐색 71] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:27,317 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:27,317 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:27,895 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:27,896 - INFO - ==================================================
2026-02-10 01:27:27,899 - INFO -   [탐색 72] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:27,901 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:27,902 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:28,451 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:28,452 - INFO - ==================================================
2026-02-10 01:27:28,454 - INFO -   [탐색 73] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:28,457 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:28,457 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:29,020 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:29,021 - INFO - ==================================================
2026-02-10 01:27:29,024 - INFO -   [탐색 74] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:29,027 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:29,027 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:29,559 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:29,559 - INFO - ==================================================
2026-02-10 01:27:29,562 - INFO -   [탐색 75] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:29,565 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:29,565 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:30,115 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:30,115 - INFO - ==================================================
2026-02-10 01:27:30,119 - INFO -   [탐색 76] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:30,121 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:30,121 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:31,024 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:31,025 - INFO - ==================================================
2026-02-10 01:27:31,027 - INFO -   [탐색 77] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:31,030 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:31,030 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:31,575 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:31,575 - INFO - ==================================================
2026-02-10 01:27:31,578 - INFO -   [탐색 78] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:31,581 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:31,581 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:31,909 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:31,909 - INFO - ==================================================
2026-02-10 01:27:31,911 - INFO -   [탐색 79] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:31,913 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:31,913 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:32,446 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:32,446 - INFO - ==================================================
2026-02-10 01:27:32,450 - INFO -   [탐색 80] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:32,453 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:32,453 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:33,032 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:33,032 - INFO - ==================================================
2026-02-10 01:27:33,035 - INFO -   [탐색 81] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:33,037 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:33,037 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:33,561 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:33,561 - INFO - ==================================================
2026-02-10 01:27:33,563 - INFO -   [탐색 82] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:33,566 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:33,566 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,397 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:34,397 - INFO - ==================================================
2026-02-10 01:27:34,400 - INFO -   [탐색 83] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:34,402 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:34,402 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:34,903 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:34,903 - INFO - ==================================================
2026-02-10 01:27:34,906 - INFO -   [탐색 84] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:34,908 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:34,908 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,254 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:35,254 - INFO - ==================================================
2026-02-10 01:27:35,256 - INFO -   [탐색 85] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:35,257 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:35,257 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:35,660 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:35,660 - INFO - ==================================================
2026-02-10 01:27:35,663 - INFO -   [탐색 86] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:35,665 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:35,665 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,134 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:36,134 - INFO - ==================================================
2026-02-10 01:27:36,137 - INFO -   [탐색 87] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:36,139 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:36,140 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:36,640 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:36,640 - INFO - ==================================================
2026-02-10 01:27:36,644 - INFO -   [탐색 88] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:36,646 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:36,646 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:37,175 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:37,176 - INFO - ==================================================
2026-02-10 01:27:37,179 - INFO -   [탐색 89] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:37,181 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:37,181 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,006 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:38,007 - INFO - ==================================================
2026-02-10 01:27:38,009 - INFO -   [탐색 90] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:38,011 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:38,011 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,497 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:38,498 - INFO - ==================================================
2026-02-10 01:27:38,501 - INFO -   [탐색 91] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:38,503 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:38,504 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:38,960 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:38,960 - INFO - ==================================================
2026-02-10 01:27:38,963 - INFO -   [탐색 92] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:38,965 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:38,965 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:39,446 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:39,446 - INFO - ==================================================
2026-02-10 01:27:39,448 - INFO -   [탐색 93] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:39,450 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:39,450 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:39,987 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:39,987 - INFO - ==================================================
2026-02-10 01:27:39,990 - INFO -   [탐색 94] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:39,993 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:39,993 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:40,508 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:40,509 - INFO - ==================================================
2026-02-10 01:27:40,512 - INFO -   [탐색 95] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:40,514 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:40,514 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,403 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:41,404 - INFO - ==================================================
2026-02-10 01:27:41,406 - INFO -   [탐색 96] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:41,407 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:41,407 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:41,770 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:41,770 - INFO - ==================================================
2026-02-10 01:27:41,773 - INFO -   [탐색 97] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:41,776 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:41,776 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,302 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:42,302 - INFO - ==================================================
2026-02-10 01:27:42,305 - INFO -   [탐색 98] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:42,308 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:42,308 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:42,840 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:42,840 - INFO - ==================================================
2026-02-10 01:27:42,843 - INFO -   [탐색 99] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:42,846 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:42,846 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,368 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:27:43,368 - INFO - ==================================================
2026-02-10 01:27:43,371 - INFO -   [탐색 100] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:27:43,371 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.9214 입니다.
2026-02-10 01:27:43,371 - INFO - ================================================================================
2026-02-10 01:27:43,372 - INFO - 계산된 Pruning 정보(희소도: 0.9214)를 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:27:43,375 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:27:43,380 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:27:43,380 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:27:43,758 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-02-10 01:27:43,758 - INFO - ==================================================
2026-02-10 01:27:43,758 - INFO - ==================================================
2026-02-10 01:27:43,758 - INFO - 모델 파라미터 수:
2026-02-10 01:27:43,758 - INFO -   - 총 파라미터: 57,792 개
2026-02-10 01:27:43,758 - INFO -   - 학습 가능한 파라미터: 57,792 개
2026-02-10 01:27:43,763 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:27:43,768 - INFO - FLOPs가 2.8696 GFLOPs에서 0.1854 GFLOPs로 감소했습니다 (감소율: 93.54%).
2026-02-10 01:27:43,768 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:27:43,768 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:27:43,768 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:27:43,768 - INFO - ==================================================
2026-02-10 01:27:43,768 - INFO - train 모드를 시작합니다.
2026-02-10 01:27:43,768 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:27:43,768 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:27:43,768 - INFO - --------------------------------------------------
2026-02-10 01:27:43,768 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:27:47,743 - INFO - [Train] [11/90] | Loss: 0.5905 | Train Acc: 73.07%
2026-02-10 01:27:48,511 - INFO - [Valid] [11/90] | Loss: 0.5525 | Val Acc: 74.34%
2026-02-10 01:27:48,516 - INFO - [Metrics for 'abnormal'] | Precision: 0.7035 | Recall: 0.7707 | F1: 0.7356
2026-02-10 01:27:48,516 - INFO - [Metrics for 'normal'] | Precision: 0.7844 | Recall: 0.7198 | F1: 0.7507
2026-02-10 01:27:48,523 - INFO - [Best Model Saved] (val loss: 0.5525) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:27:48,523 - INFO - --------------------------------------------------
2026-02-10 01:27:48,523 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:27:51,906 - INFO - [Train] [12/90] | Loss: 0.5135 | Train Acc: 81.18%
2026-02-10 01:27:52,781 - INFO - [Valid] [12/90] | Loss: 0.5642 | Val Acc: 74.04%
2026-02-10 01:27:52,785 - INFO - [Metrics for 'abnormal'] | Precision: 0.6751 | Recall: 0.8471 | F1: 0.7514
2026-02-10 01:27:52,785 - INFO - [Metrics for 'normal'] | Precision: 0.8310 | Recall: 0.6484 | F1: 0.7284
2026-02-10 01:27:52,786 - INFO - --------------------------------------------------
2026-02-10 01:27:52,786 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:27:55,790 - INFO - [Train] [13/90] | Loss: 0.5084 | Train Acc: 80.95%
2026-02-10 01:27:57,076 - INFO - [Valid] [13/90] | Loss: 0.5449 | Val Acc: 74.34%
2026-02-10 01:27:57,081 - INFO - [Metrics for 'abnormal'] | Precision: 0.7188 | Recall: 0.7325 | F1: 0.7256
2026-02-10 01:27:57,081 - INFO - [Metrics for 'normal'] | Precision: 0.7654 | Recall: 0.7527 | F1: 0.7590
2026-02-10 01:27:57,085 - INFO - [Best Model Saved] (val loss: 0.5449) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:27:57,085 - INFO - --------------------------------------------------
2026-02-10 01:27:57,086 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:28:01,919 - INFO - [Train] [14/90] | Loss: 0.4971 | Train Acc: 80.80%
2026-02-10 01:28:02,902 - INFO - [Valid] [14/90] | Loss: 0.5412 | Val Acc: 75.52%
2026-02-10 01:28:02,912 - INFO - [Metrics for 'abnormal'] | Precision: 0.6989 | Recall: 0.8280 | F1: 0.7580
2026-02-10 01:28:02,912 - INFO - [Metrics for 'normal'] | Precision: 0.8235 | Recall: 0.6923 | F1: 0.7522
2026-02-10 01:28:02,918 - INFO - [Best Model Saved] (val loss: 0.5412) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:28:02,918 - INFO - --------------------------------------------------
2026-02-10 01:28:02,919 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:28:08,018 - INFO - [Train] [15/90] | Loss: 0.4876 | Train Acc: 82.66%
2026-02-10 01:28:08,925 - INFO - [Valid] [15/90] | Loss: 0.5452 | Val Acc: 76.11%
2026-02-10 01:28:08,930 - INFO - [Metrics for 'abnormal'] | Precision: 0.6939 | Recall: 0.8662 | F1: 0.7705
2026-02-10 01:28:08,930 - INFO - [Metrics for 'normal'] | Precision: 0.8531 | Recall: 0.6703 | F1: 0.7508
2026-02-10 01:28:08,932 - INFO - --------------------------------------------------
2026-02-10 01:28:08,932 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:28:13,588 - INFO - [Train] [16/90] | Loss: 0.4857 | Train Acc: 83.26%
2026-02-10 01:28:14,599 - INFO - [Valid] [16/90] | Loss: 0.5259 | Val Acc: 76.99%
2026-02-10 01:28:14,607 - INFO - [Metrics for 'abnormal'] | Precision: 0.7257 | Recall: 0.8089 | F1: 0.7651
2026-02-10 01:28:14,607 - INFO - [Metrics for 'normal'] | Precision: 0.8171 | Recall: 0.7363 | F1: 0.7746
2026-02-10 01:28:14,611 - INFO - [Best Model Saved] (val loss: 0.5259) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:28:14,611 - INFO - --------------------------------------------------
2026-02-10 01:28:14,611 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:28:19,340 - INFO - [Train] [17/90] | Loss: 0.4807 | Train Acc: 82.89%
2026-02-10 01:28:20,617 - INFO - [Valid] [17/90] | Loss: 0.5115 | Val Acc: 78.76%
2026-02-10 01:28:20,621 - INFO - [Metrics for 'abnormal'] | Precision: 0.7607 | Recall: 0.7898 | F1: 0.7750
2026-02-10 01:28:20,622 - INFO - [Metrics for 'normal'] | Precision: 0.8125 | Recall: 0.7857 | F1: 0.7989
2026-02-10 01:28:20,625 - INFO - [Best Model Saved] (val loss: 0.5115) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:28:20,625 - INFO - --------------------------------------------------
2026-02-10 01:28:20,625 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:28:24,460 - INFO - [Train] [18/90] | Loss: 0.4730 | Train Acc: 84.15%
2026-02-10 01:28:25,782 - INFO - [Valid] [18/90] | Loss: 0.5143 | Val Acc: 78.17%
2026-02-10 01:28:25,788 - INFO - [Metrics for 'abnormal'] | Precision: 0.7515 | Recall: 0.7898 | F1: 0.7702
2026-02-10 01:28:25,788 - INFO - [Metrics for 'normal'] | Precision: 0.8103 | Recall: 0.7747 | F1: 0.7921
2026-02-10 01:28:25,790 - INFO - --------------------------------------------------
2026-02-10 01:28:25,790 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:28:29,515 - INFO - [Train] [19/90] | Loss: 0.4606 | Train Acc: 84.08%
2026-02-10 01:28:30,833 - INFO - [Valid] [19/90] | Loss: 0.5249 | Val Acc: 76.70%
2026-02-10 01:28:30,841 - INFO - [Metrics for 'abnormal'] | Precision: 0.7566 | Recall: 0.7325 | F1: 0.7443
2026-02-10 01:28:30,845 - INFO - [Metrics for 'normal'] | Precision: 0.7754 | Recall: 0.7967 | F1: 0.7859
2026-02-10 01:28:30,848 - INFO - --------------------------------------------------
2026-02-10 01:28:30,848 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:28:35,208 - INFO - [Train] [20/90] | Loss: 0.4606 | Train Acc: 84.45%
2026-02-10 01:28:36,207 - INFO - [Valid] [20/90] | Loss: 0.5449 | Val Acc: 76.70%
2026-02-10 01:28:36,212 - INFO - [Metrics for 'abnormal'] | Precision: 0.7097 | Recall: 0.8408 | F1: 0.7697
2026-02-10 01:28:36,217 - INFO - [Metrics for 'normal'] | Precision: 0.8366 | Recall: 0.7033 | F1: 0.7642
2026-02-10 01:28:36,220 - INFO - --------------------------------------------------
2026-02-10 01:28:36,221 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:28:42,020 - INFO - [Train] [21/90] | Loss: 0.4596 | Train Acc: 83.18%
2026-02-10 01:28:43,422 - INFO - [Valid] [21/90] | Loss: 0.5059 | Val Acc: 79.94%
2026-02-10 01:28:43,427 - INFO - [Metrics for 'abnormal'] | Precision: 0.7799 | Recall: 0.7898 | F1: 0.7848
2026-02-10 01:28:43,427 - INFO - [Metrics for 'normal'] | Precision: 0.8167 | Recall: 0.8077 | F1: 0.8122
2026-02-10 01:28:43,431 - INFO - [Best Model Saved] (val loss: 0.5059) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:28:43,431 - INFO - --------------------------------------------------
2026-02-10 01:28:43,431 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:28:48,951 - INFO - [Train] [22/90] | Loss: 0.4568 | Train Acc: 85.12%
2026-02-10 01:28:50,312 - INFO - [Valid] [22/90] | Loss: 0.5213 | Val Acc: 79.06%
2026-02-10 01:28:50,331 - INFO - [Metrics for 'abnormal'] | Precision: 0.7560 | Recall: 0.8089 | F1: 0.7815
2026-02-10 01:28:50,332 - INFO - [Metrics for 'normal'] | Precision: 0.8246 | Recall: 0.7747 | F1: 0.7989
2026-02-10 01:28:50,333 - INFO - --------------------------------------------------
2026-02-10 01:28:50,334 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:28:55,333 - INFO - [Train] [23/90] | Loss: 0.4605 | Train Acc: 85.12%
2026-02-10 01:28:57,037 - INFO - [Valid] [23/90] | Loss: 0.5190 | Val Acc: 78.17%
2026-02-10 01:28:57,043 - INFO - [Metrics for 'abnormal'] | Precision: 0.7456 | Recall: 0.8025 | F1: 0.7730
2026-02-10 01:28:57,043 - INFO - [Metrics for 'normal'] | Precision: 0.8176 | Recall: 0.7637 | F1: 0.7898
2026-02-10 01:28:57,047 - INFO - --------------------------------------------------
2026-02-10 01:28:57,047 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:29:01,602 - INFO - [Train] [24/90] | Loss: 0.4431 | Train Acc: 85.42%
2026-02-10 01:29:03,252 - INFO - [Valid] [24/90] | Loss: 0.5344 | Val Acc: 78.17%
2026-02-10 01:29:03,261 - INFO - [Metrics for 'abnormal'] | Precision: 0.7196 | Recall: 0.8662 | F1: 0.7861
2026-02-10 01:29:03,261 - INFO - [Metrics for 'normal'] | Precision: 0.8600 | Recall: 0.7088 | F1: 0.7771
2026-02-10 01:29:03,264 - INFO - --------------------------------------------------
2026-02-10 01:29:03,264 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:29:08,734 - INFO - [Train] [25/90] | Loss: 0.4438 | Train Acc: 87.05%
2026-02-10 01:29:09,877 - INFO - [Valid] [25/90] | Loss: 0.5383 | Val Acc: 79.65%
2026-02-10 01:29:09,882 - INFO - [Metrics for 'abnormal'] | Precision: 0.7750 | Recall: 0.7898 | F1: 0.7823
2026-02-10 01:29:09,883 - INFO - [Metrics for 'normal'] | Precision: 0.8156 | Recall: 0.8022 | F1: 0.8089
2026-02-10 01:29:09,884 - INFO - --------------------------------------------------
2026-02-10 01:29:09,886 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:29:15,441 - INFO - [Train] [26/90] | Loss: 0.4252 | Train Acc: 86.76%
2026-02-10 01:29:16,879 - INFO - [Valid] [26/90] | Loss: 0.5303 | Val Acc: 78.17%
2026-02-10 01:29:16,889 - INFO - [Metrics for 'abnormal'] | Precision: 0.7243 | Recall: 0.8535 | F1: 0.7836
2026-02-10 01:29:16,890 - INFO - [Metrics for 'normal'] | Precision: 0.8506 | Recall: 0.7198 | F1: 0.7798
2026-02-10 01:29:16,892 - INFO - --------------------------------------------------
2026-02-10 01:29:16,896 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 01:29:22,077 - INFO - [Train] [27/90] | Loss: 0.4273 | Train Acc: 85.86%
2026-02-10 01:29:23,618 - INFO - [Valid] [27/90] | Loss: 0.5145 | Val Acc: 80.24%
2026-02-10 01:29:23,623 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7643 | F1: 0.7818
2026-02-10 01:29:23,623 - INFO - [Metrics for 'normal'] | Precision: 0.8042 | Recall: 0.8352 | F1: 0.8194
2026-02-10 01:29:23,625 - INFO - --------------------------------------------------
2026-02-10 01:29:23,625 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 01:29:28,836 - INFO - [Train] [28/90] | Loss: 0.4274 | Train Acc: 86.09%
2026-02-10 01:29:30,099 - INFO - [Valid] [28/90] | Loss: 0.5236 | Val Acc: 78.76%
2026-02-10 01:29:30,104 - INFO - [Metrics for 'abnormal'] | Precision: 0.7374 | Recall: 0.8408 | F1: 0.7857
2026-02-10 01:29:30,104 - INFO - [Metrics for 'normal'] | Precision: 0.8438 | Recall: 0.7418 | F1: 0.7895
2026-02-10 01:29:30,106 - INFO - --------------------------------------------------
2026-02-10 01:29:30,106 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 01:29:35,893 - INFO - [Train] [29/90] | Loss: 0.4135 | Train Acc: 87.35%
2026-02-10 01:29:37,297 - INFO - [Valid] [29/90] | Loss: 0.5026 | Val Acc: 80.24%
2026-02-10 01:29:37,305 - INFO - [Metrics for 'abnormal'] | Precision: 0.7647 | Recall: 0.8280 | F1: 0.7951
2026-02-10 01:29:37,306 - INFO - [Metrics for 'normal'] | Precision: 0.8402 | Recall: 0.7802 | F1: 0.8091
2026-02-10 01:29:37,315 - INFO - [Best Model Saved] (val loss: 0.5026) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:29:37,315 - INFO - --------------------------------------------------
2026-02-10 01:29:37,315 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 01:29:43,298 - INFO - [Train] [30/90] | Loss: 0.4172 | Train Acc: 87.28%
2026-02-10 01:29:44,140 - INFO - [Valid] [30/90] | Loss: 0.5002 | Val Acc: 78.76%
2026-02-10 01:29:44,145 - INFO - [Metrics for 'abnormal'] | Precision: 0.7485 | Recall: 0.8153 | F1: 0.7805
2026-02-10 01:29:44,146 - INFO - [Metrics for 'normal'] | Precision: 0.8274 | Recall: 0.7637 | F1: 0.7943
2026-02-10 01:29:44,151 - INFO - [Best Model Saved] (val loss: 0.5002) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:29:44,151 - INFO - --------------------------------------------------
2026-02-10 01:29:44,152 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 01:29:50,404 - INFO - [Train] [31/90] | Loss: 0.4220 | Train Acc: 87.57%
2026-02-10 01:29:51,318 - INFO - [Valid] [31/90] | Loss: 0.5115 | Val Acc: 79.94%
2026-02-10 01:29:51,322 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.7580 | F1: 0.7778
2026-02-10 01:29:51,323 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.8352 | F1: 0.8172
2026-02-10 01:29:51,325 - INFO - --------------------------------------------------
2026-02-10 01:29:51,325 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 01:29:56,707 - INFO - [Train] [32/90] | Loss: 0.4109 | Train Acc: 87.95%
2026-02-10 01:29:58,196 - INFO - [Valid] [32/90] | Loss: 0.5020 | Val Acc: 79.65%
2026-02-10 01:29:58,207 - INFO - [Metrics for 'abnormal'] | Precision: 0.7683 | Recall: 0.8025 | F1: 0.7850
2026-02-10 01:29:58,208 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.7912 | F1: 0.8067
2026-02-10 01:29:58,209 - INFO - --------------------------------------------------
2026-02-10 01:29:58,210 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 01:30:03,205 - INFO - [Train] [33/90] | Loss: 0.4118 | Train Acc: 87.57%
2026-02-10 01:30:04,721 - INFO - [Valid] [33/90] | Loss: 0.5047 | Val Acc: 81.12%
2026-02-10 01:30:04,731 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 01:30:04,732 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 01:30:04,733 - INFO - --------------------------------------------------
2026-02-10 01:30:04,733 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 01:30:10,033 - INFO - [Train] [34/90] | Loss: 0.4076 | Train Acc: 88.76%
2026-02-10 01:30:11,581 - INFO - [Valid] [34/90] | Loss: 0.5127 | Val Acc: 81.71%
2026-02-10 01:30:11,592 - INFO - [Metrics for 'abnormal'] | Precision: 0.7950 | Recall: 0.8153 | F1: 0.8050
2026-02-10 01:30:11,592 - INFO - [Metrics for 'normal'] | Precision: 0.8371 | Recall: 0.8187 | F1: 0.8278
2026-02-10 01:30:11,594 - INFO - --------------------------------------------------
2026-02-10 01:30:11,594 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 01:30:17,177 - INFO - [Train] [35/90] | Loss: 0.4005 | Train Acc: 88.62%
2026-02-10 01:30:18,561 - INFO - [Valid] [35/90] | Loss: 0.5127 | Val Acc: 79.94%
2026-02-10 01:30:18,566 - INFO - [Metrics for 'abnormal'] | Precision: 0.7633 | Recall: 0.8217 | F1: 0.7914
2026-02-10 01:30:18,566 - INFO - [Metrics for 'normal'] | Precision: 0.8353 | Recall: 0.7802 | F1: 0.8068
2026-02-10 01:30:18,567 - INFO - --------------------------------------------------
2026-02-10 01:30:18,568 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 01:30:24,170 - INFO - [Train] [36/90] | Loss: 0.4100 | Train Acc: 87.13%
2026-02-10 01:30:25,647 - INFO - [Valid] [36/90] | Loss: 0.4987 | Val Acc: 80.24%
2026-02-10 01:30:25,651 - INFO - [Metrics for 'abnormal'] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-02-10 01:30:25,651 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-02-10 01:30:25,656 - INFO - [Best Model Saved] (val loss: 0.4987) -> 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:30:25,656 - INFO - --------------------------------------------------
2026-02-10 01:30:25,656 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 01:30:30,796 - INFO - [Train] [37/90] | Loss: 0.3911 | Train Acc: 89.43%
2026-02-10 01:30:32,430 - INFO - [Valid] [37/90] | Loss: 0.5332 | Val Acc: 78.76%
2026-02-10 01:30:32,435 - INFO - [Metrics for 'abnormal'] | Precision: 0.7297 | Recall: 0.8599 | F1: 0.7895
2026-02-10 01:30:32,438 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.7253 | F1: 0.7857
2026-02-10 01:30:32,440 - INFO - --------------------------------------------------
2026-02-10 01:30:32,441 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 01:30:37,559 - INFO - [Train] [38/90] | Loss: 0.3929 | Train Acc: 89.81%
2026-02-10 01:30:38,990 - INFO - [Valid] [38/90] | Loss: 0.5113 | Val Acc: 81.12%
2026-02-10 01:30:38,994 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-02-10 01:30:38,995 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-02-10 01:30:38,998 - INFO - --------------------------------------------------
2026-02-10 01:30:38,998 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 01:30:44,167 - INFO - [Train] [39/90] | Loss: 0.3895 | Train Acc: 90.03%
2026-02-10 01:30:45,736 - INFO - [Valid] [39/90] | Loss: 0.5218 | Val Acc: 80.24%
2026-02-10 01:30:45,741 - INFO - [Metrics for 'abnormal'] | Precision: 0.7528 | Recall: 0.8535 | F1: 0.8000
2026-02-10 01:30:45,742 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.7582 | F1: 0.8047
2026-02-10 01:30:45,743 - INFO - --------------------------------------------------
2026-02-10 01:30:45,744 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 01:30:51,509 - INFO - [Train] [40/90] | Loss: 0.3911 | Train Acc: 89.14%
2026-02-10 01:30:52,773 - INFO - [Valid] [40/90] | Loss: 0.5276 | Val Acc: 79.65%
2026-02-10 01:30:52,784 - INFO - [Metrics for 'abnormal'] | Precision: 0.7933 | Recall: 0.7580 | F1: 0.7752
2026-02-10 01:30:52,785 - INFO - [Metrics for 'normal'] | Precision: 0.7989 | Recall: 0.8297 | F1: 0.8140
2026-02-10 01:30:52,787 - INFO - --------------------------------------------------
2026-02-10 01:30:52,787 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 01:30:58,336 - INFO - [Train] [41/90] | Loss: 0.3913 | Train Acc: 89.73%
2026-02-10 01:30:59,704 - INFO - [Valid] [41/90] | Loss: 0.5323 | Val Acc: 80.83%
2026-02-10 01:30:59,713 - INFO - [Metrics for 'abnormal'] | Precision: 0.8067 | Recall: 0.7707 | F1: 0.7883
2026-02-10 01:30:59,713 - INFO - [Metrics for 'normal'] | Precision: 0.8095 | Recall: 0.8407 | F1: 0.8248
2026-02-10 01:30:59,715 - INFO - --------------------------------------------------
2026-02-10 01:30:59,715 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 01:31:04,775 - INFO - [Train] [42/90] | Loss: 0.3869 | Train Acc: 90.10%
2026-02-10 01:31:06,320 - INFO - [Valid] [42/90] | Loss: 0.5234 | Val Acc: 79.65%
2026-02-10 01:31:06,329 - INFO - [Metrics for 'abnormal'] | Precision: 0.7391 | Recall: 0.8662 | F1: 0.7977
2026-02-10 01:31:06,329 - INFO - [Metrics for 'normal'] | Precision: 0.8645 | Recall: 0.7363 | F1: 0.7953
2026-02-10 01:31:06,330 - INFO - --------------------------------------------------
2026-02-10 01:31:06,331 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 01:31:10,666 - INFO - [Train] [43/90] | Loss: 0.3772 | Train Acc: 90.48%
2026-02-10 01:31:12,245 - INFO - [Valid] [43/90] | Loss: 0.5227 | Val Acc: 80.24%
2026-02-10 01:31:12,253 - INFO - [Metrics for 'abnormal'] | Precision: 0.7616 | Recall: 0.8344 | F1: 0.7964
2026-02-10 01:31:12,253 - INFO - [Metrics for 'normal'] | Precision: 0.8443 | Recall: 0.7747 | F1: 0.8080
2026-02-10 01:31:12,255 - INFO - --------------------------------------------------
2026-02-10 01:31:12,255 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 01:31:17,438 - INFO - [Train] [44/90] | Loss: 0.3682 | Train Acc: 90.40%
2026-02-10 01:31:18,707 - INFO - [Valid] [44/90] | Loss: 0.5327 | Val Acc: 80.53%
2026-02-10 01:31:18,715 - INFO - [Metrics for 'abnormal'] | Precision: 0.7571 | Recall: 0.8535 | F1: 0.8024
2026-02-10 01:31:18,715 - INFO - [Metrics for 'normal'] | Precision: 0.8580 | Recall: 0.7637 | F1: 0.8081
2026-02-10 01:31:18,718 - INFO - --------------------------------------------------
2026-02-10 01:31:18,718 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 01:31:24,443 - INFO - [Train] [45/90] | Loss: 0.3697 | Train Acc: 91.15%
2026-02-10 01:31:25,875 - INFO - [Valid] [45/90] | Loss: 0.5245 | Val Acc: 80.83%
2026-02-10 01:31:25,880 - INFO - [Metrics for 'abnormal'] | Precision: 0.8067 | Recall: 0.7707 | F1: 0.7883
2026-02-10 01:31:25,881 - INFO - [Metrics for 'normal'] | Precision: 0.8095 | Recall: 0.8407 | F1: 0.8248
2026-02-10 01:31:25,882 - INFO - --------------------------------------------------
2026-02-10 01:31:25,882 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 01:31:30,492 - INFO - [Train] [46/90] | Loss: 0.3682 | Train Acc: 90.62%
2026-02-10 01:31:31,825 - INFO - [Valid] [46/90] | Loss: 0.5483 | Val Acc: 78.76%
2026-02-10 01:31:31,830 - INFO - [Metrics for 'abnormal'] | Precision: 0.7297 | Recall: 0.8599 | F1: 0.7895
2026-02-10 01:31:31,832 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.7253 | F1: 0.7857
2026-02-10 01:31:31,834 - INFO - --------------------------------------------------
2026-02-10 01:31:31,834 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 01:31:36,258 - INFO - [Train] [47/90] | Loss: 0.3529 | Train Acc: 92.11%
2026-02-10 01:31:37,675 - INFO - [Valid] [47/90] | Loss: 0.5514 | Val Acc: 78.47%
2026-02-10 01:31:37,684 - INFO - [Metrics for 'abnormal'] | Precision: 0.7386 | Recall: 0.8280 | F1: 0.7808
2026-02-10 01:31:37,684 - INFO - [Metrics for 'normal'] | Precision: 0.8344 | Recall: 0.7473 | F1: 0.7884
2026-02-10 01:31:37,686 - INFO - --------------------------------------------------
2026-02-10 01:31:37,686 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 01:31:42,975 - INFO - [Train] [48/90] | Loss: 0.3641 | Train Acc: 91.37%
2026-02-10 01:31:44,417 - INFO - [Valid] [48/90] | Loss: 0.5400 | Val Acc: 80.24%
2026-02-10 01:31:44,423 - INFO - [Metrics for 'abnormal'] | Precision: 0.7647 | Recall: 0.8280 | F1: 0.7951
2026-02-10 01:31:44,424 - INFO - [Metrics for 'normal'] | Precision: 0.8402 | Recall: 0.7802 | F1: 0.8091
2026-02-10 01:31:44,425 - INFO - --------------------------------------------------
2026-02-10 01:31:44,429 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 01:31:49,722 - INFO - [Train] [49/90] | Loss: 0.3671 | Train Acc: 91.52%
2026-02-10 01:31:50,959 - INFO - [Valid] [49/90] | Loss: 0.5156 | Val Acc: 82.01%
2026-02-10 01:31:50,968 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 01:31:50,968 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 01:31:50,969 - INFO - --------------------------------------------------
2026-02-10 01:31:50,969 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 01:31:56,284 - INFO - [Train] [50/90] | Loss: 0.3701 | Train Acc: 91.52%
2026-02-10 01:31:57,563 - INFO - [Valid] [50/90] | Loss: 0.5132 | Val Acc: 81.42%
2026-02-10 01:31:57,570 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-02-10 01:31:57,571 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-02-10 01:31:57,572 - INFO - --------------------------------------------------
2026-02-10 01:31:57,572 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 01:32:01,883 - INFO - [Train] [51/90] | Loss: 0.3660 | Train Acc: 90.70%
2026-02-10 01:32:02,891 - INFO - [Valid] [51/90] | Loss: 0.5245 | Val Acc: 79.94%
2026-02-10 01:32:02,896 - INFO - [Metrics for 'abnormal'] | Precision: 0.7459 | Recall: 0.8599 | F1: 0.7988
2026-02-10 01:32:02,897 - INFO - [Metrics for 'normal'] | Precision: 0.8608 | Recall: 0.7473 | F1: 0.8000
2026-02-10 01:32:02,899 - INFO - --------------------------------------------------
2026-02-10 01:32:02,899 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 01:32:07,565 - INFO - [Train] [52/90] | Loss: 0.3595 | Train Acc: 91.29%
2026-02-10 01:32:08,214 - INFO - [Valid] [52/90] | Loss: 0.5206 | Val Acc: 79.94%
2026-02-10 01:32:08,221 - INFO - [Metrics for 'abnormal'] | Precision: 0.7543 | Recall: 0.8408 | F1: 0.7952
2026-02-10 01:32:08,225 - INFO - [Metrics for 'normal'] | Precision: 0.8476 | Recall: 0.7637 | F1: 0.8035
2026-02-10 01:32:08,228 - INFO - --------------------------------------------------
2026-02-10 01:32:08,229 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 01:32:12,898 - INFO - [Train] [53/90] | Loss: 0.3595 | Train Acc: 90.77%
2026-02-10 01:32:13,887 - INFO - [Valid] [53/90] | Loss: 0.5283 | Val Acc: 79.35%
2026-02-10 01:32:13,891 - INFO - [Metrics for 'abnormal'] | Precision: 0.7544 | Recall: 0.8217 | F1: 0.7866
2026-02-10 01:32:13,891 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7692 | F1: 0.8000
2026-02-10 01:32:13,893 - INFO - --------------------------------------------------
2026-02-10 01:32:13,893 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 01:32:18,755 - INFO - [Train] [54/90] | Loss: 0.3510 | Train Acc: 91.89%
2026-02-10 01:32:20,219 - INFO - [Valid] [54/90] | Loss: 0.5104 | Val Acc: 80.53%
2026-02-10 01:32:20,224 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.8471 | F1: 0.8012
2026-02-10 01:32:20,225 - INFO - [Metrics for 'normal'] | Precision: 0.8537 | Recall: 0.7692 | F1: 0.8092
2026-02-10 01:32:20,226 - INFO - --------------------------------------------------
2026-02-10 01:32:20,226 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 01:32:25,657 - INFO - [Train] [55/90] | Loss: 0.3571 | Train Acc: 92.49%
2026-02-10 01:32:27,266 - INFO - [Valid] [55/90] | Loss: 0.5261 | Val Acc: 79.94%
2026-02-10 01:32:27,271 - INFO - [Metrics for 'abnormal'] | Precision: 0.7543 | Recall: 0.8408 | F1: 0.7952
2026-02-10 01:32:27,272 - INFO - [Metrics for 'normal'] | Precision: 0.8476 | Recall: 0.7637 | F1: 0.8035
2026-02-10 01:32:27,273 - INFO - --------------------------------------------------
2026-02-10 01:32:27,273 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 01:32:32,025 - INFO - [Train] [56/90] | Loss: 0.3511 | Train Acc: 92.49%
2026-02-10 01:32:33,547 - INFO - [Valid] [56/90] | Loss: 0.5202 | Val Acc: 80.83%
2026-02-10 01:32:33,555 - INFO - [Metrics for 'abnormal'] | Precision: 0.7738 | Recall: 0.8280 | F1: 0.8000
2026-02-10 01:32:33,555 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.7912 | F1: 0.8159
2026-02-10 01:32:33,557 - INFO - --------------------------------------------------
2026-02-10 01:32:33,557 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 01:32:39,136 - INFO - [Train] [57/90] | Loss: 0.3526 | Train Acc: 91.96%
2026-02-10 01:32:40,575 - INFO - [Valid] [57/90] | Loss: 0.5225 | Val Acc: 80.53%
2026-02-10 01:32:40,579 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-02-10 01:32:40,580 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-02-10 01:32:40,582 - INFO - --------------------------------------------------
2026-02-10 01:32:40,582 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 01:32:46,402 - INFO - [Train] [58/90] | Loss: 0.3436 | Train Acc: 92.86%
2026-02-10 01:32:47,750 - INFO - [Valid] [58/90] | Loss: 0.5168 | Val Acc: 80.53%
2026-02-10 01:32:47,757 - INFO - [Metrics for 'abnormal'] | Precision: 0.7661 | Recall: 0.8344 | F1: 0.7988
2026-02-10 01:32:47,761 - INFO - [Metrics for 'normal'] | Precision: 0.8452 | Recall: 0.7802 | F1: 0.8114
2026-02-10 01:32:47,764 - INFO - --------------------------------------------------
2026-02-10 01:32:47,764 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 01:32:53,499 - INFO - [Train] [59/90] | Loss: 0.3413 | Train Acc: 92.26%
2026-02-10 01:32:54,690 - INFO - [Valid] [59/90] | Loss: 0.5230 | Val Acc: 79.65%
2026-02-10 01:32:54,696 - INFO - [Metrics for 'abnormal'] | Precision: 0.7588 | Recall: 0.8217 | F1: 0.7890
2026-02-10 01:32:54,700 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.7747 | F1: 0.8034
2026-02-10 01:32:54,702 - INFO - --------------------------------------------------
2026-02-10 01:32:54,703 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 01:33:00,324 - INFO - [Train] [60/90] | Loss: 0.3379 | Train Acc: 93.53%
2026-02-10 01:33:01,686 - INFO - [Valid] [60/90] | Loss: 0.5325 | Val Acc: 80.53%
2026-02-10 01:33:01,690 - INFO - [Metrics for 'abnormal'] | Precision: 0.7630 | Recall: 0.8408 | F1: 0.8000
2026-02-10 01:33:01,690 - INFO - [Metrics for 'normal'] | Precision: 0.8494 | Recall: 0.7747 | F1: 0.8103
2026-02-10 01:33:01,692 - INFO - --------------------------------------------------
2026-02-10 01:33:01,692 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 01:33:06,899 - INFO - [Train] [61/90] | Loss: 0.3363 | Train Acc: 93.08%
2026-02-10 01:33:08,344 - INFO - [Valid] [61/90] | Loss: 0.5283 | Val Acc: 81.12%
2026-02-10 01:33:08,348 - INFO - [Metrics for 'abnormal'] | Precision: 0.7751 | Recall: 0.8344 | F1: 0.8037
2026-02-10 01:33:08,349 - INFO - [Metrics for 'normal'] | Precision: 0.8471 | Recall: 0.7912 | F1: 0.8182
2026-02-10 01:33:08,350 - INFO - --------------------------------------------------
2026-02-10 01:33:08,351 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 01:33:13,704 - INFO - [Train] [62/90] | Loss: 0.3386 | Train Acc: 93.30%
2026-02-10 01:33:15,156 - INFO - [Valid] [62/90] | Loss: 0.5466 | Val Acc: 79.94%
2026-02-10 01:33:15,162 - INFO - [Metrics for 'abnormal'] | Precision: 0.7543 | Recall: 0.8408 | F1: 0.7952
2026-02-10 01:33:15,162 - INFO - [Metrics for 'normal'] | Precision: 0.8476 | Recall: 0.7637 | F1: 0.8035
2026-02-10 01:33:15,165 - INFO - --------------------------------------------------
2026-02-10 01:33:15,165 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 01:33:20,926 - INFO - [Train] [63/90] | Loss: 0.3352 | Train Acc: 93.08%
2026-02-10 01:33:22,356 - INFO - [Valid] [63/90] | Loss: 0.5377 | Val Acc: 81.71%
2026-02-10 01:33:22,369 - INFO - [Metrics for 'abnormal'] | Precision: 0.7844 | Recall: 0.8344 | F1: 0.8086
2026-02-10 01:33:22,370 - INFO - [Metrics for 'normal'] | Precision: 0.8488 | Recall: 0.8022 | F1: 0.8249
2026-02-10 01:33:22,376 - INFO - --------------------------------------------------
2026-02-10 01:33:22,376 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 01:33:27,720 - INFO - [Train] [64/90] | Loss: 0.3398 | Train Acc: 92.49%
2026-02-10 01:33:29,274 - INFO - [Valid] [64/90] | Loss: 0.5515 | Val Acc: 80.24%
2026-02-10 01:33:29,278 - INFO - [Metrics for 'abnormal'] | Precision: 0.7473 | Recall: 0.8662 | F1: 0.8024
2026-02-10 01:33:29,279 - INFO - [Metrics for 'normal'] | Precision: 0.8662 | Recall: 0.7473 | F1: 0.8024
2026-02-10 01:33:29,280 - INFO - --------------------------------------------------
2026-02-10 01:33:29,280 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 01:33:33,453 - INFO - [Train] [65/90] | Loss: 0.3355 | Train Acc: 94.12%
2026-02-10 01:33:34,429 - INFO - [Valid] [65/90] | Loss: 0.5295 | Val Acc: 81.42%
2026-02-10 01:33:34,434 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-02-10 01:33:34,434 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-02-10 01:33:34,436 - INFO - --------------------------------------------------
2026-02-10 01:33:34,436 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 01:33:39,022 - INFO - [Train] [66/90] | Loss: 0.3261 | Train Acc: 93.60%
2026-02-10 01:33:39,915 - INFO - [Valid] [66/90] | Loss: 0.5493 | Val Acc: 79.06%
2026-02-10 01:33:39,919 - INFO - [Metrics for 'abnormal'] | Precision: 0.7443 | Recall: 0.8344 | F1: 0.7868
2026-02-10 01:33:39,919 - INFO - [Metrics for 'normal'] | Precision: 0.8405 | Recall: 0.7527 | F1: 0.7942
2026-02-10 01:33:39,920 - INFO - --------------------------------------------------
2026-02-10 01:33:39,921 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 01:33:43,276 - INFO - [Train] [67/90] | Loss: 0.3260 | Train Acc: 93.90%
2026-02-10 01:33:44,363 - INFO - [Valid] [67/90] | Loss: 0.5584 | Val Acc: 79.94%
2026-02-10 01:33:44,367 - INFO - [Metrics for 'abnormal'] | Precision: 0.7730 | Recall: 0.8025 | F1: 0.7875
2026-02-10 01:33:44,368 - INFO - [Metrics for 'normal'] | Precision: 0.8239 | Recall: 0.7967 | F1: 0.8101
2026-02-10 01:33:44,369 - INFO - --------------------------------------------------
2026-02-10 01:33:44,369 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 01:33:47,800 - INFO - [Train] [68/90] | Loss: 0.3225 | Train Acc: 94.20%
2026-02-10 01:33:48,940 - INFO - [Valid] [68/90] | Loss: 0.5597 | Val Acc: 79.06%
2026-02-10 01:33:48,944 - INFO - [Metrics for 'abnormal'] | Precision: 0.7590 | Recall: 0.8025 | F1: 0.7802
2026-02-10 01:33:48,944 - INFO - [Metrics for 'normal'] | Precision: 0.8208 | Recall: 0.7802 | F1: 0.8000
2026-02-10 01:33:48,947 - INFO - --------------------------------------------------
2026-02-10 01:33:48,948 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 01:33:53,328 - INFO - [Train] [69/90] | Loss: 0.3253 | Train Acc: 94.05%
2026-02-10 01:33:54,524 - INFO - [Valid] [69/90] | Loss: 0.5491 | Val Acc: 79.35%
2026-02-10 01:33:54,529 - INFO - [Metrics for 'abnormal'] | Precision: 0.7377 | Recall: 0.8599 | F1: 0.7941
2026-02-10 01:33:54,529 - INFO - [Metrics for 'normal'] | Precision: 0.8590 | Recall: 0.7363 | F1: 0.7929
2026-02-10 01:33:54,530 - INFO - --------------------------------------------------
2026-02-10 01:33:54,531 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 01:33:58,168 - INFO - [Train] [70/90] | Loss: 0.3267 | Train Acc: 94.20%
2026-02-10 01:33:59,037 - INFO - [Valid] [70/90] | Loss: 0.5471 | Val Acc: 81.42%
2026-02-10 01:33:59,041 - INFO - [Metrics for 'abnormal'] | Precision: 0.7765 | Recall: 0.8408 | F1: 0.8073
2026-02-10 01:33:59,042 - INFO - [Metrics for 'normal'] | Precision: 0.8521 | Recall: 0.7912 | F1: 0.8205
2026-02-10 01:33:59,043 - INFO - --------------------------------------------------
2026-02-10 01:33:59,043 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 01:34:02,734 - INFO - [Train] [71/90] | Loss: 0.3170 | Train Acc: 94.35%
2026-02-10 01:34:03,560 - INFO - [Valid] [71/90] | Loss: 0.5565 | Val Acc: 79.35%
2026-02-10 01:34:03,568 - INFO - [Metrics for 'abnormal'] | Precision: 0.7458 | Recall: 0.8408 | F1: 0.7904
2026-02-10 01:34:03,568 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.7527 | F1: 0.7965
2026-02-10 01:34:03,570 - INFO - --------------------------------------------------
2026-02-10 01:34:03,570 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 01:34:06,181 - INFO - [Train] [72/90] | Loss: 0.3199 | Train Acc: 94.72%
2026-02-10 01:34:07,268 - INFO - [Valid] [72/90] | Loss: 0.5541 | Val Acc: 79.65%
2026-02-10 01:34:07,273 - INFO - [Metrics for 'abnormal'] | Precision: 0.7588 | Recall: 0.8217 | F1: 0.7890
2026-02-10 01:34:07,274 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.7747 | F1: 0.8034
2026-02-10 01:34:07,275 - INFO - --------------------------------------------------
2026-02-10 01:34:07,275 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 01:34:10,660 - INFO - [Train] [73/90] | Loss: 0.3228 | Train Acc: 93.75%
2026-02-10 01:34:11,691 - INFO - [Valid] [73/90] | Loss: 0.5541 | Val Acc: 78.47%
2026-02-10 01:34:11,696 - INFO - [Metrics for 'abnormal'] | Precision: 0.7500 | Recall: 0.8025 | F1: 0.7754
2026-02-10 01:34:11,696 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.7692 | F1: 0.7932
2026-02-10 01:34:11,697 - INFO - --------------------------------------------------
2026-02-10 01:34:11,698 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 01:34:14,875 - INFO - [Train] [74/90] | Loss: 0.3177 | Train Acc: 94.42%
2026-02-10 01:34:15,846 - INFO - [Valid] [74/90] | Loss: 0.5471 | Val Acc: 79.94%
2026-02-10 01:34:15,849 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8089 | F1: 0.7888
2026-02-10 01:34:15,850 - INFO - [Metrics for 'normal'] | Precision: 0.8276 | Recall: 0.7912 | F1: 0.8090
2026-02-10 01:34:15,850 - INFO - --------------------------------------------------
2026-02-10 01:34:15,851 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 01:34:20,596 - INFO - [Train] [75/90] | Loss: 0.3254 | Train Acc: 93.38%
2026-02-10 01:34:21,676 - INFO - [Valid] [75/90] | Loss: 0.5462 | Val Acc: 79.35%
2026-02-10 01:34:21,681 - INFO - [Metrics for 'abnormal'] | Precision: 0.7574 | Recall: 0.8153 | F1: 0.7853
2026-02-10 01:34:21,681 - INFO - [Metrics for 'normal'] | Precision: 0.8294 | Recall: 0.7747 | F1: 0.8011
2026-02-10 01:34:21,683 - INFO - --------------------------------------------------
2026-02-10 01:34:21,683 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 01:34:25,642 - INFO - [Train] [76/90] | Loss: 0.3167 | Train Acc: 94.42%
2026-02-10 01:34:27,055 - INFO - [Valid] [76/90] | Loss: 0.5525 | Val Acc: 78.76%
2026-02-10 01:34:27,060 - INFO - [Metrics for 'abnormal'] | Precision: 0.7485 | Recall: 0.8153 | F1: 0.7805
2026-02-10 01:34:27,064 - INFO - [Metrics for 'normal'] | Precision: 0.8274 | Recall: 0.7637 | F1: 0.7943
2026-02-10 01:34:27,067 - INFO - --------------------------------------------------
2026-02-10 01:34:27,067 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 01:34:31,003 - INFO - [Train] [77/90] | Loss: 0.3164 | Train Acc: 94.57%
2026-02-10 01:34:32,282 - INFO - [Valid] [77/90] | Loss: 0.5510 | Val Acc: 79.35%
2026-02-10 01:34:32,292 - INFO - [Metrics for 'abnormal'] | Precision: 0.7486 | Recall: 0.8344 | F1: 0.7892
2026-02-10 01:34:32,292 - INFO - [Metrics for 'normal'] | Precision: 0.8415 | Recall: 0.7582 | F1: 0.7977
2026-02-10 01:34:32,294 - INFO - --------------------------------------------------
2026-02-10 01:34:32,294 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 01:34:38,418 - INFO - [Train] [78/90] | Loss: 0.3150 | Train Acc: 94.87%
2026-02-10 01:34:39,866 - INFO - [Valid] [78/90] | Loss: 0.5500 | Val Acc: 78.76%
2026-02-10 01:34:39,874 - INFO - [Metrics for 'abnormal'] | Precision: 0.7429 | Recall: 0.8280 | F1: 0.7831
2026-02-10 01:34:39,874 - INFO - [Metrics for 'normal'] | Precision: 0.8354 | Recall: 0.7527 | F1: 0.7919
2026-02-10 01:34:39,876 - INFO - --------------------------------------------------
2026-02-10 01:34:39,877 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 01:34:45,639 - INFO - [Train] [79/90] | Loss: 0.3181 | Train Acc: 94.87%
2026-02-10 01:34:47,061 - INFO - [Valid] [79/90] | Loss: 0.5476 | Val Acc: 79.65%
2026-02-10 01:34:47,066 - INFO - [Metrics for 'abnormal'] | Precision: 0.7558 | Recall: 0.8280 | F1: 0.7903
2026-02-10 01:34:47,066 - INFO - [Metrics for 'normal'] | Precision: 0.8383 | Recall: 0.7692 | F1: 0.8023
2026-02-10 01:34:47,068 - INFO - --------------------------------------------------
2026-02-10 01:34:47,068 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 01:34:52,769 - INFO - [Train] [80/90] | Loss: 0.3138 | Train Acc: 94.64%
2026-02-10 01:34:54,296 - INFO - [Valid] [80/90] | Loss: 0.5472 | Val Acc: 80.24%
2026-02-10 01:34:54,302 - INFO - [Metrics for 'abnormal'] | Precision: 0.7679 | Recall: 0.8217 | F1: 0.7938
2026-02-10 01:34:54,303 - INFO - [Metrics for 'normal'] | Precision: 0.8363 | Recall: 0.7857 | F1: 0.8102
2026-02-10 01:34:54,306 - INFO - --------------------------------------------------
2026-02-10 01:34:54,307 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 01:34:59,736 - INFO - [Train] [81/90] | Loss: 0.3112 | Train Acc: 95.83%
2026-02-10 01:35:01,280 - INFO - [Valid] [81/90] | Loss: 0.5556 | Val Acc: 79.35%
2026-02-10 01:35:01,286 - INFO - [Metrics for 'abnormal'] | Precision: 0.7669 | Recall: 0.7962 | F1: 0.7812
2026-02-10 01:35:01,286 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.7912 | F1: 0.8045
2026-02-10 01:35:01,288 - INFO - --------------------------------------------------
2026-02-10 01:35:01,288 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 01:35:06,806 - INFO - [Train] [82/90] | Loss: 0.3140 | Train Acc: 95.24%
2026-02-10 01:35:08,288 - INFO - [Valid] [82/90] | Loss: 0.5525 | Val Acc: 78.47%
2026-02-10 01:35:08,293 - INFO - [Metrics for 'abnormal'] | Precision: 0.7471 | Recall: 0.8089 | F1: 0.7768
2026-02-10 01:35:08,294 - INFO - [Metrics for 'normal'] | Precision: 0.8225 | Recall: 0.7637 | F1: 0.7920
2026-02-10 01:35:08,295 - INFO - --------------------------------------------------
2026-02-10 01:35:08,299 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 01:35:14,171 - INFO - [Train] [83/90] | Loss: 0.3071 | Train Acc: 94.94%
2026-02-10 01:35:15,368 - INFO - [Valid] [83/90] | Loss: 0.5514 | Val Acc: 79.94%
2026-02-10 01:35:15,374 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8089 | F1: 0.7888
2026-02-10 01:35:15,377 - INFO - [Metrics for 'normal'] | Precision: 0.8276 | Recall: 0.7912 | F1: 0.8090
2026-02-10 01:35:15,379 - INFO - --------------------------------------------------
2026-02-10 01:35:15,379 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 01:35:21,211 - INFO - [Train] [84/90] | Loss: 0.3118 | Train Acc: 95.61%
2026-02-10 01:35:22,733 - INFO - [Valid] [84/90] | Loss: 0.5539 | Val Acc: 79.06%
2026-02-10 01:35:22,742 - INFO - [Metrics for 'abnormal'] | Precision: 0.7471 | Recall: 0.8280 | F1: 0.7855
2026-02-10 01:35:22,743 - INFO - [Metrics for 'normal'] | Precision: 0.8364 | Recall: 0.7582 | F1: 0.7954
2026-02-10 01:35:22,744 - INFO - --------------------------------------------------
2026-02-10 01:35:22,744 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 01:35:28,788 - INFO - [Train] [85/90] | Loss: 0.3126 | Train Acc: 95.01%
2026-02-10 01:35:30,269 - INFO - [Valid] [85/90] | Loss: 0.5484 | Val Acc: 79.06%
2026-02-10 01:35:30,274 - INFO - [Metrics for 'abnormal'] | Precision: 0.7560 | Recall: 0.8089 | F1: 0.7815
2026-02-10 01:35:30,278 - INFO - [Metrics for 'normal'] | Precision: 0.8246 | Recall: 0.7747 | F1: 0.7989
2026-02-10 01:35:30,281 - INFO - --------------------------------------------------
2026-02-10 01:35:30,281 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 01:35:35,135 - INFO - [Train] [86/90] | Loss: 0.3100 | Train Acc: 94.79%
2026-02-10 01:35:36,821 - INFO - [Valid] [86/90] | Loss: 0.5560 | Val Acc: 78.76%
2026-02-10 01:35:36,827 - INFO - [Metrics for 'abnormal'] | Precision: 0.7545 | Recall: 0.8025 | F1: 0.7778
2026-02-10 01:35:36,827 - INFO - [Metrics for 'normal'] | Precision: 0.8198 | Recall: 0.7747 | F1: 0.7966
2026-02-10 01:35:36,833 - INFO - --------------------------------------------------
2026-02-10 01:35:36,833 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 01:35:42,354 - INFO - [Train] [87/90] | Loss: 0.3096 | Train Acc: 94.79%
2026-02-10 01:35:43,839 - INFO - [Valid] [87/90] | Loss: 0.5606 | Val Acc: 78.47%
2026-02-10 01:35:43,850 - INFO - [Metrics for 'abnormal'] | Precision: 0.7442 | Recall: 0.8153 | F1: 0.7781
2026-02-10 01:35:43,850 - INFO - [Metrics for 'normal'] | Precision: 0.8263 | Recall: 0.7582 | F1: 0.7908
2026-02-10 01:35:43,852 - INFO - --------------------------------------------------
2026-02-10 01:35:43,853 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 01:35:49,700 - INFO - [Train] [88/90] | Loss: 0.3079 | Train Acc: 95.31%
2026-02-10 01:35:50,970 - INFO - [Valid] [88/90] | Loss: 0.5546 | Val Acc: 79.35%
2026-02-10 01:35:50,975 - INFO - [Metrics for 'abnormal'] | Precision: 0.7574 | Recall: 0.8153 | F1: 0.7853
2026-02-10 01:35:50,978 - INFO - [Metrics for 'normal'] | Precision: 0.8294 | Recall: 0.7747 | F1: 0.8011
2026-02-10 01:35:50,980 - INFO - --------------------------------------------------
2026-02-10 01:35:50,980 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 01:35:56,925 - INFO - [Train] [89/90] | Loss: 0.2985 | Train Acc: 95.39%
2026-02-10 01:35:58,062 - INFO - [Valid] [89/90] | Loss: 0.5583 | Val Acc: 78.47%
2026-02-10 01:35:58,074 - INFO - [Metrics for 'abnormal'] | Precision: 0.7442 | Recall: 0.8153 | F1: 0.7781
2026-02-10 01:35:58,075 - INFO - [Metrics for 'normal'] | Precision: 0.8263 | Recall: 0.7582 | F1: 0.7908
2026-02-10 01:35:58,077 - INFO - --------------------------------------------------
2026-02-10 01:35:58,077 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 01:36:03,911 - INFO - [Train] [90/90] | Loss: 0.2973 | Train Acc: 95.98%
2026-02-10 01:36:05,442 - INFO - [Valid] [90/90] | Loss: 0.5609 | Val Acc: 78.47%
2026-02-10 01:36:05,448 - INFO - [Metrics for 'abnormal'] | Precision: 0.7471 | Recall: 0.8089 | F1: 0.7768
2026-02-10 01:36:05,452 - INFO - [Metrics for 'normal'] | Precision: 0.8225 | Recall: 0.7637 | F1: 0.7920
2026-02-10 01:36:05,454 - INFO - ==================================================
2026-02-10 01:36:05,455 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 01:36:05,455 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 01:36:05,455 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:36:05,523 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 01:36:05,524 - INFO - FPGM Pruning을 시작합니다.
2026-02-10 01:36:05,524 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:06,105 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-02-10 01:36:06,106 - INFO - ==================================================
2026-02-10 01:36:06,109 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/best_model.pth'
2026-02-10 01:36:06,109 - INFO - ==================================================
2026-02-10 01:36:06,109 - INFO - Test 모드를 시작합니다.
2026-02-10 01:36:06,237 - INFO - 연산량 (MACs): 0.0927 GMACs per sample
2026-02-10 01:36:06,241 - INFO - 연산량 (FLOPs): 0.1854 GFLOPs per sample
2026-02-10 01:36:06,241 - INFO - ==================================================
2026-02-10 01:36:06,241 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 01:36:07,172 - INFO - 샘플 당 평균 Forward Pass 시간: 0.23ms (std: 0.25ms), FPS: 7474.59 (std: 4239.64) (1개 샘플 x 100회 반복)
2026-02-10 01:36:07,172 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 160.22 MB
2026-02-10 01:36:07,173 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 01:36:09,297 - INFO - [Test] Loss: 0.4384 | Test Acc: 80.24%
2026-02-10 01:36:09,304 - INFO - [Metrics for 'abnormal'] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-02-10 01:36:09,305 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-02-10 01:36:09,595 - INFO - ==================================================
2026-02-10 01:36:09,595 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/confusion_matrix_20260210_012548.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/confusion_matrix_20260210_012548.pdf'
2026-02-10 01:36:09,596 - INFO - ==================================================
2026-02-10 01:36:09,596 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 01:36:09,912 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/model_fp32_20260210_012548.onnx'에 저장되었습니다. (크기: 0.22 MB)
2026-02-10 01:36:10,401 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 9.38 MB
2026-02-10 01:36:10,402 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 01:36:12,235 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 12.79ms (std: 8.13ms)
2026-02-10 01:36:12,235 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 170.60 FPS (std: 238.47) (1개 샘플 x 100회 반복)
2026-02-10 01:36:12,235 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 9.19 MB
2026-02-10 01:36:12,236 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 18.84 MB
2026-02-10 01:36:15,247 - INFO - [Test (ONNX)] | Test Acc (ONNX): 80.24%
2026-02-10 01:36:15,264 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-02-10 01:36:15,264 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-02-10 01:36:15,521 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/val_acc.pdf'
2026-02-10 01:36:15,762 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/train_val_acc.pdf'
2026-02-10 01:36:15,957 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/F1_normal.pdf'
2026-02-10 01:36:16,162 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/val_loss.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/val_loss.pdf'
2026-02-10 01:36:16,365 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/learning_rate.pdf'
2026-02-10 01:36:18,757 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/compile.png' and 'log/Sewer-TAPNEW/baseline_xie2019_fpgm_20260210_012548/graph_20260210_012548/compile.pdf'
