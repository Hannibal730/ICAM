2026-02-10 01:34:15,544 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/log_20260210_013415.log'에 저장됩니다.
2026-02-10 01:34:15,547 - INFO - ==================================================
2026-02-10 01:34:15,547 - INFO - config.yaml:
2026-02-10 01:34:15,547 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: mobilenet_v4_s
  use_wanda_pruning: true
  num_wanda_calib_samples: 1353
  pruning_flops_target: 0.1816

2026-02-10 01:34:15,547 - INFO - ==================================================
2026-02-10 01:34:15,588 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:34:15,591 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:34:15,591 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:34:15,597 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:34:15,597 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:34:15,597 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:34:15,597 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:34:15,963 - INFO - ==================================================
2026-02-10 01:34:15,964 - INFO - 모델 파라미터 수:
2026-02-10 01:34:15,964 - INFO -   - 총 파라미터: 2,495,586 개
2026-02-10 01:34:15,964 - INFO -   - 학습 가능한 파라미터: 2,495,586 개
2026-02-10 01:34:15,964 - INFO - ================================================================================
2026-02-10 01:34:15,964 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:34:15,964 - INFO - ================================================================================
2026-02-10 01:34:15,964 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:34:15,964 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:34:15,965 - INFO - ==================================================
2026-02-10 01:34:15,965 - INFO - train 모드를 시작합니다.
2026-02-10 01:34:15,965 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:34:15,965 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:34:15,965 - INFO - --------------------------------------------------
2026-02-10 01:34:15,966 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:34:21,358 - INFO - [Train] [1/10] | Loss: 2.8133 | Train Acc: 64.14%
2026-02-10 01:34:23,555 - INFO - [Valid] [1/10] | Loss: 1.0776 | Val Acc: 65.78%
2026-02-10 01:34:23,561 - INFO - [Metrics for 'abnormal'] | Precision: 0.7887 | Recall: 0.3567 | F1: 0.4912
2026-02-10 01:34:23,561 - INFO - [Metrics for 'normal'] | Precision: 0.6231 | Recall: 0.9176 | F1: 0.7422
2026-02-10 01:34:23,586 - INFO - [Best Model Saved] (val loss: 1.0776) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:34:23,586 - INFO - --------------------------------------------------
2026-02-10 01:34:23,587 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:34:28,272 - INFO - [Train] [2/10] | Loss: 0.7206 | Train Acc: 72.84%
2026-02-10 01:34:29,706 - INFO - [Valid] [2/10] | Loss: 0.6890 | Val Acc: 69.32%
2026-02-10 01:34:29,711 - INFO - [Metrics for 'abnormal'] | Precision: 0.6828 | Recall: 0.6306 | F1: 0.6556
2026-02-10 01:34:29,711 - INFO - [Metrics for 'normal'] | Precision: 0.7010 | Recall: 0.7473 | F1: 0.7234
2026-02-10 01:34:29,751 - INFO - [Best Model Saved] (val loss: 0.6890) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:34:29,751 - INFO - --------------------------------------------------
2026-02-10 01:34:29,753 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:34:34,292 - INFO - [Train] [3/10] | Loss: 0.6884 | Train Acc: 71.95%
2026-02-10 01:34:35,869 - INFO - [Valid] [3/10] | Loss: 0.6845 | Val Acc: 76.40%
2026-02-10 01:34:35,877 - INFO - [Metrics for 'abnormal'] | Precision: 0.6860 | Recall: 0.9045 | F1: 0.7802
2026-02-10 01:34:35,877 - INFO - [Metrics for 'normal'] | Precision: 0.8864 | Recall: 0.6429 | F1: 0.7452
2026-02-10 01:34:35,927 - INFO - [Best Model Saved] (val loss: 0.6845) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:34:35,927 - INFO - --------------------------------------------------
2026-02-10 01:34:35,929 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:34:41,325 - INFO - [Train] [4/10] | Loss: 0.5962 | Train Acc: 73.81%
2026-02-10 01:34:42,857 - INFO - [Valid] [4/10] | Loss: 0.6146 | Val Acc: 74.93%
2026-02-10 01:34:42,862 - INFO - [Metrics for 'abnormal'] | Precision: 0.7069 | Recall: 0.7834 | F1: 0.7432
2026-02-10 01:34:42,863 - INFO - [Metrics for 'normal'] | Precision: 0.7939 | Recall: 0.7198 | F1: 0.7550
2026-02-10 01:34:42,905 - INFO - [Best Model Saved] (val loss: 0.6146) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:34:42,906 - INFO - --------------------------------------------------
2026-02-10 01:34:42,907 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:34:48,903 - INFO - [Train] [5/10] | Loss: 0.5904 | Train Acc: 75.60%
2026-02-10 01:34:50,426 - INFO - [Valid] [5/10] | Loss: 0.5864 | Val Acc: 75.81%
2026-02-10 01:34:50,431 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.6369 | F1: 0.7092
2026-02-10 01:34:50,432 - INFO - [Metrics for 'normal'] | Precision: 0.7336 | Recall: 0.8626 | F1: 0.7929
2026-02-10 01:34:50,483 - INFO - [Best Model Saved] (val loss: 0.5864) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:34:50,483 - INFO - --------------------------------------------------
2026-02-10 01:34:50,485 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:34:56,532 - INFO - [Train] [6/10] | Loss: 0.5340 | Train Acc: 78.35%
2026-02-10 01:34:57,863 - INFO - [Valid] [6/10] | Loss: 0.5686 | Val Acc: 73.16%
2026-02-10 01:34:57,867 - INFO - [Metrics for 'abnormal'] | Precision: 0.6634 | Recall: 0.8535 | F1: 0.7465
2026-02-10 01:34:57,868 - INFO - [Metrics for 'normal'] | Precision: 0.8321 | Recall: 0.6264 | F1: 0.7147
2026-02-10 01:34:57,903 - INFO - [Best Model Saved] (val loss: 0.5686) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:34:57,903 - INFO - --------------------------------------------------
2026-02-10 01:34:57,904 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:35:04,061 - INFO - [Train] [7/10] | Loss: 0.4888 | Train Acc: 81.70%
2026-02-10 01:35:05,592 - INFO - [Valid] [7/10] | Loss: 0.5423 | Val Acc: 78.76%
2026-02-10 01:35:05,606 - INFO - [Metrics for 'abnormal'] | Precision: 0.8455 | Recall: 0.6624 | F1: 0.7429
2026-02-10 01:35:05,606 - INFO - [Metrics for 'normal'] | Precision: 0.7546 | Recall: 0.8956 | F1: 0.8191
2026-02-10 01:35:05,667 - INFO - [Best Model Saved] (val loss: 0.5423) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:35:05,667 - INFO - --------------------------------------------------
2026-02-10 01:35:05,668 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:35:11,832 - INFO - [Train] [8/10] | Loss: 0.4766 | Train Acc: 82.81%
2026-02-10 01:35:13,293 - INFO - [Valid] [8/10] | Loss: 0.6005 | Val Acc: 72.57%
2026-02-10 01:35:13,299 - INFO - [Metrics for 'abnormal'] | Precision: 0.6616 | Recall: 0.8344 | F1: 0.7380
2026-02-10 01:35:13,299 - INFO - [Metrics for 'normal'] | Precision: 0.8156 | Recall: 0.6319 | F1: 0.7121
2026-02-10 01:35:13,301 - INFO - --------------------------------------------------
2026-02-10 01:35:13,304 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:35:19,236 - INFO - [Train] [9/10] | Loss: 0.4449 | Train Acc: 83.63%
2026-02-10 01:35:20,699 - INFO - [Valid] [9/10] | Loss: 0.5215 | Val Acc: 78.17%
2026-02-10 01:35:20,705 - INFO - [Metrics for 'abnormal'] | Precision: 0.7785 | Recall: 0.7389 | F1: 0.7582
2026-02-10 01:35:20,705 - INFO - [Metrics for 'normal'] | Precision: 0.7842 | Recall: 0.8187 | F1: 0.8011
2026-02-10 01:35:20,756 - INFO - [Best Model Saved] (val loss: 0.5215) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:35:20,756 - INFO - --------------------------------------------------
2026-02-10 01:35:20,758 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:35:26,382 - INFO - [Train] [10/10] | Loss: 0.4470 | Train Acc: 83.78%
2026-02-10 01:35:27,794 - INFO - [Valid] [10/10] | Loss: 0.5442 | Val Acc: 78.47%
2026-02-10 01:35:27,799 - INFO - [Metrics for 'abnormal'] | Precision: 0.8134 | Recall: 0.6943 | F1: 0.7491
2026-02-10 01:35:27,799 - INFO - [Metrics for 'normal'] | Precision: 0.7659 | Recall: 0.8626 | F1: 0.8114
2026-02-10 01:35:27,801 - INFO - ================================================================================
2026-02-10 01:35:27,801 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:35:27,801 - INFO - ================================================================================
2026-02-10 01:35:27,853 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:35:27,853 - INFO - ================================================================================
2026-02-10 01:35:27,853 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:35:27,908 - INFO - 원본 모델 FLOPs: 0.3853 GFLOPs
2026-02-10 01:35:27,977 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:35:27,977 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:35:27,979 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:35:33,907 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:35:34,011 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:35:34,011 - INFO - ==================================================
2026-02-10 01:35:34,054 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.1092 GFLOPs (감소율: 71.66%)
2026-02-10 01:35:34,087 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:35:34,087 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:35:34,088 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:35:39,485 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:35:39,572 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.2475)에 맞춰 변경되었습니다.
2026-02-10 01:35:39,572 - INFO - ==================================================
2026-02-10 01:35:39,621 - INFO -   [탐색  2] 희소도: 0.2475 -> FLOPs: 0.2263 GFLOPs (감소율: 41.26%)
2026-02-10 01:35:39,655 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:35:39,655 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:35:39,657 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:35:45,175 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:35:45,256 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.37124999999999997)에 맞춰 변경되었습니다.
2026-02-10 01:35:45,257 - INFO - ==================================================
2026-02-10 01:35:45,307 - INFO -   [탐색  3] 희소도: 0.3712 -> FLOPs: 0.1626 GFLOPs (감소율: 57.81%)
2026-02-10 01:35:45,785 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:35:45,785 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:35:45,787 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:35:50,949 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:35:51,051 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.30937499999999996)에 맞춰 변경되었습니다.
2026-02-10 01:35:51,051 - INFO - ==================================================
2026-02-10 01:35:51,107 - INFO -   [탐색  4] 희소도: 0.3094 -> FLOPs: 0.1932 GFLOPs (감소율: 49.86%)
2026-02-10 01:35:51,147 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:35:51,147 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:35:51,149 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:35:57,289 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:35:57,393 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.34031249999999996)에 맞춰 변경되었습니다.
2026-02-10 01:35:57,393 - INFO - ==================================================
2026-02-10 01:35:57,439 - INFO -   [탐색  5] 희소도: 0.3403 -> FLOPs: 0.1775 GFLOPs (감소율: 53.92%)
2026-02-10 01:35:57,473 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:35:57,474 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:35:57,476 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:03,249 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:03,382 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32484375)에 맞춰 변경되었습니다.
2026-02-10 01:36:03,382 - INFO - ==================================================
2026-02-10 01:36:03,427 - INFO -   [탐색  6] 희소도: 0.3248 -> FLOPs: 0.1825 GFLOPs (감소율: 52.64%)
2026-02-10 01:36:03,458 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:03,458 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:03,460 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:08,068 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:08,144 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.33257812499999995)에 맞춰 변경되었습니다.
2026-02-10 01:36:08,144 - INFO - ==================================================
2026-02-10 01:36:08,188 - INFO -   [탐색  7] 희소도: 0.3326 -> FLOPs: 0.1806 GFLOPs (감소율: 53.13%)
2026-02-10 01:36:08,221 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:08,222 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:08,223 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:13,787 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:13,891 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32871093749999997)에 맞춰 변경되었습니다.
2026-02-10 01:36:13,891 - INFO - ==================================================
2026-02-10 01:36:13,928 - INFO -   [탐색  8] 희소도: 0.3287 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:36:13,953 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:13,953 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:13,954 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:18,776 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:18,879 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32677734375)에 맞춰 변경되었습니다.
2026-02-10 01:36:18,879 - INFO - ==================================================
2026-02-10 01:36:18,919 - INFO -   [탐색  9] 희소도: 0.3268 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:36:18,951 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:18,952 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:18,954 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:23,218 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:23,324 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.327744140625)에 맞춰 변경되었습니다.
2026-02-10 01:36:23,325 - INFO - ==================================================
2026-02-10 01:36:23,361 - INFO -   [탐색 10] 희소도: 0.3277 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:36:23,396 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:23,396 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:23,398 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:27,542 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:27,656 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3282275390625)에 맞춰 변경되었습니다.
2026-02-10 01:36:27,656 - INFO - ==================================================
2026-02-10 01:36:27,686 - INFO -   [탐색 11] 희소도: 0.3282 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:36:27,717 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:27,717 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:27,719 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:32,397 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:32,470 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32798583984375)에 맞춰 변경되었습니다.
2026-02-10 01:36:32,471 - INFO - ==================================================
2026-02-10 01:36:32,497 - INFO -   [탐색 12] 희소도: 0.3280 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:36:32,526 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:32,527 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:32,528 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:37,692 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:37,856 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32810668945312504)에 맞춰 변경되었습니다.
2026-02-10 01:36:37,857 - INFO - ==================================================
2026-02-10 01:36:37,886 - INFO -   [탐색 13] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:36:37,921 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:37,921 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:37,923 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:43,114 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:43,200 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32816711425781253)에 맞춰 변경되었습니다.
2026-02-10 01:36:43,200 - INFO - ==================================================
2026-02-10 01:36:43,232 - INFO -   [탐색 14] 희소도: 0.3282 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:36:43,267 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:43,267 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:43,270 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:48,415 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:49,036 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281369018554688)에 맞춰 변경되었습니다.
2026-02-10 01:36:49,037 - INFO - ==================================================
2026-02-10 01:36:49,071 - INFO -   [탐색 15] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:36:49,105 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:49,106 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:49,108 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:36:54,861 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:36:55,283 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281217956542969)에 맞춰 변경되었습니다.
2026-02-10 01:36:55,283 - INFO - ==================================================
2026-02-10 01:36:55,320 - INFO -   [탐색 16] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:36:55,352 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:36:55,352 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:36:55,353 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:01,371 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:01,483 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812934875488287)에 맞춰 변경되었습니다.
2026-02-10 01:37:01,484 - INFO - ==================================================
2026-02-10 01:37:01,514 - INFO -   [탐색 17] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:37:01,549 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:01,549 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:01,551 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:07,233 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:07,354 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281255722045899)에 맞춰 변경되었습니다.
2026-02-10 01:37:07,354 - INFO - ==================================================
2026-02-10 01:37:07,390 - INFO -   [탐색 18] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:37:07,421 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:07,422 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:07,424 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:12,996 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:13,227 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812368392944335)에 맞춰 변경되었습니다.
2026-02-10 01:37:13,227 - INFO - ==================================================
2026-02-10 01:37:13,258 - INFO -   [탐색 19] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:37:13,291 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:13,291 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:13,293 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:19,020 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:19,161 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281246280670166)에 맞춰 변경되었습니다.
2026-02-10 01:37:19,161 - INFO - ==================================================
2026-02-10 01:37:19,192 - INFO -   [탐색 20] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:37:19,228 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:19,229 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:19,231 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:24,507 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:24,599 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281251001358032)에 맞춰 변경되었습니다.
2026-02-10 01:37:24,599 - INFO - ==================================================
2026-02-10 01:37:24,633 - INFO -   [탐색 21] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:37:24,667 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:24,668 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:24,671 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:30,487 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:30,647 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281248641014099)에 맞춰 변경되었습니다.
2026-02-10 01:37:30,647 - INFO - ==================================================
2026-02-10 01:37:30,678 - INFO -   [탐색 22] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:37:30,710 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:30,710 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:30,712 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:36,157 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:36,264 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249821186065)에 맞춰 변경되었습니다.
2026-02-10 01:37:36,265 - INFO - ==================================================
2026-02-10 01:37:36,293 - INFO -   [탐색 23] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:37:36,324 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:36,324 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:36,326 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:40,879 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:40,976 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812504112720486)에 맞춰 변경되었습니다.
2026-02-10 01:37:40,976 - INFO - ==================================================
2026-02-10 01:37:41,011 - INFO -   [탐색 24] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:37:41,042 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:41,043 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:41,044 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:45,939 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:46,019 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250116229057)에 맞춰 변경되었습니다.
2026-02-10 01:37:46,019 - INFO - ==================================================
2026-02-10 01:37:46,057 - INFO -   [탐색 25] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:37:46,091 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:46,092 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:46,094 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:51,588 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:51,672 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499687075614)에 맞춰 변경되었습니다.
2026-02-10 01:37:51,672 - INFO - ==================================================
2026-02-10 01:37:51,703 - INFO -   [탐색 26] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:37:51,733 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:51,733 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:51,735 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:57,402 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:57,934 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250042468309)에 맞춰 변경되었습니다.
2026-02-10 01:37:57,934 - INFO - ==================================================
2026-02-10 01:37:57,959 - INFO -   [탐색 27] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:37:57,987 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:57,987 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:57,988 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:03,431 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:03,516 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250005587935)에 맞춰 변경되었습니다.
2026-02-10 01:38:03,517 - INFO - ==================================================
2026-02-10 01:38:03,547 - INFO -   [탐색 28] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:38:03,573 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:03,573 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:03,574 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:08,908 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:09,008 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499871477485)에 맞춰 변경되었습니다.
2026-02-10 01:38:09,009 - INFO - ==================================================
2026-02-10 01:38:09,040 - INFO -   [탐색 29] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:38:09,076 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:09,076 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:09,077 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:13,627 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:13,849 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249996367842)에 맞춰 변경되었습니다.
2026-02-10 01:38:13,849 - INFO - ==================================================
2026-02-10 01:38:13,867 - INFO -   [탐색 30] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:38:13,888 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:13,888 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:13,889 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:18,394 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:18,513 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000977889)에 맞춰 변경되었습니다.
2026-02-10 01:38:18,513 - INFO - ==================================================
2026-02-10 01:38:18,533 - INFO -   [탐색 31] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:38:18,564 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:18,565 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:18,567 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:23,983 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:24,102 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499986728655)에 맞춰 변경되었습니다.
2026-02-10 01:38:24,103 - INFO - ==================================================
2026-02-10 01:38:24,136 - INFO -   [탐색 32] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:38:24,169 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:24,169 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:24,171 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:29,629 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:29,726 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499998253775)에 맞춰 변경되었습니다.
2026-02-10 01:38:29,727 - INFO - ==================================================
2026-02-10 01:38:29,760 - INFO -   [탐색 33] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:38:29,797 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:29,797 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:29,799 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:33,873 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:33,956 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000401633)에 맞춰 변경되었습니다.
2026-02-10 01:38:33,957 - INFO - ==================================================
2026-02-10 01:38:33,988 - INFO -   [탐색 34] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:38:34,019 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:34,019 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:34,021 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:39,666 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:39,804 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500001135053)에 맞춰 변경되었습니다.
2026-02-10 01:38:39,804 - INFO - ==================================================
2026-02-10 01:38:39,834 - INFO -   [탐색 35] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:38:39,867 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:39,868 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:39,869 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:45,471 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:45,594 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999969441)에 맞춰 변경되었습니다.
2026-02-10 01:38:45,594 - INFO - ==================================================
2026-02-10 01:38:45,626 - INFO -   [탐색 36] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:38:45,659 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:45,659 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:45,661 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:50,798 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:50,938 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000414735)에 맞춰 변경되었습니다.
2026-02-10 01:38:50,938 - INFO - ==================================================
2026-02-10 01:38:51,509 - INFO -   [탐색 37] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:38:51,544 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:51,545 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:51,548 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:57,431 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:57,674 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000054573)에 맞춰 변경되었습니다.
2026-02-10 01:38:57,675 - INFO - ==================================================
2026-02-10 01:38:57,704 - INFO -   [탐색 38] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:38:57,737 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:57,737 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:57,739 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:03,254 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:03,346 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999987449)에 맞춰 변경되었습니다.
2026-02-10 01:39:03,346 - INFO - ==================================================
2026-02-10 01:39:03,380 - INFO -   [탐색 39] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:39:03,414 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:03,415 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:03,417 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:08,174 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:08,295 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999996453)에 맞춰 변경되었습니다.
2026-02-10 01:39:08,296 - INFO - ==================================================
2026-02-10 01:39:08,325 - INFO -   [탐색 40] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:39:08,360 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:08,361 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:08,362 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:14,133 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:14,231 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000955)에 맞춰 변경되었습니다.
2026-02-10 01:39:14,231 - INFO - ==================================================
2026-02-10 01:39:14,266 - INFO -   [탐색 41] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:39:14,302 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:14,302 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:14,303 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:20,040 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:20,141 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999998704)에 맞춰 변경되었습니다.
2026-02-10 01:39:20,141 - INFO - ==================================================
2026-02-10 01:39:20,177 - INFO -   [탐색 42] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:39:20,214 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:20,214 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:20,216 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:25,556 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:25,655 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999999829)에 맞춰 변경되었습니다.
2026-02-10 01:39:25,655 - INFO - ==================================================
2026-02-10 01:39:25,686 - INFO -   [탐색 43] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:39:25,720 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:25,720 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:25,722 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:31,205 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:31,349 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000392)에 맞춰 변경되었습니다.
2026-02-10 01:39:31,349 - INFO - ==================================================
2026-02-10 01:39:31,381 - INFO -   [탐색 44] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:39:31,412 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:31,413 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:31,414 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:36,309 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:36,376 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000001105)에 맞춰 변경되었습니다.
2026-02-10 01:39:36,376 - INFO - ==================================================
2026-02-10 01:39:36,408 - INFO -   [탐색 45] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:39:36,444 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:36,444 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:36,446 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:41,213 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:41,265 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.328124999999997)에 맞춰 변경되었습니다.
2026-02-10 01:39:41,265 - INFO - ==================================================
2026-02-10 01:39:41,290 - INFO -   [탐색 46] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:39:41,316 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:41,316 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:41,322 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:46,720 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:46,795 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.328125000000004)에 맞춰 변경되었습니다.
2026-02-10 01:39:46,795 - INFO - ==================================================
2026-02-10 01:39:46,825 - INFO -   [탐색 47] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:39:47,197 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:47,197 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:47,199 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:52,892 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:52,963 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000005)에 맞춰 변경되었습니다.
2026-02-10 01:39:52,963 - INFO - ==================================================
2026-02-10 01:39:52,991 - INFO -   [탐색 48] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:39:53,022 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:53,022 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:53,024 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:58,645 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:58,707 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999999988)에 맞춰 변경되었습니다.
2026-02-10 01:39:58,707 - INFO - ==================================================
2026-02-10 01:39:58,732 - INFO -   [탐색 49] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:39:58,764 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:58,764 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:58,765 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:04,173 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:04,241 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499999999967)에 맞춰 변경되었습니다.
2026-02-10 01:40:04,241 - INFO - ==================================================
2026-02-10 01:40:04,267 - INFO -   [탐색 50] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:40:04,299 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:04,299 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:04,300 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:09,578 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:09,646 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:40:09,646 - INFO - ==================================================
2026-02-10 01:40:09,677 - INFO -   [탐색 51] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:40:09,709 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:09,710 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:09,713 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:14,852 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:14,932 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999999999)에 맞춰 변경되었습니다.
2026-02-10 01:40:14,932 - INFO - ==================================================
2026-02-10 01:40:14,967 - INFO -   [탐색 52] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:40:15,000 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:15,001 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:15,003 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:19,848 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:19,919 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.328125)에 맞춰 변경되었습니다.
2026-02-10 01:40:19,919 - INFO - ==================================================
2026-02-10 01:40:19,951 - INFO -   [탐색 53] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:40:19,984 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:19,984 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:19,986 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:25,189 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:25,335 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000000006)에 맞춰 변경되었습니다.
2026-02-10 01:40:25,335 - INFO - ==================================================
2026-02-10 01:40:25,364 - INFO -   [탐색 54] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 01:40:25,394 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:25,394 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:25,396 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:31,565 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:31,686 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:40:31,686 - INFO - ==================================================
2026-02-10 01:40:31,716 - INFO -   [탐색 55] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:40:31,745 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:31,745 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:31,746 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:37,903 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:38,112 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:40:38,112 - INFO - ==================================================
2026-02-10 01:40:38,148 - INFO -   [탐색 56] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:40:38,186 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:38,186 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:38,188 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:44,441 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:44,763 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:40:44,763 - INFO - ==================================================
2026-02-10 01:40:44,794 - INFO -   [탐색 57] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:40:44,823 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:44,823 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:44,825 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:50,850 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:50,935 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:40:50,935 - INFO - ==================================================
2026-02-10 01:40:50,977 - INFO -   [탐색 58] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:40:51,011 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:51,011 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:51,013 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:55,962 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:56,641 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:40:56,641 - INFO - ==================================================
2026-02-10 01:40:56,679 - INFO -   [탐색 59] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:40:56,715 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:56,715 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:56,718 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:02,238 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:02,459 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:02,459 - INFO - ==================================================
2026-02-10 01:41:02,488 - INFO -   [탐색 60] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:02,522 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:02,522 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:02,524 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:08,439 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:08,543 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:08,544 - INFO - ==================================================
2026-02-10 01:41:08,577 - INFO -   [탐색 61] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:08,609 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:08,610 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:08,611 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:14,833 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:14,985 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:14,985 - INFO - ==================================================
2026-02-10 01:41:15,022 - INFO -   [탐색 62] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:15,055 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:15,055 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:15,057 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:21,151 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:21,389 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:21,390 - INFO - ==================================================
2026-02-10 01:41:21,423 - INFO -   [탐색 63] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:21,456 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:21,457 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:21,459 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:26,705 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:26,822 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:26,823 - INFO - ==================================================
2026-02-10 01:41:26,860 - INFO -   [탐색 64] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:26,894 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:26,894 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:26,898 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:32,487 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:32,559 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:32,559 - INFO - ==================================================
2026-02-10 01:41:32,595 - INFO -   [탐색 65] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:32,627 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:32,627 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:32,629 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:38,138 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:38,203 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:38,204 - INFO - ==================================================
2026-02-10 01:41:38,240 - INFO -   [탐색 66] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:38,271 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:38,271 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:38,273 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:44,517 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:44,586 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:44,587 - INFO - ==================================================
2026-02-10 01:41:44,618 - INFO -   [탐색 67] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:44,651 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:44,652 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:44,654 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:51,047 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:51,124 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:51,124 - INFO - ==================================================
2026-02-10 01:41:51,154 - INFO -   [탐색 68] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:51,186 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:51,186 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:51,188 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:56,912 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:56,992 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:41:56,992 - INFO - ==================================================
2026-02-10 01:41:57,026 - INFO -   [탐색 69] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:41:57,059 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:57,060 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:57,061 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:03,089 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:03,161 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:03,161 - INFO - ==================================================
2026-02-10 01:42:03,192 - INFO -   [탐색 70] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:03,225 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:03,226 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:03,228 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:09,000 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:09,125 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:09,125 - INFO - ==================================================
2026-02-10 01:42:09,156 - INFO -   [탐색 71] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:09,188 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:09,188 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:09,190 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:14,904 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:15,594 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:15,594 - INFO - ==================================================
2026-02-10 01:42:15,630 - INFO -   [탐색 72] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:15,665 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:15,665 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:15,667 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:21,540 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:21,628 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:21,628 - INFO - ==================================================
2026-02-10 01:42:21,661 - INFO -   [탐색 73] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:21,696 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:21,696 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:21,698 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:27,254 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:27,350 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:27,350 - INFO - ==================================================
2026-02-10 01:42:27,381 - INFO -   [탐색 74] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:27,419 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:27,420 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:27,422 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:33,457 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:33,543 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:33,544 - INFO - ==================================================
2026-02-10 01:42:33,574 - INFO -   [탐색 75] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:33,609 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:33,609 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:33,611 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:39,536 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:39,610 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:39,610 - INFO - ==================================================
2026-02-10 01:42:39,642 - INFO -   [탐색 76] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:39,677 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:39,677 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:39,680 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:45,433 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:45,557 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:45,557 - INFO - ==================================================
2026-02-10 01:42:45,591 - INFO -   [탐색 77] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:45,629 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:45,629 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:45,631 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:51,208 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:51,290 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:51,291 - INFO - ==================================================
2026-02-10 01:42:51,323 - INFO -   [탐색 78] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:51,358 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:51,359 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:51,360 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:56,972 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:57,049 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:42:57,050 - INFO - ==================================================
2026-02-10 01:42:57,079 - INFO -   [탐색 79] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:42:57,114 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:57,114 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:57,116 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:02,940 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:03,025 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:03,025 - INFO - ==================================================
2026-02-10 01:43:03,057 - INFO -   [탐색 80] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:03,091 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:03,091 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:03,093 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:08,782 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:08,905 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:08,906 - INFO - ==================================================
2026-02-10 01:43:08,935 - INFO -   [탐색 81] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:08,967 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:08,967 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:08,969 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:14,943 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:15,544 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:15,545 - INFO - ==================================================
2026-02-10 01:43:15,579 - INFO -   [탐색 82] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:15,613 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:15,614 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:15,615 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:21,696 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:21,770 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:21,770 - INFO - ==================================================
2026-02-10 01:43:21,803 - INFO -   [탐색 83] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:21,842 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:21,842 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:21,843 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:27,189 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:27,264 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:27,265 - INFO - ==================================================
2026-02-10 01:43:27,296 - INFO -   [탐색 84] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:27,330 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:27,330 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:27,331 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:32,864 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:32,960 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:32,960 - INFO - ==================================================
2026-02-10 01:43:32,999 - INFO -   [탐색 85] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:33,032 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:33,033 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:33,036 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:39,356 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:39,451 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:39,451 - INFO - ==================================================
2026-02-10 01:43:39,483 - INFO -   [탐색 86] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:39,520 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:39,520 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:39,522 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:44,636 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:44,712 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:44,713 - INFO - ==================================================
2026-02-10 01:43:44,744 - INFO -   [탐색 87] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:44,775 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:44,775 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:44,777 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:49,964 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:50,042 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:50,043 - INFO - ==================================================
2026-02-10 01:43:50,074 - INFO -   [탐색 88] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:50,099 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:50,100 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:50,101 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:54,521 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:54,587 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:54,587 - INFO - ==================================================
2026-02-10 01:43:54,619 - INFO -   [탐색 89] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:54,650 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:54,650 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:54,652 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:59,418 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:59,484 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:43:59,484 - INFO - ==================================================
2026-02-10 01:43:59,516 - INFO -   [탐색 90] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:43:59,546 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:59,546 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:59,547 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:04,585 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:04,646 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:04,646 - INFO - ==================================================
2026-02-10 01:44:04,676 - INFO -   [탐색 91] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:04,706 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:04,706 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:04,708 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:08,911 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:08,999 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:08,999 - INFO - ==================================================
2026-02-10 01:44:09,029 - INFO -   [탐색 92] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:09,061 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:09,062 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:09,063 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:13,738 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:13,810 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:13,811 - INFO - ==================================================
2026-02-10 01:44:13,844 - INFO -   [탐색 93] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:14,436 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:14,437 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:14,440 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:18,535 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:18,610 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:18,610 - INFO - ==================================================
2026-02-10 01:44:18,649 - INFO -   [탐색 94] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:18,686 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:18,687 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:18,691 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:22,762 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:22,867 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:22,868 - INFO - ==================================================
2026-02-10 01:44:22,898 - INFO -   [탐색 95] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:22,931 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:22,931 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:22,933 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:27,020 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:27,100 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:27,100 - INFO - ==================================================
2026-02-10 01:44:27,130 - INFO -   [탐색 96] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:27,162 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:27,162 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:27,164 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:31,572 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:31,692 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:31,692 - INFO - ==================================================
2026-02-10 01:44:31,721 - INFO -   [탐색 97] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:31,754 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:31,754 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:31,756 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:36,260 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:36,322 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:36,323 - INFO - ==================================================
2026-02-10 01:44:36,349 - INFO -   [탐색 98] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:36,378 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:36,378 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:36,380 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:41,153 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:41,259 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:41,259 - INFO - ==================================================
2026-02-10 01:44:41,291 - INFO -   [탐색 99] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:41,322 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:41,322 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:41,324 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:45,648 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:45,753 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 01:44:45,753 - INFO - ==================================================
2026-02-10 01:44:45,786 - INFO -   [탐색 100] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 01:44:45,787 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.3277 입니다.
2026-02-10 01:44:45,787 - INFO - ================================================================================
2026-02-10 01:44:45,791 - INFO - 계산된 Pruning 정보(희소도: 0.3277)를 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:44:45,824 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:44:45,887 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:45,887 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:45,889 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:49,713 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:49,809 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.327744140625)에 맞춰 변경되었습니다.
2026-02-10 01:44:49,810 - INFO - ==================================================
2026-02-10 01:44:49,811 - INFO - ==================================================
2026-02-10 01:44:49,812 - INFO - 모델 파라미터 수:
2026-02-10 01:44:49,812 - INFO -   - 총 파라미터: 1,146,105 개
2026-02-10 01:44:49,812 - INFO -   - 학습 가능한 파라미터: 1,146,105 개
2026-02-10 01:44:49,841 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:44:49,909 - INFO - FLOPs가 0.3853 GFLOPs에서 0.1822 GFLOPs로 감소했습니다 (감소율: 52.70%).
2026-02-10 01:44:49,910 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:44:49,910 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:44:49,911 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:44:49,911 - INFO - ==================================================
2026-02-10 01:44:49,911 - INFO - train 모드를 시작합니다.
2026-02-10 01:44:49,912 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:44:49,912 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:44:49,912 - INFO - --------------------------------------------------
2026-02-10 01:44:49,913 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:44:54,587 - INFO - [Train] [11/90] | Loss: 0.9495 | Train Acc: 65.62%
2026-02-10 01:44:55,814 - INFO - [Valid] [11/90] | Loss: 0.7026 | Val Acc: 67.55%
2026-02-10 01:44:55,819 - INFO - [Metrics for 'abnormal'] | Precision: 0.6009 | Recall: 0.8917 | F1: 0.7179
2026-02-10 01:44:55,819 - INFO - [Metrics for 'normal'] | Precision: 0.8396 | Recall: 0.4890 | F1: 0.6181
2026-02-10 01:44:55,868 - INFO - [Best Model Saved] (val loss: 0.7026) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:44:55,869 - INFO - --------------------------------------------------
2026-02-10 01:44:55,875 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:45:00,424 - INFO - [Train] [12/90] | Loss: 0.5994 | Train Acc: 73.51%
2026-02-10 01:45:01,645 - INFO - [Valid] [12/90] | Loss: 1.0338 | Val Acc: 67.26%
2026-02-10 01:45:01,649 - INFO - [Metrics for 'abnormal'] | Precision: 0.8026 | Recall: 0.3885 | F1: 0.5236
2026-02-10 01:45:01,649 - INFO - [Metrics for 'normal'] | Precision: 0.6350 | Recall: 0.9176 | F1: 0.7506
2026-02-10 01:45:01,651 - INFO - --------------------------------------------------
2026-02-10 01:45:01,652 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:45:06,348 - INFO - [Train] [13/90] | Loss: 0.5936 | Train Acc: 73.81%
2026-02-10 01:45:07,254 - INFO - [Valid] [13/90] | Loss: 0.9139 | Val Acc: 63.72%
2026-02-10 01:45:07,258 - INFO - [Metrics for 'abnormal'] | Precision: 0.7073 | Recall: 0.3694 | F1: 0.4854
2026-02-10 01:45:07,258 - INFO - [Metrics for 'normal'] | Precision: 0.6148 | Recall: 0.8681 | F1: 0.7198
2026-02-10 01:45:07,260 - INFO - --------------------------------------------------
2026-02-10 01:45:07,261 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:45:11,861 - INFO - [Train] [14/90] | Loss: 0.5456 | Train Acc: 76.93%
2026-02-10 01:45:13,069 - INFO - [Valid] [14/90] | Loss: 0.6648 | Val Acc: 70.80%
2026-02-10 01:45:13,073 - INFO - [Metrics for 'abnormal'] | Precision: 0.6318 | Recall: 0.8854 | F1: 0.7374
2026-02-10 01:45:13,073 - INFO - [Metrics for 'normal'] | Precision: 0.8487 | Recall: 0.5549 | F1: 0.6711
2026-02-10 01:45:13,115 - INFO - [Best Model Saved] (val loss: 0.6648) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:45:13,116 - INFO - --------------------------------------------------
2026-02-10 01:45:13,117 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:45:17,735 - INFO - [Train] [15/90] | Loss: 0.5933 | Train Acc: 76.93%
2026-02-10 01:45:18,895 - INFO - [Valid] [15/90] | Loss: 0.7753 | Val Acc: 61.36%
2026-02-10 01:45:18,901 - INFO - [Metrics for 'abnormal'] | Precision: 0.8250 | Recall: 0.2102 | F1: 0.3350
2026-02-10 01:45:18,901 - INFO - [Metrics for 'normal'] | Precision: 0.5853 | Recall: 0.9615 | F1: 0.7277
2026-02-10 01:45:18,903 - INFO - --------------------------------------------------
2026-02-10 01:45:18,904 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:45:23,633 - INFO - [Train] [16/90] | Loss: 0.5820 | Train Acc: 75.22%
2026-02-10 01:45:24,725 - INFO - [Valid] [16/90] | Loss: 0.8246 | Val Acc: 63.72%
2026-02-10 01:45:24,734 - INFO - [Metrics for 'abnormal'] | Precision: 0.5649 | Recall: 0.9427 | F1: 0.7064
2026-02-10 01:45:24,734 - INFO - [Metrics for 'normal'] | Precision: 0.8831 | Recall: 0.3736 | F1: 0.5251
2026-02-10 01:45:24,737 - INFO - --------------------------------------------------
2026-02-10 01:45:24,738 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:45:29,626 - INFO - [Train] [17/90] | Loss: 0.5307 | Train Acc: 81.47%
2026-02-10 01:45:30,829 - INFO - [Valid] [17/90] | Loss: 0.5415 | Val Acc: 77.88%
2026-02-10 01:45:30,833 - INFO - [Metrics for 'abnormal'] | Precision: 0.7440 | Recall: 0.7962 | F1: 0.7692
2026-02-10 01:45:30,834 - INFO - [Metrics for 'normal'] | Precision: 0.8129 | Recall: 0.7637 | F1: 0.7875
2026-02-10 01:45:30,875 - INFO - [Best Model Saved] (val loss: 0.5415) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:45:30,876 - INFO - --------------------------------------------------
2026-02-10 01:45:30,877 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:45:35,281 - INFO - [Train] [18/90] | Loss: 0.5103 | Train Acc: 79.24%
2026-02-10 01:45:36,619 - INFO - [Valid] [18/90] | Loss: 0.5322 | Val Acc: 80.24%
2026-02-10 01:45:36,623 - INFO - [Metrics for 'abnormal'] | Precision: 0.7744 | Recall: 0.8089 | F1: 0.7913
2026-02-10 01:45:36,624 - INFO - [Metrics for 'normal'] | Precision: 0.8286 | Recall: 0.7967 | F1: 0.8123
2026-02-10 01:45:36,665 - INFO - [Best Model Saved] (val loss: 0.5322) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:45:36,669 - INFO - --------------------------------------------------
2026-02-10 01:45:36,671 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:45:41,005 - INFO - [Train] [19/90] | Loss: 0.5157 | Train Acc: 81.25%
2026-02-10 01:45:42,216 - INFO - [Valid] [19/90] | Loss: 0.6140 | Val Acc: 69.03%
2026-02-10 01:45:42,220 - INFO - [Metrics for 'abnormal'] | Precision: 0.8514 | Recall: 0.4013 | F1: 0.5455
2026-02-10 01:45:42,220 - INFO - [Metrics for 'normal'] | Precision: 0.6453 | Recall: 0.9396 | F1: 0.7651
2026-02-10 01:45:42,222 - INFO - --------------------------------------------------
2026-02-10 01:45:42,224 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:45:46,638 - INFO - [Train] [20/90] | Loss: 0.4546 | Train Acc: 84.15%
2026-02-10 01:45:47,974 - INFO - [Valid] [20/90] | Loss: 0.6003 | Val Acc: 76.40%
2026-02-10 01:45:47,982 - INFO - [Metrics for 'abnormal'] | Precision: 0.6825 | Recall: 0.9172 | F1: 0.7826
2026-02-10 01:45:47,982 - INFO - [Metrics for 'normal'] | Precision: 0.8984 | Recall: 0.6319 | F1: 0.7419
2026-02-10 01:45:47,984 - INFO - --------------------------------------------------
2026-02-10 01:45:47,985 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:45:52,121 - INFO - [Train] [21/90] | Loss: 0.4888 | Train Acc: 82.89%
2026-02-10 01:45:53,560 - INFO - [Valid] [21/90] | Loss: 0.5778 | Val Acc: 76.40%
2026-02-10 01:45:53,565 - INFO - [Metrics for 'abnormal'] | Precision: 0.8291 | Recall: 0.6178 | F1: 0.7080
2026-02-10 01:45:53,573 - INFO - [Metrics for 'normal'] | Precision: 0.7297 | Recall: 0.8901 | F1: 0.8020
2026-02-10 01:45:53,577 - INFO - --------------------------------------------------
2026-02-10 01:45:53,578 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:45:57,657 - INFO - [Train] [22/90] | Loss: 0.4440 | Train Acc: 85.71%
2026-02-10 01:45:58,938 - INFO - [Valid] [22/90] | Loss: 0.5554 | Val Acc: 78.76%
2026-02-10 01:45:58,949 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.7580 | F1: 0.7677
2026-02-10 01:45:58,949 - INFO - [Metrics for 'normal'] | Precision: 0.7957 | Recall: 0.8132 | F1: 0.8043
2026-02-10 01:45:58,951 - INFO - --------------------------------------------------
2026-02-10 01:45:58,954 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:46:02,564 - INFO - [Train] [23/90] | Loss: 0.4602 | Train Acc: 83.93%
2026-02-10 01:46:03,893 - INFO - [Valid] [23/90] | Loss: 0.5205 | Val Acc: 80.53%
2026-02-10 01:46:03,899 - INFO - [Metrics for 'abnormal'] | Precision: 0.7898 | Recall: 0.7898 | F1: 0.7898
2026-02-10 01:46:03,901 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8187 | F1: 0.8187
2026-02-10 01:46:03,961 - INFO - [Best Model Saved] (val loss: 0.5205) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:46:03,961 - INFO - --------------------------------------------------
2026-02-10 01:46:03,963 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:46:07,695 - INFO - [Train] [24/90] | Loss: 0.4387 | Train Acc: 85.86%
2026-02-10 01:46:08,975 - INFO - [Valid] [24/90] | Loss: 0.6570 | Val Acc: 74.63%
2026-02-10 01:46:08,992 - INFO - [Metrics for 'abnormal'] | Precision: 0.7152 | Recall: 0.7516 | F1: 0.7329
2026-02-10 01:46:08,992 - INFO - [Metrics for 'normal'] | Precision: 0.7759 | Recall: 0.7418 | F1: 0.7584
2026-02-10 01:46:08,994 - INFO - --------------------------------------------------
2026-02-10 01:46:08,995 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:46:12,827 - INFO - [Train] [25/90] | Loss: 0.4519 | Train Acc: 84.38%
2026-02-10 01:46:14,101 - INFO - [Valid] [25/90] | Loss: 0.6415 | Val Acc: 76.40%
2026-02-10 01:46:14,111 - INFO - [Metrics for 'abnormal'] | Precision: 0.7655 | Recall: 0.7070 | F1: 0.7351
2026-02-10 01:46:14,112 - INFO - [Metrics for 'normal'] | Precision: 0.7629 | Recall: 0.8132 | F1: 0.7872
2026-02-10 01:46:14,113 - INFO - --------------------------------------------------
2026-02-10 01:46:14,115 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:46:18,539 - INFO - [Train] [26/90] | Loss: 0.4501 | Train Acc: 85.12%
2026-02-10 01:46:19,951 - INFO - [Valid] [26/90] | Loss: 1.0483 | Val Acc: 68.73%
2026-02-10 01:46:19,956 - INFO - [Metrics for 'abnormal'] | Precision: 0.6024 | Recall: 0.9554 | F1: 0.7389
2026-02-10 01:46:19,956 - INFO - [Metrics for 'normal'] | Precision: 0.9222 | Recall: 0.4560 | F1: 0.6103
2026-02-10 01:46:19,958 - INFO - --------------------------------------------------
2026-02-10 01:46:19,959 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 01:46:23,860 - INFO - [Train] [27/90] | Loss: 0.4532 | Train Acc: 84.38%
2026-02-10 01:46:25,075 - INFO - [Valid] [27/90] | Loss: 0.5134 | Val Acc: 81.71%
2026-02-10 01:46:25,081 - INFO - [Metrics for 'abnormal'] | Precision: 0.8231 | Recall: 0.7707 | F1: 0.7961
2026-02-10 01:46:25,081 - INFO - [Metrics for 'normal'] | Precision: 0.8125 | Recall: 0.8571 | F1: 0.8342
2026-02-10 01:46:25,117 - INFO - [Best Model Saved] (val loss: 0.5134) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:46:25,117 - INFO - --------------------------------------------------
2026-02-10 01:46:25,119 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 01:46:29,596 - INFO - [Train] [28/90] | Loss: 0.4330 | Train Acc: 85.49%
2026-02-10 01:46:30,696 - INFO - [Valid] [28/90] | Loss: 0.6075 | Val Acc: 76.40%
2026-02-10 01:46:30,702 - INFO - [Metrics for 'abnormal'] | Precision: 0.8598 | Recall: 0.5860 | F1: 0.6970
2026-02-10 01:46:30,702 - INFO - [Metrics for 'normal'] | Precision: 0.7198 | Recall: 0.9176 | F1: 0.8068
2026-02-10 01:46:30,704 - INFO - --------------------------------------------------
2026-02-10 01:46:30,705 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 01:46:35,425 - INFO - [Train] [29/90] | Loss: 0.4612 | Train Acc: 84.60%
2026-02-10 01:46:36,664 - INFO - [Valid] [29/90] | Loss: 0.6877 | Val Acc: 78.47%
2026-02-10 01:46:36,669 - INFO - [Metrics for 'abnormal'] | Precision: 0.7234 | Recall: 0.8662 | F1: 0.7884
2026-02-10 01:46:36,670 - INFO - [Metrics for 'normal'] | Precision: 0.8609 | Recall: 0.7143 | F1: 0.7808
2026-02-10 01:46:36,671 - INFO - --------------------------------------------------
2026-02-10 01:46:36,673 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 01:46:41,361 - INFO - [Train] [30/90] | Loss: 0.4623 | Train Acc: 84.45%
2026-02-10 01:46:42,308 - INFO - [Valid] [30/90] | Loss: 0.6859 | Val Acc: 74.93%
2026-02-10 01:46:42,312 - INFO - [Metrics for 'abnormal'] | Precision: 0.7769 | Recall: 0.6433 | F1: 0.7038
2026-02-10 01:46:42,312 - INFO - [Metrics for 'normal'] | Precision: 0.7321 | Recall: 0.8407 | F1: 0.7826
2026-02-10 01:46:42,314 - INFO - --------------------------------------------------
2026-02-10 01:46:42,315 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 01:46:47,244 - INFO - [Train] [31/90] | Loss: 0.4096 | Train Acc: 87.43%
2026-02-10 01:46:47,901 - INFO - [Valid] [31/90] | Loss: 0.4975 | Val Acc: 81.12%
2026-02-10 01:46:47,906 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 01:46:47,906 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 01:46:47,929 - INFO - [Best Model Saved] (val loss: 0.4975) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:46:47,929 - INFO - --------------------------------------------------
2026-02-10 01:46:47,930 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 01:46:52,901 - INFO - [Train] [32/90] | Loss: 0.4111 | Train Acc: 88.69%
2026-02-10 01:46:53,536 - INFO - [Valid] [32/90] | Loss: 0.5181 | Val Acc: 79.35%
2026-02-10 01:46:53,542 - INFO - [Metrics for 'abnormal'] | Precision: 0.7736 | Recall: 0.7834 | F1: 0.7785
2026-02-10 01:46:53,543 - INFO - [Metrics for 'normal'] | Precision: 0.8111 | Recall: 0.8022 | F1: 0.8066
2026-02-10 01:46:53,544 - INFO - --------------------------------------------------
2026-02-10 01:46:53,546 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 01:46:58,530 - INFO - [Train] [33/90] | Loss: 0.4144 | Train Acc: 87.65%
2026-02-10 01:46:59,702 - INFO - [Valid] [33/90] | Loss: 0.5889 | Val Acc: 79.94%
2026-02-10 01:46:59,707 - INFO - [Metrics for 'abnormal'] | Precision: 0.7799 | Recall: 0.7898 | F1: 0.7848
2026-02-10 01:46:59,707 - INFO - [Metrics for 'normal'] | Precision: 0.8167 | Recall: 0.8077 | F1: 0.8122
2026-02-10 01:46:59,709 - INFO - --------------------------------------------------
2026-02-10 01:46:59,710 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 01:47:04,075 - INFO - [Train] [34/90] | Loss: 0.3706 | Train Acc: 90.55%
2026-02-10 01:47:05,463 - INFO - [Valid] [34/90] | Loss: 0.5185 | Val Acc: 81.71%
2026-02-10 01:47:05,468 - INFO - [Metrics for 'abnormal'] | Precision: 0.8146 | Recall: 0.7834 | F1: 0.7987
2026-02-10 01:47:05,468 - INFO - [Metrics for 'normal'] | Precision: 0.8191 | Recall: 0.8462 | F1: 0.8324
2026-02-10 01:47:05,470 - INFO - --------------------------------------------------
2026-02-10 01:47:05,471 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 01:47:09,650 - INFO - [Train] [35/90] | Loss: 0.3680 | Train Acc: 90.55%
2026-02-10 01:47:10,902 - INFO - [Valid] [35/90] | Loss: 0.5080 | Val Acc: 81.42%
2026-02-10 01:47:10,907 - INFO - [Metrics for 'abnormal'] | Precision: 0.7733 | Recall: 0.8471 | F1: 0.8085
2026-02-10 01:47:10,908 - INFO - [Metrics for 'normal'] | Precision: 0.8563 | Recall: 0.7857 | F1: 0.8195
2026-02-10 01:47:10,909 - INFO - --------------------------------------------------
2026-02-10 01:47:10,910 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 01:47:15,576 - INFO - [Train] [36/90] | Loss: 0.3436 | Train Acc: 92.19%
2026-02-10 01:47:16,842 - INFO - [Valid] [36/90] | Loss: 0.5489 | Val Acc: 81.71%
2026-02-10 01:47:16,849 - INFO - [Metrics for 'abnormal'] | Precision: 0.8417 | Recall: 0.7452 | F1: 0.7905
2026-02-10 01:47:16,850 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.8791 | F1: 0.8377
2026-02-10 01:47:16,851 - INFO - --------------------------------------------------
2026-02-10 01:47:16,853 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 01:47:21,707 - INFO - [Train] [37/90] | Loss: 0.3703 | Train Acc: 91.52%
2026-02-10 01:47:22,898 - INFO - [Valid] [37/90] | Loss: 0.5563 | Val Acc: 81.12%
2026-02-10 01:47:22,902 - INFO - [Metrics for 'abnormal'] | Precision: 0.8345 | Recall: 0.7389 | F1: 0.7838
2026-02-10 01:47:22,902 - INFO - [Metrics for 'normal'] | Precision: 0.7950 | Recall: 0.8736 | F1: 0.8325
2026-02-10 01:47:22,904 - INFO - --------------------------------------------------
2026-02-10 01:47:22,905 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 01:47:27,409 - INFO - [Train] [38/90] | Loss: 0.3043 | Train Acc: 95.09%
2026-02-10 01:47:28,686 - INFO - [Valid] [38/90] | Loss: 0.6128 | Val Acc: 75.22%
2026-02-10 01:47:28,693 - INFO - [Metrics for 'abnormal'] | Precision: 0.7589 | Recall: 0.6815 | F1: 0.7181
2026-02-10 01:47:28,693 - INFO - [Metrics for 'normal'] | Precision: 0.7475 | Recall: 0.8132 | F1: 0.7789
2026-02-10 01:47:28,696 - INFO - --------------------------------------------------
2026-02-10 01:47:28,702 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 01:47:33,090 - INFO - [Train] [39/90] | Loss: 0.3297 | Train Acc: 93.30%
2026-02-10 01:47:34,407 - INFO - [Valid] [39/90] | Loss: 0.5362 | Val Acc: 83.78%
2026-02-10 01:47:34,417 - INFO - [Metrics for 'abnormal'] | Precision: 0.8446 | Recall: 0.7962 | F1: 0.8197
2026-02-10 01:47:34,417 - INFO - [Metrics for 'normal'] | Precision: 0.8325 | Recall: 0.8736 | F1: 0.8525
2026-02-10 01:47:34,419 - INFO - --------------------------------------------------
2026-02-10 01:47:34,420 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 01:47:39,027 - INFO - [Train] [40/90] | Loss: 0.3033 | Train Acc: 94.20%
2026-02-10 01:47:40,148 - INFO - [Valid] [40/90] | Loss: 0.5696 | Val Acc: 79.35%
2026-02-10 01:47:40,154 - INFO - [Metrics for 'abnormal'] | Precision: 0.8222 | Recall: 0.7070 | F1: 0.7603
2026-02-10 01:47:40,158 - INFO - [Metrics for 'normal'] | Precision: 0.7745 | Recall: 0.8681 | F1: 0.8187
2026-02-10 01:47:40,160 - INFO - --------------------------------------------------
2026-02-10 01:47:40,161 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 01:47:44,517 - INFO - [Train] [41/90] | Loss: 0.3633 | Train Acc: 91.29%
2026-02-10 01:47:45,917 - INFO - [Valid] [41/90] | Loss: 0.9310 | Val Acc: 75.22%
2026-02-10 01:47:45,923 - INFO - [Metrics for 'abnormal'] | Precision: 0.7786 | Recall: 0.6497 | F1: 0.7083
2026-02-10 01:47:45,923 - INFO - [Metrics for 'normal'] | Precision: 0.7356 | Recall: 0.8407 | F1: 0.7846
2026-02-10 01:47:45,924 - INFO - --------------------------------------------------
2026-02-10 01:47:45,926 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 01:47:50,394 - INFO - [Train] [42/90] | Loss: 0.3896 | Train Acc: 89.14%
2026-02-10 01:47:51,597 - INFO - [Valid] [42/90] | Loss: 0.5888 | Val Acc: 78.17%
2026-02-10 01:47:51,606 - INFO - [Metrics for 'abnormal'] | Precision: 0.8739 | Recall: 0.6178 | F1: 0.7239
2026-02-10 01:47:51,607 - INFO - [Metrics for 'normal'] | Precision: 0.7368 | Recall: 0.9231 | F1: 0.8195
2026-02-10 01:47:51,608 - INFO - --------------------------------------------------
2026-02-10 01:47:51,610 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 01:47:56,206 - INFO - [Train] [43/90] | Loss: 0.3475 | Train Acc: 92.78%
2026-02-10 01:47:57,370 - INFO - [Valid] [43/90] | Loss: 0.6113 | Val Acc: 76.99%
2026-02-10 01:47:57,378 - INFO - [Metrics for 'abnormal'] | Precision: 0.7842 | Recall: 0.6943 | F1: 0.7365
2026-02-10 01:47:57,378 - INFO - [Metrics for 'normal'] | Precision: 0.7600 | Recall: 0.8352 | F1: 0.7958
2026-02-10 01:47:57,384 - INFO - --------------------------------------------------
2026-02-10 01:47:57,386 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 01:48:01,900 - INFO - [Train] [44/90] | Loss: 0.3267 | Train Acc: 92.41%
2026-02-10 01:48:03,158 - INFO - [Valid] [44/90] | Loss: 0.5122 | Val Acc: 82.30%
2026-02-10 01:48:03,167 - INFO - [Metrics for 'abnormal'] | Precision: 0.8089 | Recall: 0.8089 | F1: 0.8089
2026-02-10 01:48:03,167 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8352 | F1: 0.8352
2026-02-10 01:48:03,170 - INFO - --------------------------------------------------
2026-02-10 01:48:03,171 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 01:48:07,886 - INFO - [Train] [45/90] | Loss: 0.3137 | Train Acc: 94.12%
2026-02-10 01:48:08,966 - INFO - [Valid] [45/90] | Loss: 0.5737 | Val Acc: 78.76%
2026-02-10 01:48:08,972 - INFO - [Metrics for 'abnormal'] | Precision: 0.8058 | Recall: 0.7134 | F1: 0.7568
2026-02-10 01:48:08,973 - INFO - [Metrics for 'normal'] | Precision: 0.7750 | Recall: 0.8516 | F1: 0.8115
2026-02-10 01:48:08,974 - INFO - --------------------------------------------------
2026-02-10 01:48:08,976 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 01:48:13,651 - INFO - [Train] [46/90] | Loss: 0.3284 | Train Acc: 93.38%
2026-02-10 01:48:14,640 - INFO - [Valid] [46/90] | Loss: 0.5882 | Val Acc: 79.94%
2026-02-10 01:48:14,646 - INFO - [Metrics for 'abnormal'] | Precision: 0.8504 | Recall: 0.6879 | F1: 0.7606
2026-02-10 01:48:14,646 - INFO - [Metrics for 'normal'] | Precision: 0.7689 | Recall: 0.8956 | F1: 0.8274
2026-02-10 01:48:14,647 - INFO - --------------------------------------------------
2026-02-10 01:48:14,649 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 01:48:19,908 - INFO - [Train] [47/90] | Loss: 0.2816 | Train Acc: 96.58%
2026-02-10 01:48:20,796 - INFO - [Valid] [47/90] | Loss: 0.5572 | Val Acc: 79.94%
2026-02-10 01:48:20,806 - INFO - [Metrics for 'abnormal'] | Precision: 0.7730 | Recall: 0.8025 | F1: 0.7875
2026-02-10 01:48:20,806 - INFO - [Metrics for 'normal'] | Precision: 0.8239 | Recall: 0.7967 | F1: 0.8101
2026-02-10 01:48:20,808 - INFO - --------------------------------------------------
2026-02-10 01:48:20,810 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 01:48:25,580 - INFO - [Train] [48/90] | Loss: 0.2755 | Train Acc: 96.43%
2026-02-10 01:48:26,728 - INFO - [Valid] [48/90] | Loss: 0.5755 | Val Acc: 80.83%
2026-02-10 01:48:26,733 - INFO - [Metrics for 'abnormal'] | Precision: 0.8538 | Recall: 0.7070 | F1: 0.7735
2026-02-10 01:48:26,733 - INFO - [Metrics for 'normal'] | Precision: 0.7799 | Recall: 0.8956 | F1: 0.8338
2026-02-10 01:48:26,735 - INFO - --------------------------------------------------
2026-02-10 01:48:26,736 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 01:48:31,406 - INFO - [Train] [49/90] | Loss: 0.2769 | Train Acc: 96.35%
2026-02-10 01:48:32,980 - INFO - [Valid] [49/90] | Loss: 0.5664 | Val Acc: 82.30%
2026-02-10 01:48:32,985 - INFO - [Metrics for 'abnormal'] | Precision: 0.8392 | Recall: 0.7643 | F1: 0.8000
2026-02-10 01:48:32,985 - INFO - [Metrics for 'normal'] | Precision: 0.8112 | Recall: 0.8736 | F1: 0.8413
2026-02-10 01:48:32,987 - INFO - --------------------------------------------------
2026-02-10 01:48:32,988 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 01:48:37,186 - INFO - [Train] [50/90] | Loss: 0.3041 | Train Acc: 95.24%
2026-02-10 01:48:38,434 - INFO - [Valid] [50/90] | Loss: 0.5524 | Val Acc: 82.89%
2026-02-10 01:48:38,439 - INFO - [Metrics for 'abnormal'] | Precision: 0.7964 | Recall: 0.8471 | F1: 0.8210
2026-02-10 01:48:38,440 - INFO - [Metrics for 'normal'] | Precision: 0.8605 | Recall: 0.8132 | F1: 0.8362
2026-02-10 01:48:38,441 - INFO - --------------------------------------------------
2026-02-10 01:48:38,442 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 01:48:42,434 - INFO - [Train] [51/90] | Loss: 0.2688 | Train Acc: 97.25%
2026-02-10 01:48:43,765 - INFO - [Valid] [51/90] | Loss: 0.5541 | Val Acc: 82.89%
2026-02-10 01:48:43,770 - INFO - [Metrics for 'abnormal'] | Precision: 0.8153 | Recall: 0.8153 | F1: 0.8153
2026-02-10 01:48:43,774 - INFO - [Metrics for 'normal'] | Precision: 0.8407 | Recall: 0.8407 | F1: 0.8407
2026-02-10 01:48:43,776 - INFO - --------------------------------------------------
2026-02-10 01:48:43,778 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 01:48:48,183 - INFO - [Train] [52/90] | Loss: 0.2793 | Train Acc: 96.35%
2026-02-10 01:48:49,310 - INFO - [Valid] [52/90] | Loss: 0.5864 | Val Acc: 80.24%
2026-02-10 01:48:49,315 - INFO - [Metrics for 'abnormal'] | Precision: 0.8261 | Recall: 0.7261 | F1: 0.7729
2026-02-10 01:48:49,315 - INFO - [Metrics for 'normal'] | Precision: 0.7861 | Recall: 0.8681 | F1: 0.8251
2026-02-10 01:48:49,318 - INFO - --------------------------------------------------
2026-02-10 01:48:49,320 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 01:48:54,002 - INFO - [Train] [53/90] | Loss: 0.2629 | Train Acc: 96.88%
2026-02-10 01:48:55,231 - INFO - [Valid] [53/90] | Loss: 0.5692 | Val Acc: 83.19%
2026-02-10 01:48:55,237 - INFO - [Metrics for 'abnormal'] | Precision: 0.7907 | Recall: 0.8662 | F1: 0.8267
2026-02-10 01:48:55,237 - INFO - [Metrics for 'normal'] | Precision: 0.8743 | Recall: 0.8022 | F1: 0.8367
2026-02-10 01:48:55,239 - INFO - --------------------------------------------------
2026-02-10 01:48:55,240 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 01:48:59,752 - INFO - [Train] [54/90] | Loss: 0.2568 | Train Acc: 97.77%
2026-02-10 01:49:01,088 - INFO - [Valid] [54/90] | Loss: 0.6091 | Val Acc: 82.89%
2026-02-10 01:49:01,092 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8408 | F1: 0.8199
2026-02-10 01:49:01,092 - INFO - [Metrics for 'normal'] | Precision: 0.8563 | Recall: 0.8187 | F1: 0.8371
2026-02-10 01:49:01,094 - INFO - --------------------------------------------------
2026-02-10 01:49:01,095 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 01:49:05,352 - INFO - [Train] [55/90] | Loss: 0.2461 | Train Acc: 98.66%
2026-02-10 01:49:06,256 - INFO - [Valid] [55/90] | Loss: 0.5813 | Val Acc: 82.89%
2026-02-10 01:49:06,262 - INFO - [Metrics for 'abnormal'] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-02-10 01:49:06,263 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-02-10 01:49:06,265 - INFO - --------------------------------------------------
2026-02-10 01:49:06,266 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 01:49:11,123 - INFO - [Train] [56/90] | Loss: 0.2626 | Train Acc: 97.40%
2026-02-10 01:49:12,144 - INFO - [Valid] [56/90] | Loss: 0.5407 | Val Acc: 84.37%
2026-02-10 01:49:12,149 - INFO - [Metrics for 'abnormal'] | Precision: 0.8421 | Recall: 0.8153 | F1: 0.8285
2026-02-10 01:49:12,149 - INFO - [Metrics for 'normal'] | Precision: 0.8449 | Recall: 0.8681 | F1: 0.8564
2026-02-10 01:49:12,150 - INFO - --------------------------------------------------
2026-02-10 01:49:12,151 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 01:49:17,006 - INFO - [Train] [57/90] | Loss: 0.2579 | Train Acc: 97.25%
2026-02-10 01:49:17,896 - INFO - [Valid] [57/90] | Loss: 0.5931 | Val Acc: 82.89%
2026-02-10 01:49:17,901 - INFO - [Metrics for 'abnormal'] | Precision: 0.8153 | Recall: 0.8153 | F1: 0.8153
2026-02-10 01:49:17,901 - INFO - [Metrics for 'normal'] | Precision: 0.8407 | Recall: 0.8407 | F1: 0.8407
2026-02-10 01:49:17,903 - INFO - --------------------------------------------------
2026-02-10 01:49:17,904 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 01:49:22,752 - INFO - [Train] [58/90] | Loss: 0.2839 | Train Acc: 95.98%
2026-02-10 01:49:23,779 - INFO - [Valid] [58/90] | Loss: 0.6694 | Val Acc: 76.40%
2026-02-10 01:49:23,784 - INFO - [Metrics for 'abnormal'] | Precision: 0.7655 | Recall: 0.7070 | F1: 0.7351
2026-02-10 01:49:23,784 - INFO - [Metrics for 'normal'] | Precision: 0.7629 | Recall: 0.8132 | F1: 0.7872
2026-02-10 01:49:23,786 - INFO - --------------------------------------------------
2026-02-10 01:49:23,788 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 01:49:28,359 - INFO - [Train] [59/90] | Loss: 0.2732 | Train Acc: 96.13%
2026-02-10 01:49:29,429 - INFO - [Valid] [59/90] | Loss: 0.5742 | Val Acc: 80.83%
2026-02-10 01:49:29,435 - INFO - [Metrics for 'abnormal'] | Precision: 0.8194 | Recall: 0.7516 | F1: 0.7841
2026-02-10 01:49:29,435 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.8571 | F1: 0.8276
2026-02-10 01:49:29,437 - INFO - --------------------------------------------------
2026-02-10 01:49:29,438 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 01:49:33,619 - INFO - [Train] [60/90] | Loss: 0.2857 | Train Acc: 95.91%
2026-02-10 01:49:34,914 - INFO - [Valid] [60/90] | Loss: 0.5792 | Val Acc: 81.12%
2026-02-10 01:49:34,919 - INFO - [Metrics for 'abnormal'] | Precision: 0.7598 | Recall: 0.8662 | F1: 0.8095
2026-02-10 01:49:34,919 - INFO - [Metrics for 'normal'] | Precision: 0.8688 | Recall: 0.7637 | F1: 0.8129
2026-02-10 01:49:34,920 - INFO - --------------------------------------------------
2026-02-10 01:49:34,921 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 01:49:38,618 - INFO - [Train] [61/90] | Loss: 0.2508 | Train Acc: 98.14%
2026-02-10 01:49:39,869 - INFO - [Valid] [61/90] | Loss: 0.5717 | Val Acc: 81.42%
2026-02-10 01:49:39,874 - INFO - [Metrics for 'abnormal'] | Precision: 0.7937 | Recall: 0.8089 | F1: 0.8013
2026-02-10 01:49:39,876 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8187 | F1: 0.8255
2026-02-10 01:49:39,879 - INFO - --------------------------------------------------
2026-02-10 01:49:39,880 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 01:49:44,346 - INFO - [Train] [62/90] | Loss: 0.2630 | Train Acc: 97.40%
2026-02-10 01:49:45,329 - INFO - [Valid] [62/90] | Loss: 0.5277 | Val Acc: 82.01%
2026-02-10 01:49:45,334 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-02-10 01:49:45,334 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-02-10 01:49:45,336 - INFO - --------------------------------------------------
2026-02-10 01:49:45,337 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 01:49:49,758 - INFO - [Train] [63/90] | Loss: 0.2577 | Train Acc: 97.40%
2026-02-10 01:49:50,881 - INFO - [Valid] [63/90] | Loss: 0.6030 | Val Acc: 78.47%
2026-02-10 01:49:50,885 - INFO - [Metrics for 'abnormal'] | Precision: 0.7838 | Recall: 0.7389 | F1: 0.7607
2026-02-10 01:49:50,886 - INFO - [Metrics for 'normal'] | Precision: 0.7853 | Recall: 0.8242 | F1: 0.8043
2026-02-10 01:49:50,887 - INFO - --------------------------------------------------
2026-02-10 01:49:50,889 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 01:49:55,309 - INFO - [Train] [64/90] | Loss: 0.2583 | Train Acc: 97.77%
2026-02-10 01:49:56,281 - INFO - [Valid] [64/90] | Loss: 0.5334 | Val Acc: 82.30%
2026-02-10 01:49:56,286 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-02-10 01:49:56,286 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-02-10 01:49:56,288 - INFO - --------------------------------------------------
2026-02-10 01:49:56,290 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 01:50:00,349 - INFO - [Train] [65/90] | Loss: 0.2407 | Train Acc: 98.51%
2026-02-10 01:50:01,676 - INFO - [Valid] [65/90] | Loss: 0.5519 | Val Acc: 82.01%
2026-02-10 01:50:01,684 - INFO - [Metrics for 'abnormal'] | Precision: 0.8243 | Recall: 0.7771 | F1: 0.8000
2026-02-10 01:50:01,684 - INFO - [Metrics for 'normal'] | Precision: 0.8168 | Recall: 0.8571 | F1: 0.8365
2026-02-10 01:50:01,685 - INFO - --------------------------------------------------
2026-02-10 01:50:01,687 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 01:50:05,869 - INFO - [Train] [66/90] | Loss: 0.2387 | Train Acc: 98.66%
2026-02-10 01:50:06,922 - INFO - [Valid] [66/90] | Loss: 0.5459 | Val Acc: 83.48%
2026-02-10 01:50:06,928 - INFO - [Metrics for 'abnormal'] | Precision: 0.8435 | Recall: 0.7898 | F1: 0.8158
2026-02-10 01:50:06,928 - INFO - [Metrics for 'normal'] | Precision: 0.8281 | Recall: 0.8736 | F1: 0.8503
2026-02-10 01:50:06,930 - INFO - --------------------------------------------------
2026-02-10 01:50:06,931 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 01:50:11,416 - INFO - [Train] [67/90] | Loss: 0.2396 | Train Acc: 98.88%
2026-02-10 01:50:12,597 - INFO - [Valid] [67/90] | Loss: 0.5401 | Val Acc: 82.30%
2026-02-10 01:50:12,602 - INFO - [Metrics for 'abnormal'] | Precision: 0.8050 | Recall: 0.8153 | F1: 0.8101
2026-02-10 01:50:12,602 - INFO - [Metrics for 'normal'] | Precision: 0.8389 | Recall: 0.8297 | F1: 0.8343
2026-02-10 01:50:12,604 - INFO - --------------------------------------------------
2026-02-10 01:50:12,605 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 01:50:17,222 - INFO - [Train] [68/90] | Loss: 0.2355 | Train Acc: 98.96%
2026-02-10 01:50:18,367 - INFO - [Valid] [68/90] | Loss: 0.5810 | Val Acc: 81.42%
2026-02-10 01:50:18,372 - INFO - [Metrics for 'abnormal'] | Precision: 0.7765 | Recall: 0.8408 | F1: 0.8073
2026-02-10 01:50:18,376 - INFO - [Metrics for 'normal'] | Precision: 0.8521 | Recall: 0.7912 | F1: 0.8205
2026-02-10 01:50:18,378 - INFO - --------------------------------------------------
2026-02-10 01:50:18,380 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 01:50:22,423 - INFO - [Train] [69/90] | Loss: 0.2507 | Train Acc: 98.21%
2026-02-10 01:50:23,525 - INFO - [Valid] [69/90] | Loss: 0.5679 | Val Acc: 83.48%
2026-02-10 01:50:23,531 - INFO - [Metrics for 'abnormal'] | Precision: 0.8531 | Recall: 0.7771 | F1: 0.8133
2026-02-10 01:50:23,531 - INFO - [Metrics for 'normal'] | Precision: 0.8214 | Recall: 0.8846 | F1: 0.8519
2026-02-10 01:50:23,532 - INFO - --------------------------------------------------
2026-02-10 01:50:23,534 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 01:50:28,008 - INFO - [Train] [70/90] | Loss: 0.2415 | Train Acc: 98.59%
2026-02-10 01:50:29,132 - INFO - [Valid] [70/90] | Loss: 0.5696 | Val Acc: 82.60%
2026-02-10 01:50:29,137 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8280 | F1: 0.8150
2026-02-10 01:50:29,137 - INFO - [Metrics for 'normal'] | Precision: 0.8475 | Recall: 0.8242 | F1: 0.8357
2026-02-10 01:50:29,139 - INFO - --------------------------------------------------
2026-02-10 01:50:29,140 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 01:50:33,447 - INFO - [Train] [71/90] | Loss: 0.2340 | Train Acc: 98.88%
2026-02-10 01:50:34,369 - INFO - [Valid] [71/90] | Loss: 0.5642 | Val Acc: 80.83%
2026-02-10 01:50:34,374 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-02-10 01:50:34,375 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-02-10 01:50:34,377 - INFO - --------------------------------------------------
2026-02-10 01:50:34,378 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 01:50:38,888 - INFO - [Train] [72/90] | Loss: 0.2315 | Train Acc: 98.88%
2026-02-10 01:50:40,004 - INFO - [Valid] [72/90] | Loss: 0.5889 | Val Acc: 82.01%
2026-02-10 01:50:40,013 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 01:50:40,014 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 01:50:40,016 - INFO - --------------------------------------------------
2026-02-10 01:50:40,017 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 01:50:44,095 - INFO - [Train] [73/90] | Loss: 0.2283 | Train Acc: 99.11%
2026-02-10 01:50:45,316 - INFO - [Valid] [73/90] | Loss: 0.6124 | Val Acc: 81.71%
2026-02-10 01:50:45,321 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.8089 | F1: 0.8038
2026-02-10 01:50:45,321 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8242 | F1: 0.8287
2026-02-10 01:50:45,322 - INFO - --------------------------------------------------
2026-02-10 01:50:45,324 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 01:50:49,932 - INFO - [Train] [74/90] | Loss: 0.2259 | Train Acc: 99.18%
2026-02-10 01:50:51,228 - INFO - [Valid] [74/90] | Loss: 0.5926 | Val Acc: 82.01%
2026-02-10 01:50:51,232 - INFO - [Metrics for 'abnormal'] | Precision: 0.8243 | Recall: 0.7771 | F1: 0.8000
2026-02-10 01:50:51,232 - INFO - [Metrics for 'normal'] | Precision: 0.8168 | Recall: 0.8571 | F1: 0.8365
2026-02-10 01:50:51,234 - INFO - --------------------------------------------------
2026-02-10 01:50:51,235 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 01:50:55,608 - INFO - [Train] [75/90] | Loss: 0.2298 | Train Acc: 99.11%
2026-02-10 01:50:56,329 - INFO - [Valid] [75/90] | Loss: 0.5899 | Val Acc: 82.60%
2026-02-10 01:50:56,338 - INFO - [Metrics for 'abnormal'] | Precision: 0.7952 | Recall: 0.8408 | F1: 0.8173
2026-02-10 01:50:56,338 - INFO - [Metrics for 'normal'] | Precision: 0.8555 | Recall: 0.8132 | F1: 0.8338
2026-02-10 01:50:56,340 - INFO - --------------------------------------------------
2026-02-10 01:50:56,341 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 01:51:01,466 - INFO - [Train] [76/90] | Loss: 0.2269 | Train Acc: 99.03%
2026-02-10 01:51:02,559 - INFO - [Valid] [76/90] | Loss: 0.5918 | Val Acc: 81.12%
2026-02-10 01:51:02,564 - INFO - [Metrics for 'abnormal'] | Precision: 0.8207 | Recall: 0.7580 | F1: 0.7881
2026-02-10 01:51:02,564 - INFO - [Metrics for 'normal'] | Precision: 0.8041 | Recall: 0.8571 | F1: 0.8298
2026-02-10 01:51:02,566 - INFO - --------------------------------------------------
2026-02-10 01:51:02,566 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 01:51:07,519 - INFO - [Train] [77/90] | Loss: 0.2388 | Train Acc: 98.21%
2026-02-10 01:51:08,779 - INFO - [Valid] [77/90] | Loss: 0.5671 | Val Acc: 80.83%
2026-02-10 01:51:08,784 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-02-10 01:51:08,784 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-02-10 01:51:08,786 - INFO - --------------------------------------------------
2026-02-10 01:51:08,787 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 01:51:13,726 - INFO - [Train] [78/90] | Loss: 0.2290 | Train Acc: 98.74%
2026-02-10 01:51:14,891 - INFO - [Valid] [78/90] | Loss: 0.5906 | Val Acc: 81.12%
2026-02-10 01:51:14,896 - INFO - [Metrics for 'abnormal'] | Precision: 0.8039 | Recall: 0.7834 | F1: 0.7935
2026-02-10 01:51:14,896 - INFO - [Metrics for 'normal'] | Precision: 0.8172 | Recall: 0.8352 | F1: 0.8261
2026-02-10 01:51:14,902 - INFO - --------------------------------------------------
2026-02-10 01:51:14,903 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 01:51:19,386 - INFO - [Train] [79/90] | Loss: 0.2412 | Train Acc: 98.44%
2026-02-10 01:51:20,662 - INFO - [Valid] [79/90] | Loss: 0.5809 | Val Acc: 82.01%
2026-02-10 01:51:20,668 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-02-10 01:51:20,668 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-02-10 01:51:20,669 - INFO - --------------------------------------------------
2026-02-10 01:51:20,671 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 01:51:25,203 - INFO - [Train] [80/90] | Loss: 0.2238 | Train Acc: 99.18%
2026-02-10 01:51:26,124 - INFO - [Valid] [80/90] | Loss: 0.5593 | Val Acc: 82.30%
2026-02-10 01:51:26,129 - INFO - [Metrics for 'abnormal'] | Precision: 0.8050 | Recall: 0.8153 | F1: 0.8101
2026-02-10 01:51:26,129 - INFO - [Metrics for 'normal'] | Precision: 0.8389 | Recall: 0.8297 | F1: 0.8343
2026-02-10 01:51:26,130 - INFO - --------------------------------------------------
2026-02-10 01:51:26,132 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 01:51:30,941 - INFO - [Train] [81/90] | Loss: 0.2218 | Train Acc: 99.11%
2026-02-10 01:51:31,978 - INFO - [Valid] [81/90] | Loss: 0.5516 | Val Acc: 82.60%
2026-02-10 01:51:31,981 - INFO - [Metrics for 'abnormal'] | Precision: 0.7882 | Recall: 0.8535 | F1: 0.8196
2026-02-10 01:51:31,981 - INFO - [Metrics for 'normal'] | Precision: 0.8639 | Recall: 0.8022 | F1: 0.8319
2026-02-10 01:51:31,982 - INFO - --------------------------------------------------
2026-02-10 01:51:31,982 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 01:51:36,511 - INFO - [Train] [82/90] | Loss: 0.2172 | Train Acc: 99.48%
2026-02-10 01:51:37,860 - INFO - [Valid] [82/90] | Loss: 0.6184 | Val Acc: 81.42%
2026-02-10 01:51:37,866 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-02-10 01:51:37,866 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-02-10 01:51:37,868 - INFO - --------------------------------------------------
2026-02-10 01:51:37,869 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 01:51:41,863 - INFO - [Train] [83/90] | Loss: 0.2232 | Train Acc: 99.11%
2026-02-10 01:51:43,300 - INFO - [Valid] [83/90] | Loss: 0.5836 | Val Acc: 81.42%
2026-02-10 01:51:43,306 - INFO - [Metrics for 'abnormal'] | Precision: 0.7701 | Recall: 0.8535 | F1: 0.8097
2026-02-10 01:51:43,306 - INFO - [Metrics for 'normal'] | Precision: 0.8606 | Recall: 0.7802 | F1: 0.8184
2026-02-10 01:51:43,307 - INFO - --------------------------------------------------
2026-02-10 01:51:43,309 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 01:51:47,448 - INFO - [Train] [84/90] | Loss: 0.2239 | Train Acc: 99.18%
2026-02-10 01:51:48,584 - INFO - [Valid] [84/90] | Loss: 0.5510 | Val Acc: 81.12%
2026-02-10 01:51:48,590 - INFO - [Metrics for 'abnormal'] | Precision: 0.8252 | Recall: 0.7516 | F1: 0.7867
2026-02-10 01:51:48,590 - INFO - [Metrics for 'normal'] | Precision: 0.8010 | Recall: 0.8626 | F1: 0.8307
2026-02-10 01:51:48,592 - INFO - --------------------------------------------------
2026-02-10 01:51:48,594 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 01:51:53,116 - INFO - [Train] [85/90] | Loss: 0.2175 | Train Acc: 99.40%
2026-02-10 01:51:54,119 - INFO - [Valid] [85/90] | Loss: 0.5624 | Val Acc: 81.71%
2026-02-10 01:51:54,124 - INFO - [Metrics for 'abnormal'] | Precision: 0.8065 | Recall: 0.7962 | F1: 0.8013
2026-02-10 01:51:54,124 - INFO - [Metrics for 'normal'] | Precision: 0.8261 | Recall: 0.8352 | F1: 0.8306
2026-02-10 01:51:54,126 - INFO - --------------------------------------------------
2026-02-10 01:51:54,128 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 01:51:58,933 - INFO - [Train] [86/90] | Loss: 0.2136 | Train Acc: 99.55%
2026-02-10 01:51:59,977 - INFO - [Valid] [86/90] | Loss: 0.5576 | Val Acc: 82.60%
2026-02-10 01:51:59,982 - INFO - [Metrics for 'abnormal'] | Precision: 0.8267 | Recall: 0.7898 | F1: 0.8078
2026-02-10 01:51:59,982 - INFO - [Metrics for 'normal'] | Precision: 0.8254 | Recall: 0.8571 | F1: 0.8410
2026-02-10 01:51:59,983 - INFO - --------------------------------------------------
2026-02-10 01:51:59,985 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 01:52:04,170 - INFO - [Train] [87/90] | Loss: 0.2091 | Train Acc: 99.85%
2026-02-10 01:52:05,110 - INFO - [Valid] [87/90] | Loss: 0.5490 | Val Acc: 81.71%
2026-02-10 01:52:05,114 - INFO - [Metrics for 'abnormal'] | Precision: 0.8188 | Recall: 0.7771 | F1: 0.7974
2026-02-10 01:52:05,123 - INFO - [Metrics for 'normal'] | Precision: 0.8158 | Recall: 0.8516 | F1: 0.8333
2026-02-10 01:52:05,127 - INFO - --------------------------------------------------
2026-02-10 01:52:05,128 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 01:52:09,600 - INFO - [Train] [88/90] | Loss: 0.2147 | Train Acc: 99.55%
2026-02-10 01:52:10,589 - INFO - [Valid] [88/90] | Loss: 0.5538 | Val Acc: 82.60%
2026-02-10 01:52:10,595 - INFO - [Metrics for 'abnormal'] | Precision: 0.8311 | Recall: 0.7834 | F1: 0.8066
2026-02-10 01:52:10,596 - INFO - [Metrics for 'normal'] | Precision: 0.8220 | Recall: 0.8626 | F1: 0.8418
2026-02-10 01:52:10,597 - INFO - --------------------------------------------------
2026-02-10 01:52:10,600 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 01:52:15,476 - INFO - [Train] [89/90] | Loss: 0.2151 | Train Acc: 99.48%
2026-02-10 01:52:16,037 - INFO - [Valid] [89/90] | Loss: 0.5588 | Val Acc: 83.78%
2026-02-10 01:52:16,043 - INFO - [Metrics for 'abnormal'] | Precision: 0.8269 | Recall: 0.8217 | F1: 0.8243
2026-02-10 01:52:16,044 - INFO - [Metrics for 'normal'] | Precision: 0.8470 | Recall: 0.8516 | F1: 0.8493
2026-02-10 01:52:16,045 - INFO - --------------------------------------------------
2026-02-10 01:52:16,047 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 01:52:21,023 - INFO - [Train] [90/90] | Loss: 0.2148 | Train Acc: 99.48%
2026-02-10 01:52:21,878 - INFO - [Valid] [90/90] | Loss: 0.5271 | Val Acc: 83.19%
2026-02-10 01:52:21,884 - INFO - [Metrics for 'abnormal'] | Precision: 0.8205 | Recall: 0.8153 | F1: 0.8179
2026-02-10 01:52:21,884 - INFO - [Metrics for 'normal'] | Precision: 0.8415 | Recall: 0.8462 | F1: 0.8438
2026-02-10 01:52:21,886 - INFO - ==================================================
2026-02-10 01:52:21,887 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 01:52:21,891 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 01:52:21,891 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:52:22,035 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 01:52:22,037 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:52:22,038 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:52:22,039 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:52:26,729 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:52:26,870 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.327744140625)에 맞춰 변경되었습니다.
2026-02-10 01:52:26,871 - INFO - ==================================================
2026-02-10 01:52:26,934 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/best_model.pth'
2026-02-10 01:52:26,934 - INFO - ==================================================
2026-02-10 01:52:26,935 - INFO - Test 모드를 시작합니다.
2026-02-10 01:52:27,080 - INFO - 연산량 (MACs): 0.0911 GMACs per sample
2026-02-10 01:52:27,081 - INFO - 연산량 (FLOPs): 0.1822 GFLOPs per sample
2026-02-10 01:52:27,081 - INFO - ==================================================
2026-02-10 01:52:27,081 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 01:52:28,221 - INFO - 샘플 당 평균 Forward Pass 시간: 2.85ms (std: 0.66ms), FPS: 371.56 (std: 105.04) (1개 샘플 x 100회 반복)
2026-02-10 01:52:28,221 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 81.55 MB
2026-02-10 01:52:28,221 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 01:52:30,462 - INFO - [Test] Loss: 0.4185 | Test Acc: 81.12%
2026-02-10 01:52:30,470 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 01:52:30,470 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 01:52:30,775 - INFO - ==================================================
2026-02-10 01:52:30,775 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/confusion_matrix_20260210_013415.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/confusion_matrix_20260210_013415.pdf'
2026-02-10 01:52:30,775 - INFO - ==================================================
2026-02-10 01:52:30,775 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 01:52:33,929 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/model_fp32_20260210_013415.onnx'에 저장되었습니다. (크기: 4.36 MB)
2026-02-10 01:52:34,455 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 20.06 MB
2026-02-10 01:52:34,455 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 01:52:36,535 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 15.57ms (std: 8.99ms)
2026-02-10 01:52:36,536 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 108.53 FPS (std: 107.23) (1개 샘플 x 100회 반복)
2026-02-10 01:52:36,536 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 4.69 MB
2026-02-10 01:52:36,536 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 19.83 MB
2026-02-10 01:52:40,712 - INFO - [Test (ONNX)] | Test Acc (ONNX): 81.12%
2026-02-10 01:52:40,722 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 01:52:40,723 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 01:52:40,993 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/val_acc.pdf'
2026-02-10 01:52:41,224 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/train_val_acc.pdf'
2026-02-10 01:52:41,415 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/F1_normal.pdf'
2026-02-10 01:52:41,630 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/val_loss.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/val_loss.pdf'
2026-02-10 01:52:41,825 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/learning_rate.pdf'
2026-02-10 01:52:44,194 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/compile.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_wanda_20260210_013415/graph_20260210_013415/compile.pdf'
