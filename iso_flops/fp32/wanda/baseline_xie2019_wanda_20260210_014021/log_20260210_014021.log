2026-02-10 01:40:21,772 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/log_20260210_014021.log'에 저장됩니다.
2026-02-10 01:40:21,775 - INFO - ==================================================
2026-02-10 01:40:21,775 - INFO - config.yaml:
2026-02-10 01:40:21,775 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: xie2019
  use_wanda_pruning: true
  num_wanda_calib_samples: 1353
  pruning_flops_target: 0.1816

2026-02-10 01:40:21,775 - INFO - ==================================================
2026-02-10 01:40:21,814 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:40:21,817 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:40:21,817 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:40:21,823 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:40:21,823 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:40:21,824 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:40:21,824 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:40:22,064 - INFO - ==================================================
2026-02-10 01:40:22,065 - INFO - 모델 파라미터 수:
2026-02-10 01:40:22,065 - INFO -   - 총 파라미터: 9,160,194 개
2026-02-10 01:40:22,065 - INFO -   - 학습 가능한 파라미터: 9,160,194 개
2026-02-10 01:40:22,065 - INFO - ================================================================================
2026-02-10 01:40:22,065 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:40:22,065 - INFO - ================================================================================
2026-02-10 01:40:22,066 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:40:22,066 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:40:22,066 - INFO - ==================================================
2026-02-10 01:40:22,066 - INFO - train 모드를 시작합니다.
2026-02-10 01:40:22,066 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:40:22,066 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:40:22,066 - INFO - --------------------------------------------------
2026-02-10 01:40:22,066 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:40:29,171 - INFO - [Train] [1/10] | Loss: 0.5918 | Train Acc: 74.26%
2026-02-10 01:40:31,590 - INFO - [Valid] [1/10] | Loss: 0.5699 | Val Acc: 74.04%
2026-02-10 01:40:31,595 - INFO - [Metrics for 'abnormal'] | Precision: 0.7949 | Recall: 0.5924 | F1: 0.6788
2026-02-10 01:40:31,595 - INFO - [Metrics for 'normal'] | Precision: 0.7117 | Recall: 0.8681 | F1: 0.7822
2026-02-10 01:40:31,627 - INFO - [Best Model Saved] (val loss: 0.5699) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:40:31,627 - INFO - --------------------------------------------------
2026-02-10 01:40:31,627 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:40:37,463 - INFO - [Train] [2/10] | Loss: 0.5492 | Train Acc: 78.65%
2026-02-10 01:40:38,485 - INFO - [Valid] [2/10] | Loss: 0.5790 | Val Acc: 71.98%
2026-02-10 01:40:38,491 - INFO - [Metrics for 'abnormal'] | Precision: 0.6914 | Recall: 0.7134 | F1: 0.7022
2026-02-10 01:40:38,491 - INFO - [Metrics for 'normal'] | Precision: 0.7458 | Recall: 0.7253 | F1: 0.7354
2026-02-10 01:40:38,492 - INFO - --------------------------------------------------
2026-02-10 01:40:38,493 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:40:44,718 - INFO - [Train] [3/10] | Loss: 0.5280 | Train Acc: 79.09%
2026-02-10 01:40:45,714 - INFO - [Valid] [3/10] | Loss: 0.5520 | Val Acc: 74.34%
2026-02-10 01:40:45,719 - INFO - [Metrics for 'abnormal'] | Precision: 0.6842 | Recall: 0.8280 | F1: 0.7493
2026-02-10 01:40:45,720 - INFO - [Metrics for 'normal'] | Precision: 0.8188 | Recall: 0.6703 | F1: 0.7372
2026-02-10 01:40:45,852 - INFO - [Best Model Saved] (val loss: 0.5520) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:40:45,852 - INFO - --------------------------------------------------
2026-02-10 01:40:45,853 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:40:51,895 - INFO - [Train] [4/10] | Loss: 0.5109 | Train Acc: 80.95%
2026-02-10 01:40:53,394 - INFO - [Valid] [4/10] | Loss: 0.5523 | Val Acc: 75.81%
2026-02-10 01:40:53,403 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.6369 | F1: 0.7092
2026-02-10 01:40:53,403 - INFO - [Metrics for 'normal'] | Precision: 0.7336 | Recall: 0.8626 | F1: 0.7929
2026-02-10 01:40:53,405 - INFO - --------------------------------------------------
2026-02-10 01:40:53,405 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:40:59,082 - INFO - [Train] [5/10] | Loss: 0.4925 | Train Acc: 80.51%
2026-02-10 01:41:00,553 - INFO - [Valid] [5/10] | Loss: 0.5229 | Val Acc: 77.29%
2026-02-10 01:41:00,558 - INFO - [Metrics for 'abnormal'] | Precision: 0.7469 | Recall: 0.7707 | F1: 0.7586
2026-02-10 01:41:00,558 - INFO - [Metrics for 'normal'] | Precision: 0.7966 | Recall: 0.7747 | F1: 0.7855
2026-02-10 01:41:00,692 - INFO - [Best Model Saved] (val loss: 0.5229) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:41:00,692 - INFO - --------------------------------------------------
2026-02-10 01:41:00,692 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:41:06,503 - INFO - [Train] [6/10] | Loss: 0.4830 | Train Acc: 82.51%
2026-02-10 01:41:07,782 - INFO - [Valid] [6/10] | Loss: 0.5052 | Val Acc: 79.06%
2026-02-10 01:41:07,792 - INFO - [Metrics for 'abnormal'] | Precision: 0.7622 | Recall: 0.7962 | F1: 0.7788
2026-02-10 01:41:07,792 - INFO - [Metrics for 'normal'] | Precision: 0.8171 | Recall: 0.7857 | F1: 0.8011
2026-02-10 01:41:07,964 - INFO - [Best Model Saved] (val loss: 0.5052) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:41:07,964 - INFO - --------------------------------------------------
2026-02-10 01:41:07,965 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:41:13,738 - INFO - [Train] [7/10] | Loss: 0.4543 | Train Acc: 84.08%
2026-02-10 01:41:15,271 - INFO - [Valid] [7/10] | Loss: 0.5250 | Val Acc: 78.17%
2026-02-10 01:41:15,276 - INFO - [Metrics for 'abnormal'] | Precision: 0.7610 | Recall: 0.7707 | F1: 0.7658
2026-02-10 01:41:15,276 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.7912 | F1: 0.7956
2026-02-10 01:41:15,278 - INFO - --------------------------------------------------
2026-02-10 01:41:15,278 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:41:21,531 - INFO - [Train] [8/10] | Loss: 0.4498 | Train Acc: 84.08%
2026-02-10 01:41:22,897 - INFO - [Valid] [8/10] | Loss: 0.5049 | Val Acc: 80.53%
2026-02-10 01:41:22,903 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.8471 | F1: 0.8012
2026-02-10 01:41:22,903 - INFO - [Metrics for 'normal'] | Precision: 0.8537 | Recall: 0.7692 | F1: 0.8092
2026-02-10 01:41:23,073 - INFO - [Best Model Saved] (val loss: 0.5049) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:41:23,074 - INFO - --------------------------------------------------
2026-02-10 01:41:23,074 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:41:28,562 - INFO - [Train] [9/10] | Loss: 0.4301 | Train Acc: 85.19%
2026-02-10 01:41:29,942 - INFO - [Valid] [9/10] | Loss: 0.5042 | Val Acc: 80.24%
2026-02-10 01:41:29,949 - INFO - [Metrics for 'abnormal'] | Precision: 0.7812 | Recall: 0.7962 | F1: 0.7886
2026-02-10 01:41:29,949 - INFO - [Metrics for 'normal'] | Precision: 0.8212 | Recall: 0.8077 | F1: 0.8144
2026-02-10 01:41:30,164 - INFO - [Best Model Saved] (val loss: 0.5042) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:41:30,164 - INFO - --------------------------------------------------
2026-02-10 01:41:30,165 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:41:36,033 - INFO - [Train] [10/10] | Loss: 0.4285 | Train Acc: 85.49%
2026-02-10 01:41:37,468 - INFO - [Valid] [10/10] | Loss: 0.5159 | Val Acc: 79.35%
2026-02-10 01:41:37,473 - INFO - [Metrics for 'abnormal'] | Precision: 0.7544 | Recall: 0.8217 | F1: 0.7866
2026-02-10 01:41:37,474 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7692 | F1: 0.8000
2026-02-10 01:41:37,475 - INFO - ================================================================================
2026-02-10 01:41:37,479 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:41:37,479 - INFO - ================================================================================
2026-02-10 01:41:37,518 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:41:37,519 - INFO - ================================================================================
2026-02-10 01:41:37,519 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:41:37,536 - INFO - 원본 모델 FLOPs: 2.8696 GFLOPs
2026-02-10 01:41:37,542 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:37,543 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:37,543 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:43,394 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:43,967 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:41:43,967 - INFO - ==================================================
2026-02-10 01:41:43,975 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 1.3003 GFLOPs (감소율: 54.69%)
2026-02-10 01:41:43,977 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:43,977 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:43,978 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:49,914 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:50,433 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-02-10 01:41:50,434 - INFO - ==================================================
2026-02-10 01:41:50,440 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.6166 GFLOPs (감소율: 78.51%)
2026-02-10 01:41:50,442 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:50,442 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:50,443 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:56,478 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:57,011 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-02-10 01:41:57,012 - INFO - ==================================================
2026-02-10 01:41:57,018 - INFO -   [탐색  3] 희소도: 0.8662 -> FLOPs: 0.3005 GFLOPs (감소율: 89.53%)
2026-02-10 01:41:57,021 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:57,021 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:57,021 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:02,515 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:03,083 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.928125)에 맞춰 변경되었습니다.
2026-02-10 01:42:03,084 - INFO - ==================================================
2026-02-10 01:42:03,088 - INFO -   [탐색  4] 희소도: 0.9281 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:42:03,090 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:03,090 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:03,090 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:09,118 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:10,069 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8971875)에 맞춰 변경되었습니다.
2026-02-10 01:42:10,070 - INFO - ==================================================
2026-02-10 01:42:10,075 - INFO -   [탐색  5] 희소도: 0.8972 -> FLOPs: 0.2238 GFLOPs (감소율: 92.20%)
2026-02-10 01:42:10,077 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:10,078 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:10,078 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:15,726 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:16,258 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.91265625)에 맞춰 변경되었습니다.
2026-02-10 01:42:16,258 - INFO - ==================================================
2026-02-10 01:42:16,264 - INFO -   [탐색  6] 희소도: 0.9127 -> FLOPs: 0.1858 GFLOPs (감소율: 93.52%)
2026-02-10 01:42:16,267 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:16,267 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:16,267 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:22,115 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:22,664 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.920390625)에 맞춰 변경되었습니다.
2026-02-10 01:42:22,664 - INFO - ==================================================
2026-02-10 01:42:22,677 - INFO -   [탐색  7] 희소도: 0.9204 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:42:22,680 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:22,680 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:22,680 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:28,441 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:28,998 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9242578125)에 맞춰 변경되었습니다.
2026-02-10 01:42:28,999 - INFO - ==================================================
2026-02-10 01:42:29,003 - INFO -   [탐색  8] 희소도: 0.9243 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:42:29,005 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:29,006 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:29,006 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:34,463 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:35,045 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.92232421875)에 맞춰 변경되었습니다.
2026-02-10 01:42:35,045 - INFO - ==================================================
2026-02-10 01:42:35,049 - INFO -   [탐색  9] 희소도: 0.9223 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:42:35,053 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:35,053 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:35,054 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:41,023 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:41,556 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-02-10 01:42:41,557 - INFO - ==================================================
2026-02-10 01:42:41,560 - INFO -   [탐색 10] 희소도: 0.9214 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:42:41,564 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:41,564 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:41,564 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:47,147 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:48,103 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218408203125)에 맞춰 변경되었습니다.
2026-02-10 01:42:48,103 - INFO - ==================================================
2026-02-10 01:42:48,106 - INFO -   [탐색 11] 희소도: 0.9218 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:42:48,110 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:48,110 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:48,110 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:54,198 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:54,740 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9220825195312501)에 맞춰 변경되었습니다.
2026-02-10 01:42:54,740 - INFO - ==================================================
2026-02-10 01:42:54,743 - INFO -   [탐색 12] 희소도: 0.9221 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:42:54,747 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:54,747 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:54,747 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:00,574 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:01,119 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9219616699218751)에 맞춰 변경되었습니다.
2026-02-10 01:43:01,120 - INFO - ==================================================
2026-02-10 01:43:01,123 - INFO -   [탐색 13] 희소도: 0.9220 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:43:01,125 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:01,126 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:01,126 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:06,882 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:07,441 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9219012451171875)에 맞춰 변경되었습니다.
2026-02-10 01:43:07,442 - INFO - ==================================================
2026-02-10 01:43:07,445 - INFO -   [탐색 14] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:43:07,448 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:07,448 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:07,448 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:13,091 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:13,660 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218710327148438)에 맞춰 변경되었습니다.
2026-02-10 01:43:13,660 - INFO - ==================================================
2026-02-10 01:43:13,663 - INFO -   [탐색 15] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:43:13,667 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:13,667 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:13,667 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:18,949 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:19,509 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218861389160157)에 맞춰 변경되었습니다.
2026-02-10 01:43:19,510 - INFO - ==================================================
2026-02-10 01:43:19,513 - INFO -   [탐색 16] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:43:19,516 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:19,516 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:19,516 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:25,029 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:25,636 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218785858154297)에 맞춰 변경되었습니다.
2026-02-10 01:43:25,636 - INFO - ==================================================
2026-02-10 01:43:25,639 - INFO -   [탐색 17] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:43:25,642 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:25,642 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:25,642 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:31,142 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:32,161 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218748092651368)에 맞춰 변경되었습니다.
2026-02-10 01:43:32,161 - INFO - ==================================================
2026-02-10 01:43:32,165 - INFO -   [탐색 18] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:43:32,167 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:32,167 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:32,167 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:37,762 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:38,341 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218766975402832)에 맞춰 변경되었습니다.
2026-02-10 01:43:38,341 - INFO - ==================================================
2026-02-10 01:43:38,345 - INFO -   [탐색 19] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:43:38,349 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:38,349 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:38,349 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:43,313 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:43,901 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.92187575340271)에 맞춰 변경되었습니다.
2026-02-10 01:43:43,901 - INFO - ==================================================
2026-02-10 01:43:43,905 - INFO -   [탐색 20] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:43:43,908 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:43,908 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:43,909 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:49,231 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:49,735 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218752813339234)에 맞춰 변경되었습니다.
2026-02-10 01:43:49,736 - INFO - ==================================================
2026-02-10 01:43:49,739 - INFO -   [탐색 21] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:43:49,741 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:49,741 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:49,741 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:54,326 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:54,859 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750452995301)에 맞춰 변경되었습니다.
2026-02-10 01:43:54,859 - INFO - ==================================================
2026-02-10 01:43:54,862 - INFO -   [탐색 22] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:43:54,865 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:54,865 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:54,865 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:59,588 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:00,105 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749272823334)에 맞춰 변경되었습니다.
2026-02-10 01:44:00,105 - INFO - ==================================================
2026-02-10 01:44:00,108 - INFO -   [탐색 23] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:44:00,111 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:00,111 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:00,112 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:04,739 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:05,692 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749862909318)에 맞춰 변경되었습니다.
2026-02-10 01:44:05,692 - INFO - ==================================================
2026-02-10 01:44:05,695 - INFO -   [탐색 24] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:44:05,698 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:05,698 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:05,698 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:10,188 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:10,706 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750157952309)에 맞춰 변경되었습니다.
2026-02-10 01:44:10,707 - INFO - ==================================================
2026-02-10 01:44:10,710 - INFO -   [탐색 25] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:44:10,713 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:10,713 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:10,713 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:15,206 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:15,765 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750010430814)에 맞춰 변경되었습니다.
2026-02-10 01:44:15,766 - INFO - ==================================================
2026-02-10 01:44:15,770 - INFO -   [탐색 26] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:44:15,772 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:15,772 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:15,772 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:20,023 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:20,564 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749936670065)에 맞춰 변경되었습니다.
2026-02-10 01:44:20,564 - INFO - ==================================================
2026-02-10 01:44:20,567 - INFO -   [탐색 27] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:44:20,569 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:20,569 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:20,570 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:25,287 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:25,836 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921874997355044)에 맞춰 변경되었습니다.
2026-02-10 01:44:25,837 - INFO - ==================================================
2026-02-10 01:44:25,839 - INFO -   [탐색 28] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:44:25,842 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:25,842 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:25,842 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:30,388 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:30,967 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749991990627)에 맞춰 변경되었습니다.
2026-02-10 01:44:30,968 - INFO - ==================================================
2026-02-10 01:44:30,971 - INFO -   [탐색 29] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:44:30,973 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:30,973 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:30,973 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:35,207 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:36,113 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875000121072)에 맞춰 변경되었습니다.
2026-02-10 01:44:36,113 - INFO - ==================================================
2026-02-10 01:44:36,116 - INFO -   [탐색 30] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:44:36,117 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:36,117 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:36,117 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:40,229 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:40,806 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749996600674)에 맞춰 변경되었습니다.
2026-02-10 01:44:40,806 - INFO - ==================================================
2026-02-10 01:44:40,810 - INFO -   [탐색 31] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:44:40,812 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:40,813 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:40,813 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:44,788 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:45,373 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749998905698)에 맞춰 변경되었습니다.
2026-02-10 01:44:45,373 - INFO - ==================================================
2026-02-10 01:44:45,376 - INFO -   [탐색 32] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:44:45,378 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:45,378 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:45,378 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:49,616 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:50,153 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000058209)에 맞춰 변경되었습니다.
2026-02-10 01:44:50,153 - INFO - ==================================================
2026-02-10 01:44:50,157 - INFO -   [탐색 33] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:44:50,160 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:50,160 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:50,160 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:54,730 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:55,315 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999481953)에 맞춰 변경되었습니다.
2026-02-10 01:44:55,315 - INFO - ==================================================
2026-02-10 01:44:55,320 - INFO -   [탐색 34] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:44:55,323 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:55,323 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:55,323 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:59,755 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:00,310 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999770082)에 맞춰 변경되었습니다.
2026-02-10 01:45:00,310 - INFO - ==================================================
2026-02-10 01:45:00,313 - INFO -   [탐색 35] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:45:00,317 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:00,318 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:00,318 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:04,700 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:05,697 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999914145)에 맞춰 변경되었습니다.
2026-02-10 01:45:05,698 - INFO - ==================================================
2026-02-10 01:45:05,701 - INFO -   [탐색 36] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:45:05,704 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:05,704 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:05,704 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:09,091 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:09,648 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999986178)에 맞춰 변경되었습니다.
2026-02-10 01:45:09,648 - INFO - ==================================================
2026-02-10 01:45:09,651 - INFO -   [탐색 37] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:45:09,654 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:09,654 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:09,654 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:13,659 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:14,245 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000022193)에 맞춰 변경되었습니다.
2026-02-10 01:45:14,245 - INFO - ==================================================
2026-02-10 01:45:14,248 - INFO -   [탐색 38] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:45:14,250 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:14,250 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:14,250 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:18,943 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:19,506 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000004186)에 맞춰 변경되었습니다.
2026-02-10 01:45:19,506 - INFO - ==================================================
2026-02-10 01:45:19,510 - INFO -   [탐색 39] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:45:19,512 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:19,512 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:19,512 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:24,047 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:24,628 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999995182)에 맞춰 변경되었습니다.
2026-02-10 01:45:24,628 - INFO - ==================================================
2026-02-10 01:45:24,632 - INFO -   [탐색 40] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:45:24,634 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:24,634 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:24,634 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:29,137 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:29,708 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999999684)에 맞춰 변경되었습니다.
2026-02-10 01:45:29,708 - INFO - ==================================================
2026-02-10 01:45:29,711 - INFO -   [탐색 41] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:45:29,714 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:29,715 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:29,715 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:33,953 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:34,498 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000001934)에 맞춰 변경되었습니다.
2026-02-10 01:45:34,498 - INFO - ==================================================
2026-02-10 01:45:34,501 - INFO -   [탐색 42] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:45:34,502 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:34,502 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:34,502 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:39,222 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:40,152 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000808)에 맞춰 변경되었습니다.
2026-02-10 01:45:40,153 - INFO - ==================================================
2026-02-10 01:45:40,156 - INFO -   [탐색 43] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:45:40,159 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:40,159 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:40,159 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:44,824 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:45,401 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000246)에 맞춰 변경되었습니다.
2026-02-10 01:45:45,402 - INFO - ==================================================
2026-02-10 01:45:45,405 - INFO -   [탐색 44] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:45:45,407 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:45,407 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:45,408 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:49,863 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:50,404 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999999964)에 맞춰 변경되었습니다.
2026-02-10 01:45:50,405 - INFO - ==================================================
2026-02-10 01:45:50,411 - INFO -   [탐색 45] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:45:50,413 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:50,414 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:50,414 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:54,934 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:55,438 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000105)에 맞춰 변경되었습니다.
2026-02-10 01:45:55,438 - INFO - ==================================================
2026-02-10 01:45:55,441 - INFO -   [탐색 46] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:45:55,443 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:55,443 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:55,443 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:00,030 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:00,494 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000036)에 맞춰 변경되었습니다.
2026-02-10 01:46:00,494 - INFO - ==================================================
2026-02-10 01:46:00,497 - INFO -   [탐색 47] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:46:00,500 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:00,500 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:00,500 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:05,332 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:05,794 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:46:05,794 - INFO - ==================================================
2026-02-10 01:46:05,797 - INFO -   [탐색 48] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:46:05,800 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:05,800 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:05,800 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:10,578 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:11,398 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000018)에 맞춰 변경되었습니다.
2026-02-10 01:46:11,399 - INFO - ==================================================
2026-02-10 01:46:11,402 - INFO -   [탐색 49] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:46:11,405 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:11,406 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:11,406 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:16,260 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:16,808 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000009)에 맞춰 변경되었습니다.
2026-02-10 01:46:16,809 - INFO - ==================================================
2026-02-10 01:46:16,813 - INFO -   [탐색 50] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:46:16,815 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:16,816 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:16,816 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:21,174 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:21,694 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000004)에 맞춰 변경되었습니다.
2026-02-10 01:46:21,694 - INFO - ==================================================
2026-02-10 01:46:21,697 - INFO -   [탐색 51] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:46:21,700 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:21,700 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:21,700 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:26,447 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:26,964 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000002)에 맞춰 변경되었습니다.
2026-02-10 01:46:26,964 - INFO - ==================================================
2026-02-10 01:46:26,968 - INFO -   [탐색 52] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:46:26,970 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:26,970 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:26,970 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:31,737 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:32,266 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000001)에 맞춰 변경되었습니다.
2026-02-10 01:46:32,266 - INFO - ==================================================
2026-02-10 01:46:32,270 - INFO -   [탐색 53] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 01:46:32,272 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:32,272 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:32,272 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:36,846 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:37,378 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:46:37,379 - INFO - ==================================================
2026-02-10 01:46:37,383 - INFO -   [탐색 54] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:46:37,386 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:37,386 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:37,386 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:41,999 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:42,914 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:46:42,914 - INFO - ==================================================
2026-02-10 01:46:42,918 - INFO -   [탐색 55] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:46:42,920 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:42,920 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:42,921 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:47,485 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:47,861 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:46:47,861 - INFO - ==================================================
2026-02-10 01:46:47,862 - INFO -   [탐색 56] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:46:47,864 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:47,864 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:47,864 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:52,358 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:52,906 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:46:52,906 - INFO - ==================================================
2026-02-10 01:46:52,908 - INFO -   [탐색 57] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:46:52,910 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:52,910 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:52,910 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:57,513 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:58,033 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:46:58,033 - INFO - ==================================================
2026-02-10 01:46:58,036 - INFO -   [탐색 58] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:46:58,038 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:58,038 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:58,038 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:02,323 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:02,891 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:02,891 - INFO - ==================================================
2026-02-10 01:47:02,909 - INFO -   [탐색 59] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:02,912 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:02,913 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:02,913 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:06,519 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:07,496 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:07,496 - INFO - ==================================================
2026-02-10 01:47:07,500 - INFO -   [탐색 60] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:07,502 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:07,503 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:07,503 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:11,748 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:12,318 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:12,318 - INFO - ==================================================
2026-02-10 01:47:12,321 - INFO -   [탐색 61] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:12,325 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:12,325 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:12,325 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:16,851 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:17,381 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:17,382 - INFO - ==================================================
2026-02-10 01:47:17,385 - INFO -   [탐색 62] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:17,387 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:17,388 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:17,388 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:21,490 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:22,050 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:22,050 - INFO - ==================================================
2026-02-10 01:47:22,054 - INFO -   [탐색 63] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:22,056 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:22,056 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:22,057 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:26,615 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:27,180 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:27,181 - INFO - ==================================================
2026-02-10 01:47:27,184 - INFO -   [탐색 64] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:27,187 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:27,187 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:27,187 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:31,774 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:32,316 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:32,317 - INFO - ==================================================
2026-02-10 01:47:32,319 - INFO -   [탐색 65] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:32,322 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:32,322 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:32,322 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:36,479 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:37,035 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:37,036 - INFO - ==================================================
2026-02-10 01:47:37,038 - INFO -   [탐색 66] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:37,041 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:37,041 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:37,041 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:41,398 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:42,364 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:42,365 - INFO - ==================================================
2026-02-10 01:47:42,368 - INFO -   [탐색 67] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:42,371 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:42,371 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:42,371 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:46,432 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:46,967 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:46,967 - INFO - ==================================================
2026-02-10 01:47:46,970 - INFO -   [탐색 68] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:46,973 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:46,973 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:46,973 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:51,141 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:51,694 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:51,695 - INFO - ==================================================
2026-02-10 01:47:51,698 - INFO -   [탐색 69] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:51,700 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:51,701 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:51,701 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:56,127 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:56,704 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:47:56,704 - INFO - ==================================================
2026-02-10 01:47:56,707 - INFO -   [탐색 70] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:47:56,709 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:56,709 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:56,709 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:01,423 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:01,941 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:01,941 - INFO - ==================================================
2026-02-10 01:48:01,945 - INFO -   [탐색 71] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:01,947 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:01,947 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:01,947 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:06,703 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:07,274 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:07,274 - INFO - ==================================================
2026-02-10 01:48:07,277 - INFO -   [탐색 72] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:07,281 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:07,281 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:07,281 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:11,370 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:12,332 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:12,333 - INFO - ==================================================
2026-02-10 01:48:12,336 - INFO -   [탐색 73] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:12,338 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:12,339 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:12,339 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:16,131 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:16,671 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:16,671 - INFO - ==================================================
2026-02-10 01:48:16,674 - INFO -   [탐색 74] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:16,677 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:16,677 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:16,677 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:20,539 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:21,085 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:21,085 - INFO - ==================================================
2026-02-10 01:48:21,088 - INFO -   [탐색 75] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:21,090 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:21,090 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:21,090 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:25,899 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:26,468 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:26,468 - INFO - ==================================================
2026-02-10 01:48:26,472 - INFO -   [탐색 76] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:26,474 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:26,474 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:26,475 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:30,879 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:31,421 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:31,421 - INFO - ==================================================
2026-02-10 01:48:31,424 - INFO -   [탐색 77] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:31,427 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:31,427 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:31,427 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:35,808 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:36,376 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:36,377 - INFO - ==================================================
2026-02-10 01:48:36,380 - INFO -   [탐색 78] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:36,383 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:36,383 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:36,383 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:40,401 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:41,398 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:41,398 - INFO - ==================================================
2026-02-10 01:48:41,402 - INFO -   [탐색 79] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:41,406 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:41,406 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:41,406 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:46,122 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:46,699 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:46,699 - INFO - ==================================================
2026-02-10 01:48:46,703 - INFO -   [탐색 80] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:46,706 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:46,706 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:46,707 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:50,774 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:51,362 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:51,363 - INFO - ==================================================
2026-02-10 01:48:51,366 - INFO -   [탐색 81] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:51,378 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:51,379 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:51,379 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:55,661 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:56,243 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:48:56,244 - INFO - ==================================================
2026-02-10 01:48:56,247 - INFO -   [탐색 82] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:48:56,249 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:56,249 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:56,250 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:00,573 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:01,141 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:01,141 - INFO - ==================================================
2026-02-10 01:49:01,144 - INFO -   [탐색 83] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:01,146 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:01,146 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:01,146 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:05,384 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:06,388 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:06,388 - INFO - ==================================================
2026-02-10 01:49:06,392 - INFO -   [탐색 84] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:06,394 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:06,394 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:06,395 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:11,288 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:11,896 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:11,896 - INFO - ==================================================
2026-02-10 01:49:11,899 - INFO -   [탐색 85] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:11,901 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:11,902 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:11,902 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:16,671 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:17,204 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:17,205 - INFO - ==================================================
2026-02-10 01:49:17,207 - INFO -   [탐색 86] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:17,210 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:17,210 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:17,211 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:21,448 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:22,001 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:22,001 - INFO - ==================================================
2026-02-10 01:49:22,005 - INFO -   [탐색 87] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:22,007 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:22,007 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:22,008 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:25,597 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:26,135 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:26,136 - INFO - ==================================================
2026-02-10 01:49:26,139 - INFO -   [탐색 88] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:26,142 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:26,142 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:26,142 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:30,242 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:30,781 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:30,781 - INFO - ==================================================
2026-02-10 01:49:30,785 - INFO -   [탐색 89] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:30,787 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:30,787 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:30,787 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:35,306 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:36,278 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:36,279 - INFO - ==================================================
2026-02-10 01:49:36,283 - INFO -   [탐색 90] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:36,285 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:36,285 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:36,285 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:40,470 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:41,041 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:41,041 - INFO - ==================================================
2026-02-10 01:49:41,045 - INFO -   [탐색 91] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:41,048 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:41,048 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:41,048 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:45,248 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:45,811 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:45,811 - INFO - ==================================================
2026-02-10 01:49:45,816 - INFO -   [탐색 92] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:45,819 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:45,819 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:45,819 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:50,708 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:51,246 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:51,246 - INFO - ==================================================
2026-02-10 01:49:51,249 - INFO -   [탐색 93] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:51,252 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:51,252 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:51,252 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:55,304 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:55,885 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:49:55,886 - INFO - ==================================================
2026-02-10 01:49:55,890 - INFO -   [탐색 94] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:49:55,893 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:55,893 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:55,893 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:50:00,080 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:50:00,453 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:50:00,453 - INFO - ==================================================
2026-02-10 01:50:00,456 - INFO -   [탐색 95] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:50:00,458 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:50:00,458 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:50:00,459 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:50:05,307 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:50:06,037 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:50:06,037 - INFO - ==================================================
2026-02-10 01:50:06,040 - INFO -   [탐색 96] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:50:06,042 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:50:06,042 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:50:06,042 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:50:10,848 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:50:11,264 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:50:11,264 - INFO - ==================================================
2026-02-10 01:50:11,266 - INFO -   [탐색 97] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:50:11,268 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:50:11,269 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:50:11,269 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:50:14,981 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:50:15,559 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:50:15,560 - INFO - ==================================================
2026-02-10 01:50:15,563 - INFO -   [탐색 98] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:50:15,565 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:50:15,566 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:50:15,566 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:50:19,686 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:50:20,223 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:50:20,224 - INFO - ==================================================
2026-02-10 01:50:20,227 - INFO -   [탐색 99] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:50:20,229 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:50:20,229 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:50:20,229 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:50:24,557 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:50:25,096 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 01:50:25,096 - INFO - ==================================================
2026-02-10 01:50:25,100 - INFO -   [탐색 100] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 01:50:25,100 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.9214 입니다.
2026-02-10 01:50:25,100 - INFO - ================================================================================
2026-02-10 01:50:25,101 - INFO - 계산된 Pruning 정보(희소도: 0.9214)를 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:50:25,106 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:50:25,113 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:50:25,113 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:50:25,113 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:50:29,434 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:50:29,999 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-02-10 01:50:29,999 - INFO - ==================================================
2026-02-10 01:50:29,999 - INFO - ==================================================
2026-02-10 01:50:29,999 - INFO - 모델 파라미터 수:
2026-02-10 01:50:29,999 - INFO -   - 총 파라미터: 57,792 개
2026-02-10 01:50:29,999 - INFO -   - 학습 가능한 파라미터: 57,792 개
2026-02-10 01:50:30,003 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:50:30,008 - INFO - FLOPs가 2.8696 GFLOPs에서 0.1854 GFLOPs로 감소했습니다 (감소율: 93.54%).
2026-02-10 01:50:30,008 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:50:30,008 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:50:30,008 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:50:30,009 - INFO - ==================================================
2026-02-10 01:50:30,009 - INFO - train 모드를 시작합니다.
2026-02-10 01:50:30,009 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:50:30,009 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:50:30,009 - INFO - --------------------------------------------------
2026-02-10 01:50:30,009 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:50:34,262 - INFO - [Train] [11/90] | Loss: 0.5651 | Train Acc: 74.55%
2026-02-10 01:50:35,300 - INFO - [Valid] [11/90] | Loss: 0.5552 | Val Acc: 74.34%
2026-02-10 01:50:35,306 - INFO - [Metrics for 'abnormal'] | Precision: 0.7500 | Recall: 0.6688 | F1: 0.7071
2026-02-10 01:50:35,310 - INFO - [Metrics for 'normal'] | Precision: 0.7387 | Recall: 0.8077 | F1: 0.7717
2026-02-10 01:50:35,322 - INFO - [Best Model Saved] (val loss: 0.5552) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:50:35,322 - INFO - --------------------------------------------------
2026-02-10 01:50:35,322 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:50:39,757 - INFO - [Train] [12/90] | Loss: 0.5127 | Train Acc: 81.03%
2026-02-10 01:50:40,878 - INFO - [Valid] [12/90] | Loss: 0.5469 | Val Acc: 76.99%
2026-02-10 01:50:40,883 - INFO - [Metrics for 'abnormal'] | Precision: 0.8015 | Recall: 0.6688 | F1: 0.7292
2026-02-10 01:50:40,884 - INFO - [Metrics for 'normal'] | Precision: 0.7500 | Recall: 0.8571 | F1: 0.8000
2026-02-10 01:50:40,887 - INFO - [Best Model Saved] (val loss: 0.5469) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:50:40,887 - INFO - --------------------------------------------------
2026-02-10 01:50:40,888 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:50:44,862 - INFO - [Train] [13/90] | Loss: 0.5179 | Train Acc: 80.13%
2026-02-10 01:50:46,117 - INFO - [Valid] [13/90] | Loss: 0.5399 | Val Acc: 75.22%
2026-02-10 01:50:46,126 - INFO - [Metrics for 'abnormal'] | Precision: 0.7186 | Recall: 0.7643 | F1: 0.7407
2026-02-10 01:50:46,126 - INFO - [Metrics for 'normal'] | Precision: 0.7849 | Recall: 0.7418 | F1: 0.7627
2026-02-10 01:50:46,129 - INFO - [Best Model Saved] (val loss: 0.5399) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:50:46,129 - INFO - --------------------------------------------------
2026-02-10 01:50:46,129 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:50:49,950 - INFO - [Train] [14/90] | Loss: 0.5095 | Train Acc: 80.73%
2026-02-10 01:50:51,314 - INFO - [Valid] [14/90] | Loss: 0.5316 | Val Acc: 75.81%
2026-02-10 01:50:51,318 - INFO - [Metrics for 'abnormal'] | Precision: 0.7273 | Recall: 0.7643 | F1: 0.7453
2026-02-10 01:50:51,322 - INFO - [Metrics for 'normal'] | Precision: 0.7874 | Recall: 0.7527 | F1: 0.7697
2026-02-10 01:50:51,328 - INFO - [Best Model Saved] (val loss: 0.5316) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:50:51,332 - INFO - --------------------------------------------------
2026-02-10 01:50:51,332 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:50:55,853 - INFO - [Train] [15/90] | Loss: 0.4874 | Train Acc: 81.32%
2026-02-10 01:50:56,711 - INFO - [Valid] [15/90] | Loss: 0.5362 | Val Acc: 76.99%
2026-02-10 01:50:56,721 - INFO - [Metrics for 'abnormal'] | Precision: 0.7516 | Recall: 0.7516 | F1: 0.7516
2026-02-10 01:50:56,721 - INFO - [Metrics for 'normal'] | Precision: 0.7857 | Recall: 0.7857 | F1: 0.7857
2026-02-10 01:50:56,723 - INFO - --------------------------------------------------
2026-02-10 01:50:56,723 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:51:01,237 - INFO - [Train] [16/90] | Loss: 0.4821 | Train Acc: 82.59%
2026-02-10 01:51:02,149 - INFO - [Valid] [16/90] | Loss: 0.5282 | Val Acc: 76.11%
2026-02-10 01:51:02,154 - INFO - [Metrics for 'abnormal'] | Precision: 0.7021 | Recall: 0.8408 | F1: 0.7652
2026-02-10 01:51:02,155 - INFO - [Metrics for 'normal'] | Precision: 0.8344 | Recall: 0.6923 | F1: 0.7568
2026-02-10 01:51:02,161 - INFO - [Best Model Saved] (val loss: 0.5282) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:51:02,162 - INFO - --------------------------------------------------
2026-02-10 01:51:02,162 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:51:06,194 - INFO - [Train] [17/90] | Loss: 0.4787 | Train Acc: 83.71%
2026-02-10 01:51:07,458 - INFO - [Valid] [17/90] | Loss: 0.5249 | Val Acc: 79.06%
2026-02-10 01:51:07,460 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.7834 | F1: 0.7760
2026-02-10 01:51:07,460 - INFO - [Metrics for 'normal'] | Precision: 0.8101 | Recall: 0.7967 | F1: 0.8033
2026-02-10 01:51:07,462 - INFO - [Best Model Saved] (val loss: 0.5249) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:51:07,463 - INFO - --------------------------------------------------
2026-02-10 01:51:07,463 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:51:11,528 - INFO - [Train] [18/90] | Loss: 0.4792 | Train Acc: 83.56%
2026-02-10 01:51:12,818 - INFO - [Valid] [18/90] | Loss: 0.5058 | Val Acc: 80.24%
2026-02-10 01:51:12,828 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8025 | F1: 0.7900
2026-02-10 01:51:12,828 - INFO - [Metrics for 'normal'] | Precision: 0.8249 | Recall: 0.8022 | F1: 0.8134
2026-02-10 01:51:12,832 - INFO - [Best Model Saved] (val loss: 0.5058) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:51:12,832 - INFO - --------------------------------------------------
2026-02-10 01:51:12,832 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:51:16,809 - INFO - [Train] [19/90] | Loss: 0.4684 | Train Acc: 83.63%
2026-02-10 01:51:17,920 - INFO - [Valid] [19/90] | Loss: 0.5055 | Val Acc: 81.42%
2026-02-10 01:51:17,925 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-02-10 01:51:17,925 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-02-10 01:51:17,929 - INFO - [Best Model Saved] (val loss: 0.5055) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:51:17,929 - INFO - --------------------------------------------------
2026-02-10 01:51:17,929 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:51:22,118 - INFO - [Train] [20/90] | Loss: 0.4714 | Train Acc: 83.26%
2026-02-10 01:51:23,454 - INFO - [Valid] [20/90] | Loss: 0.5023 | Val Acc: 79.06%
2026-02-10 01:51:23,462 - INFO - [Metrics for 'abnormal'] | Precision: 0.7529 | Recall: 0.8153 | F1: 0.7829
2026-02-10 01:51:23,462 - INFO - [Metrics for 'normal'] | Precision: 0.8284 | Recall: 0.7692 | F1: 0.7977
2026-02-10 01:51:23,465 - INFO - [Best Model Saved] (val loss: 0.5023) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:51:23,465 - INFO - --------------------------------------------------
2026-02-10 01:51:23,465 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:51:27,346 - INFO - [Train] [21/90] | Loss: 0.4647 | Train Acc: 82.81%
2026-02-10 01:51:28,621 - INFO - [Valid] [21/90] | Loss: 0.5069 | Val Acc: 78.17%
2026-02-10 01:51:28,636 - INFO - [Metrics for 'abnormal'] | Precision: 0.7268 | Recall: 0.8471 | F1: 0.7824
2026-02-10 01:51:28,636 - INFO - [Metrics for 'normal'] | Precision: 0.8462 | Recall: 0.7253 | F1: 0.7811
2026-02-10 01:51:28,638 - INFO - --------------------------------------------------
2026-02-10 01:51:28,641 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:51:32,258 - INFO - [Train] [22/90] | Loss: 0.4486 | Train Acc: 84.82%
2026-02-10 01:51:33,492 - INFO - [Valid] [22/90] | Loss: 0.4999 | Val Acc: 79.94%
2026-02-10 01:51:33,497 - INFO - [Metrics for 'abnormal'] | Precision: 0.7543 | Recall: 0.8408 | F1: 0.7952
2026-02-10 01:51:33,497 - INFO - [Metrics for 'normal'] | Precision: 0.8476 | Recall: 0.7637 | F1: 0.8035
2026-02-10 01:51:33,501 - INFO - [Best Model Saved] (val loss: 0.4999) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:51:33,501 - INFO - --------------------------------------------------
2026-02-10 01:51:33,501 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:51:38,159 - INFO - [Train] [23/90] | Loss: 0.4484 | Train Acc: 84.75%
2026-02-10 01:51:39,198 - INFO - [Valid] [23/90] | Loss: 0.5031 | Val Acc: 79.06%
2026-02-10 01:51:39,203 - INFO - [Metrics for 'abnormal'] | Precision: 0.7443 | Recall: 0.8344 | F1: 0.7868
2026-02-10 01:51:39,203 - INFO - [Metrics for 'normal'] | Precision: 0.8405 | Recall: 0.7527 | F1: 0.7942
2026-02-10 01:51:39,205 - INFO - --------------------------------------------------
2026-02-10 01:51:39,205 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:51:43,686 - INFO - [Train] [24/90] | Loss: 0.4368 | Train Acc: 85.12%
2026-02-10 01:51:44,848 - INFO - [Valid] [24/90] | Loss: 0.4865 | Val Acc: 81.42%
2026-02-10 01:51:44,854 - INFO - [Metrics for 'abnormal'] | Precision: 0.8052 | Recall: 0.7898 | F1: 0.7974
2026-02-10 01:51:44,854 - INFO - [Metrics for 'normal'] | Precision: 0.8216 | Recall: 0.8352 | F1: 0.8283
2026-02-10 01:51:44,858 - INFO - [Best Model Saved] (val loss: 0.4865) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:51:44,858 - INFO - --------------------------------------------------
2026-02-10 01:51:44,858 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:51:49,112 - INFO - [Train] [25/90] | Loss: 0.4382 | Train Acc: 85.12%
2026-02-10 01:51:50,459 - INFO - [Valid] [25/90] | Loss: 0.5043 | Val Acc: 79.06%
2026-02-10 01:51:50,467 - INFO - [Metrics for 'abnormal'] | Precision: 0.7654 | Recall: 0.7898 | F1: 0.7774
2026-02-10 01:51:50,467 - INFO - [Metrics for 'normal'] | Precision: 0.8136 | Recall: 0.7912 | F1: 0.8022
2026-02-10 01:51:50,469 - INFO - --------------------------------------------------
2026-02-10 01:51:50,469 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:51:54,380 - INFO - [Train] [26/90] | Loss: 0.4334 | Train Acc: 85.64%
2026-02-10 01:51:55,659 - INFO - [Valid] [26/90] | Loss: 0.5101 | Val Acc: 81.71%
2026-02-10 01:51:55,665 - INFO - [Metrics for 'abnormal'] | Precision: 0.8146 | Recall: 0.7834 | F1: 0.7987
2026-02-10 01:51:55,665 - INFO - [Metrics for 'normal'] | Precision: 0.8191 | Recall: 0.8462 | F1: 0.8324
2026-02-10 01:51:55,667 - INFO - --------------------------------------------------
2026-02-10 01:51:55,667 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 01:51:59,517 - INFO - [Train] [27/90] | Loss: 0.4313 | Train Acc: 85.71%
2026-02-10 01:52:00,677 - INFO - [Valid] [27/90] | Loss: 0.4937 | Val Acc: 79.94%
2026-02-10 01:52:00,682 - INFO - [Metrics for 'abnormal'] | Precision: 0.7665 | Recall: 0.8153 | F1: 0.7901
2026-02-10 01:52:00,682 - INFO - [Metrics for 'normal'] | Precision: 0.8314 | Recall: 0.7857 | F1: 0.8079
2026-02-10 01:52:00,685 - INFO - --------------------------------------------------
2026-02-10 01:52:00,685 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 01:52:05,129 - INFO - [Train] [28/90] | Loss: 0.4310 | Train Acc: 86.24%
2026-02-10 01:52:06,414 - INFO - [Valid] [28/90] | Loss: 0.5030 | Val Acc: 79.35%
2026-02-10 01:52:06,423 - INFO - [Metrics for 'abnormal'] | Precision: 0.7514 | Recall: 0.8280 | F1: 0.7879
2026-02-10 01:52:06,424 - INFO - [Metrics for 'normal'] | Precision: 0.8373 | Recall: 0.7637 | F1: 0.7989
2026-02-10 01:52:06,429 - INFO - --------------------------------------------------
2026-02-10 01:52:06,430 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 01:52:10,220 - INFO - [Train] [29/90] | Loss: 0.4229 | Train Acc: 86.09%
2026-02-10 01:52:11,142 - INFO - [Valid] [29/90] | Loss: 0.4925 | Val Acc: 82.01%
2026-02-10 01:52:11,152 - INFO - [Metrics for 'abnormal'] | Precision: 0.8288 | Recall: 0.7707 | F1: 0.7987
2026-02-10 01:52:11,153 - INFO - [Metrics for 'normal'] | Precision: 0.8135 | Recall: 0.8626 | F1: 0.8373
2026-02-10 01:52:11,154 - INFO - --------------------------------------------------
2026-02-10 01:52:11,154 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 01:52:15,786 - INFO - [Train] [30/90] | Loss: 0.4177 | Train Acc: 86.53%
2026-02-10 01:52:16,938 - INFO - [Valid] [30/90] | Loss: 0.4932 | Val Acc: 79.94%
2026-02-10 01:52:16,943 - INFO - [Metrics for 'abnormal'] | Precision: 0.7665 | Recall: 0.8153 | F1: 0.7901
2026-02-10 01:52:16,944 - INFO - [Metrics for 'normal'] | Precision: 0.8314 | Recall: 0.7857 | F1: 0.8079
2026-02-10 01:52:16,945 - INFO - --------------------------------------------------
2026-02-10 01:52:16,946 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 01:52:21,721 - INFO - [Train] [31/90] | Loss: 0.4196 | Train Acc: 86.98%
2026-02-10 01:52:22,403 - INFO - [Valid] [31/90] | Loss: 0.4978 | Val Acc: 79.65%
2026-02-10 01:52:22,407 - INFO - [Metrics for 'abnormal'] | Precision: 0.7588 | Recall: 0.8217 | F1: 0.7890
2026-02-10 01:52:22,408 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.7747 | F1: 0.8034
2026-02-10 01:52:22,409 - INFO - --------------------------------------------------
2026-02-10 01:52:22,409 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 01:52:26,665 - INFO - [Train] [32/90] | Loss: 0.4184 | Train Acc: 88.02%
2026-02-10 01:52:27,577 - INFO - [Valid] [32/90] | Loss: 0.4809 | Val Acc: 81.71%
2026-02-10 01:52:27,581 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.8089 | F1: 0.8038
2026-02-10 01:52:27,581 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8242 | F1: 0.8287
2026-02-10 01:52:27,583 - INFO - [Best Model Saved] (val loss: 0.4809) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:52:27,583 - INFO - --------------------------------------------------
2026-02-10 01:52:27,583 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 01:52:30,403 - INFO - [Train] [33/90] | Loss: 0.4070 | Train Acc: 87.72%
2026-02-10 01:52:31,222 - INFO - [Valid] [33/90] | Loss: 0.5074 | Val Acc: 79.35%
2026-02-10 01:52:31,230 - INFO - [Metrics for 'abnormal'] | Precision: 0.7403 | Recall: 0.8535 | F1: 0.7929
2026-02-10 01:52:31,230 - INFO - [Metrics for 'normal'] | Precision: 0.8544 | Recall: 0.7418 | F1: 0.7941
2026-02-10 01:52:31,231 - INFO - --------------------------------------------------
2026-02-10 01:52:31,232 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 01:52:34,518 - INFO - [Train] [34/90] | Loss: 0.4188 | Train Acc: 87.43%
2026-02-10 01:52:35,586 - INFO - [Valid] [34/90] | Loss: 0.4797 | Val Acc: 79.06%
2026-02-10 01:52:35,590 - INFO - [Metrics for 'abnormal'] | Precision: 0.7560 | Recall: 0.8089 | F1: 0.7815
2026-02-10 01:52:35,590 - INFO - [Metrics for 'normal'] | Precision: 0.8246 | Recall: 0.7747 | F1: 0.7989
2026-02-10 01:52:35,593 - INFO - [Best Model Saved] (val loss: 0.4797) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:52:35,597 - INFO - --------------------------------------------------
2026-02-10 01:52:35,597 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 01:52:39,928 - INFO - [Train] [35/90] | Loss: 0.4070 | Train Acc: 88.10%
2026-02-10 01:52:41,061 - INFO - [Valid] [35/90] | Loss: 0.4745 | Val Acc: 80.53%
2026-02-10 01:52:41,066 - INFO - [Metrics for 'abnormal'] | Precision: 0.7898 | Recall: 0.7898 | F1: 0.7898
2026-02-10 01:52:41,066 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8187 | F1: 0.8187
2026-02-10 01:52:41,070 - INFO - [Best Model Saved] (val loss: 0.4745) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:52:41,070 - INFO - --------------------------------------------------
2026-02-10 01:52:41,071 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 01:52:43,969 - INFO - [Train] [36/90] | Loss: 0.3982 | Train Acc: 88.84%
2026-02-10 01:52:44,913 - INFO - [Valid] [36/90] | Loss: 0.4792 | Val Acc: 81.12%
2026-02-10 01:52:44,920 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-02-10 01:52:44,920 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-02-10 01:52:44,922 - INFO - --------------------------------------------------
2026-02-10 01:52:44,922 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 01:52:48,015 - INFO - [Train] [37/90] | Loss: 0.4039 | Train Acc: 87.80%
2026-02-10 01:52:48,634 - INFO - [Valid] [37/90] | Loss: 0.4631 | Val Acc: 83.19%
2026-02-10 01:52:48,639 - INFO - [Metrics for 'abnormal'] | Precision: 0.8378 | Recall: 0.7898 | F1: 0.8131
2026-02-10 01:52:48,639 - INFO - [Metrics for 'normal'] | Precision: 0.8272 | Recall: 0.8681 | F1: 0.8472
2026-02-10 01:52:48,642 - INFO - [Best Model Saved] (val loss: 0.4631) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:52:48,643 - INFO - --------------------------------------------------
2026-02-10 01:52:48,643 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 01:52:52,110 - INFO - [Train] [38/90] | Loss: 0.3917 | Train Acc: 88.10%
2026-02-10 01:52:52,784 - INFO - [Valid] [38/90] | Loss: 0.4917 | Val Acc: 81.42%
2026-02-10 01:52:52,786 - INFO - [Metrics for 'abnormal'] | Precision: 0.8133 | Recall: 0.7771 | F1: 0.7948
2026-02-10 01:52:52,786 - INFO - [Metrics for 'normal'] | Precision: 0.8148 | Recall: 0.8462 | F1: 0.8302
2026-02-10 01:52:52,787 - INFO - --------------------------------------------------
2026-02-10 01:52:52,787 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 01:52:55,882 - INFO - [Train] [39/90] | Loss: 0.3880 | Train Acc: 88.39%
2026-02-10 01:52:56,774 - INFO - [Valid] [39/90] | Loss: 0.4925 | Val Acc: 80.24%
2026-02-10 01:52:56,779 - INFO - [Metrics for 'abnormal'] | Precision: 0.7922 | Recall: 0.7771 | F1: 0.7846
2026-02-10 01:52:56,780 - INFO - [Metrics for 'normal'] | Precision: 0.8108 | Recall: 0.8242 | F1: 0.8174
2026-02-10 01:52:56,781 - INFO - --------------------------------------------------
2026-02-10 01:52:56,783 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 01:52:59,159 - INFO - [Train] [40/90] | Loss: 0.3945 | Train Acc: 88.91%
2026-02-10 01:53:00,018 - INFO - [Valid] [40/90] | Loss: 0.4946 | Val Acc: 79.35%
2026-02-10 01:53:00,024 - INFO - [Metrics for 'abnormal'] | Precision: 0.7574 | Recall: 0.8153 | F1: 0.7853
2026-02-10 01:53:00,024 - INFO - [Metrics for 'normal'] | Precision: 0.8294 | Recall: 0.7747 | F1: 0.8011
2026-02-10 01:53:00,026 - INFO - --------------------------------------------------
2026-02-10 01:53:00,030 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 01:53:03,045 - INFO - [Train] [41/90] | Loss: 0.3910 | Train Acc: 88.91%
2026-02-10 01:53:03,691 - INFO - [Valid] [41/90] | Loss: 0.4748 | Val Acc: 81.12%
2026-02-10 01:53:03,696 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8280 | F1: 0.8025
2026-02-10 01:53:03,696 - INFO - [Metrics for 'normal'] | Precision: 0.8430 | Recall: 0.7967 | F1: 0.8192
2026-02-10 01:53:03,697 - INFO - --------------------------------------------------
2026-02-10 01:53:03,697 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 01:53:06,713 - INFO - [Train] [42/90] | Loss: 0.3842 | Train Acc: 88.84%
2026-02-10 01:53:07,388 - INFO - [Valid] [42/90] | Loss: 0.4589 | Val Acc: 82.01%
2026-02-10 01:53:07,393 - INFO - [Metrics for 'abnormal'] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-02-10 01:53:07,393 - INFO - [Metrics for 'normal'] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-02-10 01:53:07,398 - INFO - [Best Model Saved] (val loss: 0.4589) -> 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:53:07,399 - INFO - --------------------------------------------------
2026-02-10 01:53:07,399 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 01:53:10,090 - INFO - [Train] [43/90] | Loss: 0.3831 | Train Acc: 89.58%
2026-02-10 01:53:10,850 - INFO - [Valid] [43/90] | Loss: 0.4752 | Val Acc: 81.42%
2026-02-10 01:53:10,855 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-02-10 01:53:10,855 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-02-10 01:53:10,857 - INFO - --------------------------------------------------
2026-02-10 01:53:10,857 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 01:53:13,814 - INFO - [Train] [44/90] | Loss: 0.3852 | Train Acc: 88.99%
2026-02-10 01:53:14,681 - INFO - [Valid] [44/90] | Loss: 0.4629 | Val Acc: 79.94%
2026-02-10 01:53:14,686 - INFO - [Metrics for 'abnormal'] | Precision: 0.7764 | Recall: 0.7962 | F1: 0.7862
2026-02-10 01:53:14,686 - INFO - [Metrics for 'normal'] | Precision: 0.8202 | Recall: 0.8022 | F1: 0.8111
2026-02-10 01:53:14,687 - INFO - --------------------------------------------------
2026-02-10 01:53:14,688 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 01:53:17,354 - INFO - [Train] [45/90] | Loss: 0.3808 | Train Acc: 89.14%
2026-02-10 01:53:18,269 - INFO - [Valid] [45/90] | Loss: 0.4705 | Val Acc: 82.60%
2026-02-10 01:53:18,274 - INFO - [Metrics for 'abnormal'] | Precision: 0.8356 | Recall: 0.7771 | F1: 0.8053
2026-02-10 01:53:18,274 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8681 | F1: 0.8427
2026-02-10 01:53:18,276 - INFO - --------------------------------------------------
2026-02-10 01:53:18,276 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 01:53:20,828 - INFO - [Train] [46/90] | Loss: 0.3793 | Train Acc: 89.73%
2026-02-10 01:53:21,640 - INFO - [Valid] [46/90] | Loss: 0.4786 | Val Acc: 80.24%
2026-02-10 01:53:21,644 - INFO - [Metrics for 'abnormal'] | Precision: 0.7647 | Recall: 0.8280 | F1: 0.7951
2026-02-10 01:53:21,644 - INFO - [Metrics for 'normal'] | Precision: 0.8402 | Recall: 0.7802 | F1: 0.8091
2026-02-10 01:53:21,646 - INFO - --------------------------------------------------
2026-02-10 01:53:21,646 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 01:53:25,010 - INFO - [Train] [47/90] | Loss: 0.3718 | Train Acc: 89.73%
2026-02-10 01:53:25,681 - INFO - [Valid] [47/90] | Loss: 0.4772 | Val Acc: 80.83%
2026-02-10 01:53:25,685 - INFO - [Metrics for 'abnormal'] | Precision: 0.7738 | Recall: 0.8280 | F1: 0.8000
2026-02-10 01:53:25,685 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.7912 | F1: 0.8159
2026-02-10 01:53:25,686 - INFO - --------------------------------------------------
2026-02-10 01:53:25,686 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 01:53:28,446 - INFO - [Train] [48/90] | Loss: 0.3665 | Train Acc: 91.00%
2026-02-10 01:53:29,288 - INFO - [Valid] [48/90] | Loss: 0.4597 | Val Acc: 81.71%
2026-02-10 01:53:29,294 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.8089 | F1: 0.8038
2026-02-10 01:53:29,294 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8242 | F1: 0.8287
2026-02-10 01:53:29,295 - INFO - --------------------------------------------------
2026-02-10 01:53:29,296 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 01:53:31,941 - INFO - [Train] [49/90] | Loss: 0.3699 | Train Acc: 90.40%
2026-02-10 01:53:32,738 - INFO - [Valid] [49/90] | Loss: 0.4613 | Val Acc: 80.53%
2026-02-10 01:53:32,742 - INFO - [Metrics for 'abnormal'] | Precision: 0.7459 | Recall: 0.8790 | F1: 0.8070
2026-02-10 01:53:32,742 - INFO - [Metrics for 'normal'] | Precision: 0.8766 | Recall: 0.7418 | F1: 0.8036
2026-02-10 01:53:32,744 - INFO - --------------------------------------------------
2026-02-10 01:53:32,744 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 01:53:35,679 - INFO - [Train] [50/90] | Loss: 0.3655 | Train Acc: 90.70%
2026-02-10 01:53:36,602 - INFO - [Valid] [50/90] | Loss: 0.4657 | Val Acc: 83.48%
2026-02-10 01:53:36,607 - INFO - [Metrics for 'abnormal'] | Precision: 0.8389 | Recall: 0.7962 | F1: 0.8170
2026-02-10 01:53:36,607 - INFO - [Metrics for 'normal'] | Precision: 0.8316 | Recall: 0.8681 | F1: 0.8495
2026-02-10 01:53:36,609 - INFO - --------------------------------------------------
2026-02-10 01:53:36,609 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 01:53:39,369 - INFO - [Train] [51/90] | Loss: 0.3632 | Train Acc: 90.55%
2026-02-10 01:53:40,192 - INFO - [Valid] [51/90] | Loss: 0.4809 | Val Acc: 80.24%
2026-02-10 01:53:40,201 - INFO - [Metrics for 'abnormal'] | Precision: 0.7812 | Recall: 0.7962 | F1: 0.7886
2026-02-10 01:53:40,202 - INFO - [Metrics for 'normal'] | Precision: 0.8212 | Recall: 0.8077 | F1: 0.8144
2026-02-10 01:53:40,203 - INFO - --------------------------------------------------
2026-02-10 01:53:40,203 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 01:53:43,188 - INFO - [Train] [52/90] | Loss: 0.3584 | Train Acc: 90.70%
2026-02-10 01:53:43,829 - INFO - [Valid] [52/90] | Loss: 0.4759 | Val Acc: 80.53%
2026-02-10 01:53:43,834 - INFO - [Metrics for 'abnormal'] | Precision: 0.7692 | Recall: 0.8280 | F1: 0.7975
2026-02-10 01:53:43,834 - INFO - [Metrics for 'normal'] | Precision: 0.8412 | Recall: 0.7857 | F1: 0.8125
2026-02-10 01:53:43,836 - INFO - --------------------------------------------------
2026-02-10 01:53:43,837 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 01:53:47,127 - INFO - [Train] [53/90] | Loss: 0.3580 | Train Acc: 91.44%
2026-02-10 01:53:47,838 - INFO - [Valid] [53/90] | Loss: 0.4845 | Val Acc: 80.53%
2026-02-10 01:53:47,840 - INFO - [Metrics for 'abnormal'] | Precision: 0.7725 | Recall: 0.8217 | F1: 0.7963
2026-02-10 01:53:47,840 - INFO - [Metrics for 'normal'] | Precision: 0.8372 | Recall: 0.7912 | F1: 0.8136
2026-02-10 01:53:47,841 - INFO - --------------------------------------------------
2026-02-10 01:53:47,841 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 01:53:50,843 - INFO - [Train] [54/90] | Loss: 0.3570 | Train Acc: 91.15%
2026-02-10 01:53:51,615 - INFO - [Valid] [54/90] | Loss: 0.4710 | Val Acc: 80.53%
2026-02-10 01:53:51,621 - INFO - [Metrics for 'abnormal'] | Precision: 0.7692 | Recall: 0.8280 | F1: 0.7975
2026-02-10 01:53:51,622 - INFO - [Metrics for 'normal'] | Precision: 0.8412 | Recall: 0.7857 | F1: 0.8125
2026-02-10 01:53:51,624 - INFO - --------------------------------------------------
2026-02-10 01:53:51,625 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 01:53:54,209 - INFO - [Train] [55/90] | Loss: 0.3501 | Train Acc: 91.29%
2026-02-10 01:53:55,232 - INFO - [Valid] [55/90] | Loss: 0.4754 | Val Acc: 79.94%
2026-02-10 01:53:55,236 - INFO - [Metrics for 'abnormal'] | Precision: 0.7633 | Recall: 0.8217 | F1: 0.7914
2026-02-10 01:53:55,236 - INFO - [Metrics for 'normal'] | Precision: 0.8353 | Recall: 0.7802 | F1: 0.8068
2026-02-10 01:53:55,240 - INFO - --------------------------------------------------
2026-02-10 01:53:55,241 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 01:53:58,169 - INFO - [Train] [56/90] | Loss: 0.3576 | Train Acc: 91.29%
2026-02-10 01:53:58,749 - INFO - [Valid] [56/90] | Loss: 0.4757 | Val Acc: 79.35%
2026-02-10 01:53:58,756 - INFO - [Metrics for 'abnormal'] | Precision: 0.7544 | Recall: 0.8217 | F1: 0.7866
2026-02-10 01:53:58,756 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7692 | F1: 0.8000
2026-02-10 01:53:58,758 - INFO - --------------------------------------------------
2026-02-10 01:53:58,758 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 01:54:02,066 - INFO - [Train] [57/90] | Loss: 0.3474 | Train Acc: 91.22%
2026-02-10 01:54:02,728 - INFO - [Valid] [57/90] | Loss: 0.4805 | Val Acc: 81.42%
2026-02-10 01:54:02,733 - INFO - [Metrics for 'abnormal'] | Precision: 0.8133 | Recall: 0.7771 | F1: 0.7948
2026-02-10 01:54:02,733 - INFO - [Metrics for 'normal'] | Precision: 0.8148 | Recall: 0.8462 | F1: 0.8302
2026-02-10 01:54:02,734 - INFO - --------------------------------------------------
2026-02-10 01:54:02,734 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 01:54:05,712 - INFO - [Train] [58/90] | Loss: 0.3588 | Train Acc: 91.15%
2026-02-10 01:54:06,567 - INFO - [Valid] [58/90] | Loss: 0.4747 | Val Acc: 81.71%
2026-02-10 01:54:06,570 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8025 | F1: 0.8025
2026-02-10 01:54:06,570 - INFO - [Metrics for 'normal'] | Precision: 0.8297 | Recall: 0.8297 | F1: 0.8297
2026-02-10 01:54:06,571 - INFO - --------------------------------------------------
2026-02-10 01:54:06,571 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 01:54:09,278 - INFO - [Train] [59/90] | Loss: 0.3543 | Train Acc: 91.67%
2026-02-10 01:54:10,069 - INFO - [Valid] [59/90] | Loss: 0.4651 | Val Acc: 81.12%
2026-02-10 01:54:10,074 - INFO - [Metrics for 'abnormal'] | Precision: 0.7751 | Recall: 0.8344 | F1: 0.8037
2026-02-10 01:54:10,074 - INFO - [Metrics for 'normal'] | Precision: 0.8471 | Recall: 0.7912 | F1: 0.8182
2026-02-10 01:54:10,075 - INFO - --------------------------------------------------
2026-02-10 01:54:10,075 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 01:54:12,959 - INFO - [Train] [60/90] | Loss: 0.3480 | Train Acc: 91.37%
2026-02-10 01:54:13,824 - INFO - [Valid] [60/90] | Loss: 0.4796 | Val Acc: 80.83%
2026-02-10 01:54:13,829 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8217 | F1: 0.7988
2026-02-10 01:54:13,829 - INFO - [Metrics for 'normal'] | Precision: 0.8382 | Recall: 0.7967 | F1: 0.8169
2026-02-10 01:54:13,831 - INFO - --------------------------------------------------
2026-02-10 01:54:13,831 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 01:54:16,343 - INFO - [Train] [61/90] | Loss: 0.3430 | Train Acc: 92.19%
2026-02-10 01:54:17,380 - INFO - [Valid] [61/90] | Loss: 0.4738 | Val Acc: 82.01%
2026-02-10 01:54:17,387 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-02-10 01:54:17,392 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-02-10 01:54:17,400 - INFO - --------------------------------------------------
2026-02-10 01:54:17,400 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 01:54:20,290 - INFO - [Train] [62/90] | Loss: 0.3404 | Train Acc: 92.11%
2026-02-10 01:54:21,019 - INFO - [Valid] [62/90] | Loss: 0.4860 | Val Acc: 80.83%
2026-02-10 01:54:21,024 - INFO - [Metrics for 'abnormal'] | Precision: 0.7644 | Recall: 0.8471 | F1: 0.8036
2026-02-10 01:54:21,025 - INFO - [Metrics for 'normal'] | Precision: 0.8545 | Recall: 0.7747 | F1: 0.8127
2026-02-10 01:54:21,026 - INFO - --------------------------------------------------
2026-02-10 01:54:21,026 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 01:54:24,273 - INFO - [Train] [63/90] | Loss: 0.3397 | Train Acc: 92.86%
2026-02-10 01:54:25,034 - INFO - [Valid] [63/90] | Loss: 0.4878 | Val Acc: 79.35%
2026-02-10 01:54:25,037 - INFO - [Metrics for 'abnormal'] | Precision: 0.7669 | Recall: 0.7962 | F1: 0.7812
2026-02-10 01:54:25,037 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.7912 | F1: 0.8045
2026-02-10 01:54:25,038 - INFO - --------------------------------------------------
2026-02-10 01:54:25,038 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 01:54:27,901 - INFO - [Train] [64/90] | Loss: 0.3451 | Train Acc: 92.19%
2026-02-10 01:54:28,799 - INFO - [Valid] [64/90] | Loss: 0.4633 | Val Acc: 82.30%
2026-02-10 01:54:28,803 - INFO - [Metrics for 'abnormal'] | Precision: 0.7904 | Recall: 0.8408 | F1: 0.8148
2026-02-10 01:54:28,803 - INFO - [Metrics for 'normal'] | Precision: 0.8547 | Recall: 0.8077 | F1: 0.8305
2026-02-10 01:54:28,805 - INFO - --------------------------------------------------
2026-02-10 01:54:28,805 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 01:54:31,292 - INFO - [Train] [65/90] | Loss: 0.3307 | Train Acc: 93.45%
2026-02-10 01:54:32,219 - INFO - [Valid] [65/90] | Loss: 0.4749 | Val Acc: 80.83%
2026-02-10 01:54:32,226 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-02-10 01:54:32,227 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-02-10 01:54:32,228 - INFO - --------------------------------------------------
2026-02-10 01:54:32,229 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 01:54:35,125 - INFO - [Train] [66/90] | Loss: 0.3358 | Train Acc: 92.41%
2026-02-10 01:54:35,969 - INFO - [Valid] [66/90] | Loss: 0.4860 | Val Acc: 80.24%
2026-02-10 01:54:35,974 - INFO - [Metrics for 'abnormal'] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-02-10 01:54:35,974 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-02-10 01:54:35,976 - INFO - --------------------------------------------------
2026-02-10 01:54:35,976 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 01:54:38,652 - INFO - [Train] [67/90] | Loss: 0.3374 | Train Acc: 92.86%
2026-02-10 01:54:39,394 - INFO - [Valid] [67/90] | Loss: 0.4743 | Val Acc: 81.71%
2026-02-10 01:54:39,398 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8025 | F1: 0.8025
2026-02-10 01:54:39,399 - INFO - [Metrics for 'normal'] | Precision: 0.8297 | Recall: 0.8297 | F1: 0.8297
2026-02-10 01:54:39,401 - INFO - --------------------------------------------------
2026-02-10 01:54:39,401 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 01:54:42,341 - INFO - [Train] [68/90] | Loss: 0.3341 | Train Acc: 93.30%
2026-02-10 01:54:43,053 - INFO - [Valid] [68/90] | Loss: 0.4831 | Val Acc: 81.42%
2026-02-10 01:54:43,055 - INFO - [Metrics for 'abnormal'] | Precision: 0.8052 | Recall: 0.7898 | F1: 0.7974
2026-02-10 01:54:43,055 - INFO - [Metrics for 'normal'] | Precision: 0.8216 | Recall: 0.8352 | F1: 0.8283
2026-02-10 01:54:43,056 - INFO - --------------------------------------------------
2026-02-10 01:54:43,056 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 01:54:45,654 - INFO - [Train] [69/90] | Loss: 0.3347 | Train Acc: 92.86%
2026-02-10 01:54:46,605 - INFO - [Valid] [69/90] | Loss: 0.4859 | Val Acc: 80.24%
2026-02-10 01:54:46,610 - INFO - [Metrics for 'abnormal'] | Precision: 0.7711 | Recall: 0.8153 | F1: 0.7926
2026-02-10 01:54:46,610 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.7912 | F1: 0.8113
2026-02-10 01:54:46,612 - INFO - --------------------------------------------------
2026-02-10 01:54:46,612 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 01:54:49,179 - INFO - [Train] [70/90] | Loss: 0.3256 | Train Acc: 93.82%
2026-02-10 01:54:49,826 - INFO - [Valid] [70/90] | Loss: 0.4843 | Val Acc: 80.24%
2026-02-10 01:54:49,829 - INFO - [Metrics for 'abnormal'] | Precision: 0.7711 | Recall: 0.8153 | F1: 0.7926
2026-02-10 01:54:49,829 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.7912 | F1: 0.8113
2026-02-10 01:54:49,830 - INFO - --------------------------------------------------
2026-02-10 01:54:49,830 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 01:54:52,041 - INFO - [Train] [71/90] | Loss: 0.3369 | Train Acc: 93.68%
2026-02-10 01:54:52,694 - INFO - [Valid] [71/90] | Loss: 0.4832 | Val Acc: 80.83%
2026-02-10 01:54:52,699 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8217 | F1: 0.7988
2026-02-10 01:54:52,699 - INFO - [Metrics for 'normal'] | Precision: 0.8382 | Recall: 0.7967 | F1: 0.8169
2026-02-10 01:54:52,700 - INFO - --------------------------------------------------
2026-02-10 01:54:52,700 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 01:54:56,221 - INFO - [Train] [72/90] | Loss: 0.3239 | Train Acc: 93.75%
2026-02-10 01:54:56,953 - INFO - [Valid] [72/90] | Loss: 0.4766 | Val Acc: 80.53%
2026-02-10 01:54:56,957 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-02-10 01:54:56,958 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-02-10 01:54:56,962 - INFO - --------------------------------------------------
2026-02-10 01:54:56,963 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 01:55:00,353 - INFO - [Train] [73/90] | Loss: 0.3226 | Train Acc: 94.27%
2026-02-10 01:55:01,139 - INFO - [Valid] [73/90] | Loss: 0.4830 | Val Acc: 80.53%
2026-02-10 01:55:01,144 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-02-10 01:55:01,144 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-02-10 01:55:01,146 - INFO - --------------------------------------------------
2026-02-10 01:55:01,146 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 01:55:03,551 - INFO - [Train] [74/90] | Loss: 0.3220 | Train Acc: 93.60%
2026-02-10 01:55:04,105 - INFO - [Valid] [74/90] | Loss: 0.4835 | Val Acc: 79.65%
2026-02-10 01:55:04,109 - INFO - [Metrics for 'abnormal'] | Precision: 0.7750 | Recall: 0.7898 | F1: 0.7823
2026-02-10 01:55:04,109 - INFO - [Metrics for 'normal'] | Precision: 0.8156 | Recall: 0.8022 | F1: 0.8089
2026-02-10 01:55:04,111 - INFO - --------------------------------------------------
2026-02-10 01:55:04,111 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 01:55:06,089 - INFO - [Train] [75/90] | Loss: 0.3169 | Train Acc: 94.20%
2026-02-10 01:55:06,680 - INFO - [Valid] [75/90] | Loss: 0.4891 | Val Acc: 79.94%
2026-02-10 01:55:06,685 - INFO - [Metrics for 'abnormal'] | Precision: 0.7665 | Recall: 0.8153 | F1: 0.7901
2026-02-10 01:55:06,685 - INFO - [Metrics for 'normal'] | Precision: 0.8314 | Recall: 0.7857 | F1: 0.8079
2026-02-10 01:55:06,686 - INFO - --------------------------------------------------
2026-02-10 01:55:06,686 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 01:55:08,719 - INFO - [Train] [76/90] | Loss: 0.3194 | Train Acc: 93.82%
2026-02-10 01:55:09,178 - INFO - [Valid] [76/90] | Loss: 0.4836 | Val Acc: 80.53%
2026-02-10 01:55:09,181 - INFO - [Metrics for 'abnormal'] | Precision: 0.7725 | Recall: 0.8217 | F1: 0.7963
2026-02-10 01:55:09,181 - INFO - [Metrics for 'normal'] | Precision: 0.8372 | Recall: 0.7912 | F1: 0.8136
2026-02-10 01:55:09,182 - INFO - --------------------------------------------------
2026-02-10 01:55:09,182 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 01:55:11,365 - INFO - [Train] [77/90] | Loss: 0.3174 | Train Acc: 94.12%
2026-02-10 01:55:11,917 - INFO - [Valid] [77/90] | Loss: 0.4868 | Val Acc: 80.53%
2026-02-10 01:55:11,921 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-02-10 01:55:11,921 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-02-10 01:55:11,922 - INFO - --------------------------------------------------
2026-02-10 01:55:11,922 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 01:55:13,687 - INFO - [Train] [78/90] | Loss: 0.3096 | Train Acc: 94.94%
2026-02-10 01:55:14,226 - INFO - [Valid] [78/90] | Loss: 0.4883 | Val Acc: 80.83%
2026-02-10 01:55:14,231 - INFO - [Metrics for 'abnormal'] | Precision: 0.7949 | Recall: 0.7898 | F1: 0.7923
2026-02-10 01:55:14,232 - INFO - [Metrics for 'normal'] | Precision: 0.8197 | Recall: 0.8242 | F1: 0.8219
2026-02-10 01:55:14,233 - INFO - --------------------------------------------------
2026-02-10 01:55:14,234 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 01:55:16,315 - INFO - [Train] [79/90] | Loss: 0.3240 | Train Acc: 93.53%
2026-02-10 01:55:16,769 - INFO - [Valid] [79/90] | Loss: 0.4870 | Val Acc: 80.83%
2026-02-10 01:55:16,772 - INFO - [Metrics for 'abnormal'] | Precision: 0.7805 | Recall: 0.8153 | F1: 0.7975
2026-02-10 01:55:16,772 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8022 | F1: 0.8179
2026-02-10 01:55:16,772 - INFO - --------------------------------------------------
2026-02-10 01:55:16,772 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 01:55:18,857 - INFO - [Train] [80/90] | Loss: 0.3220 | Train Acc: 93.45%
2026-02-10 01:55:19,398 - INFO - [Valid] [80/90] | Loss: 0.4874 | Val Acc: 80.83%
2026-02-10 01:55:19,402 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-02-10 01:55:19,402 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-02-10 01:55:19,403 - INFO - --------------------------------------------------
2026-02-10 01:55:19,403 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 01:55:21,162 - INFO - [Train] [81/90] | Loss: 0.3129 | Train Acc: 94.20%
2026-02-10 01:55:21,689 - INFO - [Valid] [81/90] | Loss: 0.4899 | Val Acc: 80.53%
2026-02-10 01:55:21,693 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-02-10 01:55:21,693 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-02-10 01:55:21,694 - INFO - --------------------------------------------------
2026-02-10 01:55:21,694 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 01:55:23,826 - INFO - [Train] [82/90] | Loss: 0.3227 | Train Acc: 93.90%
2026-02-10 01:55:24,323 - INFO - [Valid] [82/90] | Loss: 0.4878 | Val Acc: 81.42%
2026-02-10 01:55:24,326 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-02-10 01:55:24,326 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-02-10 01:55:24,326 - INFO - --------------------------------------------------
2026-02-10 01:55:24,327 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 01:55:26,343 - INFO - [Train] [83/90] | Loss: 0.3131 | Train Acc: 94.64%
2026-02-10 01:55:26,869 - INFO - [Valid] [83/90] | Loss: 0.4937 | Val Acc: 81.12%
2026-02-10 01:55:26,872 - INFO - [Metrics for 'abnormal'] | Precision: 0.7853 | Recall: 0.8153 | F1: 0.8000
2026-02-10 01:55:26,873 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8077 | F1: 0.8212
2026-02-10 01:55:26,874 - INFO - --------------------------------------------------
2026-02-10 01:55:26,874 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 01:55:28,636 - INFO - [Train] [84/90] | Loss: 0.3116 | Train Acc: 94.57%
2026-02-10 01:55:29,273 - INFO - [Valid] [84/90] | Loss: 0.4950 | Val Acc: 80.53%
2026-02-10 01:55:29,277 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-02-10 01:55:29,278 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-02-10 01:55:29,279 - INFO - --------------------------------------------------
2026-02-10 01:55:29,279 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 01:55:31,521 - INFO - [Train] [85/90] | Loss: 0.3207 | Train Acc: 94.12%
2026-02-10 01:55:32,027 - INFO - [Valid] [85/90] | Loss: 0.4944 | Val Acc: 80.83%
2026-02-10 01:55:32,030 - INFO - [Metrics for 'abnormal'] | Precision: 0.7805 | Recall: 0.8153 | F1: 0.7975
2026-02-10 01:55:32,030 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8022 | F1: 0.8179
2026-02-10 01:55:32,031 - INFO - --------------------------------------------------
2026-02-10 01:55:32,031 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 01:55:34,146 - INFO - [Train] [86/90] | Loss: 0.3176 | Train Acc: 94.72%
2026-02-10 01:55:34,683 - INFO - [Valid] [86/90] | Loss: 0.4902 | Val Acc: 81.12%
2026-02-10 01:55:34,687 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 01:55:34,687 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 01:55:34,688 - INFO - --------------------------------------------------
2026-02-10 01:55:34,689 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 01:55:36,534 - INFO - [Train] [87/90] | Loss: 0.3081 | Train Acc: 94.64%
2026-02-10 01:55:37,039 - INFO - [Valid] [87/90] | Loss: 0.4889 | Val Acc: 80.53%
2026-02-10 01:55:37,043 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-02-10 01:55:37,043 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-02-10 01:55:37,045 - INFO - --------------------------------------------------
2026-02-10 01:55:37,045 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 01:55:39,241 - INFO - [Train] [88/90] | Loss: 0.3040 | Train Acc: 95.39%
2026-02-10 01:55:39,681 - INFO - [Valid] [88/90] | Loss: 0.4890 | Val Acc: 79.35%
2026-02-10 01:55:39,683 - INFO - [Metrics for 'abnormal'] | Precision: 0.7574 | Recall: 0.8153 | F1: 0.7853
2026-02-10 01:55:39,683 - INFO - [Metrics for 'normal'] | Precision: 0.8294 | Recall: 0.7747 | F1: 0.8011
2026-02-10 01:55:39,684 - INFO - --------------------------------------------------
2026-02-10 01:55:39,684 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 01:55:41,862 - INFO - [Train] [89/90] | Loss: 0.3101 | Train Acc: 95.01%
2026-02-10 01:55:42,363 - INFO - [Valid] [89/90] | Loss: 0.4995 | Val Acc: 80.53%
2026-02-10 01:55:42,366 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-02-10 01:55:42,366 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-02-10 01:55:42,367 - INFO - --------------------------------------------------
2026-02-10 01:55:42,367 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 01:55:44,169 - INFO - [Train] [90/90] | Loss: 0.3081 | Train Acc: 95.09%
2026-02-10 01:55:44,717 - INFO - [Valid] [90/90] | Loss: 0.4920 | Val Acc: 80.24%
2026-02-10 01:55:44,733 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8025 | F1: 0.7900
2026-02-10 01:55:44,733 - INFO - [Metrics for 'normal'] | Precision: 0.8249 | Recall: 0.8022 | F1: 0.8134
2026-02-10 01:55:44,734 - INFO - ==================================================
2026-02-10 01:55:44,734 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 01:55:44,734 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 01:55:44,734 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:55:44,792 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 01:55:44,793 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:55:44,793 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:55:44,794 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:55:46,838 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:55:47,232 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-02-10 01:55:47,232 - INFO - ==================================================
2026-02-10 01:55:47,234 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/best_model.pth'
2026-02-10 01:55:47,234 - INFO - ==================================================
2026-02-10 01:55:47,234 - INFO - Test 모드를 시작합니다.
2026-02-10 01:55:47,308 - INFO - 연산량 (MACs): 0.0927 GMACs per sample
2026-02-10 01:55:47,308 - INFO - 연산량 (FLOPs): 0.1854 GFLOPs per sample
2026-02-10 01:55:47,308 - INFO - ==================================================
2026-02-10 01:55:47,308 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 01:55:47,711 - INFO - 샘플 당 평균 Forward Pass 시간: 0.13ms (std: 0.05ms), FPS: 7846.26 (std: 923.75) (1개 샘플 x 100회 반복)
2026-02-10 01:55:47,711 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 160.22 MB
2026-02-10 01:55:47,712 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 01:55:48,825 - INFO - [Test] Loss: 0.3847 | Test Acc: 82.01%
2026-02-10 01:55:48,829 - INFO - [Metrics for 'abnormal'] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-02-10 01:55:48,829 - INFO - [Metrics for 'normal'] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-02-10 01:55:49,004 - INFO - ==================================================
2026-02-10 01:55:49,004 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/confusion_matrix_20260210_014021.png' and 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/confusion_matrix_20260210_014021.pdf'
2026-02-10 01:55:49,004 - INFO - ==================================================
2026-02-10 01:55:49,004 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 01:55:49,046 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/model_fp32_20260210_014021.onnx'에 저장되었습니다. (크기: 0.22 MB)
2026-02-10 01:55:49,276 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 9.56 MB
2026-02-10 01:55:49,276 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 01:55:49,536 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 0.97ms (std: 1.31ms)
2026-02-10 01:55:49,536 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 1837.71 FPS (std: 619.65) (1개 샘플 x 100회 반복)
2026-02-10 01:55:49,536 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 8.44 MB
2026-02-10 01:55:49,536 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 18.31 MB
2026-02-10 01:55:50,462 - INFO - [Test (ONNX)] | Test Acc (ONNX): 82.01%
2026-02-10 01:55:50,468 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-02-10 01:55:50,469 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-02-10 01:55:50,669 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/val_acc.pdf'
2026-02-10 01:55:50,858 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/train_val_acc.pdf'
2026-02-10 01:55:50,987 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/F1_normal.pdf'
2026-02-10 01:55:51,144 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/val_loss.png' and 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/val_loss.pdf'
2026-02-10 01:55:51,271 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/learning_rate.pdf'
2026-02-10 01:55:52,465 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/compile.png' and 'log/Sewer-TAPNEW/baseline_xie2019_wanda_20260210_014021/graph_20260210_014021/compile.pdf'
