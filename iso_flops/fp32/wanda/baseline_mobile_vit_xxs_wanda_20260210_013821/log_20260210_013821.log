2026-02-10 01:38:21,925 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/log_20260210_013821.log'에 저장됩니다.
2026-02-10 01:38:21,928 - INFO - ==================================================
2026-02-10 01:38:21,928 - INFO - config.yaml:
2026-02-10 01:38:21,928 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: mobile_vit_xxs
  use_wanda_pruning: true
  num_wanda_calib_samples: 1353
  pruning_flops_target: 0.1816

2026-02-10 01:38:21,928 - INFO - ==================================================
2026-02-10 01:38:21,969 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:38:21,971 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:38:21,971 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:38:21,977 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:38:21,978 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:38:21,978 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:38:21,978 - INFO - Baseline 모델 'mobile_vit_xxs'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:38:22,233 - INFO - timm 모델(mobile_vit_xxs)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:38:22,285 - INFO - ==================================================
2026-02-10 01:38:22,285 - INFO - 모델 파라미터 수:
2026-02-10 01:38:22,285 - INFO -   - 총 파라미터: 951,666 개
2026-02-10 01:38:22,285 - INFO -   - 학습 가능한 파라미터: 951,666 개
2026-02-10 01:38:22,285 - INFO - ================================================================================
2026-02-10 01:38:22,285 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:38:22,285 - INFO - ================================================================================
2026-02-10 01:38:22,285 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:38:22,286 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:38:22,286 - INFO - ==================================================
2026-02-10 01:38:22,286 - INFO - train 모드를 시작합니다.
2026-02-10 01:38:22,286 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:38:22,286 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:38:22,286 - INFO - --------------------------------------------------
2026-02-10 01:38:22,287 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:38:30,659 - INFO - [Train] [1/10] | Loss: 0.5206 | Train Acc: 78.35%
2026-02-10 01:38:33,003 - INFO - [Valid] [1/10] | Loss: 0.5402 | Val Acc: 81.42%
2026-02-10 01:38:33,011 - INFO - [Metrics for 'abnormal'] | Precision: 0.8052 | Recall: 0.7898 | F1: 0.7974
2026-02-10 01:38:33,012 - INFO - [Metrics for 'normal'] | Precision: 0.8216 | Recall: 0.8352 | F1: 0.8283
2026-02-10 01:38:33,037 - INFO - [Best Model Saved] (val loss: 0.5402) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:38:33,037 - INFO - --------------------------------------------------
2026-02-10 01:38:33,039 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:38:40,450 - INFO - [Train] [2/10] | Loss: 0.4619 | Train Acc: 83.26%
2026-02-10 01:38:41,925 - INFO - [Valid] [2/10] | Loss: 0.5207 | Val Acc: 82.30%
2026-02-10 01:38:41,930 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-02-10 01:38:41,930 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-02-10 01:38:41,960 - INFO - [Best Model Saved] (val loss: 0.5207) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:38:41,961 - INFO - --------------------------------------------------
2026-02-10 01:38:41,963 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:38:49,462 - INFO - [Train] [3/10] | Loss: 0.4301 | Train Acc: 85.27%
2026-02-10 01:38:50,786 - INFO - [Valid] [3/10] | Loss: 0.5232 | Val Acc: 81.12%
2026-02-10 01:38:50,791 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-02-10 01:38:50,792 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-02-10 01:38:50,793 - INFO - --------------------------------------------------
2026-02-10 01:38:50,794 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:38:58,197 - INFO - [Train] [4/10] | Loss: 0.3909 | Train Acc: 87.72%
2026-02-10 01:38:59,647 - INFO - [Valid] [4/10] | Loss: 0.5014 | Val Acc: 81.71%
2026-02-10 01:38:59,652 - INFO - [Metrics for 'abnormal'] | Precision: 0.7950 | Recall: 0.8153 | F1: 0.8050
2026-02-10 01:38:59,652 - INFO - [Metrics for 'normal'] | Precision: 0.8371 | Recall: 0.8187 | F1: 0.8278
2026-02-10 01:38:59,683 - INFO - [Best Model Saved] (val loss: 0.5014) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:38:59,683 - INFO - --------------------------------------------------
2026-02-10 01:38:59,685 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:39:06,856 - INFO - [Train] [5/10] | Loss: 0.3819 | Train Acc: 89.51%
2026-02-10 01:39:08,368 - INFO - [Valid] [5/10] | Loss: 0.5344 | Val Acc: 80.83%
2026-02-10 01:39:08,378 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8217 | F1: 0.7988
2026-02-10 01:39:08,378 - INFO - [Metrics for 'normal'] | Precision: 0.8382 | Recall: 0.7967 | F1: 0.8169
2026-02-10 01:39:08,379 - INFO - --------------------------------------------------
2026-02-10 01:39:08,381 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:39:15,794 - INFO - [Train] [6/10] | Loss: 0.3507 | Train Acc: 91.44%
2026-02-10 01:39:17,280 - INFO - [Valid] [6/10] | Loss: 0.5011 | Val Acc: 81.12%
2026-02-10 01:39:17,284 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7898 | F1: 0.7949
2026-02-10 01:39:17,285 - INFO - [Metrics for 'normal'] | Precision: 0.8207 | Recall: 0.8297 | F1: 0.8251
2026-02-10 01:39:17,329 - INFO - [Best Model Saved] (val loss: 0.5011) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:39:17,329 - INFO - --------------------------------------------------
2026-02-10 01:39:17,330 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:39:24,589 - INFO - [Train] [7/10] | Loss: 0.3083 | Train Acc: 94.05%
2026-02-10 01:39:25,739 - INFO - [Valid] [7/10] | Loss: 0.5027 | Val Acc: 80.24%
2026-02-10 01:39:25,743 - INFO - [Metrics for 'abnormal'] | Precision: 0.8082 | Recall: 0.7516 | F1: 0.7789
2026-02-10 01:39:25,744 - INFO - [Metrics for 'normal'] | Precision: 0.7979 | Recall: 0.8462 | F1: 0.8213
2026-02-10 01:39:25,745 - INFO - --------------------------------------------------
2026-02-10 01:39:25,746 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:39:33,229 - INFO - [Train] [8/10] | Loss: 0.2844 | Train Acc: 96.43%
2026-02-10 01:39:35,140 - INFO - [Valid] [8/10] | Loss: 0.5018 | Val Acc: 82.60%
2026-02-10 01:39:35,144 - INFO - [Metrics for 'abnormal'] | Precision: 0.7952 | Recall: 0.8408 | F1: 0.8173
2026-02-10 01:39:35,144 - INFO - [Metrics for 'normal'] | Precision: 0.8555 | Recall: 0.8132 | F1: 0.8338
2026-02-10 01:39:35,146 - INFO - --------------------------------------------------
2026-02-10 01:39:35,148 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:39:41,175 - INFO - [Train] [9/10] | Loss: 0.2630 | Train Acc: 97.17%
2026-02-10 01:39:42,398 - INFO - [Valid] [9/10] | Loss: 0.4704 | Val Acc: 83.19%
2026-02-10 01:39:42,402 - INFO - [Metrics for 'abnormal'] | Precision: 0.8125 | Recall: 0.8280 | F1: 0.8202
2026-02-10 01:39:42,402 - INFO - [Metrics for 'normal'] | Precision: 0.8492 | Recall: 0.8352 | F1: 0.8421
2026-02-10 01:39:42,429 - INFO - [Best Model Saved] (val loss: 0.4704) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:39:42,429 - INFO - --------------------------------------------------
2026-02-10 01:39:42,431 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:39:48,287 - INFO - [Train] [10/10] | Loss: 0.2581 | Train Acc: 97.77%
2026-02-10 01:39:49,758 - INFO - [Valid] [10/10] | Loss: 0.4794 | Val Acc: 81.42%
2026-02-10 01:39:49,763 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-02-10 01:39:49,764 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-02-10 01:39:49,766 - INFO - ================================================================================
2026-02-10 01:39:49,767 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:39:49,767 - INFO - ================================================================================
2026-02-10 01:39:49,814 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:39:49,815 - INFO - ================================================================================
2026-02-10 01:39:49,815 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:39:49,932 - INFO - 원본 모델 FLOPs: 0.5384 GFLOPs
2026-02-10 01:39:50,013 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:50,013 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:50,016 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:55,836 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:55,837 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:56,155 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:39:56,155 - INFO - ==================================================
2026-02-10 01:39:56,235 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.1747 GFLOPs (감소율: 67.56%)
2026-02-10 01:39:56,676 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:56,676 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:56,678 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:01,937 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:01,938 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:02,271 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.2475)에 맞춰 변경되었습니다.
2026-02-10 01:40:02,272 - INFO - ==================================================
2026-02-10 01:40:02,329 - INFO -   [탐색  2] 희소도: 0.2475 -> FLOPs: 0.3329 GFLOPs (감소율: 38.17%)
2026-02-10 01:40:02,376 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:02,376 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:02,378 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:07,962 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:07,963 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:08,278 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.37124999999999997)에 맞춰 변경되었습니다.
2026-02-10 01:40:08,279 - INFO - ==================================================
2026-02-10 01:40:08,422 - INFO -   [탐색  3] 희소도: 0.3712 -> FLOPs: 0.2479 GFLOPs (감소율: 53.96%)
2026-02-10 01:40:08,465 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:08,466 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:08,468 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:13,499 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:13,500 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:13,810 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.433125)에 맞춰 변경되었습니다.
2026-02-10 01:40:13,810 - INFO - ==================================================
2026-02-10 01:40:13,889 - INFO -   [탐색  4] 희소도: 0.4331 -> FLOPs: 0.2093 GFLOPs (감소율: 61.13%)
2026-02-10 01:40:13,932 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:13,932 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:13,934 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:18,939 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:18,940 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:19,245 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4640625)에 맞춰 변경되었습니다.
2026-02-10 01:40:19,246 - INFO - ==================================================
2026-02-10 01:40:19,319 - INFO -   [탐색  5] 희소도: 0.4641 -> FLOPs: 0.1880 GFLOPs (감소율: 65.08%)
2026-02-10 01:40:19,363 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:19,363 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:19,365 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:24,940 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:24,941 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:25,807 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47953124999999996)에 맞춰 변경되었습니다.
2026-02-10 01:40:25,807 - INFO - ==================================================
2026-02-10 01:40:25,876 - INFO -   [탐색  6] 희소도: 0.4795 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:40:25,919 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:25,919 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:25,921 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:31,580 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:31,581 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:31,830 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.471796875)에 맞춰 변경되었습니다.
2026-02-10 01:40:31,830 - INFO - ==================================================
2026-02-10 01:40:31,890 - INFO -   [탐색  7] 희소도: 0.4718 -> FLOPs: 0.1838 GFLOPs (감소율: 65.85%)
2026-02-10 01:40:31,931 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:31,932 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:31,934 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:38,333 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:38,335 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:38,644 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4756640625)에 맞춰 변경되었습니다.
2026-02-10 01:40:38,645 - INFO - ==================================================
2026-02-10 01:40:38,697 - INFO -   [탐색  8] 희소도: 0.4757 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-02-10 01:40:38,741 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:38,741 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:38,744 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:44,684 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:44,684 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:44,937 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47759765624999995)에 맞춰 변경되었습니다.
2026-02-10 01:40:44,937 - INFO - ==================================================
2026-02-10 01:40:44,986 - INFO -   [탐색  9] 희소도: 0.4776 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:40:45,028 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:45,028 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:45,030 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:51,516 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:51,517 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:51,911 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47856445312499996)에 맞춰 변경되었습니다.
2026-02-10 01:40:51,911 - INFO - ==================================================
2026-02-10 01:40:51,954 - INFO -   [탐색 10] 희소도: 0.4786 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:40:51,994 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:51,994 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:51,996 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:58,033 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:58,037 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:58,929 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47904785156249996)에 맞춰 변경되었습니다.
2026-02-10 01:40:58,930 - INFO - ==================================================
2026-02-10 01:40:58,988 - INFO -   [탐색 11] 희소도: 0.4790 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:40:59,031 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:59,032 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:59,035 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:04,994 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:04,995 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:05,301 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47928955078124996)에 맞춰 변경되었습니다.
2026-02-10 01:41:05,302 - INFO - ==================================================
2026-02-10 01:41:05,354 - INFO -   [탐색 12] 희소도: 0.4793 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:41:05,401 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:05,401 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:05,404 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:11,577 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:11,578 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:11,923 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916870117187493)에 맞춰 변경되었습니다.
2026-02-10 01:41:11,923 - INFO - ==================================================
2026-02-10 01:41:11,980 - INFO -   [탐색 13] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:41:12,031 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:12,031 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:12,036 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:17,628 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:17,629 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:18,033 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47910827636718745)에 맞춰 변경되었습니다.
2026-02-10 01:41:18,033 - INFO - ==================================================
2026-02-10 01:41:18,080 - INFO -   [탐색 14] 희소도: 0.4791 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:41:18,124 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:18,124 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:18,126 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:23,975 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:23,976 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:24,357 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791384887695312)에 맞춰 변경되었습니다.
2026-02-10 01:41:24,357 - INFO - ==================================================
2026-02-10 01:41:24,408 - INFO -   [탐색 15] 희소도: 0.4791 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:41:25,035 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:25,035 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:25,038 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:31,457 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:31,458 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:31,832 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791535949707031)에 맞춰 변경되었습니다.
2026-02-10 01:41:31,836 - INFO - ==================================================
2026-02-10 01:41:31,890 - INFO -   [탐색 16] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:41:31,935 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:31,936 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:31,938 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:37,639 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:37,640 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:37,933 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.479161148071289)에 맞춰 변경되었습니다.
2026-02-10 01:41:37,934 - INFO - ==================================================
2026-02-10 01:41:37,988 - INFO -   [탐색 17] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:41:38,034 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:38,034 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:38,036 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:43,813 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:43,814 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:44,141 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.479164924621582)에 맞춰 변경되었습니다.
2026-02-10 01:41:44,141 - INFO - ==================================================
2026-02-10 01:41:44,193 - INFO -   [탐색 18] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:41:44,233 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:44,234 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:44,236 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:50,182 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:50,183 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:50,500 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916681289672847)에 맞춰 변경되었습니다.
2026-02-10 01:41:50,500 - INFO - ==================================================
2026-02-10 01:41:50,548 - INFO -   [탐색 19] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:41:50,598 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:50,598 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:50,600 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:56,547 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:56,548 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:57,396 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916586875915523)에 맞춰 변경되었습니다.
2026-02-10 01:41:57,396 - INFO - ==================================================
2026-02-10 01:41:57,447 - INFO -   [탐색 20] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:41:57,494 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:57,495 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:57,499 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:03,870 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:03,871 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:04,207 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916634082794185)에 맞춰 변경되었습니다.
2026-02-10 01:42:04,207 - INFO - ==================================================
2026-02-10 01:42:04,256 - INFO -   [탐색 21] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:42:04,305 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:04,306 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:04,308 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:10,202 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:10,203 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:10,495 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916657686233516)에 맞춰 변경되었습니다.
2026-02-10 01:42:10,495 - INFO - ==================================================
2026-02-10 01:42:10,543 - INFO -   [탐색 22] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:42:10,589 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:10,590 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:10,592 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:16,953 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:16,954 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:17,275 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666948795318)에 맞춰 변경되었습니다.
2026-02-10 01:42:17,275 - INFO - ==================================================
2026-02-10 01:42:17,324 - INFO -   [탐색 23] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:42:17,376 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:17,376 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:17,378 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:23,515 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:23,516 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:23,836 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916663587093344)에 맞춰 변경되었습니다.
2026-02-10 01:42:23,836 - INFO - ==================================================
2026-02-10 01:42:23,885 - INFO -   [탐색 24] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:42:24,523 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:24,524 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:24,526 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:30,189 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:30,190 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:30,578 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666653752326)에 맞춰 변경되었습니다.
2026-02-10 01:42:30,579 - INFO - ==================================================
2026-02-10 01:42:30,629 - INFO -   [탐색 25] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:42:30,678 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:30,679 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:30,682 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:36,954 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:36,955 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:37,279 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916668012738217)에 맞춰 변경되었습니다.
2026-02-10 01:42:37,279 - INFO - ==================================================
2026-02-10 01:42:37,332 - INFO -   [탐색 26] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:42:37,382 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:37,382 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:37,385 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:43,615 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:43,620 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:43,969 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666727513074)에 맞춰 변경되었습니다.
2026-02-10 01:42:43,970 - INFO - ==================================================
2026-02-10 01:42:44,012 - INFO -   [탐색 27] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:42:44,052 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:44,052 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:44,055 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:49,572 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:49,572 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:49,919 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666906327)에 맞춰 변경되었습니다.
2026-02-10 01:42:49,920 - INFO - ==================================================
2026-02-10 01:42:49,975 - INFO -   [탐색 28] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:42:50,025 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:50,025 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:50,027 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:56,181 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:56,182 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:57,178 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666672192513)에 맞춰 변경되었습니다.
2026-02-10 01:42:57,179 - INFO - ==================================================
2026-02-10 01:42:57,227 - INFO -   [탐색 29] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:42:57,272 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:57,272 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:57,275 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:03,297 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:03,298 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:03,643 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666662972419)에 맞춰 변경되었습니다.
2026-02-10 01:43:03,644 - INFO - ==================================================
2026-02-10 01:43:03,698 - INFO -   [탐색 30] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:43:03,742 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:03,742 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:03,744 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:09,516 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:09,517 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:09,896 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666667582466)에 맞춰 변경되었습니다.
2026-02-10 01:43:09,896 - INFO - ==================================================
2026-02-10 01:43:09,941 - INFO -   [탐색 31] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:43:09,984 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:09,984 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:09,986 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:16,004 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:16,005 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:16,336 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666652774426)에 맞춰 변경되었습니다.
2026-02-10 01:43:16,336 - INFO - ==================================================
2026-02-10 01:43:16,389 - INFO -   [탐색 32] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:43:16,438 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:16,439 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:16,441 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:22,423 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:22,424 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:22,746 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666664299545)에 맞춰 변경되었습니다.
2026-02-10 01:43:22,747 - INFO - ==================================================
2026-02-10 01:43:22,794 - INFO -   [탐색 33] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:43:22,841 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:22,842 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:22,845 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:28,776 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:28,777 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:29,670 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.479166666700621)에 맞춰 변경되었습니다.
2026-02-10 01:43:29,670 - INFO - ==================================================
2026-02-10 01:43:29,721 - INFO -   [탐색 34] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:43:29,765 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:29,765 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:29,768 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:35,740 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:35,742 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:36,125 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666667180824)에 맞춰 변경되었습니다.
2026-02-10 01:43:36,125 - INFO - ==================================================
2026-02-10 01:43:36,172 - INFO -   [탐색 35] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:43:36,214 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:36,215 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:36,217 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:41,884 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:41,885 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:42,173 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666574018)에 맞춰 변경되었습니다.
2026-02-10 01:43:42,174 - INFO - ==================================================
2026-02-10 01:43:42,219 - INFO -   [탐색 36] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:43:42,259 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:42,260 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:42,261 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:47,786 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:47,792 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:48,123 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666460506)에 맞춰 변경되었습니다.
2026-02-10 01:43:48,124 - INFO - ==================================================
2026-02-10 01:43:48,167 - INFO -   [탐색 37] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:43:48,204 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:48,205 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:48,206 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:53,530 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:53,531 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:54,265 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666682066)에 맞춰 변경되었습니다.
2026-02-10 01:43:54,265 - INFO - ==================================================
2026-02-10 01:43:54,313 - INFO -   [탐색 38] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:43:54,355 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:54,356 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:54,358 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:59,342 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:59,343 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:59,641 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666640584)에 맞춰 변경되었습니다.
2026-02-10 01:43:59,641 - INFO - ==================================================
2026-02-10 01:43:59,683 - INFO -   [탐색 39] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:43:59,727 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:59,728 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:59,731 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:04,669 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:04,670 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:04,986 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666730623)에 맞춰 변경되었습니다.
2026-02-10 01:44:04,986 - INFO - ==================================================
2026-02-10 01:44:05,034 - INFO -   [탐색 40] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:44:05,077 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:05,077 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:05,079 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:09,796 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:09,797 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:10,198 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666685603)에 맞춰 변경되었습니다.
2026-02-10 01:44:10,198 - INFO - ==================================================
2026-02-10 01:44:10,249 - INFO -   [탐색 41] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:44:10,292 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:10,293 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:10,295 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:14,671 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:14,672 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:14,987 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666663094)에 맞춰 변경되었습니다.
2026-02-10 01:44:14,987 - INFO - ==================================================
2026-02-10 01:44:15,034 - INFO -   [탐색 42] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:44:15,081 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:15,082 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:15,084 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:19,607 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:19,609 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:20,544 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666674346)에 맞춰 변경되었습니다.
2026-02-10 01:44:20,544 - INFO - ==================================================
2026-02-10 01:44:20,596 - INFO -   [탐색 43] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:44:20,641 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:20,641 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:20,644 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:25,588 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:25,589 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:25,971 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666668717)에 맞춰 변경되었습니다.
2026-02-10 01:44:25,971 - INFO - ==================================================
2026-02-10 01:44:26,016 - INFO -   [탐색 44] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:44:26,059 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:26,060 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:26,062 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:30,894 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:30,895 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:31,192 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666591)에 맞춰 변경되었습니다.
2026-02-10 01:44:31,192 - INFO - ==================================================
2026-02-10 01:44:31,238 - INFO -   [탐색 45] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:44:31,281 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:31,281 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:31,284 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:36,154 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:36,154 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:36,371 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666731)에 맞춰 변경되었습니다.
2026-02-10 01:44:36,372 - INFO - ==================================================
2026-02-10 01:44:36,423 - INFO -   [탐색 46] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:44:36,461 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:36,462 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:36,464 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:41,273 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:41,274 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:41,577 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666661)에 맞춰 변경되었습니다.
2026-02-10 01:44:41,577 - INFO - ==================================================
2026-02-10 01:44:41,626 - INFO -   [탐색 47] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:44:41,669 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:41,669 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:41,672 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:46,560 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:46,561 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:47,436 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666696)에 맞춰 변경되었습니다.
2026-02-10 01:44:47,436 - INFO - ==================================================
2026-02-10 01:44:47,485 - INFO -   [탐색 48] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:44:47,531 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:47,532 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:47,535 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:52,088 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:52,088 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:52,391 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666785)에 맞춰 변경되었습니다.
2026-02-10 01:44:52,392 - INFO - ==================================================
2026-02-10 01:44:52,439 - INFO -   [탐색 49] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:44:52,481 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:52,482 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:52,485 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:57,173 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:57,175 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:57,529 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666696)에 맞춰 변경되었습니다.
2026-02-10 01:44:57,529 - INFO - ==================================================
2026-02-10 01:44:57,583 - INFO -   [탐색 50] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:44:57,629 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:57,629 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:57,632 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:02,166 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:02,167 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:02,489 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666665)에 맞춰 변경되었습니다.
2026-02-10 01:45:02,489 - INFO - ==================================================
2026-02-10 01:45:02,539 - INFO -   [탐색 51] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:45:02,583 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:02,583 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:02,586 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:07,009 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:07,011 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:07,227 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666674)에 맞춰 변경되었습니다.
2026-02-10 01:45:07,227 - INFO - ==================================================
2026-02-10 01:45:07,262 - INFO -   [탐색 52] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:45:07,855 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:07,855 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:07,858 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:12,788 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:12,790 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:13,137 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:45:13,137 - INFO - ==================================================
2026-02-10 01:45:13,186 - INFO -   [탐색 53] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:45:13,230 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:13,230 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:13,233 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:17,962 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:17,964 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:18,319 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666667)에 맞춰 변경되었습니다.
2026-02-10 01:45:18,320 - INFO - ==================================================
2026-02-10 01:45:18,375 - INFO -   [탐색 54] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:45:18,425 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:18,425 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:18,429 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:23,286 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:23,287 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:23,661 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:45:23,661 - INFO - ==================================================
2026-02-10 01:45:23,708 - INFO -   [탐색 55] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:45:23,753 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:23,753 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:23,755 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:28,548 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:28,549 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:28,896 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:45:28,896 - INFO - ==================================================
2026-02-10 01:45:28,946 - INFO -   [탐색 56] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:45:28,991 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:28,992 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:28,994 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:33,310 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:33,311 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:34,209 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:45:34,210 - INFO - ==================================================
2026-02-10 01:45:34,261 - INFO -   [탐색 57] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:45:34,305 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:34,306 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:34,308 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:38,902 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:38,904 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:39,258 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:45:39,259 - INFO - ==================================================
2026-02-10 01:45:39,305 - INFO -   [탐색 58] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:45:39,347 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:39,347 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:39,351 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:43,793 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:43,794 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:44,230 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:45:44,230 - INFO - ==================================================
2026-02-10 01:45:44,280 - INFO -   [탐색 59] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:45:44,326 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:44,326 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:44,328 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:49,203 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:49,204 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:49,614 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:45:49,614 - INFO - ==================================================
2026-02-10 01:45:49,660 - INFO -   [탐색 60] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:45:49,705 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:49,705 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:49,708 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:54,332 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:54,333 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:54,757 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:45:54,757 - INFO - ==================================================
2026-02-10 01:45:54,800 - INFO -   [탐색 61] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:45:54,825 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:54,825 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:54,826 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:59,435 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:59,437 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:00,276 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:00,276 - INFO - ==================================================
2026-02-10 01:46:00,307 - INFO -   [탐색 62] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:00,334 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:00,334 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:00,336 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:05,229 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:05,230 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:05,540 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:05,540 - INFO - ==================================================
2026-02-10 01:46:05,571 - INFO -   [탐색 63] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:05,597 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:05,597 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:05,598 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:10,556 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:10,557 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:10,817 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:10,818 - INFO - ==================================================
2026-02-10 01:46:10,849 - INFO -   [탐색 64] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:10,887 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:10,887 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:10,890 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:15,851 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:15,852 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:16,154 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:16,154 - INFO - ==================================================
2026-02-10 01:46:16,208 - INFO -   [탐색 65] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:16,249 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:16,249 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:16,252 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:20,818 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:20,819 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:21,131 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:21,131 - INFO - ==================================================
2026-02-10 01:46:21,179 - INFO -   [탐색 66] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:21,781 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:21,781 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:21,784 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:26,461 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:26,462 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:26,814 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:26,814 - INFO - ==================================================
2026-02-10 01:46:26,864 - INFO -   [탐색 67] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:26,915 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:26,915 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:26,918 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:31,555 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:31,556 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:31,929 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:31,929 - INFO - ==================================================
2026-02-10 01:46:31,977 - INFO -   [탐색 68] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:32,021 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:32,021 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:32,024 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:36,772 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:36,773 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:37,050 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:37,050 - INFO - ==================================================
2026-02-10 01:46:37,106 - INFO -   [탐색 69] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:37,152 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:37,152 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:37,154 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:42,028 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:42,030 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:42,332 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:42,332 - INFO - ==================================================
2026-02-10 01:46:42,377 - INFO -   [탐색 70] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:42,419 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:42,419 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:42,421 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:47,540 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:47,541 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:48,127 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:48,127 - INFO - ==================================================
2026-02-10 01:46:48,175 - INFO -   [탐색 71] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:48,217 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:48,217 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:48,219 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:53,090 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:53,091 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:53,424 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:53,425 - INFO - ==================================================
2026-02-10 01:46:53,477 - INFO -   [탐색 72] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:53,521 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:53,521 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:53,524 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:58,506 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:58,507 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:58,816 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:46:58,816 - INFO - ==================================================
2026-02-10 01:46:58,867 - INFO -   [탐색 73] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:46:58,909 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:58,909 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:58,912 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:03,593 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:03,595 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:03,940 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:03,941 - INFO - ==================================================
2026-02-10 01:47:03,984 - INFO -   [탐색 74] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:04,024 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:04,025 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:04,028 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:08,765 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:08,766 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:09,091 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:09,091 - INFO - ==================================================
2026-02-10 01:47:09,135 - INFO -   [탐색 75] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:09,754 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:09,754 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:09,757 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:14,545 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:14,546 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:14,911 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:14,911 - INFO - ==================================================
2026-02-10 01:47:14,959 - INFO -   [탐색 76] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:15,001 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:15,001 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:15,004 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:19,342 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:19,343 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:19,635 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:19,635 - INFO - ==================================================
2026-02-10 01:47:19,682 - INFO -   [탐색 77] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:19,725 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:19,726 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:19,728 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:24,272 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:24,272 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:24,587 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:24,587 - INFO - ==================================================
2026-02-10 01:47:24,644 - INFO -   [탐색 78] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:24,686 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:24,686 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:24,688 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:29,284 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:29,286 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:29,598 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:29,598 - INFO - ==================================================
2026-02-10 01:47:29,643 - INFO -   [탐색 79] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:29,689 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:29,689 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:29,692 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:34,113 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:34,114 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:35,011 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:35,012 - INFO - ==================================================
2026-02-10 01:47:35,061 - INFO -   [탐색 80] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:35,102 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:35,103 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:35,105 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:39,354 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:39,355 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:39,711 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:39,711 - INFO - ==================================================
2026-02-10 01:47:39,765 - INFO -   [탐색 81] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:39,812 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:39,813 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:39,815 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:44,389 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:44,390 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:44,693 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:44,693 - INFO - ==================================================
2026-02-10 01:47:44,744 - INFO -   [탐색 82] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:44,789 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:44,790 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:44,792 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:49,466 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:49,469 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:49,762 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:49,763 - INFO - ==================================================
2026-02-10 01:47:49,809 - INFO -   [탐색 83] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:49,852 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:49,852 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:49,855 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:54,325 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:54,326 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:54,639 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:54,639 - INFO - ==================================================
2026-02-10 01:47:54,689 - INFO -   [탐색 84] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:55,286 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:55,287 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:55,291 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:59,351 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:59,352 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:59,751 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:47:59,751 - INFO - ==================================================
2026-02-10 01:47:59,803 - INFO -   [탐색 85] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:47:59,847 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:59,847 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:59,850 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:04,011 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:04,012 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:04,348 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:04,349 - INFO - ==================================================
2026-02-10 01:48:04,400 - INFO -   [탐색 86] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:04,442 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:04,443 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:04,445 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:08,931 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:08,933 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:09,255 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:09,256 - INFO - ==================================================
2026-02-10 01:48:09,302 - INFO -   [탐색 87] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:09,351 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:09,351 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:09,353 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:13,805 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:13,806 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:14,189 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:14,189 - INFO - ==================================================
2026-02-10 01:48:14,236 - INFO -   [탐색 88] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:14,281 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:14,282 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:14,284 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:18,846 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:18,847 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:19,826 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:19,826 - INFO - ==================================================
2026-02-10 01:48:19,874 - INFO -   [탐색 89] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:19,918 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:19,918 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:19,920 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:24,108 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:24,109 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:24,488 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:24,488 - INFO - ==================================================
2026-02-10 01:48:24,537 - INFO -   [탐색 90] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:24,579 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:24,580 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:24,584 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:28,564 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:28,565 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:28,868 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:28,868 - INFO - ==================================================
2026-02-10 01:48:28,928 - INFO -   [탐색 91] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:28,973 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:28,973 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:28,975 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:33,369 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:33,370 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:33,650 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:33,650 - INFO - ==================================================
2026-02-10 01:48:33,699 - INFO -   [탐색 92] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:33,741 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:33,742 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:33,744 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:38,401 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:38,402 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:38,701 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:38,701 - INFO - ==================================================
2026-02-10 01:48:38,750 - INFO -   [탐색 93] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:39,379 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:39,380 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:39,382 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:43,620 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:43,621 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:43,955 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:43,956 - INFO - ==================================================
2026-02-10 01:48:44,006 - INFO -   [탐색 94] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:44,051 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:44,051 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:44,054 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:48,877 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:48,879 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:49,254 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:49,254 - INFO - ==================================================
2026-02-10 01:48:49,302 - INFO -   [탐색 95] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:49,347 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:49,348 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:49,351 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:54,126 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:54,127 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:54,436 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:54,437 - INFO - ==================================================
2026-02-10 01:48:54,485 - INFO -   [탐색 96] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:54,533 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:54,533 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:54,536 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:48:58,842 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:48:58,843 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:48:59,190 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:48:59,190 - INFO - ==================================================
2026-02-10 01:48:59,239 - INFO -   [탐색 97] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:48:59,284 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:48:59,285 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:48:59,287 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:03,982 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:03,983 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:49:04,912 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:49:04,912 - INFO - ==================================================
2026-02-10 01:49:04,958 - INFO -   [탐색 98] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:49:05,007 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:05,007 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:05,010 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:08,846 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:08,847 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:49:09,168 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:49:09,168 - INFO - ==================================================
2026-02-10 01:49:09,218 - INFO -   [탐색 99] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:49:09,264 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:09,264 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:09,266 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:13,038 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:13,039 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:49:13,412 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:49:13,413 - INFO - ==================================================
2026-02-10 01:49:13,460 - INFO -   [탐색 100] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:49:13,461 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.4786 입니다.
2026-02-10 01:49:13,461 - INFO - ================================================================================
2026-02-10 01:49:13,465 - INFO - 계산된 Pruning 정보(희소도: 0.4786)를 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:49:13,512 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:49:13,610 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:49:13,611 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:49:13,613 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:49:17,500 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:49:17,501 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:49:17,891 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47856445312499996)에 맞춰 변경되었습니다.
2026-02-10 01:49:17,892 - INFO - ==================================================
2026-02-10 01:49:17,894 - INFO - ==================================================
2026-02-10 01:49:17,894 - INFO - 모델 파라미터 수:
2026-02-10 01:49:17,894 - INFO -   - 총 파라미터: 320,501 개
2026-02-10 01:49:17,894 - INFO -   - 학습 가능한 파라미터: 320,501 개
2026-02-10 01:49:17,937 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:49:18,024 - INFO - FLOPs가 0.5384 GFLOPs에서 0.1824 GFLOPs로 감소했습니다 (감소율: 66.12%).
2026-02-10 01:49:18,025 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:49:18,025 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:49:18,026 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:49:18,027 - INFO - ==================================================
2026-02-10 01:49:18,027 - INFO - train 모드를 시작합니다.
2026-02-10 01:49:18,027 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:49:18,027 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:49:18,027 - INFO - --------------------------------------------------
2026-02-10 01:49:18,029 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:49:24,176 - INFO - [Train] [11/90] | Loss: 0.5104 | Train Acc: 79.76%
2026-02-10 01:49:25,529 - INFO - [Valid] [11/90] | Loss: 0.5406 | Val Acc: 77.29%
2026-02-10 01:49:25,535 - INFO - [Metrics for 'abnormal'] | Precision: 0.7410 | Recall: 0.7834 | F1: 0.7616
2026-02-10 01:49:25,535 - INFO - [Metrics for 'normal'] | Precision: 0.8035 | Recall: 0.7637 | F1: 0.7831
2026-02-10 01:49:25,578 - INFO - [Best Model Saved] (val loss: 0.5406) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:49:25,579 - INFO - --------------------------------------------------
2026-02-10 01:49:25,580 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:49:31,365 - INFO - [Train] [12/90] | Loss: 0.4604 | Train Acc: 84.23%
2026-02-10 01:49:32,829 - INFO - [Valid] [12/90] | Loss: 0.5352 | Val Acc: 76.11%
2026-02-10 01:49:32,833 - INFO - [Metrics for 'abnormal'] | Precision: 0.7065 | Recall: 0.8280 | F1: 0.7625
2026-02-10 01:49:32,834 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.7033 | F1: 0.7596
2026-02-10 01:49:32,859 - INFO - [Best Model Saved] (val loss: 0.5352) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:49:32,859 - INFO - --------------------------------------------------
2026-02-10 01:49:32,861 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:49:38,336 - INFO - [Train] [13/90] | Loss: 0.4261 | Train Acc: 86.01%
2026-02-10 01:49:39,573 - INFO - [Valid] [13/90] | Loss: 0.5425 | Val Acc: 77.29%
2026-02-10 01:49:39,578 - INFO - [Metrics for 'abnormal'] | Precision: 0.7632 | Recall: 0.7389 | F1: 0.7508
2026-02-10 01:49:39,578 - INFO - [Metrics for 'normal'] | Precision: 0.7807 | Recall: 0.8022 | F1: 0.7913
2026-02-10 01:49:39,579 - INFO - --------------------------------------------------
2026-02-10 01:49:39,581 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:49:45,242 - INFO - [Train] [14/90] | Loss: 0.4247 | Train Acc: 85.86%
2026-02-10 01:49:46,298 - INFO - [Valid] [14/90] | Loss: 0.5714 | Val Acc: 75.52%
2026-02-10 01:49:46,302 - INFO - [Metrics for 'abnormal'] | Precision: 0.6968 | Recall: 0.8344 | F1: 0.7594
2026-02-10 01:49:46,303 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.6868 | F1: 0.7508
2026-02-10 01:49:46,304 - INFO - --------------------------------------------------
2026-02-10 01:49:46,306 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:49:52,054 - INFO - [Train] [15/90] | Loss: 0.4194 | Train Acc: 86.38%
2026-02-10 01:49:53,344 - INFO - [Valid] [15/90] | Loss: 0.5165 | Val Acc: 80.24%
2026-02-10 01:49:53,349 - INFO - [Metrics for 'abnormal'] | Precision: 0.7922 | Recall: 0.7771 | F1: 0.7846
2026-02-10 01:49:53,349 - INFO - [Metrics for 'normal'] | Precision: 0.8108 | Recall: 0.8242 | F1: 0.8174
2026-02-10 01:49:53,381 - INFO - [Best Model Saved] (val loss: 0.5165) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:49:53,381 - INFO - --------------------------------------------------
2026-02-10 01:49:53,383 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:49:59,291 - INFO - [Train] [16/90] | Loss: 0.3847 | Train Acc: 88.91%
2026-02-10 01:50:00,284 - INFO - [Valid] [16/90] | Loss: 0.5820 | Val Acc: 80.53%
2026-02-10 01:50:00,287 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-02-10 01:50:00,287 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-02-10 01:50:00,288 - INFO - --------------------------------------------------
2026-02-10 01:50:00,289 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:50:05,914 - INFO - [Train] [17/90] | Loss: 0.3657 | Train Acc: 89.96%
2026-02-10 01:50:07,124 - INFO - [Valid] [17/90] | Loss: 0.5429 | Val Acc: 80.83%
2026-02-10 01:50:07,129 - INFO - [Metrics for 'abnormal'] | Precision: 0.8151 | Recall: 0.7580 | F1: 0.7855
2026-02-10 01:50:07,129 - INFO - [Metrics for 'normal'] | Precision: 0.8031 | Recall: 0.8516 | F1: 0.8267
2026-02-10 01:50:07,131 - INFO - --------------------------------------------------
2026-02-10 01:50:07,133 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:50:12,915 - INFO - [Train] [18/90] | Loss: 0.3313 | Train Acc: 92.26%
2026-02-10 01:50:14,273 - INFO - [Valid] [18/90] | Loss: 0.5588 | Val Acc: 77.29%
2026-02-10 01:50:14,278 - INFO - [Metrics for 'abnormal'] | Precision: 0.7247 | Recall: 0.8217 | F1: 0.7701
2026-02-10 01:50:14,278 - INFO - [Metrics for 'normal'] | Precision: 0.8261 | Recall: 0.7308 | F1: 0.7755
2026-02-10 01:50:14,279 - INFO - --------------------------------------------------
2026-02-10 01:50:14,281 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:50:20,243 - INFO - [Train] [19/90] | Loss: 0.3405 | Train Acc: 92.19%
2026-02-10 01:50:21,674 - INFO - [Valid] [19/90] | Loss: 0.5400 | Val Acc: 80.53%
2026-02-10 01:50:21,678 - INFO - [Metrics for 'abnormal'] | Precision: 0.7898 | Recall: 0.7898 | F1: 0.7898
2026-02-10 01:50:21,678 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8187 | F1: 0.8187
2026-02-10 01:50:21,679 - INFO - --------------------------------------------------
2026-02-10 01:50:21,681 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:50:27,367 - INFO - [Train] [20/90] | Loss: 0.3175 | Train Acc: 92.93%
2026-02-10 01:50:28,614 - INFO - [Valid] [20/90] | Loss: 0.5593 | Val Acc: 78.17%
2026-02-10 01:50:28,620 - INFO - [Metrics for 'abnormal'] | Precision: 0.7902 | Recall: 0.7197 | F1: 0.7533
2026-02-10 01:50:28,620 - INFO - [Metrics for 'normal'] | Precision: 0.7755 | Recall: 0.8352 | F1: 0.8042
2026-02-10 01:50:28,622 - INFO - --------------------------------------------------
2026-02-10 01:50:28,624 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:50:34,263 - INFO - [Train] [21/90] | Loss: 0.3196 | Train Acc: 92.93%
2026-02-10 01:50:35,608 - INFO - [Valid] [21/90] | Loss: 0.5715 | Val Acc: 79.94%
2026-02-10 01:50:35,613 - INFO - [Metrics for 'abnormal'] | Precision: 0.8201 | Recall: 0.7261 | F1: 0.7703
2026-02-10 01:50:35,613 - INFO - [Metrics for 'normal'] | Precision: 0.7850 | Recall: 0.8626 | F1: 0.8220
2026-02-10 01:50:35,614 - INFO - --------------------------------------------------
2026-02-10 01:50:35,615 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:50:41,856 - INFO - [Train] [22/90] | Loss: 0.3053 | Train Acc: 94.64%
2026-02-10 01:50:43,226 - INFO - [Valid] [22/90] | Loss: 0.5605 | Val Acc: 81.12%
2026-02-10 01:50:43,231 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-02-10 01:50:43,231 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-02-10 01:50:43,233 - INFO - --------------------------------------------------
2026-02-10 01:50:43,234 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:50:49,064 - INFO - [Train] [23/90] | Loss: 0.2840 | Train Acc: 95.98%
2026-02-10 01:50:50,078 - INFO - [Valid] [23/90] | Loss: 0.5823 | Val Acc: 80.83%
2026-02-10 01:50:50,083 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-02-10 01:50:50,083 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-02-10 01:50:50,084 - INFO - --------------------------------------------------
2026-02-10 01:50:50,086 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:50:56,171 - INFO - [Train] [24/90] | Loss: 0.2773 | Train Acc: 96.13%
2026-02-10 01:50:57,389 - INFO - [Valid] [24/90] | Loss: 0.5349 | Val Acc: 80.83%
2026-02-10 01:50:57,395 - INFO - [Metrics for 'abnormal'] | Precision: 0.8026 | Recall: 0.7771 | F1: 0.7896
2026-02-10 01:50:57,395 - INFO - [Metrics for 'normal'] | Precision: 0.8128 | Recall: 0.8352 | F1: 0.8238
2026-02-10 01:50:57,397 - INFO - --------------------------------------------------
2026-02-10 01:50:57,398 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:51:03,026 - INFO - [Train] [25/90] | Loss: 0.2888 | Train Acc: 95.46%
2026-02-10 01:51:04,411 - INFO - [Valid] [25/90] | Loss: 0.5010 | Val Acc: 82.60%
2026-02-10 01:51:04,416 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-02-10 01:51:04,416 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-02-10 01:51:04,442 - INFO - [Best Model Saved] (val loss: 0.5010) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:51:04,443 - INFO - --------------------------------------------------
2026-02-10 01:51:04,444 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:51:10,316 - INFO - [Train] [26/90] | Loss: 0.2676 | Train Acc: 96.95%
2026-02-10 01:51:11,640 - INFO - [Valid] [26/90] | Loss: 0.5524 | Val Acc: 82.01%
2026-02-10 01:51:11,644 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-02-10 01:51:11,645 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-02-10 01:51:11,646 - INFO - --------------------------------------------------
2026-02-10 01:51:11,648 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 01:51:17,706 - INFO - [Train] [27/90] | Loss: 0.2798 | Train Acc: 95.98%
2026-02-10 01:51:18,944 - INFO - [Valid] [27/90] | Loss: 0.5300 | Val Acc: 82.60%
2026-02-10 01:51:18,949 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.8153 | F1: 0.8127
2026-02-10 01:51:18,949 - INFO - [Metrics for 'normal'] | Precision: 0.8398 | Recall: 0.8352 | F1: 0.8375
2026-02-10 01:51:18,951 - INFO - --------------------------------------------------
2026-02-10 01:51:18,953 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 01:51:24,695 - INFO - [Train] [28/90] | Loss: 0.2634 | Train Acc: 96.88%
2026-02-10 01:51:25,871 - INFO - [Valid] [28/90] | Loss: 0.5845 | Val Acc: 80.24%
2026-02-10 01:51:25,875 - INFO - [Metrics for 'abnormal'] | Precision: 0.7922 | Recall: 0.7771 | F1: 0.7846
2026-02-10 01:51:25,875 - INFO - [Metrics for 'normal'] | Precision: 0.8108 | Recall: 0.8242 | F1: 0.8174
2026-02-10 01:51:25,876 - INFO - --------------------------------------------------
2026-02-10 01:51:25,878 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 01:51:32,035 - INFO - [Train] [29/90] | Loss: 0.2651 | Train Acc: 96.95%
2026-02-10 01:51:33,373 - INFO - [Valid] [29/90] | Loss: 0.5574 | Val Acc: 81.42%
2026-02-10 01:51:33,378 - INFO - [Metrics for 'abnormal'] | Precision: 0.7765 | Recall: 0.8408 | F1: 0.8073
2026-02-10 01:51:33,378 - INFO - [Metrics for 'normal'] | Precision: 0.8521 | Recall: 0.7912 | F1: 0.8205
2026-02-10 01:51:33,380 - INFO - --------------------------------------------------
2026-02-10 01:51:33,381 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 01:51:39,377 - INFO - [Train] [30/90] | Loss: 0.2595 | Train Acc: 97.62%
2026-02-10 01:51:40,628 - INFO - [Valid] [30/90] | Loss: 0.5034 | Val Acc: 80.53%
2026-02-10 01:51:40,633 - INFO - [Metrics for 'abnormal'] | Precision: 0.7974 | Recall: 0.7771 | F1: 0.7871
2026-02-10 01:51:40,633 - INFO - [Metrics for 'normal'] | Precision: 0.8118 | Recall: 0.8297 | F1: 0.8207
2026-02-10 01:51:40,635 - INFO - --------------------------------------------------
2026-02-10 01:51:40,637 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 01:51:46,439 - INFO - [Train] [31/90] | Loss: 0.2466 | Train Acc: 97.84%
2026-02-10 01:51:47,736 - INFO - [Valid] [31/90] | Loss: 0.5644 | Val Acc: 79.65%
2026-02-10 01:51:47,746 - INFO - [Metrics for 'abnormal'] | Precision: 0.7683 | Recall: 0.8025 | F1: 0.7850
2026-02-10 01:51:47,746 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.7912 | F1: 0.8067
2026-02-10 01:51:47,749 - INFO - --------------------------------------------------
2026-02-10 01:51:47,751 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 01:51:53,914 - INFO - [Train] [32/90] | Loss: 0.2528 | Train Acc: 97.69%
2026-02-10 01:51:54,965 - INFO - [Valid] [32/90] | Loss: 0.5783 | Val Acc: 79.94%
2026-02-10 01:51:54,970 - INFO - [Metrics for 'abnormal'] | Precision: 0.7730 | Recall: 0.8025 | F1: 0.7875
2026-02-10 01:51:54,970 - INFO - [Metrics for 'normal'] | Precision: 0.8239 | Recall: 0.7967 | F1: 0.8101
2026-02-10 01:51:54,971 - INFO - --------------------------------------------------
2026-02-10 01:51:54,973 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 01:52:01,192 - INFO - [Train] [33/90] | Loss: 0.2493 | Train Acc: 97.92%
2026-02-10 01:52:02,551 - INFO - [Valid] [33/90] | Loss: 0.5743 | Val Acc: 80.83%
2026-02-10 01:52:02,556 - INFO - [Metrics for 'abnormal'] | Precision: 0.8151 | Recall: 0.7580 | F1: 0.7855
2026-02-10 01:52:02,556 - INFO - [Metrics for 'normal'] | Precision: 0.8031 | Recall: 0.8516 | F1: 0.8267
2026-02-10 01:52:02,558 - INFO - --------------------------------------------------
2026-02-10 01:52:02,560 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 01:52:08,865 - INFO - [Train] [34/90] | Loss: 0.2454 | Train Acc: 98.21%
2026-02-10 01:52:09,902 - INFO - [Valid] [34/90] | Loss: 0.6160 | Val Acc: 79.35%
2026-02-10 01:52:09,906 - INFO - [Metrics for 'abnormal'] | Precision: 0.7736 | Recall: 0.7834 | F1: 0.7785
2026-02-10 01:52:09,907 - INFO - [Metrics for 'normal'] | Precision: 0.8111 | Recall: 0.8022 | F1: 0.8066
2026-02-10 01:52:09,908 - INFO - --------------------------------------------------
2026-02-10 01:52:09,910 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 01:52:16,016 - INFO - [Train] [35/90] | Loss: 0.2367 | Train Acc: 98.36%
2026-02-10 01:52:17,111 - INFO - [Valid] [35/90] | Loss: 0.6352 | Val Acc: 76.70%
2026-02-10 01:52:17,116 - INFO - [Metrics for 'abnormal'] | Precision: 0.7955 | Recall: 0.6688 | F1: 0.7266
2026-02-10 01:52:17,116 - INFO - [Metrics for 'normal'] | Precision: 0.7488 | Recall: 0.8516 | F1: 0.7969
2026-02-10 01:52:17,118 - INFO - --------------------------------------------------
2026-02-10 01:52:17,120 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 01:52:23,119 - INFO - [Train] [36/90] | Loss: 0.2547 | Train Acc: 97.40%
2026-02-10 01:52:24,442 - INFO - [Valid] [36/90] | Loss: 0.6345 | Val Acc: 78.17%
2026-02-10 01:52:24,447 - INFO - [Metrics for 'abnormal'] | Precision: 0.7173 | Recall: 0.8726 | F1: 0.7874
2026-02-10 01:52:24,447 - INFO - [Metrics for 'normal'] | Precision: 0.8649 | Recall: 0.7033 | F1: 0.7758
2026-02-10 01:52:24,448 - INFO - --------------------------------------------------
2026-02-10 01:52:24,450 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 01:52:29,724 - INFO - [Train] [37/90] | Loss: 0.2506 | Train Acc: 97.69%
2026-02-10 01:52:30,888 - INFO - [Valid] [37/90] | Loss: 0.6478 | Val Acc: 79.94%
2026-02-10 01:52:30,892 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8089 | F1: 0.7888
2026-02-10 01:52:30,892 - INFO - [Metrics for 'normal'] | Precision: 0.8276 | Recall: 0.7912 | F1: 0.8090
2026-02-10 01:52:30,893 - INFO - --------------------------------------------------
2026-02-10 01:52:30,894 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 01:52:35,474 - INFO - [Train] [38/90] | Loss: 0.2533 | Train Acc: 97.69%
2026-02-10 01:52:36,659 - INFO - [Valid] [38/90] | Loss: 0.5494 | Val Acc: 81.42%
2026-02-10 01:52:36,664 - INFO - [Metrics for 'abnormal'] | Precision: 0.7975 | Recall: 0.8025 | F1: 0.8000
2026-02-10 01:52:36,664 - INFO - [Metrics for 'normal'] | Precision: 0.8287 | Recall: 0.8242 | F1: 0.8264
2026-02-10 01:52:36,665 - INFO - --------------------------------------------------
2026-02-10 01:52:36,667 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 01:52:42,084 - INFO - [Train] [39/90] | Loss: 0.2501 | Train Acc: 98.14%
2026-02-10 01:52:43,189 - INFO - [Valid] [39/90] | Loss: 0.5757 | Val Acc: 80.83%
2026-02-10 01:52:43,194 - INFO - [Metrics for 'abnormal'] | Precision: 0.8026 | Recall: 0.7771 | F1: 0.7896
2026-02-10 01:52:43,194 - INFO - [Metrics for 'normal'] | Precision: 0.8128 | Recall: 0.8352 | F1: 0.8238
2026-02-10 01:52:43,196 - INFO - --------------------------------------------------
2026-02-10 01:52:43,198 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 01:52:47,835 - INFO - [Train] [40/90] | Loss: 0.2414 | Train Acc: 97.99%
2026-02-10 01:52:48,453 - INFO - [Valid] [40/90] | Loss: 0.5584 | Val Acc: 80.24%
2026-02-10 01:52:48,457 - INFO - [Metrics for 'abnormal'] | Precision: 0.7679 | Recall: 0.8217 | F1: 0.7938
2026-02-10 01:52:48,457 - INFO - [Metrics for 'normal'] | Precision: 0.8363 | Recall: 0.7857 | F1: 0.8102
2026-02-10 01:52:48,458 - INFO - --------------------------------------------------
2026-02-10 01:52:48,460 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 01:52:53,115 - INFO - [Train] [41/90] | Loss: 0.2331 | Train Acc: 98.44%
2026-02-10 01:52:54,029 - INFO - [Valid] [41/90] | Loss: 0.5795 | Val Acc: 79.94%
2026-02-10 01:52:54,034 - INFO - [Metrics for 'abnormal'] | Precision: 0.7572 | Recall: 0.8344 | F1: 0.7939
2026-02-10 01:52:54,034 - INFO - [Metrics for 'normal'] | Precision: 0.8434 | Recall: 0.7692 | F1: 0.8046
2026-02-10 01:52:54,035 - INFO - --------------------------------------------------
2026-02-10 01:52:54,037 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 01:52:58,711 - INFO - [Train] [42/90] | Loss: 0.2327 | Train Acc: 98.88%
2026-02-10 01:52:59,712 - INFO - [Valid] [42/90] | Loss: 0.6063 | Val Acc: 78.47%
2026-02-10 01:52:59,719 - INFO - [Metrics for 'abnormal'] | Precision: 0.7471 | Recall: 0.8089 | F1: 0.7768
2026-02-10 01:52:59,719 - INFO - [Metrics for 'normal'] | Precision: 0.8225 | Recall: 0.7637 | F1: 0.7920
2026-02-10 01:52:59,720 - INFO - --------------------------------------------------
2026-02-10 01:52:59,723 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 01:53:04,148 - INFO - [Train] [43/90] | Loss: 0.2383 | Train Acc: 98.51%
2026-02-10 01:53:05,193 - INFO - [Valid] [43/90] | Loss: 0.6323 | Val Acc: 80.24%
2026-02-10 01:53:05,198 - INFO - [Metrics for 'abnormal'] | Precision: 0.7586 | Recall: 0.8408 | F1: 0.7976
2026-02-10 01:53:05,205 - INFO - [Metrics for 'normal'] | Precision: 0.8485 | Recall: 0.7692 | F1: 0.8069
2026-02-10 01:53:05,207 - INFO - --------------------------------------------------
2026-02-10 01:53:05,211 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 01:53:09,795 - INFO - [Train] [44/90] | Loss: 0.2198 | Train Acc: 99.55%
2026-02-10 01:53:10,746 - INFO - [Valid] [44/90] | Loss: 0.6140 | Val Acc: 78.76%
2026-02-10 01:53:10,749 - INFO - [Metrics for 'abnormal'] | Precision: 0.7545 | Recall: 0.8025 | F1: 0.7778
2026-02-10 01:53:10,757 - INFO - [Metrics for 'normal'] | Precision: 0.8198 | Recall: 0.7747 | F1: 0.7966
2026-02-10 01:53:10,759 - INFO - --------------------------------------------------
2026-02-10 01:53:10,760 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 01:53:15,245 - INFO - [Train] [45/90] | Loss: 0.2255 | Train Acc: 99.03%
2026-02-10 01:53:16,131 - INFO - [Valid] [45/90] | Loss: 0.5800 | Val Acc: 79.35%
2026-02-10 01:53:16,139 - INFO - [Metrics for 'abnormal'] | Precision: 0.7544 | Recall: 0.8217 | F1: 0.7866
2026-02-10 01:53:16,139 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7692 | F1: 0.8000
2026-02-10 01:53:16,142 - INFO - --------------------------------------------------
2026-02-10 01:53:16,144 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 01:53:20,869 - INFO - [Train] [46/90] | Loss: 0.2289 | Train Acc: 99.26%
2026-02-10 01:53:21,708 - INFO - [Valid] [46/90] | Loss: 0.5630 | Val Acc: 80.24%
2026-02-10 01:53:21,713 - INFO - [Metrics for 'abnormal'] | Precision: 0.7885 | Recall: 0.7834 | F1: 0.7859
2026-02-10 01:53:21,713 - INFO - [Metrics for 'normal'] | Precision: 0.8142 | Recall: 0.8187 | F1: 0.8164
2026-02-10 01:53:21,714 - INFO - --------------------------------------------------
2026-02-10 01:53:21,716 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 01:53:26,451 - INFO - [Train] [47/90] | Loss: 0.2346 | Train Acc: 98.96%
2026-02-10 01:53:27,555 - INFO - [Valid] [47/90] | Loss: 0.5827 | Val Acc: 83.19%
2026-02-10 01:53:27,562 - INFO - [Metrics for 'abnormal'] | Precision: 0.8378 | Recall: 0.7898 | F1: 0.8131
2026-02-10 01:53:27,562 - INFO - [Metrics for 'normal'] | Precision: 0.8272 | Recall: 0.8681 | F1: 0.8472
2026-02-10 01:53:27,564 - INFO - --------------------------------------------------
2026-02-10 01:53:27,566 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 01:53:32,116 - INFO - [Train] [48/90] | Loss: 0.2302 | Train Acc: 98.88%
2026-02-10 01:53:32,966 - INFO - [Valid] [48/90] | Loss: 0.6020 | Val Acc: 79.65%
2026-02-10 01:53:32,970 - INFO - [Metrics for 'abnormal'] | Precision: 0.7472 | Recall: 0.8471 | F1: 0.7940
2026-02-10 01:53:32,970 - INFO - [Metrics for 'normal'] | Precision: 0.8509 | Recall: 0.7527 | F1: 0.7988
2026-02-10 01:53:32,972 - INFO - --------------------------------------------------
2026-02-10 01:53:32,974 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 01:53:37,342 - INFO - [Train] [49/90] | Loss: 0.2202 | Train Acc: 99.55%
2026-02-10 01:53:38,320 - INFO - [Valid] [49/90] | Loss: 0.5455 | Val Acc: 80.53%
2026-02-10 01:53:38,324 - INFO - [Metrics for 'abnormal'] | Precision: 0.7571 | Recall: 0.8535 | F1: 0.8024
2026-02-10 01:53:38,324 - INFO - [Metrics for 'normal'] | Precision: 0.8580 | Recall: 0.7637 | F1: 0.8081
2026-02-10 01:53:38,326 - INFO - --------------------------------------------------
2026-02-10 01:53:38,327 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 01:53:42,929 - INFO - [Train] [50/90] | Loss: 0.2193 | Train Acc: 99.78%
2026-02-10 01:53:43,545 - INFO - [Valid] [50/90] | Loss: 0.5766 | Val Acc: 80.83%
2026-02-10 01:53:43,550 - INFO - [Metrics for 'abnormal'] | Precision: 0.7674 | Recall: 0.8408 | F1: 0.8024
2026-02-10 01:53:43,550 - INFO - [Metrics for 'normal'] | Precision: 0.8503 | Recall: 0.7802 | F1: 0.8138
2026-02-10 01:53:43,551 - INFO - --------------------------------------------------
2026-02-10 01:53:43,553 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 01:53:47,922 - INFO - [Train] [51/90] | Loss: 0.2166 | Train Acc: 99.48%
2026-02-10 01:53:48,886 - INFO - [Valid] [51/90] | Loss: 0.5468 | Val Acc: 81.42%
2026-02-10 01:53:48,891 - INFO - [Metrics for 'abnormal'] | Precision: 0.7798 | Recall: 0.8344 | F1: 0.8062
2026-02-10 01:53:48,891 - INFO - [Metrics for 'normal'] | Precision: 0.8480 | Recall: 0.7967 | F1: 0.8215
2026-02-10 01:53:48,892 - INFO - --------------------------------------------------
2026-02-10 01:53:48,894 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 01:53:53,590 - INFO - [Train] [52/90] | Loss: 0.2125 | Train Acc: 99.85%
2026-02-10 01:53:54,314 - INFO - [Valid] [52/90] | Loss: 0.5540 | Val Acc: 81.12%
2026-02-10 01:53:54,319 - INFO - [Metrics for 'abnormal'] | Precision: 0.7751 | Recall: 0.8344 | F1: 0.8037
2026-02-10 01:53:54,319 - INFO - [Metrics for 'normal'] | Precision: 0.8471 | Recall: 0.7912 | F1: 0.8182
2026-02-10 01:53:54,320 - INFO - --------------------------------------------------
2026-02-10 01:53:54,322 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 01:53:58,561 - INFO - [Train] [53/90] | Loss: 0.2134 | Train Acc: 99.63%
2026-02-10 01:53:59,584 - INFO - [Valid] [53/90] | Loss: 0.5229 | Val Acc: 82.01%
2026-02-10 01:53:59,589 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-02-10 01:53:59,589 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-02-10 01:53:59,591 - INFO - --------------------------------------------------
2026-02-10 01:53:59,592 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 01:54:04,078 - INFO - [Train] [54/90] | Loss: 0.2121 | Train Acc: 99.70%
2026-02-10 01:54:05,070 - INFO - [Valid] [54/90] | Loss: 0.5606 | Val Acc: 80.83%
2026-02-10 01:54:05,074 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-02-10 01:54:05,074 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-02-10 01:54:05,076 - INFO - --------------------------------------------------
2026-02-10 01:54:05,078 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 01:54:09,473 - INFO - [Train] [55/90] | Loss: 0.2158 | Train Acc: 99.48%
2026-02-10 01:54:10,348 - INFO - [Valid] [55/90] | Loss: 0.5424 | Val Acc: 81.42%
2026-02-10 01:54:10,354 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-02-10 01:54:10,354 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-02-10 01:54:10,355 - INFO - --------------------------------------------------
2026-02-10 01:54:10,357 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 01:54:14,997 - INFO - [Train] [56/90] | Loss: 0.2135 | Train Acc: 99.55%
2026-02-10 01:54:15,902 - INFO - [Valid] [56/90] | Loss: 0.5683 | Val Acc: 80.53%
2026-02-10 01:54:15,911 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-02-10 01:54:15,912 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-02-10 01:54:15,913 - INFO - --------------------------------------------------
2026-02-10 01:54:15,915 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 01:54:20,337 - INFO - [Train] [57/90] | Loss: 0.2198 | Train Acc: 99.33%
2026-02-10 01:54:21,323 - INFO - [Valid] [57/90] | Loss: 0.5620 | Val Acc: 80.83%
2026-02-10 01:54:21,328 - INFO - [Metrics for 'abnormal'] | Precision: 0.7805 | Recall: 0.8153 | F1: 0.7975
2026-02-10 01:54:21,328 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8022 | F1: 0.8179
2026-02-10 01:54:21,331 - INFO - --------------------------------------------------
2026-02-10 01:54:21,333 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 01:54:25,782 - INFO - [Train] [58/90] | Loss: 0.2144 | Train Acc: 99.33%
2026-02-10 01:54:26,792 - INFO - [Valid] [58/90] | Loss: 0.5975 | Val Acc: 78.76%
2026-02-10 01:54:26,797 - INFO - [Metrics for 'abnormal'] | Precision: 0.7485 | Recall: 0.8153 | F1: 0.7805
2026-02-10 01:54:26,797 - INFO - [Metrics for 'normal'] | Precision: 0.8274 | Recall: 0.7637 | F1: 0.7943
2026-02-10 01:54:26,798 - INFO - --------------------------------------------------
2026-02-10 01:54:26,802 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 01:54:31,054 - INFO - [Train] [59/90] | Loss: 0.2105 | Train Acc: 99.55%
2026-02-10 01:54:32,120 - INFO - [Valid] [59/90] | Loss: 0.6280 | Val Acc: 79.94%
2026-02-10 01:54:32,124 - INFO - [Metrics for 'abnormal'] | Precision: 0.7405 | Recall: 0.8726 | F1: 0.8012
2026-02-10 01:54:32,124 - INFO - [Metrics for 'normal'] | Precision: 0.8701 | Recall: 0.7363 | F1: 0.7976
2026-02-10 01:54:32,126 - INFO - --------------------------------------------------
2026-02-10 01:54:32,127 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 01:54:36,936 - INFO - [Train] [60/90] | Loss: 0.2125 | Train Acc: 99.55%
2026-02-10 01:54:37,994 - INFO - [Valid] [60/90] | Loss: 0.6115 | Val Acc: 82.60%
2026-02-10 01:54:38,000 - INFO - [Metrics for 'abnormal'] | Precision: 0.8356 | Recall: 0.7771 | F1: 0.8053
2026-02-10 01:54:38,000 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8681 | F1: 0.8427
2026-02-10 01:54:38,001 - INFO - --------------------------------------------------
2026-02-10 01:54:38,003 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 01:54:42,532 - INFO - [Train] [61/90] | Loss: 0.2071 | Train Acc: 99.85%
2026-02-10 01:54:43,206 - INFO - [Valid] [61/90] | Loss: 0.6139 | Val Acc: 80.83%
2026-02-10 01:54:43,211 - INFO - [Metrics for 'abnormal'] | Precision: 0.7805 | Recall: 0.8153 | F1: 0.7975
2026-02-10 01:54:43,211 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8022 | F1: 0.8179
2026-02-10 01:54:43,212 - INFO - --------------------------------------------------
2026-02-10 01:54:43,213 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 01:54:47,970 - INFO - [Train] [62/90] | Loss: 0.2108 | Train Acc: 99.55%
2026-02-10 01:54:48,763 - INFO - [Valid] [62/90] | Loss: 0.6323 | Val Acc: 80.53%
2026-02-10 01:54:48,766 - INFO - [Metrics for 'abnormal'] | Precision: 0.7725 | Recall: 0.8217 | F1: 0.7963
2026-02-10 01:54:48,766 - INFO - [Metrics for 'normal'] | Precision: 0.8372 | Recall: 0.7912 | F1: 0.8136
2026-02-10 01:54:48,767 - INFO - --------------------------------------------------
2026-02-10 01:54:48,768 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 01:54:52,032 - INFO - [Train] [63/90] | Loss: 0.2070 | Train Acc: 99.78%
2026-02-10 01:54:52,615 - INFO - [Valid] [63/90] | Loss: 0.5753 | Val Acc: 82.60%
2026-02-10 01:54:52,617 - INFO - [Metrics for 'abnormal'] | Precision: 0.8403 | Recall: 0.7707 | F1: 0.8040
2026-02-10 01:54:52,618 - INFO - [Metrics for 'normal'] | Precision: 0.8154 | Recall: 0.8736 | F1: 0.8435
2026-02-10 01:54:52,618 - INFO - --------------------------------------------------
2026-02-10 01:54:52,619 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 01:54:56,451 - INFO - [Train] [64/90] | Loss: 0.2053 | Train Acc: 99.93%
2026-02-10 01:54:57,268 - INFO - [Valid] [64/90] | Loss: 0.5850 | Val Acc: 81.12%
2026-02-10 01:54:57,275 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-02-10 01:54:57,275 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-02-10 01:54:57,277 - INFO - --------------------------------------------------
2026-02-10 01:54:57,278 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 01:55:01,234 - INFO - [Train] [65/90] | Loss: 0.2083 | Train Acc: 99.63%
2026-02-10 01:55:02,068 - INFO - [Valid] [65/90] | Loss: 0.5748 | Val Acc: 81.42%
2026-02-10 01:55:02,072 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-02-10 01:55:02,072 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-02-10 01:55:02,073 - INFO - --------------------------------------------------
2026-02-10 01:55:02,075 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 01:55:05,703 - INFO - [Train] [66/90] | Loss: 0.2068 | Train Acc: 99.85%
2026-02-10 01:55:06,207 - INFO - [Valid] [66/90] | Loss: 0.5654 | Val Acc: 81.12%
2026-02-10 01:55:06,212 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 01:55:06,212 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 01:55:06,213 - INFO - --------------------------------------------------
2026-02-10 01:55:06,215 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 01:55:09,435 - INFO - [Train] [67/90] | Loss: 0.2052 | Train Acc: 99.93%
2026-02-10 01:55:10,177 - INFO - [Valid] [67/90] | Loss: 0.5873 | Val Acc: 81.12%
2026-02-10 01:55:10,181 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7898 | F1: 0.7949
2026-02-10 01:55:10,181 - INFO - [Metrics for 'normal'] | Precision: 0.8207 | Recall: 0.8297 | F1: 0.8251
2026-02-10 01:55:10,183 - INFO - --------------------------------------------------
2026-02-10 01:55:10,184 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 01:55:13,333 - INFO - [Train] [68/90] | Loss: 0.2047 | Train Acc: 99.70%
2026-02-10 01:55:13,843 - INFO - [Valid] [68/90] | Loss: 0.5641 | Val Acc: 82.01%
2026-02-10 01:55:13,847 - INFO - [Metrics for 'abnormal'] | Precision: 0.8200 | Recall: 0.7834 | F1: 0.8013
2026-02-10 01:55:13,848 - INFO - [Metrics for 'normal'] | Precision: 0.8201 | Recall: 0.8516 | F1: 0.8356
2026-02-10 01:55:13,849 - INFO - --------------------------------------------------
2026-02-10 01:55:13,851 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 01:55:16,885 - INFO - [Train] [69/90] | Loss: 0.2059 | Train Acc: 99.93%
2026-02-10 01:55:17,634 - INFO - [Valid] [69/90] | Loss: 0.5870 | Val Acc: 82.30%
2026-02-10 01:55:17,639 - INFO - [Metrics for 'abnormal'] | Precision: 0.8212 | Recall: 0.7898 | F1: 0.8052
2026-02-10 01:55:17,639 - INFO - [Metrics for 'normal'] | Precision: 0.8245 | Recall: 0.8516 | F1: 0.8378
2026-02-10 01:55:17,640 - INFO - --------------------------------------------------
2026-02-10 01:55:17,642 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 01:55:20,982 - INFO - [Train] [70/90] | Loss: 0.2045 | Train Acc: 99.78%
2026-02-10 01:55:21,518 - INFO - [Valid] [70/90] | Loss: 0.5714 | Val Acc: 82.30%
2026-02-10 01:55:21,521 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-02-10 01:55:21,521 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-02-10 01:55:21,522 - INFO - --------------------------------------------------
2026-02-10 01:55:21,523 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 01:55:24,656 - INFO - [Train] [71/90] | Loss: 0.2037 | Train Acc: 99.93%
2026-02-10 01:55:25,404 - INFO - [Valid] [71/90] | Loss: 0.5821 | Val Acc: 81.71%
2026-02-10 01:55:25,409 - INFO - [Metrics for 'abnormal'] | Precision: 0.8146 | Recall: 0.7834 | F1: 0.7987
2026-02-10 01:55:25,409 - INFO - [Metrics for 'normal'] | Precision: 0.8191 | Recall: 0.8462 | F1: 0.8324
2026-02-10 01:55:25,410 - INFO - --------------------------------------------------
2026-02-10 01:55:25,412 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 01:55:28,624 - INFO - [Train] [72/90] | Loss: 0.2027 | Train Acc: 99.93%
2026-02-10 01:55:29,225 - INFO - [Valid] [72/90] | Loss: 0.5891 | Val Acc: 82.30%
2026-02-10 01:55:29,228 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-02-10 01:55:29,228 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-02-10 01:55:29,229 - INFO - --------------------------------------------------
2026-02-10 01:55:29,229 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 01:55:32,335 - INFO - [Train] [73/90] | Loss: 0.2037 | Train Acc: 99.93%
2026-02-10 01:55:33,114 - INFO - [Valid] [73/90] | Loss: 0.5691 | Val Acc: 81.42%
2026-02-10 01:55:33,119 - INFO - [Metrics for 'abnormal'] | Precision: 0.7937 | Recall: 0.8089 | F1: 0.8013
2026-02-10 01:55:33,119 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8187 | F1: 0.8255
2026-02-10 01:55:33,121 - INFO - --------------------------------------------------
2026-02-10 01:55:33,122 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 01:55:36,261 - INFO - [Train] [74/90] | Loss: 0.2016 | Train Acc: 100.00%
2026-02-10 01:55:36,787 - INFO - [Valid] [74/90] | Loss: 0.5665 | Val Acc: 82.01%
2026-02-10 01:55:36,790 - INFO - [Metrics for 'abnormal'] | Precision: 0.8077 | Recall: 0.8025 | F1: 0.8051
2026-02-10 01:55:36,790 - INFO - [Metrics for 'normal'] | Precision: 0.8306 | Recall: 0.8352 | F1: 0.8329
2026-02-10 01:55:36,791 - INFO - --------------------------------------------------
2026-02-10 01:55:36,792 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 01:55:39,796 - INFO - [Train] [75/90] | Loss: 0.2031 | Train Acc: 99.93%
2026-02-10 01:55:40,553 - INFO - [Valid] [75/90] | Loss: 0.6004 | Val Acc: 79.94%
2026-02-10 01:55:40,557 - INFO - [Metrics for 'abnormal'] | Precision: 0.7602 | Recall: 0.8280 | F1: 0.7927
2026-02-10 01:55:40,558 - INFO - [Metrics for 'normal'] | Precision: 0.8393 | Recall: 0.7747 | F1: 0.8057
2026-02-10 01:55:40,559 - INFO - --------------------------------------------------
2026-02-10 01:55:40,561 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 01:55:43,759 - INFO - [Train] [76/90] | Loss: 0.2080 | Train Acc: 99.78%
2026-02-10 01:55:44,304 - INFO - [Valid] [76/90] | Loss: 0.5739 | Val Acc: 81.12%
2026-02-10 01:55:44,308 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8280 | F1: 0.8025
2026-02-10 01:55:44,309 - INFO - [Metrics for 'normal'] | Precision: 0.8430 | Recall: 0.7967 | F1: 0.8192
2026-02-10 01:55:44,310 - INFO - --------------------------------------------------
2026-02-10 01:55:44,312 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 01:55:47,145 - INFO - [Train] [77/90] | Loss: 0.2057 | Train Acc: 99.78%
2026-02-10 01:55:47,567 - INFO - [Valid] [77/90] | Loss: 0.5787 | Val Acc: 80.83%
2026-02-10 01:55:47,569 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-02-10 01:55:47,569 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-02-10 01:55:47,570 - INFO - --------------------------------------------------
2026-02-10 01:55:47,571 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 01:55:49,589 - INFO - [Train] [78/90] | Loss: 0.2023 | Train Acc: 99.85%
2026-02-10 01:55:50,336 - INFO - [Valid] [78/90] | Loss: 0.5674 | Val Acc: 81.42%
2026-02-10 01:55:50,339 - INFO - [Metrics for 'abnormal'] | Precision: 0.8013 | Recall: 0.7962 | F1: 0.7987
2026-02-10 01:55:50,339 - INFO - [Metrics for 'normal'] | Precision: 0.8251 | Recall: 0.8297 | F1: 0.8274
2026-02-10 01:55:50,341 - INFO - --------------------------------------------------
2026-02-10 01:55:50,341 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 01:55:52,157 - INFO - [Train] [79/90] | Loss: 0.2021 | Train Acc: 99.93%
2026-02-10 01:55:52,617 - INFO - [Valid] [79/90] | Loss: 0.5941 | Val Acc: 80.83%
2026-02-10 01:55:52,621 - INFO - [Metrics for 'abnormal'] | Precision: 0.7805 | Recall: 0.8153 | F1: 0.7975
2026-02-10 01:55:52,621 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8022 | F1: 0.8179
2026-02-10 01:55:52,622 - INFO - --------------------------------------------------
2026-02-10 01:55:52,623 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 01:55:54,435 - INFO - [Train] [80/90] | Loss: 0.2040 | Train Acc: 99.78%
2026-02-10 01:55:54,849 - INFO - [Valid] [80/90] | Loss: 0.5946 | Val Acc: 80.24%
2026-02-10 01:55:54,852 - INFO - [Metrics for 'abnormal'] | Precision: 0.7812 | Recall: 0.7962 | F1: 0.7886
2026-02-10 01:55:54,852 - INFO - [Metrics for 'normal'] | Precision: 0.8212 | Recall: 0.8077 | F1: 0.8144
2026-02-10 01:55:54,853 - INFO - --------------------------------------------------
2026-02-10 01:55:54,853 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 01:55:56,569 - INFO - [Train] [81/90] | Loss: 0.2015 | Train Acc: 99.93%
2026-02-10 01:55:56,985 - INFO - [Valid] [81/90] | Loss: 0.5761 | Val Acc: 81.71%
2026-02-10 01:55:56,987 - INFO - [Metrics for 'abnormal'] | Precision: 0.8105 | Recall: 0.7898 | F1: 0.8000
2026-02-10 01:55:56,988 - INFO - [Metrics for 'normal'] | Precision: 0.8226 | Recall: 0.8407 | F1: 0.8315
2026-02-10 01:55:56,988 - INFO - --------------------------------------------------
2026-02-10 01:55:56,989 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 01:55:58,669 - INFO - [Train] [82/90] | Loss: 0.2016 | Train Acc: 99.85%
2026-02-10 01:55:59,084 - INFO - [Valid] [82/90] | Loss: 0.6021 | Val Acc: 80.83%
2026-02-10 01:55:59,087 - INFO - [Metrics for 'abnormal'] | Precision: 0.7949 | Recall: 0.7898 | F1: 0.7923
2026-02-10 01:55:59,087 - INFO - [Metrics for 'normal'] | Precision: 0.8197 | Recall: 0.8242 | F1: 0.8219
2026-02-10 01:55:59,088 - INFO - --------------------------------------------------
2026-02-10 01:55:59,088 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 01:56:00,759 - INFO - [Train] [83/90] | Loss: 0.2021 | Train Acc: 99.93%
2026-02-10 01:56:01,168 - INFO - [Valid] [83/90] | Loss: 0.5775 | Val Acc: 81.71%
2026-02-10 01:56:01,170 - INFO - [Metrics for 'abnormal'] | Precision: 0.8105 | Recall: 0.7898 | F1: 0.8000
2026-02-10 01:56:01,170 - INFO - [Metrics for 'normal'] | Precision: 0.8226 | Recall: 0.8407 | F1: 0.8315
2026-02-10 01:56:01,171 - INFO - --------------------------------------------------
2026-02-10 01:56:01,172 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 01:56:02,864 - INFO - [Train] [84/90] | Loss: 0.2005 | Train Acc: 100.00%
2026-02-10 01:56:03,280 - INFO - [Valid] [84/90] | Loss: 0.6017 | Val Acc: 80.24%
2026-02-10 01:56:03,283 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8025 | F1: 0.7900
2026-02-10 01:56:03,283 - INFO - [Metrics for 'normal'] | Precision: 0.8249 | Recall: 0.8022 | F1: 0.8134
2026-02-10 01:56:03,284 - INFO - --------------------------------------------------
2026-02-10 01:56:03,284 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 01:56:04,979 - INFO - [Train] [85/90] | Loss: 0.2019 | Train Acc: 99.93%
2026-02-10 01:56:05,388 - INFO - [Valid] [85/90] | Loss: 0.5933 | Val Acc: 80.53%
2026-02-10 01:56:05,390 - INFO - [Metrics for 'abnormal'] | Precision: 0.7862 | Recall: 0.7962 | F1: 0.7911
2026-02-10 01:56:05,390 - INFO - [Metrics for 'normal'] | Precision: 0.8222 | Recall: 0.8132 | F1: 0.8177
2026-02-10 01:56:05,391 - INFO - --------------------------------------------------
2026-02-10 01:56:05,392 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 01:56:07,085 - INFO - [Train] [86/90] | Loss: 0.2033 | Train Acc: 99.85%
2026-02-10 01:56:07,496 - INFO - [Valid] [86/90] | Loss: 0.5787 | Val Acc: 80.53%
2026-02-10 01:56:07,499 - INFO - [Metrics for 'abnormal'] | Precision: 0.7898 | Recall: 0.7898 | F1: 0.7898
2026-02-10 01:56:07,499 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8187 | F1: 0.8187
2026-02-10 01:56:07,500 - INFO - --------------------------------------------------
2026-02-10 01:56:07,500 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 01:56:09,222 - INFO - [Train] [87/90] | Loss: 0.2001 | Train Acc: 100.00%
2026-02-10 01:56:09,650 - INFO - [Valid] [87/90] | Loss: 0.5887 | Val Acc: 80.83%
2026-02-10 01:56:09,652 - INFO - [Metrics for 'abnormal'] | Precision: 0.7911 | Recall: 0.7962 | F1: 0.7937
2026-02-10 01:56:09,653 - INFO - [Metrics for 'normal'] | Precision: 0.8232 | Recall: 0.8187 | F1: 0.8209
2026-02-10 01:56:09,653 - INFO - --------------------------------------------------
2026-02-10 01:56:09,654 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 01:56:11,334 - INFO - [Train] [88/90] | Loss: 0.2019 | Train Acc: 99.85%
2026-02-10 01:56:11,741 - INFO - [Valid] [88/90] | Loss: 0.5710 | Val Acc: 81.12%
2026-02-10 01:56:11,743 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7898 | F1: 0.7949
2026-02-10 01:56:11,743 - INFO - [Metrics for 'normal'] | Precision: 0.8207 | Recall: 0.8297 | F1: 0.8251
2026-02-10 01:56:11,744 - INFO - --------------------------------------------------
2026-02-10 01:56:11,745 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 01:56:13,407 - INFO - [Train] [89/90] | Loss: 0.2018 | Train Acc: 99.85%
2026-02-10 01:56:13,822 - INFO - [Valid] [89/90] | Loss: 0.5830 | Val Acc: 80.53%
2026-02-10 01:56:13,825 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-02-10 01:56:13,825 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-02-10 01:56:13,825 - INFO - --------------------------------------------------
2026-02-10 01:56:13,826 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 01:56:15,497 - INFO - [Train] [90/90] | Loss: 0.2014 | Train Acc: 99.93%
2026-02-10 01:56:15,899 - INFO - [Valid] [90/90] | Loss: 0.5947 | Val Acc: 80.53%
2026-02-10 01:56:15,902 - INFO - [Metrics for 'abnormal'] | Precision: 0.7862 | Recall: 0.7962 | F1: 0.7911
2026-02-10 01:56:15,902 - INFO - [Metrics for 'normal'] | Precision: 0.8222 | Recall: 0.8132 | F1: 0.8177
2026-02-10 01:56:15,903 - INFO - ==================================================
2026-02-10 01:56:15,903 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 01:56:15,903 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 01:56:15,903 - INFO - Baseline 모델 'mobile_vit_xxs'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:56:15,917 - INFO - timm 모델(mobile_vit_xxs)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:56:15,921 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 01:56:15,922 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:56:15,922 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:56:15,923 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:56:17,365 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:56:17,365 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:56:17,648 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47856445312499996)에 맞춰 변경되었습니다.
2026-02-10 01:56:17,648 - INFO - ==================================================
2026-02-10 01:56:17,667 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/best_model.pth'
2026-02-10 01:56:17,667 - INFO - ==================================================
2026-02-10 01:56:17,667 - INFO - Test 모드를 시작합니다.
2026-02-10 01:56:17,700 - INFO - 연산량 (MACs): 0.0912 GMACs per sample
2026-02-10 01:56:17,700 - INFO - 연산량 (FLOPs): 0.1824 GFLOPs per sample
2026-02-10 01:56:17,700 - INFO - ==================================================
2026-02-10 01:56:17,700 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 01:56:18,200 - INFO - 샘플 당 평균 Forward Pass 시간: 2.30ms (std: 0.10ms), FPS: 435.15 (std: 13.37) (1개 샘플 x 100회 반복)
2026-02-10 01:56:18,200 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 44.36 MB
2026-02-10 01:56:18,200 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 01:56:18,908 - INFO - [Test] Loss: 0.4083 | Test Acc: 82.60%
2026-02-10 01:56:18,912 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-02-10 01:56:18,912 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-02-10 01:56:19,036 - INFO - ==================================================
2026-02-10 01:56:19,036 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/confusion_matrix_20260210_013821.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/confusion_matrix_20260210_013821.pdf'
2026-02-10 01:56:19,036 - INFO - ==================================================
2026-02-10 01:56:19,036 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 01:56:19,251 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/model_fp32_20260210_013821.onnx'에 저장되었습니다. (크기: 1.40 MB)
2026-02-10 01:56:19,425 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 12.00 MB
2026-02-10 01:56:19,426 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 01:56:19,949 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 3.65ms (std: 0.04ms)
2026-02-10 01:56:19,949 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 274.15 FPS (std: 3.22) (1개 샘플 x 100회 반복)
2026-02-10 01:56:19,949 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 12.94 MB
2026-02-10 01:56:19,949 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 23.92 MB
2026-02-10 01:56:21,228 - INFO - [Test (ONNX)] | Test Acc (ONNX): 82.60%
2026-02-10 01:56:21,231 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-02-10 01:56:21,231 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-02-10 01:56:21,335 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/val_acc.pdf'
2026-02-10 01:56:21,442 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/train_val_acc.pdf'
2026-02-10 01:56:21,530 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/F1_normal.pdf'
2026-02-10 01:56:21,633 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/val_loss.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/val_loss.pdf'
2026-02-10 01:56:21,723 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/learning_rate.pdf'
2026-02-10 01:56:22,683 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/compile.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_wanda_20260210_013821/graph_20260210_013821/compile.pdf'
