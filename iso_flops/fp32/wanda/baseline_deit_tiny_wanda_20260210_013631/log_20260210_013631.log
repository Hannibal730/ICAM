2026-02-10 01:36:31,115 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/log_20260210_013631.log'에 저장됩니다.
2026-02-10 01:36:31,118 - INFO - ==================================================
2026-02-10 01:36:31,118 - INFO - config.yaml:
2026-02-10 01:36:31,118 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: deit_tiny
  use_wanda_pruning: true
  num_wanda_calib_samples: 1353
  pruning_flops_target: 0.1816

2026-02-10 01:36:31,118 - INFO - ==================================================
2026-02-10 01:36:31,160 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:36:31,161 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:36:31,161 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:36:31,170 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:36:31,171 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:36:31,171 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:36:31,171 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:36:34,045 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:36:34,046 - INFO - ==================================================
2026-02-10 01:36:34,047 - INFO - 모델 파라미터 수:
2026-02-10 01:36:34,047 - INFO -   - 총 파라미터: 5,524,802 개
2026-02-10 01:36:34,047 - INFO -   - 학습 가능한 파라미터: 5,524,802 개
2026-02-10 01:36:34,047 - INFO - ================================================================================
2026-02-10 01:36:34,047 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:36:34,047 - INFO - ================================================================================
2026-02-10 01:36:34,047 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:36:34,048 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:36:34,048 - INFO - ==================================================
2026-02-10 01:36:34,048 - INFO - train 모드를 시작합니다.
2026-02-10 01:36:34,048 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:36:34,048 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:36:34,048 - INFO - --------------------------------------------------
2026-02-10 01:36:34,049 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:36:41,857 - INFO - [Train] [1/10] | Loss: 0.7051 | Train Acc: 60.49%
2026-02-10 01:36:44,146 - INFO - [Valid] [1/10] | Loss: 0.6724 | Val Acc: 59.88%
2026-02-10 01:36:44,157 - INFO - [Metrics for 'abnormal'] | Precision: 0.5488 | Recall: 0.7516 | F1: 0.6344
2026-02-10 01:36:44,157 - INFO - [Metrics for 'normal'] | Precision: 0.6855 | Recall: 0.4670 | F1: 0.5556
2026-02-10 01:36:44,202 - INFO - [Best Model Saved] (val loss: 0.6724) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:36:44,202 - INFO - --------------------------------------------------
2026-02-10 01:36:44,203 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:36:49,893 - INFO - [Train] [2/10] | Loss: 0.6464 | Train Acc: 64.43%
2026-02-10 01:36:51,604 - INFO - [Valid] [2/10] | Loss: 0.6629 | Val Acc: 61.65%
2026-02-10 01:36:51,614 - INFO - [Metrics for 'abnormal'] | Precision: 0.7547 | Recall: 0.2548 | F1: 0.3810
2026-02-10 01:36:51,614 - INFO - [Metrics for 'normal'] | Precision: 0.5909 | Recall: 0.9286 | F1: 0.7222
2026-02-10 01:36:51,700 - INFO - [Best Model Saved] (val loss: 0.6629) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:36:51,700 - INFO - --------------------------------------------------
2026-02-10 01:36:51,701 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:36:57,341 - INFO - [Train] [3/10] | Loss: 0.5956 | Train Acc: 68.90%
2026-02-10 01:36:58,838 - INFO - [Valid] [3/10] | Loss: 0.5660 | Val Acc: 74.93%
2026-02-10 01:36:58,844 - INFO - [Metrics for 'abnormal'] | Precision: 0.7727 | Recall: 0.6497 | F1: 0.7059
2026-02-10 01:36:58,844 - INFO - [Metrics for 'normal'] | Precision: 0.7343 | Recall: 0.8352 | F1: 0.7815
2026-02-10 01:36:58,899 - INFO - [Best Model Saved] (val loss: 0.5660) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:36:58,899 - INFO - --------------------------------------------------
2026-02-10 01:36:58,901 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:37:05,641 - INFO - [Train] [4/10] | Loss: 0.5630 | Train Acc: 75.82%
2026-02-10 01:37:06,996 - INFO - [Valid] [4/10] | Loss: 0.5505 | Val Acc: 74.04%
2026-02-10 01:37:07,002 - INFO - [Metrics for 'abnormal'] | Precision: 0.7117 | Recall: 0.7389 | F1: 0.7250
2026-02-10 01:37:07,002 - INFO - [Metrics for 'normal'] | Precision: 0.7670 | Recall: 0.7418 | F1: 0.7542
2026-02-10 01:37:07,076 - INFO - [Best Model Saved] (val loss: 0.5505) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:37:07,076 - INFO - --------------------------------------------------
2026-02-10 01:37:07,077 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:37:13,431 - INFO - [Train] [5/10] | Loss: 0.5155 | Train Acc: 78.94%
2026-02-10 01:37:14,891 - INFO - [Valid] [5/10] | Loss: 0.5579 | Val Acc: 74.63%
2026-02-10 01:37:14,896 - INFO - [Metrics for 'abnormal'] | Precision: 0.7126 | Recall: 0.7580 | F1: 0.7346
2026-02-10 01:37:14,897 - INFO - [Metrics for 'normal'] | Precision: 0.7791 | Recall: 0.7363 | F1: 0.7571
2026-02-10 01:37:14,898 - INFO - --------------------------------------------------
2026-02-10 01:37:14,900 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:37:21,485 - INFO - [Train] [6/10] | Loss: 0.5177 | Train Acc: 78.50%
2026-02-10 01:37:23,049 - INFO - [Valid] [6/10] | Loss: 0.5393 | Val Acc: 74.63%
2026-02-10 01:37:23,055 - INFO - [Metrics for 'abnormal'] | Precision: 0.6940 | Recall: 0.8089 | F1: 0.7471
2026-02-10 01:37:23,055 - INFO - [Metrics for 'normal'] | Precision: 0.8077 | Recall: 0.6923 | F1: 0.7456
2026-02-10 01:37:23,131 - INFO - [Best Model Saved] (val loss: 0.5393) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:37:23,132 - INFO - --------------------------------------------------
2026-02-10 01:37:23,133 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:37:29,505 - INFO - [Train] [7/10] | Loss: 0.4901 | Train Acc: 81.32%
2026-02-10 01:37:30,792 - INFO - [Valid] [7/10] | Loss: 0.5521 | Val Acc: 76.99%
2026-02-10 01:37:30,798 - INFO - [Metrics for 'abnormal'] | Precision: 0.8264 | Recall: 0.6369 | F1: 0.7194
2026-02-10 01:37:30,799 - INFO - [Metrics for 'normal'] | Precision: 0.7385 | Recall: 0.8846 | F1: 0.8050
2026-02-10 01:37:30,800 - INFO - --------------------------------------------------
2026-02-10 01:37:30,801 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:37:36,959 - INFO - [Train] [8/10] | Loss: 0.4861 | Train Acc: 81.70%
2026-02-10 01:37:38,252 - INFO - [Valid] [8/10] | Loss: 0.5293 | Val Acc: 77.29%
2026-02-10 01:37:38,257 - INFO - [Metrics for 'abnormal'] | Precision: 0.8175 | Recall: 0.6561 | F1: 0.7279
2026-02-10 01:37:38,257 - INFO - [Metrics for 'normal'] | Precision: 0.7465 | Recall: 0.8736 | F1: 0.8051
2026-02-10 01:37:38,350 - INFO - [Best Model Saved] (val loss: 0.5293) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:37:38,350 - INFO - --------------------------------------------------
2026-02-10 01:37:38,351 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:37:43,724 - INFO - [Train] [9/10] | Loss: 0.4830 | Train Acc: 80.88%
2026-02-10 01:37:45,053 - INFO - [Valid] [9/10] | Loss: 0.5372 | Val Acc: 76.40%
2026-02-10 01:37:45,059 - INFO - [Metrics for 'abnormal'] | Precision: 0.7655 | Recall: 0.7070 | F1: 0.7351
2026-02-10 01:37:45,059 - INFO - [Metrics for 'normal'] | Precision: 0.7629 | Recall: 0.8132 | F1: 0.7872
2026-02-10 01:37:45,061 - INFO - --------------------------------------------------
2026-02-10 01:37:45,062 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:37:50,692 - INFO - [Train] [10/10] | Loss: 0.4752 | Train Acc: 82.29%
2026-02-10 01:37:52,097 - INFO - [Valid] [10/10] | Loss: 0.5302 | Val Acc: 76.70%
2026-02-10 01:37:52,109 - INFO - [Metrics for 'abnormal'] | Precision: 0.7378 | Recall: 0.7707 | F1: 0.7539
2026-02-10 01:37:52,113 - INFO - [Metrics for 'normal'] | Precision: 0.7943 | Recall: 0.7637 | F1: 0.7787
2026-02-10 01:37:52,116 - INFO - ================================================================================
2026-02-10 01:37:52,116 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:37:52,116 - INFO - ================================================================================
2026-02-10 01:37:52,155 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:37:52,159 - INFO - ================================================================================
2026-02-10 01:37:52,159 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:37:52,186 - INFO - 원본 모델 FLOPs: 2.1493 GFLOPs
2026-02-10 01:37:52,240 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:52,240 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:52,241 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:37:57,905 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:37:57,905 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:37:58,126 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:37:58,127 - INFO - ==================================================
2026-02-10 01:37:58,148 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.7288 GFLOPs (감소율: 66.09%)
2026-02-10 01:37:58,173 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:37:58,174 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:37:58,174 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:03,703 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:03,703 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:04,242 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-02-10 01:38:04,243 - INFO - ==================================================
2026-02-10 01:38:04,266 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.2840 GFLOPs (감소율: 86.79%)
2026-02-10 01:38:04,288 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:04,288 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:04,290 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:08,965 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:08,969 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:09,205 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-02-10 01:38:09,205 - INFO - ==================================================
2026-02-10 01:38:09,228 - INFO -   [탐색  3] 희소도: 0.8662 -> FLOPs: 0.1224 GFLOPs (감소율: 94.30%)
2026-02-10 01:38:09,260 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:09,260 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:09,262 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:13,970 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:13,971 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:14,180 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.804375)에 맞춰 변경되었습니다.
2026-02-10 01:38:14,180 - INFO - ==================================================
2026-02-10 01:38:14,203 - INFO -   [탐색  4] 희소도: 0.8044 -> FLOPs: 0.1980 GFLOPs (감소율: 90.79%)
2026-02-10 01:38:14,234 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:14,234 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:14,236 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:18,721 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:18,722 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:18,925 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8353124999999999)에 맞춰 변경되었습니다.
2026-02-10 01:38:18,925 - INFO - ==================================================
2026-02-10 01:38:18,949 - INFO -   [탐색  5] 희소도: 0.8353 -> FLOPs: 0.1588 GFLOPs (감소율: 92.61%)
2026-02-10 01:38:18,979 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:18,979 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:18,980 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:24,235 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:24,236 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:24,471 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81984375)에 맞춰 변경되었습니다.
2026-02-10 01:38:24,472 - INFO - ==================================================
2026-02-10 01:38:24,499 - INFO -   [탐색  6] 희소도: 0.8198 -> FLOPs: 0.1781 GFLOPs (감소율: 91.72%)
2026-02-10 01:38:24,527 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:24,528 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:24,529 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:30,430 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:30,432 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:31,163 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8121093749999999)에 맞춰 변경되었습니다.
2026-02-10 01:38:31,164 - INFO - ==================================================
2026-02-10 01:38:31,190 - INFO -   [탐색  7] 희소도: 0.8121 -> FLOPs: 0.1906 GFLOPs (감소율: 91.13%)
2026-02-10 01:38:31,216 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:31,216 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:31,218 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:37,322 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:37,323 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:37,574 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8159765625)에 맞춰 변경되었습니다.
2026-02-10 01:38:37,574 - INFO - ==================================================
2026-02-10 01:38:37,598 - INFO -   [탐색  8] 희소도: 0.8160 -> FLOPs: 0.1843 GFLOPs (감소율: 91.43%)
2026-02-10 01:38:37,625 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:37,625 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:37,627 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:43,035 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:43,036 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:43,288 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81791015625)에 맞춰 변경되었습니다.
2026-02-10 01:38:43,288 - INFO - ==================================================
2026-02-10 01:38:43,313 - INFO -   [탐색  9] 희소도: 0.8179 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:38:43,343 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:43,343 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:43,345 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:49,396 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:49,396 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:49,615 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-02-10 01:38:49,615 - INFO - ==================================================
2026-02-10 01:38:49,639 - INFO -   [탐색 10] 희소도: 0.8169 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:38:49,668 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:49,669 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:49,670 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:38:54,846 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:38:54,847 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:38:55,070 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8174267578125)에 맞춰 변경되었습니다.
2026-02-10 01:38:55,071 - INFO - ==================================================
2026-02-10 01:38:55,098 - INFO -   [탐색 11] 희소도: 0.8174 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:38:55,128 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:38:55,128 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:38:55,130 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:00,003 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:00,003 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:00,837 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81766845703125)에 맞춰 변경되었습니다.
2026-02-10 01:39:00,837 - INFO - ==================================================
2026-02-10 01:39:00,865 - INFO -   [탐색 12] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:39:00,898 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:00,898 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:00,900 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:06,797 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:06,798 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:07,020 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.817789306640625)에 맞춰 변경되었습니다.
2026-02-10 01:39:07,021 - INFO - ==================================================
2026-02-10 01:39:07,046 - INFO -   [탐색 13] 희소도: 0.8178 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:39:07,072 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:07,072 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:07,073 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:12,839 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:12,840 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:13,106 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177288818359375)에 맞춰 변경되었습니다.
2026-02-10 01:39:13,106 - INFO - ==================================================
2026-02-10 01:39:13,132 - INFO -   [탐색 14] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:39:13,158 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:13,158 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:13,159 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:18,782 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:18,783 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:19,012 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8176986694335937)에 맞춰 변경되었습니다.
2026-02-10 01:39:19,012 - INFO - ==================================================
2026-02-10 01:39:19,037 - INFO -   [탐색 15] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:39:19,067 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:19,067 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:19,069 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:24,934 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:24,934 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:25,172 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177137756347657)에 맞춰 변경되었습니다.
2026-02-10 01:39:25,172 - INFO - ==================================================
2026-02-10 01:39:25,198 - INFO -   [탐색 16] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:39:25,229 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:25,229 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:25,231 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:31,429 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:31,430 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:32,201 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177062225341797)에 맞춰 변경되었습니다.
2026-02-10 01:39:32,201 - INFO - ==================================================
2026-02-10 01:39:32,225 - INFO -   [탐색 17] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:39:32,256 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:32,256 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:32,257 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:36,746 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:36,747 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:37,030 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177099990844727)에 맞춰 변경되었습니다.
2026-02-10 01:39:37,030 - INFO - ==================================================
2026-02-10 01:39:37,054 - INFO -   [탐색 18] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:39:37,082 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:37,082 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:37,084 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:42,019 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:42,019 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:42,186 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177081108093263)에 맞춰 변경되었습니다.
2026-02-10 01:39:42,186 - INFO - ==================================================
2026-02-10 01:39:42,204 - INFO -   [탐색 19] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:39:42,227 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:42,228 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:42,229 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:47,487 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:47,488 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:47,688 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177090549468995)에 맞춰 변경되었습니다.
2026-02-10 01:39:47,688 - INFO - ==================================================
2026-02-10 01:39:47,707 - INFO -   [탐색 20] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:39:47,729 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:47,730 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:47,731 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:53,115 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:53,116 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:53,315 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177085828781129)에 맞춰 변경되었습니다.
2026-02-10 01:39:53,315 - INFO - ==================================================
2026-02-10 01:39:53,335 - INFO -   [탐색 21] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:39:53,358 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:53,358 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:53,359 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:39:58,828 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:39:58,832 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:39:59,436 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083468437196)에 맞춰 변경되었습니다.
2026-02-10 01:39:59,436 - INFO - ==================================================
2026-02-10 01:39:59,457 - INFO -   [탐색 22] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:39:59,483 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:39:59,484 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:39:59,485 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:04,940 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:04,941 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:05,145 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082288265229)에 맞춰 변경되었습니다.
2026-02-10 01:40:05,145 - INFO - ==================================================
2026-02-10 01:40:05,164 - INFO -   [탐색 23] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:40:05,191 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:05,191 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:05,192 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:10,144 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:10,144 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:10,360 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082878351213)에 맞춰 변경되었습니다.
2026-02-10 01:40:10,361 - INFO - ==================================================
2026-02-10 01:40:10,385 - INFO -   [탐색 24] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:40:10,414 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:10,414 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:10,416 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:15,258 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:15,259 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:15,456 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083173394204)에 맞춰 변경되었습니다.
2026-02-10 01:40:15,456 - INFO - ==================================================
2026-02-10 01:40:15,479 - INFO -   [탐색 25] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:40:15,506 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:15,507 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:15,508 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:20,059 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:20,060 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:20,793 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833209157)에 맞춰 변경되었습니다.
2026-02-10 01:40:20,793 - INFO - ==================================================
2026-02-10 01:40:20,817 - INFO -   [탐색 26] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:40:20,843 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:20,843 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:20,845 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:25,851 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:25,852 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:26,090 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083394676448)에 맞춰 변경되었습니다.
2026-02-10 01:40:26,090 - INFO - ==================================================
2026-02-10 01:40:26,112 - INFO -   [탐색 27] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:40:26,138 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:26,139 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:26,140 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:31,917 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:31,917 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:32,145 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083357796073)에 맞춰 변경되었습니다.
2026-02-10 01:40:32,146 - INFO - ==================================================
2026-02-10 01:40:32,171 - INFO -   [탐색 28] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:40:32,198 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:32,198 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:32,200 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:38,350 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:38,355 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:38,570 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083339355886)에 맞춰 변경되었습니다.
2026-02-10 01:40:38,570 - INFO - ==================================================
2026-02-10 01:40:38,594 - INFO -   [탐색 29] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:40:38,621 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:38,621 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:38,622 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:44,955 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:44,956 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:45,192 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083330135793)에 맞춰 변경되었습니다.
2026-02-10 01:40:45,193 - INFO - ==================================================
2026-02-10 01:40:45,217 - INFO -   [탐색 30] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:40:45,247 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:45,248 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:45,249 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:51,440 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:51,441 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:52,184 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083334745839)에 맞춰 변경되었습니다.
2026-02-10 01:40:52,184 - INFO - ==================================================
2026-02-10 01:40:52,208 - INFO -   [탐색 31] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:40:52,237 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:52,237 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:52,239 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:40:58,302 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:40:58,303 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:40:58,524 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083332440815)에 맞춰 변경되었습니다.
2026-02-10 01:40:58,525 - INFO - ==================================================
2026-02-10 01:40:58,551 - INFO -   [탐색 32] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:40:58,581 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:40:58,582 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:40:58,584 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:04,423 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:04,424 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:04,740 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333593327)에 맞춰 변경되었습니다.
2026-02-10 01:41:04,740 - INFO - ==================================================
2026-02-10 01:41:04,766 - INFO -   [탐색 33] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:41:04,791 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:04,792 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:04,793 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:10,610 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:10,610 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:10,856 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333017071)에 맞춰 변경되었습니다.
2026-02-10 01:41:10,857 - INFO - ==================================================
2026-02-10 01:41:10,882 - INFO -   [탐색 34] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:41:10,912 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:10,912 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:10,917 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:16,897 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:16,898 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:17,114 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833333052)에 맞춰 변경되었습니다.
2026-02-10 01:41:17,115 - INFO - ==================================================
2026-02-10 01:41:17,137 - INFO -   [탐색 35] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:41:17,162 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:17,163 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:17,165 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:23,092 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:23,095 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:23,883 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333449263)에 맞춰 변경되었습니다.
2026-02-10 01:41:23,884 - INFO - ==================================================
2026-02-10 01:41:23,907 - INFO -   [탐색 36] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:41:23,933 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:23,933 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:23,934 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:30,154 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:30,155 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:30,390 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333377231)에 맞춰 변경되었습니다.
2026-02-10 01:41:30,391 - INFO - ==================================================
2026-02-10 01:41:30,414 - INFO -   [탐색 37] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:41:30,444 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:30,445 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:30,446 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:36,663 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:36,664 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:36,894 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333341215)에 맞춰 변경되었습니다.
2026-02-10 01:41:36,894 - INFO - ==================================================
2026-02-10 01:41:36,922 - INFO -   [탐색 38] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:41:36,959 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:36,959 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:36,961 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:42,947 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:42,948 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:43,189 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333323207)에 맞춰 변경되었습니다.
2026-02-10 01:41:43,189 - INFO - ==================================================
2026-02-10 01:41:43,213 - INFO -   [탐색 39] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:41:43,244 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:43,244 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:43,245 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:48,973 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:48,973 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:49,193 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332211)에 맞춰 변경되었습니다.
2026-02-10 01:41:49,193 - INFO - ==================================================
2026-02-10 01:41:49,216 - INFO -   [탐색 40] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:41:49,247 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:49,247 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:49,248 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:41:54,972 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:41:54,972 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:41:55,798 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333336713)에 맞춰 변경되었습니다.
2026-02-10 01:41:55,798 - INFO - ==================================================
2026-02-10 01:41:55,828 - INFO -   [탐색 41] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:41:55,858 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:41:55,858 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:41:55,860 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:01,139 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:01,141 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:01,390 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333334463)에 맞춰 변경되었습니다.
2026-02-10 01:42:01,390 - INFO - ==================================================
2026-02-10 01:42:01,415 - INFO -   [탐색 42] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:42:01,443 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:01,444 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:01,446 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:07,210 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:07,211 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:07,443 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333337)에 맞춰 변경되었습니다.
2026-02-10 01:42:07,444 - INFO - ==================================================
2026-02-10 01:42:07,484 - INFO -   [탐색 43] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:42:07,510 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:07,510 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:07,512 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:13,420 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:13,420 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:13,651 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332774)에 맞춰 변경되었습니다.
2026-02-10 01:42:13,651 - INFO - ==================================================
2026-02-10 01:42:13,676 - INFO -   [탐색 44] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:42:13,706 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:13,706 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:13,708 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:19,448 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:19,449 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:19,675 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333055)에 맞춰 변경되었습니다.
2026-02-10 01:42:19,676 - INFO - ==================================================
2026-02-10 01:42:19,703 - INFO -   [탐색 45] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:42:19,733 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:19,734 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:19,735 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:25,605 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:25,606 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:26,424 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333196)에 맞춰 변경되었습니다.
2026-02-10 01:42:26,424 - INFO - ==================================================
2026-02-10 01:42:26,449 - INFO -   [탐색 46] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:42:26,479 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:26,480 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:26,482 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:32,528 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:32,529 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:32,801 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333266)에 맞춰 변경되었습니다.
2026-02-10 01:42:32,802 - INFO - ==================================================
2026-02-10 01:42:32,827 - INFO -   [탐색 47] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:42:32,859 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:32,859 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:32,860 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:39,013 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:39,014 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:39,233 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333302)에 맞춰 변경되었습니다.
2026-02-10 01:42:39,233 - INFO - ==================================================
2026-02-10 01:42:39,258 - INFO -   [탐색 48] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:42:39,284 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:39,284 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:39,286 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:45,511 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:45,511 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:45,726 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333319)에 맞춰 변경되었습니다.
2026-02-10 01:42:45,726 - INFO - ==================================================
2026-02-10 01:42:45,753 - INFO -   [탐색 49] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:42:45,780 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:45,780 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:45,781 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:51,774 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:51,775 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:52,038 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333328)에 맞춰 변경되었습니다.
2026-02-10 01:42:52,038 - INFO - ==================================================
2026-02-10 01:42:52,067 - INFO -   [탐색 50] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:42:52,096 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:52,096 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:52,097 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:42:57,474 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:42:57,475 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:42:58,281 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:42:58,282 - INFO - ==================================================
2026-02-10 01:42:58,306 - INFO -   [탐색 51] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:42:58,335 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:42:58,335 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:42:58,336 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:04,239 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:04,240 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:04,491 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333335)에 맞춰 변경되었습니다.
2026-02-10 01:43:04,491 - INFO - ==================================================
2026-02-10 01:43:04,514 - INFO -   [탐색 52] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:43:04,543 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:04,544 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:04,545 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:10,617 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:10,618 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:10,842 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333334)에 맞춰 변경되었습니다.
2026-02-10 01:43:10,842 - INFO - ==================================================
2026-02-10 01:43:10,871 - INFO -   [탐색 53] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:43:10,904 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:10,916 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:10,918 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:16,976 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:16,977 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:17,202 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:43:17,202 - INFO - ==================================================
2026-02-10 01:43:17,228 - INFO -   [탐색 54] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:43:17,256 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:17,256 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:17,258 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:23,379 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:23,380 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:24,154 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:43:24,155 - INFO - ==================================================
2026-02-10 01:43:24,182 - INFO -   [탐색 55] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:43:24,212 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:24,212 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:24,214 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:30,318 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:30,319 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:30,559 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:43:30,559 - INFO - ==================================================
2026-02-10 01:43:30,584 - INFO -   [탐색 56] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:43:30,611 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:30,611 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:30,613 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:36,270 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:36,271 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:36,494 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:43:36,494 - INFO - ==================================================
2026-02-10 01:43:36,518 - INFO -   [탐색 57] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:43:36,546 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:36,546 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:36,548 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:41,887 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:41,887 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:42,070 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:43:42,071 - INFO - ==================================================
2026-02-10 01:43:42,090 - INFO -   [탐색 58] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:43:42,114 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:42,114 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:42,115 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:47,453 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:47,454 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:47,689 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:43:47,689 - INFO - ==================================================
2026-02-10 01:43:47,720 - INFO -   [탐색 59] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:43:47,752 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:47,752 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:47,754 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:53,350 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:53,350 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:54,090 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:43:54,090 - INFO - ==================================================
2026-02-10 01:43:54,109 - INFO -   [탐색 60] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:43:54,128 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:54,129 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:54,130 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:43:58,864 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:43:58,865 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:43:59,149 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:43:59,149 - INFO - ==================================================
2026-02-10 01:43:59,166 - INFO -   [탐색 61] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:43:59,179 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:43:59,179 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:43:59,180 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:03,045 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:03,046 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:03,281 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:03,282 - INFO - ==================================================
2026-02-10 01:44:03,305 - INFO -   [탐색 62] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:03,329 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:03,329 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:03,331 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:07,142 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:07,143 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:07,352 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:07,353 - INFO - ==================================================
2026-02-10 01:44:07,377 - INFO -   [탐색 63] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:07,403 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:07,403 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:07,404 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:12,023 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:12,024 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:12,240 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:12,240 - INFO - ==================================================
2026-02-10 01:44:12,263 - INFO -   [탐색 64] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:12,291 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:12,291 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:12,293 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:17,002 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:17,003 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:17,820 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:17,820 - INFO - ==================================================
2026-02-10 01:44:17,845 - INFO -   [탐색 65] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:17,875 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:17,875 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:17,877 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:21,903 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:21,904 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:22,122 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:22,123 - INFO - ==================================================
2026-02-10 01:44:22,147 - INFO -   [탐색 66] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:22,176 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:22,176 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:22,178 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:26,620 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:26,621 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:26,882 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:26,882 - INFO - ==================================================
2026-02-10 01:44:26,907 - INFO -   [탐색 67] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:26,938 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:26,938 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:26,941 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:31,602 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:31,603 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:31,830 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:31,830 - INFO - ==================================================
2026-02-10 01:44:31,854 - INFO -   [탐색 68] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:31,879 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:31,879 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:31,880 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:36,342 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:36,343 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:36,538 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:36,539 - INFO - ==================================================
2026-02-10 01:44:36,561 - INFO -   [탐색 69] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:36,586 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:36,586 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:36,588 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:41,359 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:41,364 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:42,104 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:42,105 - INFO - ==================================================
2026-02-10 01:44:42,130 - INFO -   [탐색 70] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:42,159 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:42,159 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:42,161 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:46,924 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:46,924 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:47,113 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:47,113 - INFO - ==================================================
2026-02-10 01:44:47,134 - INFO -   [탐색 71] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:47,162 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:47,162 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:47,163 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:51,923 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:51,923 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:52,171 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:52,171 - INFO - ==================================================
2026-02-10 01:44:52,194 - INFO -   [탐색 72] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:52,227 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:52,227 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:52,228 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:44:56,539 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:44:56,540 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:44:56,769 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:44:56,770 - INFO - ==================================================
2026-02-10 01:44:56,795 - INFO -   [탐색 73] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:44:56,825 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:44:56,825 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:44:56,827 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:01,498 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:01,499 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:01,718 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:01,718 - INFO - ==================================================
2026-02-10 01:45:01,745 - INFO -   [탐색 74] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:01,776 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:01,776 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:01,778 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:06,626 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:06,627 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:07,307 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:07,307 - INFO - ==================================================
2026-02-10 01:45:07,331 - INFO -   [탐색 75] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:07,358 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:07,358 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:07,359 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:12,044 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:12,045 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:12,309 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:12,309 - INFO - ==================================================
2026-02-10 01:45:12,333 - INFO -   [탐색 76] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:12,358 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:12,358 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:12,361 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:16,839 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:16,840 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:17,079 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:17,079 - INFO - ==================================================
2026-02-10 01:45:17,118 - INFO -   [탐색 77] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:17,144 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:17,145 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:17,147 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:21,648 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:21,649 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:21,895 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:21,896 - INFO - ==================================================
2026-02-10 01:45:21,918 - INFO -   [탐색 78] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:21,947 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:21,948 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:21,949 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:25,917 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:25,918 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:26,133 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:26,133 - INFO - ==================================================
2026-02-10 01:45:26,156 - INFO -   [탐색 79] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:26,184 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:26,184 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:26,186 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:30,321 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:30,322 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:31,115 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:31,116 - INFO - ==================================================
2026-02-10 01:45:31,141 - INFO -   [탐색 80] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:31,169 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:31,169 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:31,171 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:35,181 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:35,182 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:35,414 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:35,414 - INFO - ==================================================
2026-02-10 01:45:35,438 - INFO -   [탐색 81] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:35,464 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:35,464 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:35,466 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:39,869 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:39,871 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:40,102 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:40,103 - INFO - ==================================================
2026-02-10 01:45:40,126 - INFO -   [탐색 82] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:40,151 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:40,152 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:40,153 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:44,772 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:44,773 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:45,004 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:45,005 - INFO - ==================================================
2026-02-10 01:45:45,026 - INFO -   [탐색 83] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:45,050 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:45,051 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:45,052 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:49,694 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:49,694 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:49,925 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:49,926 - INFO - ==================================================
2026-02-10 01:45:49,949 - INFO -   [탐색 84] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:49,977 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:49,978 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:49,979 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:45:54,715 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:45:54,716 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:45:55,411 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:45:55,411 - INFO - ==================================================
2026-02-10 01:45:55,438 - INFO -   [탐색 85] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:45:55,471 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:45:55,472 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:45:55,474 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:00,167 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:00,168 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:00,387 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:00,387 - INFO - ==================================================
2026-02-10 01:46:00,411 - INFO -   [탐색 86] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:00,437 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:00,438 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:00,439 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:05,420 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:05,421 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:05,614 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:05,614 - INFO - ==================================================
2026-02-10 01:46:05,636 - INFO -   [탐색 87] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:05,662 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:05,663 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:05,664 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:10,616 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:10,616 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:10,816 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:10,816 - INFO - ==================================================
2026-02-10 01:46:10,837 - INFO -   [탐색 88] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:10,861 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:10,861 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:10,862 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:15,373 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:15,374 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:15,624 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:15,624 - INFO - ==================================================
2026-02-10 01:46:15,646 - INFO -   [탐색 89] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:15,672 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:15,672 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:15,674 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:19,999 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:20,000 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:20,759 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:20,759 - INFO - ==================================================
2026-02-10 01:46:20,781 - INFO -   [탐색 90] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:20,807 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:20,807 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:20,809 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:25,125 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:25,126 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:25,354 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:25,355 - INFO - ==================================================
2026-02-10 01:46:25,378 - INFO -   [탐색 91] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:25,406 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:25,406 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:25,408 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:29,784 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:29,785 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:30,016 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:30,017 - INFO - ==================================================
2026-02-10 01:46:30,041 - INFO -   [탐색 92] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:30,068 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:30,068 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:30,069 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:34,319 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:34,319 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:34,549 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:34,549 - INFO - ==================================================
2026-02-10 01:46:34,573 - INFO -   [탐색 93] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:34,602 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:34,602 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:34,605 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:38,761 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:38,762 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:39,528 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:39,529 - INFO - ==================================================
2026-02-10 01:46:39,555 - INFO -   [탐색 94] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:39,584 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:39,584 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:39,586 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:43,204 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:43,204 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:43,433 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:43,434 - INFO - ==================================================
2026-02-10 01:46:43,457 - INFO -   [탐색 95] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:43,483 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:43,483 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:43,485 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:48,070 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:48,075 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:48,296 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:48,297 - INFO - ==================================================
2026-02-10 01:46:48,320 - INFO -   [탐색 96] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:48,347 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:48,347 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:48,349 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:52,928 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:52,928 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:53,166 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:53,166 - INFO - ==================================================
2026-02-10 01:46:53,191 - INFO -   [탐색 97] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:53,220 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:53,221 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:53,222 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:46:57,922 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:46:57,923 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:46:58,104 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:46:58,104 - INFO - ==================================================
2026-02-10 01:46:58,125 - INFO -   [탐색 98] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:46:58,153 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:46:58,153 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:46:58,155 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:02,909 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:02,910 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:03,687 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:47:03,687 - INFO - ==================================================
2026-02-10 01:47:03,709 - INFO -   [탐색 99] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:47:03,735 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:03,735 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:03,737 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:08,279 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:08,282 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:08,535 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:47:08,536 - INFO - ==================================================
2026-02-10 01:47:08,566 - INFO -   [탐색 100] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:47:08,566 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.8169 입니다.
2026-02-10 01:47:08,566 - INFO - ================================================================================
2026-02-10 01:47:08,570 - INFO - 계산된 Pruning 정보(희소도: 0.8169)를 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:47:08,595 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:47:08,651 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:47:08,651 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:47:08,653 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:47:12,729 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:47:12,730 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:47:12,953 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-02-10 01:47:12,954 - INFO - ==================================================
2026-02-10 01:47:12,955 - INFO - ==================================================
2026-02-10 01:47:12,956 - INFO - 모델 파라미터 수:
2026-02-10 01:47:12,956 - INFO -   - 총 파라미터: 485,259 개
2026-02-10 01:47:12,956 - INFO -   - 학습 가능한 파라미터: 485,259 개
2026-02-10 01:47:12,979 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:47:13,027 - INFO - FLOPs가 2.1493 GFLOPs에서 0.1840 GFLOPs로 감소했습니다 (감소율: 91.44%).
2026-02-10 01:47:13,027 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:47:13,028 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:47:13,029 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:47:13,029 - INFO - ==================================================
2026-02-10 01:47:13,029 - INFO - train 모드를 시작합니다.
2026-02-10 01:47:13,029 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:47:13,029 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:47:13,029 - INFO - --------------------------------------------------
2026-02-10 01:47:13,030 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:47:18,019 - INFO - [Train] [11/90] | Loss: 0.5454 | Train Acc: 76.71%
2026-02-10 01:47:19,318 - INFO - [Valid] [11/90] | Loss: 0.5858 | Val Acc: 72.86%
2026-02-10 01:47:19,323 - INFO - [Metrics for 'abnormal'] | Precision: 0.6570 | Recall: 0.8662 | F1: 0.7473
2026-02-10 01:47:19,323 - INFO - [Metrics for 'normal'] | Precision: 0.8409 | Recall: 0.6099 | F1: 0.7070
2026-02-10 01:47:19,356 - INFO - [Best Model Saved] (val loss: 0.5858) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:47:19,356 - INFO - --------------------------------------------------
2026-02-10 01:47:19,357 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:47:24,253 - INFO - [Train] [12/90] | Loss: 0.5087 | Train Acc: 79.84%
2026-02-10 01:47:25,576 - INFO - [Valid] [12/90] | Loss: 0.5358 | Val Acc: 76.70%
2026-02-10 01:47:25,581 - INFO - [Metrics for 'abnormal'] | Precision: 0.7566 | Recall: 0.7325 | F1: 0.7443
2026-02-10 01:47:25,582 - INFO - [Metrics for 'normal'] | Precision: 0.7754 | Recall: 0.7967 | F1: 0.7859
2026-02-10 01:47:25,601 - INFO - [Best Model Saved] (val loss: 0.5358) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:47:25,601 - INFO - --------------------------------------------------
2026-02-10 01:47:25,602 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:47:30,680 - INFO - [Train] [13/90] | Loss: 0.4940 | Train Acc: 80.65%
2026-02-10 01:47:31,864 - INFO - [Valid] [13/90] | Loss: 0.5358 | Val Acc: 77.29%
2026-02-10 01:47:31,869 - INFO - [Metrics for 'abnormal'] | Precision: 0.7817 | Recall: 0.7070 | F1: 0.7425
2026-02-10 01:47:31,870 - INFO - [Metrics for 'normal'] | Precision: 0.7665 | Recall: 0.8297 | F1: 0.7968
2026-02-10 01:47:31,888 - INFO - [Best Model Saved] (val loss: 0.5358) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:47:31,888 - INFO - --------------------------------------------------
2026-02-10 01:47:31,890 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:47:36,669 - INFO - [Train] [14/90] | Loss: 0.5144 | Train Acc: 79.69%
2026-02-10 01:47:37,956 - INFO - [Valid] [14/90] | Loss: 0.5468 | Val Acc: 76.70%
2026-02-10 01:47:37,960 - INFO - [Metrics for 'abnormal'] | Precision: 0.7074 | Recall: 0.8471 | F1: 0.7710
2026-02-10 01:47:37,961 - INFO - [Metrics for 'normal'] | Precision: 0.8411 | Recall: 0.6978 | F1: 0.7628
2026-02-10 01:47:37,962 - INFO - --------------------------------------------------
2026-02-10 01:47:37,963 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:47:42,875 - INFO - [Train] [15/90] | Loss: 0.4945 | Train Acc: 80.65%
2026-02-10 01:47:44,257 - INFO - [Valid] [15/90] | Loss: 0.5288 | Val Acc: 78.17%
2026-02-10 01:47:44,261 - INFO - [Metrics for 'abnormal'] | Precision: 0.8029 | Recall: 0.7006 | F1: 0.7483
2026-02-10 01:47:44,261 - INFO - [Metrics for 'normal'] | Precision: 0.7673 | Recall: 0.8516 | F1: 0.8073
2026-02-10 01:47:44,276 - INFO - [Best Model Saved] (val loss: 0.5288) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:47:44,276 - INFO - --------------------------------------------------
2026-02-10 01:47:44,277 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:47:49,520 - INFO - [Train] [16/90] | Loss: 0.4900 | Train Acc: 81.47%
2026-02-10 01:47:50,828 - INFO - [Valid] [16/90] | Loss: 0.5235 | Val Acc: 77.88%
2026-02-10 01:47:50,837 - INFO - [Metrics for 'abnormal'] | Precision: 0.7500 | Recall: 0.7834 | F1: 0.7664
2026-02-10 01:47:50,838 - INFO - [Metrics for 'normal'] | Precision: 0.8057 | Recall: 0.7747 | F1: 0.7899
2026-02-10 01:47:50,866 - INFO - [Best Model Saved] (val loss: 0.5235) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:47:50,866 - INFO - --------------------------------------------------
2026-02-10 01:47:50,868 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:47:55,703 - INFO - [Train] [17/90] | Loss: 0.4867 | Train Acc: 80.88%
2026-02-10 01:47:56,703 - INFO - [Valid] [17/90] | Loss: 0.5441 | Val Acc: 76.70%
2026-02-10 01:47:56,708 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.7261 | F1: 0.7427
2026-02-10 01:47:56,708 - INFO - [Metrics for 'normal'] | Precision: 0.7725 | Recall: 0.8022 | F1: 0.7871
2026-02-10 01:47:56,709 - INFO - --------------------------------------------------
2026-02-10 01:47:56,710 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:48:01,796 - INFO - [Train] [18/90] | Loss: 0.4769 | Train Acc: 82.37%
2026-02-10 01:48:02,961 - INFO - [Valid] [18/90] | Loss: 0.5417 | Val Acc: 75.81%
2026-02-10 01:48:02,966 - INFO - [Metrics for 'abnormal'] | Precision: 0.8049 | Recall: 0.6306 | F1: 0.7071
2026-02-10 01:48:02,966 - INFO - [Metrics for 'normal'] | Precision: 0.7315 | Recall: 0.8681 | F1: 0.7940
2026-02-10 01:48:02,968 - INFO - --------------------------------------------------
2026-02-10 01:48:02,969 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:48:08,235 - INFO - [Train] [19/90] | Loss: 0.4682 | Train Acc: 82.89%
2026-02-10 01:48:09,427 - INFO - [Valid] [19/90] | Loss: 0.5414 | Val Acc: 76.40%
2026-02-10 01:48:09,432 - INFO - [Metrics for 'abnormal'] | Precision: 0.7081 | Recall: 0.8344 | F1: 0.7661
2026-02-10 01:48:09,433 - INFO - [Metrics for 'normal'] | Precision: 0.8312 | Recall: 0.7033 | F1: 0.7619
2026-02-10 01:48:09,435 - INFO - --------------------------------------------------
2026-02-10 01:48:09,437 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:48:14,455 - INFO - [Train] [20/90] | Loss: 0.4717 | Train Acc: 81.99%
2026-02-10 01:48:15,762 - INFO - [Valid] [20/90] | Loss: 0.5202 | Val Acc: 77.58%
2026-02-10 01:48:15,768 - INFO - [Metrics for 'abnormal'] | Precision: 0.7832 | Recall: 0.7134 | F1: 0.7467
2026-02-10 01:48:15,768 - INFO - [Metrics for 'normal'] | Precision: 0.7704 | Recall: 0.8297 | F1: 0.7989
2026-02-10 01:48:15,791 - INFO - [Best Model Saved] (val loss: 0.5202) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:48:15,792 - INFO - --------------------------------------------------
2026-02-10 01:48:15,793 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:48:20,693 - INFO - [Train] [21/90] | Loss: 0.4618 | Train Acc: 81.92%
2026-02-10 01:48:21,812 - INFO - [Valid] [21/90] | Loss: 0.5153 | Val Acc: 79.06%
2026-02-10 01:48:21,818 - INFO - [Metrics for 'abnormal'] | Precision: 0.7722 | Recall: 0.7771 | F1: 0.7746
2026-02-10 01:48:21,818 - INFO - [Metrics for 'normal'] | Precision: 0.8066 | Recall: 0.8022 | F1: 0.8044
2026-02-10 01:48:21,843 - INFO - [Best Model Saved] (val loss: 0.5153) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:48:21,843 - INFO - --------------------------------------------------
2026-02-10 01:48:21,844 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:48:27,142 - INFO - [Train] [22/90] | Loss: 0.4642 | Train Acc: 83.11%
2026-02-10 01:48:28,487 - INFO - [Valid] [22/90] | Loss: 0.5147 | Val Acc: 76.99%
2026-02-10 01:48:28,494 - INFO - [Metrics for 'abnormal'] | Precision: 0.7484 | Recall: 0.7580 | F1: 0.7532
2026-02-10 01:48:28,494 - INFO - [Metrics for 'normal'] | Precision: 0.7889 | Recall: 0.7802 | F1: 0.7845
2026-02-10 01:48:28,518 - INFO - [Best Model Saved] (val loss: 0.5147) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:48:28,519 - INFO - --------------------------------------------------
2026-02-10 01:48:28,520 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:48:33,255 - INFO - [Train] [23/90] | Loss: 0.4655 | Train Acc: 82.59%
2026-02-10 01:48:34,646 - INFO - [Valid] [23/90] | Loss: 0.5203 | Val Acc: 76.99%
2026-02-10 01:48:34,656 - INFO - [Metrics for 'abnormal'] | Precision: 0.7548 | Recall: 0.7452 | F1: 0.7500
2026-02-10 01:48:34,657 - INFO - [Metrics for 'normal'] | Precision: 0.7826 | Recall: 0.7912 | F1: 0.7869
2026-02-10 01:48:34,658 - INFO - --------------------------------------------------
2026-02-10 01:48:34,659 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:48:40,010 - INFO - [Train] [24/90] | Loss: 0.4564 | Train Acc: 83.04%
2026-02-10 01:48:41,069 - INFO - [Valid] [24/90] | Loss: 0.5156 | Val Acc: 79.35%
2026-02-10 01:48:41,079 - INFO - [Metrics for 'abnormal'] | Precision: 0.7702 | Recall: 0.7898 | F1: 0.7799
2026-02-10 01:48:41,079 - INFO - [Metrics for 'normal'] | Precision: 0.8146 | Recall: 0.7967 | F1: 0.8056
2026-02-10 01:48:41,081 - INFO - --------------------------------------------------
2026-02-10 01:48:41,082 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:48:45,727 - INFO - [Train] [25/90] | Loss: 0.4576 | Train Acc: 82.44%
2026-02-10 01:48:46,902 - INFO - [Valid] [25/90] | Loss: 0.5137 | Val Acc: 78.17%
2026-02-10 01:48:46,907 - INFO - [Metrics for 'abnormal'] | Precision: 0.8374 | Recall: 0.6561 | F1: 0.7357
2026-02-10 01:48:46,907 - INFO - [Metrics for 'normal'] | Precision: 0.7500 | Recall: 0.8901 | F1: 0.8141
2026-02-10 01:48:46,925 - INFO - [Best Model Saved] (val loss: 0.5137) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:48:46,925 - INFO - --------------------------------------------------
2026-02-10 01:48:46,926 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:48:52,013 - INFO - [Train] [26/90] | Loss: 0.4437 | Train Acc: 84.67%
2026-02-10 01:48:53,184 - INFO - [Valid] [26/90] | Loss: 0.5018 | Val Acc: 79.65%
2026-02-10 01:48:53,189 - INFO - [Metrics for 'abnormal'] | Precision: 0.7588 | Recall: 0.8217 | F1: 0.7890
2026-02-10 01:48:53,191 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.7747 | F1: 0.8034
2026-02-10 01:48:53,213 - INFO - [Best Model Saved] (val loss: 0.5018) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:48:53,213 - INFO - --------------------------------------------------
2026-02-10 01:48:53,214 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 01:48:58,384 - INFO - [Train] [27/90] | Loss: 0.4468 | Train Acc: 83.33%
2026-02-10 01:48:59,535 - INFO - [Valid] [27/90] | Loss: 0.5360 | Val Acc: 77.29%
2026-02-10 01:48:59,543 - INFO - [Metrics for 'abnormal'] | Precision: 0.7198 | Recall: 0.8344 | F1: 0.7729
2026-02-10 01:48:59,543 - INFO - [Metrics for 'normal'] | Precision: 0.8344 | Recall: 0.7198 | F1: 0.7729
2026-02-10 01:48:59,544 - INFO - --------------------------------------------------
2026-02-10 01:48:59,546 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 01:49:04,461 - INFO - [Train] [28/90] | Loss: 0.4437 | Train Acc: 84.08%
2026-02-10 01:49:05,459 - INFO - [Valid] [28/90] | Loss: 0.4828 | Val Acc: 80.53%
2026-02-10 01:49:05,464 - INFO - [Metrics for 'abnormal'] | Precision: 0.8138 | Recall: 0.7516 | F1: 0.7815
2026-02-10 01:49:05,464 - INFO - [Metrics for 'normal'] | Precision: 0.7990 | Recall: 0.8516 | F1: 0.8245
2026-02-10 01:49:05,490 - INFO - [Best Model Saved] (val loss: 0.4828) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:49:05,490 - INFO - --------------------------------------------------
2026-02-10 01:49:05,491 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 01:49:10,968 - INFO - [Train] [29/90] | Loss: 0.4430 | Train Acc: 84.52%
2026-02-10 01:49:11,858 - INFO - [Valid] [29/90] | Loss: 0.4785 | Val Acc: 83.19%
2026-02-10 01:49:11,864 - INFO - [Metrics for 'abnormal'] | Precision: 0.7976 | Recall: 0.8535 | F1: 0.8246
2026-02-10 01:49:11,864 - INFO - [Metrics for 'normal'] | Precision: 0.8655 | Recall: 0.8132 | F1: 0.8385
2026-02-10 01:49:11,891 - INFO - [Best Model Saved] (val loss: 0.4785) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:49:11,891 - INFO - --------------------------------------------------
2026-02-10 01:49:11,892 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 01:49:17,118 - INFO - [Train] [30/90] | Loss: 0.4433 | Train Acc: 84.23%
2026-02-10 01:49:18,066 - INFO - [Valid] [30/90] | Loss: 0.4922 | Val Acc: 81.12%
2026-02-10 01:49:18,070 - INFO - [Metrics for 'abnormal'] | Precision: 0.7818 | Recall: 0.8217 | F1: 0.8012
2026-02-10 01:49:18,071 - INFO - [Metrics for 'normal'] | Precision: 0.8391 | Recall: 0.8022 | F1: 0.8202
2026-02-10 01:49:18,072 - INFO - --------------------------------------------------
2026-02-10 01:49:18,073 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 01:49:23,179 - INFO - [Train] [31/90] | Loss: 0.4361 | Train Acc: 84.45%
2026-02-10 01:49:24,184 - INFO - [Valid] [31/90] | Loss: 0.5260 | Val Acc: 79.65%
2026-02-10 01:49:24,189 - INFO - [Metrics for 'abnormal'] | Precision: 0.7500 | Recall: 0.8408 | F1: 0.7928
2026-02-10 01:49:24,189 - INFO - [Metrics for 'normal'] | Precision: 0.8466 | Recall: 0.7582 | F1: 0.8000
2026-02-10 01:49:24,191 - INFO - --------------------------------------------------
2026-02-10 01:49:24,192 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 01:49:29,421 - INFO - [Train] [32/90] | Loss: 0.4331 | Train Acc: 85.19%
2026-02-10 01:49:30,571 - INFO - [Valid] [32/90] | Loss: 0.4885 | Val Acc: 77.88%
2026-02-10 01:49:30,574 - INFO - [Metrics for 'abnormal'] | Precision: 0.8203 | Recall: 0.6688 | F1: 0.7368
2026-02-10 01:49:30,574 - INFO - [Metrics for 'normal'] | Precision: 0.7536 | Recall: 0.8736 | F1: 0.8092
2026-02-10 01:49:30,576 - INFO - --------------------------------------------------
2026-02-10 01:49:30,577 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 01:49:35,558 - INFO - [Train] [33/90] | Loss: 0.4286 | Train Acc: 85.57%
2026-02-10 01:49:36,571 - INFO - [Valid] [33/90] | Loss: 0.5172 | Val Acc: 80.53%
2026-02-10 01:49:36,576 - INFO - [Metrics for 'abnormal'] | Precision: 0.7459 | Recall: 0.8790 | F1: 0.8070
2026-02-10 01:49:36,576 - INFO - [Metrics for 'normal'] | Precision: 0.8766 | Recall: 0.7418 | F1: 0.8036
2026-02-10 01:49:36,583 - INFO - --------------------------------------------------
2026-02-10 01:49:36,584 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 01:49:41,357 - INFO - [Train] [34/90] | Loss: 0.4305 | Train Acc: 84.23%
2026-02-10 01:49:42,723 - INFO - [Valid] [34/90] | Loss: 0.4742 | Val Acc: 80.83%
2026-02-10 01:49:42,728 - INFO - [Metrics for 'abnormal'] | Precision: 0.8594 | Recall: 0.7006 | F1: 0.7719
2026-02-10 01:49:42,728 - INFO - [Metrics for 'normal'] | Precision: 0.7773 | Recall: 0.9011 | F1: 0.8346
2026-02-10 01:49:42,744 - INFO - [Best Model Saved] (val loss: 0.4742) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:49:42,744 - INFO - --------------------------------------------------
2026-02-10 01:49:42,745 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 01:49:47,224 - INFO - [Train] [35/90] | Loss: 0.4258 | Train Acc: 85.27%
2026-02-10 01:49:48,557 - INFO - [Valid] [35/90] | Loss: 0.4775 | Val Acc: 81.71%
2026-02-10 01:49:48,563 - INFO - [Metrics for 'abnormal'] | Precision: 0.7714 | Recall: 0.8599 | F1: 0.8133
2026-02-10 01:49:48,563 - INFO - [Metrics for 'normal'] | Precision: 0.8659 | Recall: 0.7802 | F1: 0.8208
2026-02-10 01:49:48,564 - INFO - --------------------------------------------------
2026-02-10 01:49:48,566 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 01:49:53,322 - INFO - [Train] [36/90] | Loss: 0.4201 | Train Acc: 86.16%
2026-02-10 01:49:54,717 - INFO - [Valid] [36/90] | Loss: 0.4879 | Val Acc: 79.65%
2026-02-10 01:49:54,722 - INFO - [Metrics for 'abnormal'] | Precision: 0.7683 | Recall: 0.8025 | F1: 0.7850
2026-02-10 01:49:54,723 - INFO - [Metrics for 'normal'] | Precision: 0.8229 | Recall: 0.7912 | F1: 0.8067
2026-02-10 01:49:54,724 - INFO - --------------------------------------------------
2026-02-10 01:49:54,726 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 01:49:59,703 - INFO - [Train] [37/90] | Loss: 0.4210 | Train Acc: 86.24%
2026-02-10 01:50:00,441 - INFO - [Valid] [37/90] | Loss: 0.4843 | Val Acc: 79.65%
2026-02-10 01:50:00,445 - INFO - [Metrics for 'abnormal'] | Precision: 0.8143 | Recall: 0.7261 | F1: 0.7677
2026-02-10 01:50:00,446 - INFO - [Metrics for 'normal'] | Precision: 0.7839 | Recall: 0.8571 | F1: 0.8189
2026-02-10 01:50:00,450 - INFO - --------------------------------------------------
2026-02-10 01:50:00,451 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 01:50:05,508 - INFO - [Train] [38/90] | Loss: 0.4216 | Train Acc: 85.71%
2026-02-10 01:50:06,065 - INFO - [Valid] [38/90] | Loss: 0.4738 | Val Acc: 81.71%
2026-02-10 01:50:06,072 - INFO - [Metrics for 'abnormal'] | Precision: 0.7811 | Recall: 0.8408 | F1: 0.8098
2026-02-10 01:50:06,072 - INFO - [Metrics for 'normal'] | Precision: 0.8529 | Recall: 0.7967 | F1: 0.8239
2026-02-10 01:50:06,094 - INFO - [Best Model Saved] (val loss: 0.4738) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:50:06,094 - INFO - --------------------------------------------------
2026-02-10 01:50:06,095 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 01:50:11,342 - INFO - [Train] [39/90] | Loss: 0.4148 | Train Acc: 86.83%
2026-02-10 01:50:12,583 - INFO - [Valid] [39/90] | Loss: 0.4774 | Val Acc: 81.12%
2026-02-10 01:50:12,586 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8280 | F1: 0.8025
2026-02-10 01:50:12,586 - INFO - [Metrics for 'normal'] | Precision: 0.8430 | Recall: 0.7967 | F1: 0.8192
2026-02-10 01:50:12,587 - INFO - --------------------------------------------------
2026-02-10 01:50:12,587 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 01:50:17,702 - INFO - [Train] [40/90] | Loss: 0.4086 | Train Acc: 86.68%
2026-02-10 01:50:18,751 - INFO - [Valid] [40/90] | Loss: 0.4685 | Val Acc: 81.71%
2026-02-10 01:50:18,756 - INFO - [Metrics for 'abnormal'] | Precision: 0.8231 | Recall: 0.7707 | F1: 0.7961
2026-02-10 01:50:18,756 - INFO - [Metrics for 'normal'] | Precision: 0.8125 | Recall: 0.8571 | F1: 0.8342
2026-02-10 01:50:18,783 - INFO - [Best Model Saved] (val loss: 0.4685) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:50:18,783 - INFO - --------------------------------------------------
2026-02-10 01:50:18,784 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 01:50:23,507 - INFO - [Train] [41/90] | Loss: 0.4032 | Train Acc: 86.16%
2026-02-10 01:50:24,711 - INFO - [Valid] [41/90] | Loss: 0.4739 | Val Acc: 81.12%
2026-02-10 01:50:24,728 - INFO - [Metrics for 'abnormal'] | Precision: 0.8252 | Recall: 0.7516 | F1: 0.7867
2026-02-10 01:50:24,728 - INFO - [Metrics for 'normal'] | Precision: 0.8010 | Recall: 0.8626 | F1: 0.8307
2026-02-10 01:50:24,729 - INFO - --------------------------------------------------
2026-02-10 01:50:24,730 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 01:50:29,784 - INFO - [Train] [42/90] | Loss: 0.4013 | Train Acc: 86.76%
2026-02-10 01:50:30,944 - INFO - [Valid] [42/90] | Loss: 0.4990 | Val Acc: 80.83%
2026-02-10 01:50:30,949 - INFO - [Metrics for 'abnormal'] | Precision: 0.7614 | Recall: 0.8535 | F1: 0.8048
2026-02-10 01:50:30,949 - INFO - [Metrics for 'normal'] | Precision: 0.8589 | Recall: 0.7692 | F1: 0.8116
2026-02-10 01:50:30,951 - INFO - --------------------------------------------------
2026-02-10 01:50:30,952 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 01:50:35,806 - INFO - [Train] [43/90] | Loss: 0.4074 | Train Acc: 86.24%
2026-02-10 01:50:37,155 - INFO - [Valid] [43/90] | Loss: 0.4653 | Val Acc: 82.30%
2026-02-10 01:50:37,163 - INFO - [Metrics for 'abnormal'] | Precision: 0.8212 | Recall: 0.7898 | F1: 0.8052
2026-02-10 01:50:37,163 - INFO - [Metrics for 'normal'] | Precision: 0.8245 | Recall: 0.8516 | F1: 0.8378
2026-02-10 01:50:37,187 - INFO - [Best Model Saved] (val loss: 0.4653) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:50:37,188 - INFO - --------------------------------------------------
2026-02-10 01:50:37,189 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 01:50:42,135 - INFO - [Train] [44/90] | Loss: 0.4132 | Train Acc: 86.09%
2026-02-10 01:50:43,358 - INFO - [Valid] [44/90] | Loss: 0.4636 | Val Acc: 81.71%
2026-02-10 01:50:43,363 - INFO - [Metrics for 'abnormal'] | Precision: 0.8188 | Recall: 0.7771 | F1: 0.7974
2026-02-10 01:50:43,363 - INFO - [Metrics for 'normal'] | Precision: 0.8158 | Recall: 0.8516 | F1: 0.8333
2026-02-10 01:50:43,395 - INFO - [Best Model Saved] (val loss: 0.4636) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:50:43,395 - INFO - --------------------------------------------------
2026-02-10 01:50:43,396 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 01:50:48,626 - INFO - [Train] [45/90] | Loss: 0.3968 | Train Acc: 87.35%
2026-02-10 01:50:49,892 - INFO - [Valid] [45/90] | Loss: 0.4634 | Val Acc: 80.53%
2026-02-10 01:50:49,895 - INFO - [Metrics for 'abnormal'] | Precision: 0.8054 | Recall: 0.7643 | F1: 0.7843
2026-02-10 01:50:49,895 - INFO - [Metrics for 'normal'] | Precision: 0.8053 | Recall: 0.8407 | F1: 0.8226
2026-02-10 01:50:49,904 - INFO - [Best Model Saved] (val loss: 0.4634) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:50:49,905 - INFO - --------------------------------------------------
2026-02-10 01:50:49,905 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 01:50:55,134 - INFO - [Train] [46/90] | Loss: 0.4070 | Train Acc: 86.53%
2026-02-10 01:50:55,967 - INFO - [Valid] [46/90] | Loss: 0.4692 | Val Acc: 81.71%
2026-02-10 01:50:55,973 - INFO - [Metrics for 'abnormal'] | Precision: 0.7879 | Recall: 0.8280 | F1: 0.8075
2026-02-10 01:50:55,973 - INFO - [Metrics for 'normal'] | Precision: 0.8448 | Recall: 0.8077 | F1: 0.8258
2026-02-10 01:50:55,974 - INFO - --------------------------------------------------
2026-02-10 01:50:55,976 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 01:51:01,578 - INFO - [Train] [47/90] | Loss: 0.3958 | Train Acc: 87.50%
2026-02-10 01:51:02,546 - INFO - [Valid] [47/90] | Loss: 0.4614 | Val Acc: 82.30%
2026-02-10 01:51:02,550 - INFO - [Metrics for 'abnormal'] | Precision: 0.8089 | Recall: 0.8089 | F1: 0.8089
2026-02-10 01:51:02,551 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8352 | F1: 0.8352
2026-02-10 01:51:02,566 - INFO - [Best Model Saved] (val loss: 0.4614) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:51:02,566 - INFO - --------------------------------------------------
2026-02-10 01:51:02,567 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 01:51:07,681 - INFO - [Train] [48/90] | Loss: 0.3926 | Train Acc: 87.20%
2026-02-10 01:51:08,890 - INFO - [Valid] [48/90] | Loss: 0.4627 | Val Acc: 81.12%
2026-02-10 01:51:08,895 - INFO - [Metrics for 'abnormal'] | Precision: 0.8720 | Recall: 0.6943 | F1: 0.7730
2026-02-10 01:51:08,895 - INFO - [Metrics for 'normal'] | Precision: 0.7757 | Recall: 0.9121 | F1: 0.8384
2026-02-10 01:51:08,897 - INFO - --------------------------------------------------
2026-02-10 01:51:08,898 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 01:51:13,472 - INFO - [Train] [49/90] | Loss: 0.3831 | Train Acc: 87.87%
2026-02-10 01:51:14,730 - INFO - [Valid] [49/90] | Loss: 0.4903 | Val Acc: 79.94%
2026-02-10 01:51:14,735 - INFO - [Metrics for 'abnormal'] | Precision: 0.7486 | Recall: 0.8535 | F1: 0.7976
2026-02-10 01:51:14,735 - INFO - [Metrics for 'normal'] | Precision: 0.8562 | Recall: 0.7527 | F1: 0.8012
2026-02-10 01:51:14,737 - INFO - --------------------------------------------------
2026-02-10 01:51:14,738 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 01:51:19,430 - INFO - [Train] [50/90] | Loss: 0.4039 | Train Acc: 86.90%
2026-02-10 01:51:20,768 - INFO - [Valid] [50/90] | Loss: 0.4812 | Val Acc: 81.12%
2026-02-10 01:51:20,773 - INFO - [Metrics for 'abnormal'] | Precision: 0.7962 | Recall: 0.7962 | F1: 0.7962
2026-02-10 01:51:20,774 - INFO - [Metrics for 'normal'] | Precision: 0.8242 | Recall: 0.8242 | F1: 0.8242
2026-02-10 01:51:20,775 - INFO - --------------------------------------------------
2026-02-10 01:51:20,776 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 01:51:25,926 - INFO - [Train] [51/90] | Loss: 0.3863 | Train Acc: 87.87%
2026-02-10 01:51:27,180 - INFO - [Valid] [51/90] | Loss: 0.4861 | Val Acc: 81.12%
2026-02-10 01:51:27,188 - INFO - [Metrics for 'abnormal'] | Precision: 0.7657 | Recall: 0.8535 | F1: 0.8072
2026-02-10 01:51:27,188 - INFO - [Metrics for 'normal'] | Precision: 0.8598 | Recall: 0.7747 | F1: 0.8150
2026-02-10 01:51:27,190 - INFO - --------------------------------------------------
2026-02-10 01:51:27,191 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 01:51:32,163 - INFO - [Train] [52/90] | Loss: 0.3810 | Train Acc: 88.10%
2026-02-10 01:51:33,344 - INFO - [Valid] [52/90] | Loss: 0.4701 | Val Acc: 82.89%
2026-02-10 01:51:33,348 - INFO - [Metrics for 'abnormal'] | Precision: 0.8075 | Recall: 0.8280 | F1: 0.8176
2026-02-10 01:51:33,348 - INFO - [Metrics for 'normal'] | Precision: 0.8483 | Recall: 0.8297 | F1: 0.8389
2026-02-10 01:51:33,350 - INFO - --------------------------------------------------
2026-02-10 01:51:33,350 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 01:51:38,477 - INFO - [Train] [53/90] | Loss: 0.3713 | Train Acc: 88.84%
2026-02-10 01:51:39,487 - INFO - [Valid] [53/90] | Loss: 0.4980 | Val Acc: 80.24%
2026-02-10 01:51:39,492 - INFO - [Metrics for 'abnormal'] | Precision: 0.7711 | Recall: 0.8153 | F1: 0.7926
2026-02-10 01:51:39,496 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.7912 | F1: 0.8113
2026-02-10 01:51:39,498 - INFO - --------------------------------------------------
2026-02-10 01:51:39,500 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 01:51:44,867 - INFO - [Train] [54/90] | Loss: 0.3817 | Train Acc: 87.95%
2026-02-10 01:51:46,079 - INFO - [Valid] [54/90] | Loss: 0.4835 | Val Acc: 81.42%
2026-02-10 01:51:46,085 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-02-10 01:51:46,085 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-02-10 01:51:46,086 - INFO - --------------------------------------------------
2026-02-10 01:51:46,088 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 01:51:50,883 - INFO - [Train] [55/90] | Loss: 0.3837 | Train Acc: 88.54%
2026-02-10 01:51:52,395 - INFO - [Valid] [55/90] | Loss: 0.4743 | Val Acc: 82.01%
2026-02-10 01:51:52,399 - INFO - [Metrics for 'abnormal'] | Precision: 0.8429 | Recall: 0.7516 | F1: 0.7946
2026-02-10 01:51:52,400 - INFO - [Metrics for 'normal'] | Precision: 0.8040 | Recall: 0.8791 | F1: 0.8399
2026-02-10 01:51:52,401 - INFO - --------------------------------------------------
2026-02-10 01:51:52,402 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 01:51:57,295 - INFO - [Train] [56/90] | Loss: 0.3738 | Train Acc: 88.10%
2026-02-10 01:51:58,650 - INFO - [Valid] [56/90] | Loss: 0.4659 | Val Acc: 82.89%
2026-02-10 01:51:58,656 - INFO - [Metrics for 'abnormal'] | Precision: 0.8153 | Recall: 0.8153 | F1: 0.8153
2026-02-10 01:51:58,656 - INFO - [Metrics for 'normal'] | Precision: 0.8407 | Recall: 0.8407 | F1: 0.8407
2026-02-10 01:51:58,657 - INFO - --------------------------------------------------
2026-02-10 01:51:58,658 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 01:52:03,457 - INFO - [Train] [57/90] | Loss: 0.3657 | Train Acc: 89.06%
2026-02-10 01:52:04,644 - INFO - [Valid] [57/90] | Loss: 0.4639 | Val Acc: 82.60%
2026-02-10 01:52:04,650 - INFO - [Metrics for 'abnormal'] | Precision: 0.7849 | Recall: 0.8599 | F1: 0.8207
2026-02-10 01:52:04,650 - INFO - [Metrics for 'normal'] | Precision: 0.8683 | Recall: 0.7967 | F1: 0.8309
2026-02-10 01:52:04,651 - INFO - --------------------------------------------------
2026-02-10 01:52:04,652 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 01:52:09,604 - INFO - [Train] [58/90] | Loss: 0.3599 | Train Acc: 90.25%
2026-02-10 01:52:10,621 - INFO - [Valid] [58/90] | Loss: 0.4523 | Val Acc: 82.89%
2026-02-10 01:52:10,626 - INFO - [Metrics for 'abnormal'] | Precision: 0.8194 | Recall: 0.8089 | F1: 0.8141
2026-02-10 01:52:10,627 - INFO - [Metrics for 'normal'] | Precision: 0.8370 | Recall: 0.8462 | F1: 0.8415
2026-02-10 01:52:10,644 - INFO - [Best Model Saved] (val loss: 0.4523) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:52:10,644 - INFO - --------------------------------------------------
2026-02-10 01:52:10,645 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 01:52:15,783 - INFO - [Train] [59/90] | Loss: 0.3529 | Train Acc: 90.03%
2026-02-10 01:52:16,709 - INFO - [Valid] [59/90] | Loss: 0.4472 | Val Acc: 84.37%
2026-02-10 01:52:16,714 - INFO - [Metrics for 'abnormal'] | Precision: 0.8467 | Recall: 0.8089 | F1: 0.8274
2026-02-10 01:52:16,714 - INFO - [Metrics for 'normal'] | Precision: 0.8413 | Recall: 0.8736 | F1: 0.8571
2026-02-10 01:52:16,768 - INFO - [Best Model Saved] (val loss: 0.4472) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:52:16,769 - INFO - --------------------------------------------------
2026-02-10 01:52:16,770 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 01:52:22,090 - INFO - [Train] [60/90] | Loss: 0.3783 | Train Acc: 88.24%
2026-02-10 01:52:23,080 - INFO - [Valid] [60/90] | Loss: 0.4611 | Val Acc: 82.60%
2026-02-10 01:52:23,085 - INFO - [Metrics for 'abnormal'] | Precision: 0.8267 | Recall: 0.7898 | F1: 0.8078
2026-02-10 01:52:23,085 - INFO - [Metrics for 'normal'] | Precision: 0.8254 | Recall: 0.8571 | F1: 0.8410
2026-02-10 01:52:23,086 - INFO - --------------------------------------------------
2026-02-10 01:52:23,088 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 01:52:27,912 - INFO - [Train] [61/90] | Loss: 0.3632 | Train Acc: 89.58%
2026-02-10 01:52:28,946 - INFO - [Valid] [61/90] | Loss: 0.4701 | Val Acc: 82.89%
2026-02-10 01:52:28,951 - INFO - [Metrics for 'abnormal'] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-02-10 01:52:28,951 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-02-10 01:52:28,953 - INFO - --------------------------------------------------
2026-02-10 01:52:28,954 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 01:52:33,197 - INFO - [Train] [62/90] | Loss: 0.3651 | Train Acc: 89.14%
2026-02-10 01:52:34,328 - INFO - [Valid] [62/90] | Loss: 0.4646 | Val Acc: 82.89%
2026-02-10 01:52:34,334 - INFO - [Metrics for 'abnormal'] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-02-10 01:52:34,334 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-02-10 01:52:34,335 - INFO - --------------------------------------------------
2026-02-10 01:52:34,336 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 01:52:38,526 - INFO - [Train] [63/90] | Loss: 0.3535 | Train Acc: 89.66%
2026-02-10 01:52:39,679 - INFO - [Valid] [63/90] | Loss: 0.4640 | Val Acc: 83.48%
2026-02-10 01:52:39,683 - INFO - [Metrics for 'abnormal'] | Precision: 0.8217 | Recall: 0.8217 | F1: 0.8217
2026-02-10 01:52:39,684 - INFO - [Metrics for 'normal'] | Precision: 0.8462 | Recall: 0.8462 | F1: 0.8462
2026-02-10 01:52:39,685 - INFO - --------------------------------------------------
2026-02-10 01:52:39,686 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 01:52:43,884 - INFO - [Train] [64/90] | Loss: 0.3463 | Train Acc: 90.18%
2026-02-10 01:52:44,795 - INFO - [Valid] [64/90] | Loss: 0.4591 | Val Acc: 84.37%
2026-02-10 01:52:44,799 - INFO - [Metrics for 'abnormal'] | Precision: 0.8171 | Recall: 0.8535 | F1: 0.8349
2026-02-10 01:52:44,799 - INFO - [Metrics for 'normal'] | Precision: 0.8686 | Recall: 0.8352 | F1: 0.8515
2026-02-10 01:52:44,800 - INFO - --------------------------------------------------
2026-02-10 01:52:44,800 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 01:52:48,445 - INFO - [Train] [65/90] | Loss: 0.3476 | Train Acc: 91.59%
2026-02-10 01:52:49,250 - INFO - [Valid] [65/90] | Loss: 0.4664 | Val Acc: 82.30%
2026-02-10 01:52:49,256 - INFO - [Metrics for 'abnormal'] | Precision: 0.8050 | Recall: 0.8153 | F1: 0.8101
2026-02-10 01:52:49,257 - INFO - [Metrics for 'normal'] | Precision: 0.8389 | Recall: 0.8297 | F1: 0.8343
2026-02-10 01:52:49,266 - INFO - --------------------------------------------------
2026-02-10 01:52:49,267 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 01:52:52,864 - INFO - [Train] [66/90] | Loss: 0.3348 | Train Acc: 90.77%
2026-02-10 01:52:53,897 - INFO - [Valid] [66/90] | Loss: 0.4704 | Val Acc: 82.01%
2026-02-10 01:52:53,902 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8726 | F1: 0.8179
2026-02-10 01:52:53,902 - INFO - [Metrics for 'normal'] | Precision: 0.8758 | Recall: 0.7747 | F1: 0.8222
2026-02-10 01:52:53,904 - INFO - --------------------------------------------------
2026-02-10 01:52:53,905 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 01:52:57,570 - INFO - [Train] [67/90] | Loss: 0.3405 | Train Acc: 90.62%
2026-02-10 01:52:58,480 - INFO - [Valid] [67/90] | Loss: 0.4667 | Val Acc: 82.60%
2026-02-10 01:52:58,484 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8280 | F1: 0.8150
2026-02-10 01:52:58,485 - INFO - [Metrics for 'normal'] | Precision: 0.8475 | Recall: 0.8242 | F1: 0.8357
2026-02-10 01:52:58,486 - INFO - --------------------------------------------------
2026-02-10 01:52:58,487 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 01:53:02,358 - INFO - [Train] [68/90] | Loss: 0.3375 | Train Acc: 91.07%
2026-02-10 01:53:03,124 - INFO - [Valid] [68/90] | Loss: 0.4604 | Val Acc: 82.89%
2026-02-10 01:53:03,129 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8408 | F1: 0.8199
2026-02-10 01:53:03,129 - INFO - [Metrics for 'normal'] | Precision: 0.8563 | Recall: 0.8187 | F1: 0.8371
2026-02-10 01:53:03,131 - INFO - --------------------------------------------------
2026-02-10 01:53:03,132 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 01:53:06,969 - INFO - [Train] [69/90] | Loss: 0.3266 | Train Acc: 91.89%
2026-02-10 01:53:07,654 - INFO - [Valid] [69/90] | Loss: 0.4753 | Val Acc: 83.48%
2026-02-10 01:53:07,662 - INFO - [Metrics for 'abnormal'] | Precision: 0.8344 | Recall: 0.8025 | F1: 0.8182
2026-02-10 01:53:07,662 - INFO - [Metrics for 'normal'] | Precision: 0.8351 | Recall: 0.8626 | F1: 0.8486
2026-02-10 01:53:07,665 - INFO - --------------------------------------------------
2026-02-10 01:53:07,666 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 01:53:11,381 - INFO - [Train] [70/90] | Loss: 0.3383 | Train Acc: 90.77%
2026-02-10 01:53:12,459 - INFO - [Valid] [70/90] | Loss: 0.4618 | Val Acc: 83.19%
2026-02-10 01:53:12,465 - INFO - [Metrics for 'abnormal'] | Precision: 0.8165 | Recall: 0.8217 | F1: 0.8190
2026-02-10 01:53:12,465 - INFO - [Metrics for 'normal'] | Precision: 0.8453 | Recall: 0.8407 | F1: 0.8430
2026-02-10 01:53:12,467 - INFO - --------------------------------------------------
2026-02-10 01:53:12,468 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 01:53:16,018 - INFO - [Train] [71/90] | Loss: 0.3204 | Train Acc: 91.89%
2026-02-10 01:53:16,932 - INFO - [Valid] [71/90] | Loss: 0.4760 | Val Acc: 84.37%
2026-02-10 01:53:16,938 - INFO - [Metrics for 'abnormal'] | Precision: 0.8023 | Recall: 0.8790 | F1: 0.8389
2026-02-10 01:53:16,939 - INFO - [Metrics for 'normal'] | Precision: 0.8862 | Recall: 0.8132 | F1: 0.8481
2026-02-10 01:53:16,941 - INFO - --------------------------------------------------
2026-02-10 01:53:16,942 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 01:53:20,696 - INFO - [Train] [72/90] | Loss: 0.3257 | Train Acc: 92.11%
2026-02-10 01:53:21,394 - INFO - [Valid] [72/90] | Loss: 0.4850 | Val Acc: 82.60%
2026-02-10 01:53:21,399 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.8153 | F1: 0.8127
2026-02-10 01:53:21,399 - INFO - [Metrics for 'normal'] | Precision: 0.8398 | Recall: 0.8352 | F1: 0.8375
2026-02-10 01:53:21,401 - INFO - --------------------------------------------------
2026-02-10 01:53:21,402 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 01:53:25,279 - INFO - [Train] [73/90] | Loss: 0.3293 | Train Acc: 91.89%
2026-02-10 01:53:25,978 - INFO - [Valid] [73/90] | Loss: 0.4588 | Val Acc: 83.78%
2026-02-10 01:53:25,983 - INFO - [Metrics for 'abnormal'] | Precision: 0.8228 | Recall: 0.8280 | F1: 0.8254
2026-02-10 01:53:25,983 - INFO - [Metrics for 'normal'] | Precision: 0.8508 | Recall: 0.8462 | F1: 0.8485
2026-02-10 01:53:25,984 - INFO - --------------------------------------------------
2026-02-10 01:53:25,985 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 01:53:30,060 - INFO - [Train] [74/90] | Loss: 0.3239 | Train Acc: 91.74%
2026-02-10 01:53:30,962 - INFO - [Valid] [74/90] | Loss: 0.4915 | Val Acc: 81.42%
2026-02-10 01:53:30,967 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-02-10 01:53:30,967 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-02-10 01:53:30,968 - INFO - --------------------------------------------------
2026-02-10 01:53:30,969 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 01:53:34,580 - INFO - [Train] [75/90] | Loss: 0.3318 | Train Acc: 91.74%
2026-02-10 01:53:35,491 - INFO - [Valid] [75/90] | Loss: 0.4756 | Val Acc: 84.66%
2026-02-10 01:53:35,494 - INFO - [Metrics for 'abnormal'] | Precision: 0.8070 | Recall: 0.8790 | F1: 0.8415
2026-02-10 01:53:35,495 - INFO - [Metrics for 'normal'] | Precision: 0.8869 | Recall: 0.8187 | F1: 0.8514
2026-02-10 01:53:35,496 - INFO - --------------------------------------------------
2026-02-10 01:53:35,497 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 01:53:39,124 - INFO - [Train] [76/90] | Loss: 0.3216 | Train Acc: 92.56%
2026-02-10 01:53:39,858 - INFO - [Valid] [76/90] | Loss: 0.4657 | Val Acc: 84.96%
2026-02-10 01:53:39,861 - INFO - [Metrics for 'abnormal'] | Precision: 0.8487 | Recall: 0.8217 | F1: 0.8350
2026-02-10 01:53:39,861 - INFO - [Metrics for 'normal'] | Precision: 0.8503 | Recall: 0.8736 | F1: 0.8618
2026-02-10 01:53:39,863 - INFO - --------------------------------------------------
2026-02-10 01:53:39,863 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 01:53:43,326 - INFO - [Train] [77/90] | Loss: 0.3184 | Train Acc: 92.56%
2026-02-10 01:53:44,117 - INFO - [Valid] [77/90] | Loss: 0.4650 | Val Acc: 84.96%
2026-02-10 01:53:44,124 - INFO - [Metrics for 'abnormal'] | Precision: 0.8442 | Recall: 0.8280 | F1: 0.8360
2026-02-10 01:53:44,124 - INFO - [Metrics for 'normal'] | Precision: 0.8541 | Recall: 0.8681 | F1: 0.8610
2026-02-10 01:53:44,126 - INFO - --------------------------------------------------
2026-02-10 01:53:44,131 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 01:53:47,963 - INFO - [Train] [78/90] | Loss: 0.3214 | Train Acc: 92.49%
2026-02-10 01:53:49,011 - INFO - [Valid] [78/90] | Loss: 0.4792 | Val Acc: 83.78%
2026-02-10 01:53:49,015 - INFO - [Metrics for 'abnormal'] | Precision: 0.7898 | Recall: 0.8854 | F1: 0.8348
2026-02-10 01:53:49,015 - INFO - [Metrics for 'normal'] | Precision: 0.8896 | Recall: 0.7967 | F1: 0.8406
2026-02-10 01:53:49,017 - INFO - --------------------------------------------------
2026-02-10 01:53:49,018 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 01:53:53,025 - INFO - [Train] [79/90] | Loss: 0.3027 | Train Acc: 93.45%
2026-02-10 01:53:53,801 - INFO - [Valid] [79/90] | Loss: 0.4722 | Val Acc: 85.84%
2026-02-10 01:53:53,808 - INFO - [Metrics for 'abnormal'] | Precision: 0.8303 | Recall: 0.8726 | F1: 0.8509
2026-02-10 01:53:53,808 - INFO - [Metrics for 'normal'] | Precision: 0.8851 | Recall: 0.8462 | F1: 0.8652
2026-02-10 01:53:53,810 - INFO - --------------------------------------------------
2026-02-10 01:53:53,811 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 01:53:57,824 - INFO - [Train] [80/90] | Loss: 0.3050 | Train Acc: 93.38%
2026-02-10 01:53:58,330 - INFO - [Valid] [80/90] | Loss: 0.4771 | Val Acc: 85.25%
2026-02-10 01:53:58,335 - INFO - [Metrics for 'abnormal'] | Precision: 0.8242 | Recall: 0.8662 | F1: 0.8447
2026-02-10 01:53:58,335 - INFO - [Metrics for 'normal'] | Precision: 0.8793 | Recall: 0.8407 | F1: 0.8596
2026-02-10 01:53:58,337 - INFO - --------------------------------------------------
2026-02-10 01:53:58,337 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 01:54:02,296 - INFO - [Train] [81/90] | Loss: 0.3059 | Train Acc: 93.23%
2026-02-10 01:54:03,081 - INFO - [Valid] [81/90] | Loss: 0.4844 | Val Acc: 83.19%
2026-02-10 01:54:03,087 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8471 | F1: 0.8235
2026-02-10 01:54:03,087 - INFO - [Metrics for 'normal'] | Precision: 0.8613 | Recall: 0.8187 | F1: 0.8394
2026-02-10 01:54:03,089 - INFO - --------------------------------------------------
2026-02-10 01:54:03,090 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 01:54:06,841 - INFO - [Train] [82/90] | Loss: 0.3024 | Train Acc: 93.38%
2026-02-10 01:54:07,824 - INFO - [Valid] [82/90] | Loss: 0.4933 | Val Acc: 82.60%
2026-02-10 01:54:07,829 - INFO - [Metrics for 'abnormal'] | Precision: 0.7816 | Recall: 0.8662 | F1: 0.8218
2026-02-10 01:54:07,829 - INFO - [Metrics for 'normal'] | Precision: 0.8727 | Recall: 0.7912 | F1: 0.8300
2026-02-10 01:54:07,830 - INFO - --------------------------------------------------
2026-02-10 01:54:07,831 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 01:54:11,373 - INFO - [Train] [83/90] | Loss: 0.2978 | Train Acc: 94.05%
2026-02-10 01:54:12,364 - INFO - [Valid] [83/90] | Loss: 0.4955 | Val Acc: 82.01%
2026-02-10 01:54:12,368 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-02-10 01:54:12,376 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-02-10 01:54:12,378 - INFO - --------------------------------------------------
2026-02-10 01:54:12,381 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 01:54:15,814 - INFO - [Train] [84/90] | Loss: 0.2996 | Train Acc: 94.12%
2026-02-10 01:54:16,609 - INFO - [Valid] [84/90] | Loss: 0.4977 | Val Acc: 82.89%
2026-02-10 01:54:16,616 - INFO - [Metrics for 'abnormal'] | Precision: 0.8037 | Recall: 0.8344 | F1: 0.8187
2026-02-10 01:54:16,616 - INFO - [Metrics for 'normal'] | Precision: 0.8523 | Recall: 0.8242 | F1: 0.8380
2026-02-10 01:54:16,618 - INFO - --------------------------------------------------
2026-02-10 01:54:16,619 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 01:54:20,354 - INFO - [Train] [85/90] | Loss: 0.2977 | Train Acc: 94.20%
2026-02-10 01:54:21,251 - INFO - [Valid] [85/90] | Loss: 0.5014 | Val Acc: 82.30%
2026-02-10 01:54:21,255 - INFO - [Metrics for 'abnormal'] | Precision: 0.7904 | Recall: 0.8408 | F1: 0.8148
2026-02-10 01:54:21,255 - INFO - [Metrics for 'normal'] | Precision: 0.8547 | Recall: 0.8077 | F1: 0.8305
2026-02-10 01:54:21,260 - INFO - --------------------------------------------------
2026-02-10 01:54:21,261 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 01:54:25,222 - INFO - [Train] [86/90] | Loss: 0.2946 | Train Acc: 93.60%
2026-02-10 01:54:26,074 - INFO - [Valid] [86/90] | Loss: 0.4882 | Val Acc: 84.07%
2026-02-10 01:54:26,083 - INFO - [Metrics for 'abnormal'] | Precision: 0.8084 | Recall: 0.8599 | F1: 0.8333
2026-02-10 01:54:26,083 - INFO - [Metrics for 'normal'] | Precision: 0.8721 | Recall: 0.8242 | F1: 0.8475
2026-02-10 01:54:26,085 - INFO - --------------------------------------------------
2026-02-10 01:54:26,087 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 01:54:30,136 - INFO - [Train] [87/90] | Loss: 0.2909 | Train Acc: 93.82%
2026-02-10 01:54:30,918 - INFO - [Valid] [87/90] | Loss: 0.5047 | Val Acc: 84.37%
2026-02-10 01:54:30,921 - INFO - [Metrics for 'abnormal'] | Precision: 0.8467 | Recall: 0.8089 | F1: 0.8274
2026-02-10 01:54:30,921 - INFO - [Metrics for 'normal'] | Precision: 0.8413 | Recall: 0.8736 | F1: 0.8571
2026-02-10 01:54:30,922 - INFO - --------------------------------------------------
2026-02-10 01:54:30,923 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 01:54:34,578 - INFO - [Train] [88/90] | Loss: 0.2966 | Train Acc: 94.05%
2026-02-10 01:54:35,271 - INFO - [Valid] [88/90] | Loss: 0.5112 | Val Acc: 82.30%
2026-02-10 01:54:35,276 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8662 | F1: 0.8193
2026-02-10 01:54:35,277 - INFO - [Metrics for 'normal'] | Precision: 0.8720 | Recall: 0.7857 | F1: 0.8266
2026-02-10 01:54:35,278 - INFO - --------------------------------------------------
2026-02-10 01:54:35,279 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 01:54:39,014 - INFO - [Train] [89/90] | Loss: 0.2978 | Train Acc: 93.90%
2026-02-10 01:54:39,787 - INFO - [Valid] [89/90] | Loss: 0.4999 | Val Acc: 82.89%
2026-02-10 01:54:39,792 - INFO - [Metrics for 'abnormal'] | Precision: 0.8194 | Recall: 0.8089 | F1: 0.8141
2026-02-10 01:54:39,792 - INFO - [Metrics for 'normal'] | Precision: 0.8370 | Recall: 0.8462 | F1: 0.8415
2026-02-10 01:54:39,794 - INFO - --------------------------------------------------
2026-02-10 01:54:39,795 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 01:54:43,355 - INFO - [Train] [90/90] | Loss: 0.2942 | Train Acc: 94.49%
2026-02-10 01:54:44,489 - INFO - [Valid] [90/90] | Loss: 0.4966 | Val Acc: 83.19%
2026-02-10 01:54:44,493 - INFO - [Metrics for 'abnormal'] | Precision: 0.7976 | Recall: 0.8535 | F1: 0.8246
2026-02-10 01:54:44,494 - INFO - [Metrics for 'normal'] | Precision: 0.8655 | Recall: 0.8132 | F1: 0.8385
2026-02-10 01:54:44,496 - INFO - ==================================================
2026-02-10 01:54:44,496 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 01:54:44,496 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 01:54:44,496 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:54:45,457 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:54:45,457 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 01:54:45,458 - INFO - Wanda Pruning을 시작합니다.
2026-02-10 01:54:45,458 - INFO - Wanda 중요도 계산을 위해 Calibration을 수행합니다.
2026-02-10 01:54:45,460 - INFO -   - Calibration Samples: 1353 (approx 85 batches)
2026-02-10 01:54:48,201 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:54:48,204 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:54:48,428 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-02-10 01:54:48,428 - INFO - ==================================================
2026-02-10 01:54:48,450 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/best_model.pth'
2026-02-10 01:54:48,451 - INFO - ==================================================
2026-02-10 01:54:48,451 - INFO - Test 모드를 시작합니다.
2026-02-10 01:54:48,565 - INFO - 연산량 (MACs): 0.0920 GMACs per sample
2026-02-10 01:54:48,566 - INFO - 연산량 (FLOPs): 0.1840 GFLOPs per sample
2026-02-10 01:54:48,566 - INFO - ==================================================
2026-02-10 01:54:48,566 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 01:54:49,766 - INFO - 샘플 당 평균 Forward Pass 시간: 3.55ms (std: 1.43ms), FPS: 348.51 (std: 195.62) (1개 샘플 x 100회 반복)
2026-02-10 01:54:49,766 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 112.93 MB
2026-02-10 01:54:49,766 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 01:54:51,603 - INFO - [Test] Loss: 0.3657 | Test Acc: 84.37%
2026-02-10 01:54:51,609 - INFO - [Metrics for 'abnormal'] | Precision: 0.8467 | Recall: 0.8089 | F1: 0.8274
2026-02-10 01:54:51,609 - INFO - [Metrics for 'normal'] | Precision: 0.8413 | Recall: 0.8736 | F1: 0.8571
2026-02-10 01:54:51,810 - INFO - ==================================================
2026-02-10 01:54:51,811 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/confusion_matrix_20260210_013631.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/confusion_matrix_20260210_013631.pdf'
2026-02-10 01:54:51,811 - INFO - ==================================================
2026-02-10 01:54:51,811 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 01:54:52,036 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/model_fp32_20260210_013631.onnx'에 저장되었습니다. (크기: 1.96 MB)
2026-02-10 01:54:52,542 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 7.31 MB
2026-02-10 01:54:52,542 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 01:54:54,826 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 18.92ms (std: 12.13ms)
2026-02-10 01:54:54,826 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 76.15 FPS (std: 46.79) (1개 샘플 x 100회 반복)
2026-02-10 01:54:54,826 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 4.31 MB
2026-02-10 01:54:54,826 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 16.19 MB
2026-02-10 01:55:01,681 - INFO - [Test (ONNX)] | Test Acc (ONNX): 84.37%
2026-02-10 01:55:01,690 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8467 | Recall: 0.8089 | F1: 0.8274
2026-02-10 01:55:01,690 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8413 | Recall: 0.8736 | F1: 0.8571
2026-02-10 01:55:01,953 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/val_acc.pdf'
2026-02-10 01:55:02,179 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/train_val_acc.pdf'
2026-02-10 01:55:02,375 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/F1_normal.pdf'
2026-02-10 01:55:02,571 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/val_loss.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/val_loss.pdf'
2026-02-10 01:55:02,771 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/learning_rate.pdf'
2026-02-10 01:55:04,784 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/compile.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_wanda_20260210_013631/graph_20260210_013631/compile.pdf'
