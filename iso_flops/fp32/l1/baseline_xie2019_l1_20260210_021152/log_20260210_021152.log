2026-02-10 02:11:52,502 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/log_20260210_021152.log'에 저장됩니다.
2026-02-10 02:11:52,503 - INFO - ==================================================
2026-02-10 02:11:52,503 - INFO - config.yaml:
2026-02-10 02:11:52,503 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: xie2019
  use_l1_pruning: true
  pruning_flops_target: 0.1816

2026-02-10 02:11:52,503 - INFO - ==================================================
2026-02-10 02:11:52,653 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 02:11:52,653 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 02:11:52,653 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 02:11:52,656 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 02:11:52,656 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 02:11:52,656 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 02:11:52,656 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 02:11:52,761 - INFO - ==================================================
2026-02-10 02:11:52,761 - INFO - 모델 파라미터 수:
2026-02-10 02:11:52,761 - INFO -   - 총 파라미터: 9,160,194 개
2026-02-10 02:11:52,761 - INFO -   - 학습 가능한 파라미터: 9,160,194 개
2026-02-10 02:11:52,761 - INFO - ================================================================================
2026-02-10 02:11:52,761 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 02:11:52,761 - INFO - ================================================================================
2026-02-10 02:11:52,761 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 02:11:52,761 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 02:11:52,761 - INFO - ==================================================
2026-02-10 02:11:52,761 - INFO - train 모드를 시작합니다.
2026-02-10 02:11:52,761 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 02:11:52,762 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 02:11:52,762 - INFO - --------------------------------------------------
2026-02-10 02:11:52,762 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 02:11:54,446 - INFO - [Train] [1/10] | Loss: 0.5864 | Train Acc: 74.40%
2026-02-10 02:11:55,036 - INFO - [Valid] [1/10] | Loss: 0.5779 | Val Acc: 74.34%
2026-02-10 02:11:55,040 - INFO - [Metrics for 'abnormal'] | Precision: 0.8723 | Recall: 0.5223 | F1: 0.6534
2026-02-10 02:11:55,040 - INFO - [Metrics for 'normal'] | Precision: 0.6939 | Recall: 0.9341 | F1: 0.7963
2026-02-10 02:11:55,066 - INFO - [Best Model Saved] (val loss: 0.5779) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:11:55,066 - INFO - --------------------------------------------------
2026-02-10 02:11:55,066 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 02:11:56,571 - INFO - [Train] [2/10] | Loss: 0.5499 | Train Acc: 79.09%
2026-02-10 02:11:56,969 - INFO - [Valid] [2/10] | Loss: 0.5618 | Val Acc: 74.34%
2026-02-10 02:11:56,971 - INFO - [Metrics for 'abnormal'] | Precision: 0.7059 | Recall: 0.7643 | F1: 0.7339
2026-02-10 02:11:56,971 - INFO - [Metrics for 'normal'] | Precision: 0.7811 | Recall: 0.7253 | F1: 0.7521
2026-02-10 02:11:57,006 - INFO - [Best Model Saved] (val loss: 0.5618) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:11:57,006 - INFO - --------------------------------------------------
2026-02-10 02:11:57,006 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 02:11:58,537 - INFO - [Train] [3/10] | Loss: 0.5357 | Train Acc: 79.32%
2026-02-10 02:11:58,936 - INFO - [Valid] [3/10] | Loss: 0.5366 | Val Acc: 76.40%
2026-02-10 02:11:58,939 - INFO - [Metrics for 'abnormal'] | Precision: 0.7278 | Recall: 0.7834 | F1: 0.7546
2026-02-10 02:11:58,939 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.7473 | F1: 0.7727
2026-02-10 02:11:58,973 - INFO - [Best Model Saved] (val loss: 0.5366) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:11:58,973 - INFO - --------------------------------------------------
2026-02-10 02:11:58,973 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 02:12:00,470 - INFO - [Train] [4/10] | Loss: 0.5056 | Train Acc: 81.77%
2026-02-10 02:12:00,845 - INFO - [Valid] [4/10] | Loss: 0.5681 | Val Acc: 75.52%
2026-02-10 02:12:00,847 - INFO - [Metrics for 'abnormal'] | Precision: 0.8190 | Recall: 0.6051 | F1: 0.6960
2026-02-10 02:12:00,847 - INFO - [Metrics for 'normal'] | Precision: 0.7220 | Recall: 0.8846 | F1: 0.7951
2026-02-10 02:12:00,848 - INFO - --------------------------------------------------
2026-02-10 02:12:00,848 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 02:12:02,366 - INFO - [Train] [5/10] | Loss: 0.4906 | Train Acc: 80.80%
2026-02-10 02:12:02,768 - INFO - [Valid] [5/10] | Loss: 0.5209 | Val Acc: 76.99%
2026-02-10 02:12:02,771 - INFO - [Metrics for 'abnormal'] | Precision: 0.7112 | Recall: 0.8471 | F1: 0.7733
2026-02-10 02:12:02,771 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.7033 | F1: 0.7665
2026-02-10 02:12:02,816 - INFO - [Best Model Saved] (val loss: 0.5209) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:02,816 - INFO - --------------------------------------------------
2026-02-10 02:12:02,816 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 02:12:04,311 - INFO - [Train] [6/10] | Loss: 0.4767 | Train Acc: 83.26%
2026-02-10 02:12:04,719 - INFO - [Valid] [6/10] | Loss: 0.4980 | Val Acc: 80.24%
2026-02-10 02:12:04,721 - INFO - [Metrics for 'abnormal'] | Precision: 0.7885 | Recall: 0.7834 | F1: 0.7859
2026-02-10 02:12:04,721 - INFO - [Metrics for 'normal'] | Precision: 0.8142 | Recall: 0.8187 | F1: 0.8164
2026-02-10 02:12:04,756 - INFO - [Best Model Saved] (val loss: 0.4980) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:04,756 - INFO - --------------------------------------------------
2026-02-10 02:12:04,756 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 02:12:06,206 - INFO - [Train] [7/10] | Loss: 0.4576 | Train Acc: 84.08%
2026-02-10 02:12:06,610 - INFO - [Valid] [7/10] | Loss: 0.5509 | Val Acc: 78.47%
2026-02-10 02:12:06,612 - INFO - [Metrics for 'abnormal'] | Precision: 0.7692 | Recall: 0.7643 | F1: 0.7668
2026-02-10 02:12:06,612 - INFO - [Metrics for 'normal'] | Precision: 0.7978 | Recall: 0.8022 | F1: 0.8000
2026-02-10 02:12:06,613 - INFO - --------------------------------------------------
2026-02-10 02:12:06,613 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 02:12:08,091 - INFO - [Train] [8/10] | Loss: 0.4425 | Train Acc: 84.60%
2026-02-10 02:12:08,496 - INFO - [Valid] [8/10] | Loss: 0.4876 | Val Acc: 82.01%
2026-02-10 02:12:08,499 - INFO - [Metrics for 'abnormal'] | Precision: 0.7963 | Recall: 0.8217 | F1: 0.8088
2026-02-10 02:12:08,499 - INFO - [Metrics for 'normal'] | Precision: 0.8418 | Recall: 0.8187 | F1: 0.8301
2026-02-10 02:12:08,543 - INFO - [Best Model Saved] (val loss: 0.4876) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:08,543 - INFO - --------------------------------------------------
2026-02-10 02:12:08,543 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 02:12:10,064 - INFO - [Train] [9/10] | Loss: 0.4324 | Train Acc: 85.42%
2026-02-10 02:12:10,468 - INFO - [Valid] [9/10] | Loss: 0.4986 | Val Acc: 78.76%
2026-02-10 02:12:10,470 - INFO - [Metrics for 'abnormal'] | Precision: 0.7607 | Recall: 0.7898 | F1: 0.7750
2026-02-10 02:12:10,470 - INFO - [Metrics for 'normal'] | Precision: 0.8125 | Recall: 0.7857 | F1: 0.7989
2026-02-10 02:12:10,471 - INFO - --------------------------------------------------
2026-02-10 02:12:10,471 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 02:12:11,955 - INFO - [Train] [10/10] | Loss: 0.4267 | Train Acc: 85.49%
2026-02-10 02:12:12,356 - INFO - [Valid] [10/10] | Loss: 0.4941 | Val Acc: 79.65%
2026-02-10 02:12:12,359 - INFO - [Metrics for 'abnormal'] | Precision: 0.7716 | Recall: 0.7962 | F1: 0.7837
2026-02-10 02:12:12,359 - INFO - [Metrics for 'normal'] | Precision: 0.8192 | Recall: 0.7967 | F1: 0.8078
2026-02-10 02:12:12,360 - INFO - ================================================================================
2026-02-10 02:12:12,360 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 02:12:12,360 - INFO - ================================================================================
2026-02-10 02:12:12,371 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'을(를) 불러왔습니다.
2026-02-10 02:12:12,372 - INFO - ================================================================================
2026-02-10 02:12:12,372 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 02:12:12,378 - INFO - 원본 모델 FLOPs: 2.8696 GFLOPs
2026-02-10 02:12:12,381 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:12,381 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:12,570 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 02:12:12,570 - INFO - ==================================================
2026-02-10 02:12:12,572 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 1.3003 GFLOPs (감소율: 54.69%)
2026-02-10 02:12:12,573 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:12,573 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:12,732 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-02-10 02:12:12,732 - INFO - ==================================================
2026-02-10 02:12:12,735 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.6166 GFLOPs (감소율: 78.51%)
2026-02-10 02:12:12,736 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:12,736 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:12,901 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-02-10 02:12:12,901 - INFO - ==================================================
2026-02-10 02:12:12,904 - INFO -   [탐색  3] 희소도: 0.8662 -> FLOPs: 0.3005 GFLOPs (감소율: 89.53%)
2026-02-10 02:12:12,905 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:12,905 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:13,073 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.928125)에 맞춰 변경되었습니다.
2026-02-10 02:12:13,073 - INFO - ==================================================
2026-02-10 02:12:13,075 - INFO -   [탐색  4] 희소도: 0.9281 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:13,076 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:13,076 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:13,348 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8971875)에 맞춰 변경되었습니다.
2026-02-10 02:12:13,348 - INFO - ==================================================
2026-02-10 02:12:13,350 - INFO -   [탐색  5] 희소도: 0.8972 -> FLOPs: 0.2238 GFLOPs (감소율: 92.20%)
2026-02-10 02:12:13,351 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:13,352 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:13,517 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.91265625)에 맞춰 변경되었습니다.
2026-02-10 02:12:13,517 - INFO - ==================================================
2026-02-10 02:12:13,519 - INFO -   [탐색  6] 희소도: 0.9127 -> FLOPs: 0.1858 GFLOPs (감소율: 93.52%)
2026-02-10 02:12:13,520 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:13,520 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:13,688 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.920390625)에 맞춰 변경되었습니다.
2026-02-10 02:12:13,688 - INFO - ==================================================
2026-02-10 02:12:13,693 - INFO -   [탐색  7] 희소도: 0.9204 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:13,694 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:13,694 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:13,862 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9242578125)에 맞춰 변경되었습니다.
2026-02-10 02:12:13,862 - INFO - ==================================================
2026-02-10 02:12:13,864 - INFO -   [탐색  8] 희소도: 0.9243 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:13,865 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:13,865 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:14,032 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.92232421875)에 맞춰 변경되었습니다.
2026-02-10 02:12:14,032 - INFO - ==================================================
2026-02-10 02:12:14,034 - INFO -   [탐색  9] 희소도: 0.9223 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:14,035 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:14,035 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:14,204 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-02-10 02:12:14,204 - INFO - ==================================================
2026-02-10 02:12:14,205 - INFO -   [탐색 10] 희소도: 0.9214 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:14,206 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:14,206 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:14,478 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218408203125)에 맞춰 변경되었습니다.
2026-02-10 02:12:14,479 - INFO - ==================================================
2026-02-10 02:12:14,480 - INFO -   [탐색 11] 희소도: 0.9218 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:14,481 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:14,481 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:14,648 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9220825195312501)에 맞춰 변경되었습니다.
2026-02-10 02:12:14,648 - INFO - ==================================================
2026-02-10 02:12:14,649 - INFO -   [탐색 12] 희소도: 0.9221 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:14,651 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:14,651 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:14,820 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9219616699218751)에 맞춰 변경되었습니다.
2026-02-10 02:12:14,820 - INFO - ==================================================
2026-02-10 02:12:14,821 - INFO -   [탐색 13] 희소도: 0.9220 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:14,822 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:14,822 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:14,992 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9219012451171875)에 맞춰 변경되었습니다.
2026-02-10 02:12:14,992 - INFO - ==================================================
2026-02-10 02:12:14,993 - INFO -   [탐색 14] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:14,994 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:14,994 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:15,162 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218710327148438)에 맞춰 변경되었습니다.
2026-02-10 02:12:15,162 - INFO - ==================================================
2026-02-10 02:12:15,164 - INFO -   [탐색 15] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:15,165 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:15,165 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:15,334 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218861389160157)에 맞춰 변경되었습니다.
2026-02-10 02:12:15,334 - INFO - ==================================================
2026-02-10 02:12:15,335 - INFO -   [탐색 16] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:15,336 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:15,336 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:15,503 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218785858154297)에 맞춰 변경되었습니다.
2026-02-10 02:12:15,503 - INFO - ==================================================
2026-02-10 02:12:15,505 - INFO -   [탐색 17] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:15,506 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:15,506 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:15,778 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218748092651368)에 맞춰 변경되었습니다.
2026-02-10 02:12:15,779 - INFO - ==================================================
2026-02-10 02:12:15,780 - INFO -   [탐색 18] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:15,781 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:15,781 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:15,946 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218766975402832)에 맞춰 변경되었습니다.
2026-02-10 02:12:15,947 - INFO - ==================================================
2026-02-10 02:12:15,948 - INFO -   [탐색 19] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:15,949 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:15,949 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:16,117 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.92187575340271)에 맞춰 변경되었습니다.
2026-02-10 02:12:16,118 - INFO - ==================================================
2026-02-10 02:12:16,119 - INFO -   [탐색 20] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:16,120 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:16,120 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:16,287 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218752813339234)에 맞춰 변경되었습니다.
2026-02-10 02:12:16,287 - INFO - ==================================================
2026-02-10 02:12:16,289 - INFO -   [탐색 21] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:16,290 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:16,290 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:16,459 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750452995301)에 맞춰 변경되었습니다.
2026-02-10 02:12:16,459 - INFO - ==================================================
2026-02-10 02:12:16,461 - INFO -   [탐색 22] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:16,462 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:16,462 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:16,629 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749272823334)에 맞춰 변경되었습니다.
2026-02-10 02:12:16,629 - INFO - ==================================================
2026-02-10 02:12:16,631 - INFO -   [탐색 23] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:16,632 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:16,632 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:16,908 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749862909318)에 맞춰 변경되었습니다.
2026-02-10 02:12:16,908 - INFO - ==================================================
2026-02-10 02:12:16,910 - INFO -   [탐색 24] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:16,911 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:16,911 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:17,078 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750157952309)에 맞춰 변경되었습니다.
2026-02-10 02:12:17,078 - INFO - ==================================================
2026-02-10 02:12:17,079 - INFO -   [탐색 25] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:17,080 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:17,081 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:17,251 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750010430814)에 맞춰 변경되었습니다.
2026-02-10 02:12:17,252 - INFO - ==================================================
2026-02-10 02:12:17,253 - INFO -   [탐색 26] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:17,254 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:17,254 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:17,423 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749936670065)에 맞춰 변경되었습니다.
2026-02-10 02:12:17,423 - INFO - ==================================================
2026-02-10 02:12:17,424 - INFO -   [탐색 27] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:17,425 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:17,425 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:17,594 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921874997355044)에 맞춰 변경되었습니다.
2026-02-10 02:12:17,594 - INFO - ==================================================
2026-02-10 02:12:17,595 - INFO -   [탐색 28] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:17,596 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:17,596 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:17,766 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749991990627)에 맞춰 변경되었습니다.
2026-02-10 02:12:17,767 - INFO - ==================================================
2026-02-10 02:12:17,768 - INFO -   [탐색 29] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:17,769 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:17,769 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:17,939 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875000121072)에 맞춰 변경되었습니다.
2026-02-10 02:12:17,939 - INFO - ==================================================
2026-02-10 02:12:17,940 - INFO -   [탐색 30] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:17,941 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:17,942 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:18,218 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749996600674)에 맞춰 변경되었습니다.
2026-02-10 02:12:18,218 - INFO - ==================================================
2026-02-10 02:12:18,219 - INFO -   [탐색 31] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:18,220 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:18,221 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:18,387 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749998905698)에 맞춰 변경되었습니다.
2026-02-10 02:12:18,388 - INFO - ==================================================
2026-02-10 02:12:18,389 - INFO -   [탐색 32] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:18,390 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:18,390 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:18,560 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000058209)에 맞춰 변경되었습니다.
2026-02-10 02:12:18,560 - INFO - ==================================================
2026-02-10 02:12:18,561 - INFO -   [탐색 33] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:18,563 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:18,563 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:18,730 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999481953)에 맞춰 변경되었습니다.
2026-02-10 02:12:18,730 - INFO - ==================================================
2026-02-10 02:12:18,732 - INFO -   [탐색 34] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:18,733 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:18,733 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:18,900 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999770082)에 맞춰 변경되었습니다.
2026-02-10 02:12:18,900 - INFO - ==================================================
2026-02-10 02:12:18,902 - INFO -   [탐색 35] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:18,903 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:18,903 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:19,072 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999914145)에 맞춰 변경되었습니다.
2026-02-10 02:12:19,073 - INFO - ==================================================
2026-02-10 02:12:19,074 - INFO -   [탐색 36] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:19,075 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:19,075 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:19,350 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999986178)에 맞춰 변경되었습니다.
2026-02-10 02:12:19,350 - INFO - ==================================================
2026-02-10 02:12:19,352 - INFO -   [탐색 37] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:19,353 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:19,353 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:19,520 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000022193)에 맞춰 변경되었습니다.
2026-02-10 02:12:19,520 - INFO - ==================================================
2026-02-10 02:12:19,521 - INFO -   [탐색 38] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:19,523 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:19,523 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:19,692 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000004186)에 맞춰 변경되었습니다.
2026-02-10 02:12:19,692 - INFO - ==================================================
2026-02-10 02:12:19,693 - INFO -   [탐색 39] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:19,694 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:19,694 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:19,864 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999995182)에 맞춰 변경되었습니다.
2026-02-10 02:12:19,864 - INFO - ==================================================
2026-02-10 02:12:19,865 - INFO -   [탐색 40] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:19,866 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:19,866 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:20,035 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999999684)에 맞춰 변경되었습니다.
2026-02-10 02:12:20,035 - INFO - ==================================================
2026-02-10 02:12:20,036 - INFO -   [탐색 41] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:20,037 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:20,037 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:20,207 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000001934)에 맞춰 변경되었습니다.
2026-02-10 02:12:20,207 - INFO - ==================================================
2026-02-10 02:12:20,209 - INFO -   [탐색 42] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:20,210 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:20,210 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:20,378 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000808)에 맞춰 변경되었습니다.
2026-02-10 02:12:20,378 - INFO - ==================================================
2026-02-10 02:12:20,379 - INFO -   [탐색 43] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:20,380 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:20,380 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:20,656 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000246)에 맞춰 변경되었습니다.
2026-02-10 02:12:20,656 - INFO - ==================================================
2026-02-10 02:12:20,658 - INFO -   [탐색 44] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:20,659 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:20,659 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:20,824 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218749999999964)에 맞춰 변경되었습니다.
2026-02-10 02:12:20,825 - INFO - ==================================================
2026-02-10 02:12:20,826 - INFO -   [탐색 45] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:20,827 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:20,827 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:20,997 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000105)에 맞춰 변경되었습니다.
2026-02-10 02:12:20,997 - INFO - ==================================================
2026-02-10 02:12:20,998 - INFO -   [탐색 46] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:20,999 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:20,999 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:21,167 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000036)에 맞춰 변경되었습니다.
2026-02-10 02:12:21,167 - INFO - ==================================================
2026-02-10 02:12:21,169 - INFO -   [탐색 47] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:21,170 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:21,170 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:21,340 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:21,340 - INFO - ==================================================
2026-02-10 02:12:21,342 - INFO -   [탐색 48] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:21,343 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:21,343 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:21,510 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000018)에 맞춰 변경되었습니다.
2026-02-10 02:12:21,510 - INFO - ==================================================
2026-02-10 02:12:21,512 - INFO -   [탐색 49] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:21,513 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:21,513 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:21,790 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000009)에 맞춰 변경되었습니다.
2026-02-10 02:12:21,790 - INFO - ==================================================
2026-02-10 02:12:21,792 - INFO -   [탐색 50] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:21,793 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:21,793 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:21,960 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000004)에 맞춰 변경되었습니다.
2026-02-10 02:12:21,960 - INFO - ==================================================
2026-02-10 02:12:21,962 - INFO -   [탐색 51] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:21,963 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:21,963 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:22,135 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000002)에 맞춰 변경되었습니다.
2026-02-10 02:12:22,135 - INFO - ==================================================
2026-02-10 02:12:22,136 - INFO -   [탐색 52] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:22,137 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:22,137 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:22,307 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.9218750000000001)에 맞춰 변경되었습니다.
2026-02-10 02:12:22,307 - INFO - ==================================================
2026-02-10 02:12:22,308 - INFO -   [탐색 53] 희소도: 0.9219 -> FLOPs: 0.1481 GFLOPs (감소율: 94.84%)
2026-02-10 02:12:22,309 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:22,309 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:22,480 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:22,480 - INFO - ==================================================
2026-02-10 02:12:22,482 - INFO -   [탐색 54] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:22,483 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:22,483 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:22,654 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:22,654 - INFO - ==================================================
2026-02-10 02:12:22,656 - INFO -   [탐색 55] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:22,657 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:22,657 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:22,827 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:22,827 - INFO - ==================================================
2026-02-10 02:12:22,828 - INFO -   [탐색 56] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:22,829 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:22,829 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:23,104 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:23,104 - INFO - ==================================================
2026-02-10 02:12:23,106 - INFO -   [탐색 57] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:23,107 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:23,107 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:23,274 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:23,274 - INFO - ==================================================
2026-02-10 02:12:23,276 - INFO -   [탐색 58] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:23,277 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:23,277 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:23,447 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:23,447 - INFO - ==================================================
2026-02-10 02:12:23,448 - INFO -   [탐색 59] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:23,449 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:23,449 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:23,618 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:23,618 - INFO - ==================================================
2026-02-10 02:12:23,619 - INFO -   [탐색 60] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:23,620 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:23,621 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:23,789 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:23,789 - INFO - ==================================================
2026-02-10 02:12:23,790 - INFO -   [탐색 61] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:23,791 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:23,791 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:23,966 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:23,966 - INFO - ==================================================
2026-02-10 02:12:23,967 - INFO -   [탐색 62] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:23,968 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:23,968 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:24,246 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:24,246 - INFO - ==================================================
2026-02-10 02:12:24,247 - INFO -   [탐색 63] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:24,248 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:24,248 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:24,417 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:24,417 - INFO - ==================================================
2026-02-10 02:12:24,418 - INFO -   [탐색 64] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:24,419 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:24,419 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:24,589 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:24,589 - INFO - ==================================================
2026-02-10 02:12:24,590 - INFO -   [탐색 65] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:24,591 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:24,591 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:24,761 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:24,761 - INFO - ==================================================
2026-02-10 02:12:24,762 - INFO -   [탐색 66] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:24,763 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:24,763 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:24,932 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:24,932 - INFO - ==================================================
2026-02-10 02:12:24,934 - INFO -   [탐색 67] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:24,935 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:24,935 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:25,105 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:25,105 - INFO - ==================================================
2026-02-10 02:12:25,106 - INFO -   [탐색 68] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:25,107 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:25,107 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:25,274 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:25,274 - INFO - ==================================================
2026-02-10 02:12:25,276 - INFO -   [탐색 69] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:25,277 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:25,277 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:25,553 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:25,553 - INFO - ==================================================
2026-02-10 02:12:25,554 - INFO -   [탐색 70] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:25,555 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:25,556 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:25,723 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:25,723 - INFO - ==================================================
2026-02-10 02:12:25,724 - INFO -   [탐색 71] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:25,725 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:25,725 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:25,895 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:25,895 - INFO - ==================================================
2026-02-10 02:12:25,896 - INFO -   [탐색 72] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:25,897 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:25,897 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:26,064 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:26,065 - INFO - ==================================================
2026-02-10 02:12:26,066 - INFO -   [탐색 73] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:26,067 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:26,067 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:26,238 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:26,238 - INFO - ==================================================
2026-02-10 02:12:26,240 - INFO -   [탐색 74] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:26,241 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:26,241 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:26,409 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:26,409 - INFO - ==================================================
2026-02-10 02:12:26,410 - INFO -   [탐색 75] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:26,411 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:26,411 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:26,689 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:26,689 - INFO - ==================================================
2026-02-10 02:12:26,690 - INFO -   [탐색 76] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:26,691 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:26,691 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:26,859 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:26,859 - INFO - ==================================================
2026-02-10 02:12:26,860 - INFO -   [탐색 77] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:26,862 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:26,862 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:27,034 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:27,034 - INFO - ==================================================
2026-02-10 02:12:27,036 - INFO -   [탐색 78] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:27,037 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:27,037 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:27,207 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:27,208 - INFO - ==================================================
2026-02-10 02:12:27,209 - INFO -   [탐색 79] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:27,210 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:27,210 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:27,382 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:27,382 - INFO - ==================================================
2026-02-10 02:12:27,384 - INFO -   [탐색 80] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:27,385 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:27,385 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:27,556 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:27,556 - INFO - ==================================================
2026-02-10 02:12:27,558 - INFO -   [탐색 81] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:27,559 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:27,559 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:27,730 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:27,730 - INFO - ==================================================
2026-02-10 02:12:27,731 - INFO -   [탐색 82] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:27,732 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:27,732 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:28,007 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:28,007 - INFO - ==================================================
2026-02-10 02:12:28,009 - INFO -   [탐색 83] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:28,010 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:28,010 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:28,180 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:28,180 - INFO - ==================================================
2026-02-10 02:12:28,181 - INFO -   [탐색 84] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:28,183 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:28,183 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:28,353 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:28,353 - INFO - ==================================================
2026-02-10 02:12:28,355 - INFO -   [탐색 85] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:28,356 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:28,356 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:28,525 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:28,525 - INFO - ==================================================
2026-02-10 02:12:28,526 - INFO -   [탐색 86] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:28,527 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:28,527 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:28,695 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:28,696 - INFO - ==================================================
2026-02-10 02:12:28,697 - INFO -   [탐색 87] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:28,698 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:28,698 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:28,869 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:28,869 - INFO - ==================================================
2026-02-10 02:12:28,870 - INFO -   [탐색 88] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:28,871 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:28,871 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:29,147 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:29,147 - INFO - ==================================================
2026-02-10 02:12:29,149 - INFO -   [탐색 89] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:29,150 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:29,150 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:29,319 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:29,319 - INFO - ==================================================
2026-02-10 02:12:29,320 - INFO -   [탐색 90] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:29,321 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:29,321 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:29,491 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:29,491 - INFO - ==================================================
2026-02-10 02:12:29,492 - INFO -   [탐색 91] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:29,493 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:29,493 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:29,665 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:29,665 - INFO - ==================================================
2026-02-10 02:12:29,666 - INFO -   [탐색 92] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:29,667 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:29,667 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:29,839 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:29,839 - INFO - ==================================================
2026-02-10 02:12:29,840 - INFO -   [탐색 93] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:29,841 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:29,841 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:30,012 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:30,012 - INFO - ==================================================
2026-02-10 02:12:30,013 - INFO -   [탐색 94] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:30,014 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:30,014 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:30,183 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:30,183 - INFO - ==================================================
2026-02-10 02:12:30,185 - INFO -   [탐색 95] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:30,186 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:30,186 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:30,463 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:30,463 - INFO - ==================================================
2026-02-10 02:12:30,465 - INFO -   [탐색 96] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:30,466 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:30,466 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:30,632 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:30,632 - INFO - ==================================================
2026-02-10 02:12:30,633 - INFO -   [탐색 97] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:30,634 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:30,634 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:30,805 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:30,805 - INFO - ==================================================
2026-02-10 02:12:30,806 - INFO -   [탐색 98] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:30,807 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:30,807 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:30,976 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:30,976 - INFO - ==================================================
2026-02-10 02:12:30,978 - INFO -   [탐색 99] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:30,979 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:30,979 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:31,149 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921875)에 맞춰 변경되었습니다.
2026-02-10 02:12:31,149 - INFO - ==================================================
2026-02-10 02:12:31,150 - INFO -   [탐색 100] 희소도: 0.9219 -> FLOPs: 0.1854 GFLOPs (감소율: 93.54%)
2026-02-10 02:12:31,150 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.9214 입니다.
2026-02-10 02:12:31,150 - INFO - ================================================================================
2026-02-10 02:12:31,151 - INFO - 계산된 Pruning 정보(희소도: 0.9214)를 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/pruning_info.yaml'에 저장했습니다.
2026-02-10 02:12:31,152 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 02:12:31,155 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:12:31,155 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:12:31,324 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-02-10 02:12:31,324 - INFO - ==================================================
2026-02-10 02:12:31,324 - INFO - ==================================================
2026-02-10 02:12:31,324 - INFO - 모델 파라미터 수:
2026-02-10 02:12:31,325 - INFO -   - 총 파라미터: 57,792 개
2026-02-10 02:12:31,325 - INFO -   - 학습 가능한 파라미터: 57,792 개
2026-02-10 02:12:31,326 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 02:12:31,328 - INFO - FLOPs가 2.8696 GFLOPs에서 0.1854 GFLOPs로 감소했습니다 (감소율: 93.54%).
2026-02-10 02:12:31,328 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 02:12:31,329 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 02:12:31,329 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 02:12:31,329 - INFO - ==================================================
2026-02-10 02:12:31,329 - INFO - train 모드를 시작합니다.
2026-02-10 02:12:31,329 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 02:12:31,329 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 02:12:31,329 - INFO - --------------------------------------------------
2026-02-10 02:12:31,329 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 02:12:32,820 - INFO - [Train] [11/90] | Loss: 0.5531 | Train Acc: 77.98%
2026-02-10 02:12:33,211 - INFO - [Valid] [11/90] | Loss: 0.5426 | Val Acc: 75.22%
2026-02-10 02:12:33,214 - INFO - [Metrics for 'abnormal'] | Precision: 0.7355 | Recall: 0.7261 | F1: 0.7308
2026-02-10 02:12:33,214 - INFO - [Metrics for 'normal'] | Precision: 0.7663 | Recall: 0.7747 | F1: 0.7705
2026-02-10 02:12:33,218 - INFO - [Best Model Saved] (val loss: 0.5426) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:33,218 - INFO - --------------------------------------------------
2026-02-10 02:12:33,218 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 02:12:34,666 - INFO - [Train] [12/90] | Loss: 0.5229 | Train Acc: 80.43%
2026-02-10 02:12:35,061 - INFO - [Valid] [12/90] | Loss: 0.5418 | Val Acc: 74.93%
2026-02-10 02:12:35,063 - INFO - [Metrics for 'abnormal'] | Precision: 0.7093 | Recall: 0.7771 | F1: 0.7416
2026-02-10 02:12:35,064 - INFO - [Metrics for 'normal'] | Precision: 0.7904 | Recall: 0.7253 | F1: 0.7564
2026-02-10 02:12:35,065 - INFO - [Best Model Saved] (val loss: 0.5418) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:35,065 - INFO - --------------------------------------------------
2026-02-10 02:12:35,065 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 02:12:36,554 - INFO - [Train] [13/90] | Loss: 0.5209 | Train Acc: 80.36%
2026-02-10 02:12:36,944 - INFO - [Valid] [13/90] | Loss: 0.5322 | Val Acc: 76.70%
2026-02-10 02:12:36,946 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.7261 | F1: 0.7427
2026-02-10 02:12:36,946 - INFO - [Metrics for 'normal'] | Precision: 0.7725 | Recall: 0.8022 | F1: 0.7871
2026-02-10 02:12:36,948 - INFO - [Best Model Saved] (val loss: 0.5322) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:36,948 - INFO - --------------------------------------------------
2026-02-10 02:12:36,948 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 02:12:38,385 - INFO - [Train] [14/90] | Loss: 0.5123 | Train Acc: 81.40%
2026-02-10 02:12:38,784 - INFO - [Valid] [14/90] | Loss: 0.5229 | Val Acc: 77.58%
2026-02-10 02:12:38,787 - INFO - [Metrics for 'abnormal'] | Precision: 0.7793 | Recall: 0.7197 | F1: 0.7483
2026-02-10 02:12:38,787 - INFO - [Metrics for 'normal'] | Precision: 0.7732 | Recall: 0.8242 | F1: 0.7979
2026-02-10 02:12:38,788 - INFO - [Best Model Saved] (val loss: 0.5229) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:38,789 - INFO - --------------------------------------------------
2026-02-10 02:12:38,789 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 02:12:40,272 - INFO - [Train] [15/90] | Loss: 0.4923 | Train Acc: 81.55%
2026-02-10 02:12:40,665 - INFO - [Valid] [15/90] | Loss: 0.5174 | Val Acc: 76.70%
2026-02-10 02:12:40,667 - INFO - [Metrics for 'abnormal'] | Precision: 0.7294 | Recall: 0.7898 | F1: 0.7584
2026-02-10 02:12:40,667 - INFO - [Metrics for 'normal'] | Precision: 0.8047 | Recall: 0.7473 | F1: 0.7749
2026-02-10 02:12:40,669 - INFO - [Best Model Saved] (val loss: 0.5174) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:40,669 - INFO - --------------------------------------------------
2026-02-10 02:12:40,669 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 02:12:42,158 - INFO - [Train] [16/90] | Loss: 0.4906 | Train Acc: 82.81%
2026-02-10 02:12:42,550 - INFO - [Valid] [16/90] | Loss: 0.5082 | Val Acc: 76.11%
2026-02-10 02:12:42,553 - INFO - [Metrics for 'abnormal'] | Precision: 0.7111 | Recall: 0.8153 | F1: 0.7596
2026-02-10 02:12:42,553 - INFO - [Metrics for 'normal'] | Precision: 0.8176 | Recall: 0.7143 | F1: 0.7625
2026-02-10 02:12:42,555 - INFO - [Best Model Saved] (val loss: 0.5082) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:42,555 - INFO - --------------------------------------------------
2026-02-10 02:12:42,555 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 02:12:44,004 - INFO - [Train] [17/90] | Loss: 0.4943 | Train Acc: 82.22%
2026-02-10 02:12:44,391 - INFO - [Valid] [17/90] | Loss: 0.5038 | Val Acc: 79.65%
2026-02-10 02:12:44,394 - INFO - [Metrics for 'abnormal'] | Precision: 0.7716 | Recall: 0.7962 | F1: 0.7837
2026-02-10 02:12:44,394 - INFO - [Metrics for 'normal'] | Precision: 0.8192 | Recall: 0.7967 | F1: 0.8078
2026-02-10 02:12:44,396 - INFO - [Best Model Saved] (val loss: 0.5038) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:44,396 - INFO - --------------------------------------------------
2026-02-10 02:12:44,396 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 02:12:45,885 - INFO - [Train] [18/90] | Loss: 0.4805 | Train Acc: 83.04%
2026-02-10 02:12:46,287 - INFO - [Valid] [18/90] | Loss: 0.5014 | Val Acc: 79.06%
2026-02-10 02:12:46,289 - INFO - [Metrics for 'abnormal'] | Precision: 0.7590 | Recall: 0.8025 | F1: 0.7802
2026-02-10 02:12:46,289 - INFO - [Metrics for 'normal'] | Precision: 0.8208 | Recall: 0.7802 | F1: 0.8000
2026-02-10 02:12:46,291 - INFO - [Best Model Saved] (val loss: 0.5014) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:46,291 - INFO - --------------------------------------------------
2026-02-10 02:12:46,291 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 02:12:47,774 - INFO - [Train] [19/90] | Loss: 0.4763 | Train Acc: 83.11%
2026-02-10 02:12:48,175 - INFO - [Valid] [19/90] | Loss: 0.4978 | Val Acc: 79.65%
2026-02-10 02:12:48,178 - INFO - [Metrics for 'abnormal'] | Precision: 0.7619 | Recall: 0.8153 | F1: 0.7877
2026-02-10 02:12:48,178 - INFO - [Metrics for 'normal'] | Precision: 0.8304 | Recall: 0.7802 | F1: 0.8045
2026-02-10 02:12:48,180 - INFO - [Best Model Saved] (val loss: 0.4978) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:48,180 - INFO - --------------------------------------------------
2026-02-10 02:12:48,180 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 02:12:49,637 - INFO - [Train] [20/90] | Loss: 0.4768 | Train Acc: 82.14%
2026-02-10 02:12:50,033 - INFO - [Valid] [20/90] | Loss: 0.4941 | Val Acc: 79.06%
2026-02-10 02:12:50,036 - INFO - [Metrics for 'abnormal'] | Precision: 0.7654 | Recall: 0.7898 | F1: 0.7774
2026-02-10 02:12:50,036 - INFO - [Metrics for 'normal'] | Precision: 0.8136 | Recall: 0.7912 | F1: 0.8022
2026-02-10 02:12:50,037 - INFO - [Best Model Saved] (val loss: 0.4941) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:50,037 - INFO - --------------------------------------------------
2026-02-10 02:12:50,037 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 02:12:51,513 - INFO - [Train] [21/90] | Loss: 0.4686 | Train Acc: 83.18%
2026-02-10 02:12:51,915 - INFO - [Valid] [21/90] | Loss: 0.4879 | Val Acc: 80.53%
2026-02-10 02:12:51,917 - INFO - [Metrics for 'abnormal'] | Precision: 0.7862 | Recall: 0.7962 | F1: 0.7911
2026-02-10 02:12:51,917 - INFO - [Metrics for 'normal'] | Precision: 0.8222 | Recall: 0.8132 | F1: 0.8177
2026-02-10 02:12:51,919 - INFO - [Best Model Saved] (val loss: 0.4879) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:12:51,919 - INFO - --------------------------------------------------
2026-02-10 02:12:51,919 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 02:12:53,382 - INFO - [Train] [22/90] | Loss: 0.4770 | Train Acc: 84.08%
2026-02-10 02:12:53,783 - INFO - [Valid] [22/90] | Loss: 0.4977 | Val Acc: 80.24%
2026-02-10 02:12:53,786 - INFO - [Metrics for 'abnormal'] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-02-10 02:12:53,786 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-02-10 02:12:53,787 - INFO - --------------------------------------------------
2026-02-10 02:12:53,787 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 02:12:55,248 - INFO - [Train] [23/90] | Loss: 0.4581 | Train Acc: 84.23%
2026-02-10 02:12:55,651 - INFO - [Valid] [23/90] | Loss: 0.4952 | Val Acc: 82.01%
2026-02-10 02:12:55,653 - INFO - [Metrics for 'abnormal'] | Precision: 0.8117 | Recall: 0.7962 | F1: 0.8039
2026-02-10 02:12:55,653 - INFO - [Metrics for 'normal'] | Precision: 0.8270 | Recall: 0.8407 | F1: 0.8338
2026-02-10 02:12:55,654 - INFO - --------------------------------------------------
2026-02-10 02:12:55,654 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 02:12:57,174 - INFO - [Train] [24/90] | Loss: 0.4561 | Train Acc: 84.30%
2026-02-10 02:12:57,575 - INFO - [Valid] [24/90] | Loss: 0.5182 | Val Acc: 76.99%
2026-02-10 02:12:57,578 - INFO - [Metrics for 'abnormal'] | Precision: 0.7026 | Recall: 0.8726 | F1: 0.7784
2026-02-10 02:12:57,578 - INFO - [Metrics for 'normal'] | Precision: 0.8611 | Recall: 0.6813 | F1: 0.7607
2026-02-10 02:12:57,579 - INFO - --------------------------------------------------
2026-02-10 02:12:57,579 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 02:12:59,074 - INFO - [Train] [25/90] | Loss: 0.4653 | Train Acc: 84.38%
2026-02-10 02:12:59,470 - INFO - [Valid] [25/90] | Loss: 0.5085 | Val Acc: 80.24%
2026-02-10 02:12:59,473 - INFO - [Metrics for 'abnormal'] | Precision: 0.7961 | Recall: 0.7707 | F1: 0.7832
2026-02-10 02:12:59,473 - INFO - [Metrics for 'normal'] | Precision: 0.8075 | Recall: 0.8297 | F1: 0.8184
2026-02-10 02:12:59,474 - INFO - --------------------------------------------------
2026-02-10 02:12:59,474 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 02:13:00,919 - INFO - [Train] [26/90] | Loss: 0.4589 | Train Acc: 84.90%
2026-02-10 02:13:01,316 - INFO - [Valid] [26/90] | Loss: 0.5120 | Val Acc: 77.88%
2026-02-10 02:13:01,319 - INFO - [Metrics for 'abnormal'] | Precision: 0.7181 | Recall: 0.8599 | F1: 0.7826
2026-02-10 02:13:01,319 - INFO - [Metrics for 'normal'] | Precision: 0.8543 | Recall: 0.7088 | F1: 0.7748
2026-02-10 02:13:01,319 - INFO - --------------------------------------------------
2026-02-10 02:13:01,319 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 02:13:02,812 - INFO - [Train] [27/90] | Loss: 0.4471 | Train Acc: 85.34%
2026-02-10 02:13:03,214 - INFO - [Valid] [27/90] | Loss: 0.4993 | Val Acc: 82.01%
2026-02-10 02:13:03,217 - INFO - [Metrics for 'abnormal'] | Precision: 0.8288 | Recall: 0.7707 | F1: 0.7987
2026-02-10 02:13:03,217 - INFO - [Metrics for 'normal'] | Precision: 0.8135 | Recall: 0.8626 | F1: 0.8373
2026-02-10 02:13:03,217 - INFO - --------------------------------------------------
2026-02-10 02:13:03,218 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 02:13:04,671 - INFO - [Train] [28/90] | Loss: 0.4436 | Train Acc: 85.19%
2026-02-10 02:13:05,066 - INFO - [Valid] [28/90] | Loss: 0.4988 | Val Acc: 79.94%
2026-02-10 02:13:05,068 - INFO - [Metrics for 'abnormal'] | Precision: 0.7799 | Recall: 0.7898 | F1: 0.7848
2026-02-10 02:13:05,068 - INFO - [Metrics for 'normal'] | Precision: 0.8167 | Recall: 0.8077 | F1: 0.8122
2026-02-10 02:13:05,069 - INFO - --------------------------------------------------
2026-02-10 02:13:05,069 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 02:13:06,523 - INFO - [Train] [29/90] | Loss: 0.4395 | Train Acc: 85.79%
2026-02-10 02:13:06,919 - INFO - [Valid] [29/90] | Loss: 0.5224 | Val Acc: 78.17%
2026-02-10 02:13:06,922 - INFO - [Metrics for 'abnormal'] | Precision: 0.7268 | Recall: 0.8471 | F1: 0.7824
2026-02-10 02:13:06,922 - INFO - [Metrics for 'normal'] | Precision: 0.8462 | Recall: 0.7253 | F1: 0.7811
2026-02-10 02:13:06,923 - INFO - --------------------------------------------------
2026-02-10 02:13:06,923 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 02:13:08,391 - INFO - [Train] [30/90] | Loss: 0.4312 | Train Acc: 86.61%
2026-02-10 02:13:08,790 - INFO - [Valid] [30/90] | Loss: 0.5066 | Val Acc: 81.71%
2026-02-10 02:13:08,793 - INFO - [Metrics for 'abnormal'] | Precision: 0.8231 | Recall: 0.7707 | F1: 0.7961
2026-02-10 02:13:08,793 - INFO - [Metrics for 'normal'] | Precision: 0.8125 | Recall: 0.8571 | F1: 0.8342
2026-02-10 02:13:08,794 - INFO - --------------------------------------------------
2026-02-10 02:13:08,794 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 02:13:10,172 - INFO - [Train] [31/90] | Loss: 0.4303 | Train Acc: 86.76%
2026-02-10 02:13:10,557 - INFO - [Valid] [31/90] | Loss: 0.5022 | Val Acc: 80.53%
2026-02-10 02:13:10,560 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-02-10 02:13:10,560 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-02-10 02:13:10,561 - INFO - --------------------------------------------------
2026-02-10 02:13:10,561 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 02:13:12,045 - INFO - [Train] [32/90] | Loss: 0.4180 | Train Acc: 87.57%
2026-02-10 02:13:12,445 - INFO - [Valid] [32/90] | Loss: 0.5045 | Val Acc: 79.65%
2026-02-10 02:13:12,447 - INFO - [Metrics for 'abnormal'] | Precision: 0.7651 | Recall: 0.8089 | F1: 0.7864
2026-02-10 02:13:12,448 - INFO - [Metrics for 'normal'] | Precision: 0.8266 | Recall: 0.7857 | F1: 0.8056
2026-02-10 02:13:12,448 - INFO - --------------------------------------------------
2026-02-10 02:13:12,448 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 02:13:13,943 - INFO - [Train] [33/90] | Loss: 0.4157 | Train Acc: 87.50%
2026-02-10 02:13:14,331 - INFO - [Valid] [33/90] | Loss: 0.4917 | Val Acc: 82.60%
2026-02-10 02:13:14,333 - INFO - [Metrics for 'abnormal'] | Precision: 0.8356 | Recall: 0.7771 | F1: 0.8053
2026-02-10 02:13:14,333 - INFO - [Metrics for 'normal'] | Precision: 0.8187 | Recall: 0.8681 | F1: 0.8427
2026-02-10 02:13:14,334 - INFO - --------------------------------------------------
2026-02-10 02:13:14,334 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 02:13:15,839 - INFO - [Train] [34/90] | Loss: 0.4176 | Train Acc: 87.87%
2026-02-10 02:13:16,244 - INFO - [Valid] [34/90] | Loss: 0.5278 | Val Acc: 79.06%
2026-02-10 02:13:16,246 - INFO - [Metrics for 'abnormal'] | Precision: 0.8308 | Recall: 0.6879 | F1: 0.7526
2026-02-10 02:13:16,246 - INFO - [Metrics for 'normal'] | Precision: 0.7656 | Recall: 0.8791 | F1: 0.8184
2026-02-10 02:13:16,247 - INFO - --------------------------------------------------
2026-02-10 02:13:16,247 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 02:13:17,708 - INFO - [Train] [35/90] | Loss: 0.4148 | Train Acc: 88.76%
2026-02-10 02:13:18,109 - INFO - [Valid] [35/90] | Loss: 0.4958 | Val Acc: 80.53%
2026-02-10 02:13:18,112 - INFO - [Metrics for 'abnormal'] | Precision: 0.7826 | Recall: 0.8025 | F1: 0.7925
2026-02-10 02:13:18,112 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.8077 | F1: 0.8167
2026-02-10 02:13:18,112 - INFO - --------------------------------------------------
2026-02-10 02:13:18,113 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 02:13:19,574 - INFO - [Train] [36/90] | Loss: 0.4008 | Train Acc: 88.02%
2026-02-10 02:13:19,976 - INFO - [Valid] [36/90] | Loss: 0.4975 | Val Acc: 79.94%
2026-02-10 02:13:19,979 - INFO - [Metrics for 'abnormal'] | Precision: 0.7633 | Recall: 0.8217 | F1: 0.7914
2026-02-10 02:13:19,979 - INFO - [Metrics for 'normal'] | Precision: 0.8353 | Recall: 0.7802 | F1: 0.8068
2026-02-10 02:13:19,980 - INFO - --------------------------------------------------
2026-02-10 02:13:19,980 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 02:13:21,503 - INFO - [Train] [37/90] | Loss: 0.4136 | Train Acc: 88.47%
2026-02-10 02:13:21,905 - INFO - [Valid] [37/90] | Loss: 0.5423 | Val Acc: 77.88%
2026-02-10 02:13:21,908 - INFO - [Metrics for 'abnormal'] | Precision: 0.7071 | Recall: 0.8917 | F1: 0.7887
2026-02-10 02:13:21,908 - INFO - [Metrics for 'normal'] | Precision: 0.8794 | Recall: 0.6813 | F1: 0.7678
2026-02-10 02:13:21,909 - INFO - --------------------------------------------------
2026-02-10 02:13:21,909 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 02:13:23,409 - INFO - [Train] [38/90] | Loss: 0.3860 | Train Acc: 88.76%
2026-02-10 02:13:23,810 - INFO - [Valid] [38/90] | Loss: 0.4980 | Val Acc: 82.01%
2026-02-10 02:13:23,812 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 02:13:23,812 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 02:13:23,813 - INFO - --------------------------------------------------
2026-02-10 02:13:23,813 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 02:13:25,329 - INFO - [Train] [39/90] | Loss: 0.3929 | Train Acc: 89.43%
2026-02-10 02:13:25,730 - INFO - [Valid] [39/90] | Loss: 0.5191 | Val Acc: 81.12%
2026-02-10 02:13:25,733 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-02-10 02:13:25,733 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-02-10 02:13:25,734 - INFO - --------------------------------------------------
2026-02-10 02:13:25,734 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 02:13:27,186 - INFO - [Train] [40/90] | Loss: 0.3840 | Train Acc: 89.06%
2026-02-10 02:13:27,585 - INFO - [Valid] [40/90] | Loss: 0.4799 | Val Acc: 82.89%
2026-02-10 02:13:27,588 - INFO - [Metrics for 'abnormal'] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-02-10 02:13:27,588 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-02-10 02:13:27,590 - INFO - [Best Model Saved] (val loss: 0.4799) -> 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:13:27,590 - INFO - --------------------------------------------------
2026-02-10 02:13:27,590 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 02:13:29,097 - INFO - [Train] [41/90] | Loss: 0.3854 | Train Acc: 89.51%
2026-02-10 02:13:29,497 - INFO - [Valid] [41/90] | Loss: 0.4899 | Val Acc: 83.19%
2026-02-10 02:13:29,499 - INFO - [Metrics for 'abnormal'] | Precision: 0.8205 | Recall: 0.8153 | F1: 0.8179
2026-02-10 02:13:29,499 - INFO - [Metrics for 'normal'] | Precision: 0.8415 | Recall: 0.8462 | F1: 0.8438
2026-02-10 02:13:29,500 - INFO - --------------------------------------------------
2026-02-10 02:13:29,500 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 02:13:30,961 - INFO - [Train] [42/90] | Loss: 0.3811 | Train Acc: 89.51%
2026-02-10 02:13:31,352 - INFO - [Valid] [42/90] | Loss: 0.4977 | Val Acc: 81.42%
2026-02-10 02:13:31,354 - INFO - [Metrics for 'abnormal'] | Precision: 0.7798 | Recall: 0.8344 | F1: 0.8062
2026-02-10 02:13:31,354 - INFO - [Metrics for 'normal'] | Precision: 0.8480 | Recall: 0.7967 | F1: 0.8215
2026-02-10 02:13:31,355 - INFO - --------------------------------------------------
2026-02-10 02:13:31,355 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 02:13:32,837 - INFO - [Train] [43/90] | Loss: 0.3796 | Train Acc: 89.66%
2026-02-10 02:13:33,233 - INFO - [Valid] [43/90] | Loss: 0.5314 | Val Acc: 79.35%
2026-02-10 02:13:33,236 - INFO - [Metrics for 'abnormal'] | Precision: 0.7351 | Recall: 0.8662 | F1: 0.7953
2026-02-10 02:13:33,236 - INFO - [Metrics for 'normal'] | Precision: 0.8636 | Recall: 0.7308 | F1: 0.7917
2026-02-10 02:13:33,237 - INFO - --------------------------------------------------
2026-02-10 02:13:33,237 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 02:13:34,723 - INFO - [Train] [44/90] | Loss: 0.3802 | Train Acc: 90.48%
2026-02-10 02:13:35,112 - INFO - [Valid] [44/90] | Loss: 0.4983 | Val Acc: 81.71%
2026-02-10 02:13:35,115 - INFO - [Metrics for 'abnormal'] | Precision: 0.7914 | Recall: 0.8217 | F1: 0.8063
2026-02-10 02:13:35,115 - INFO - [Metrics for 'normal'] | Precision: 0.8409 | Recall: 0.8132 | F1: 0.8268
2026-02-10 02:13:35,115 - INFO - --------------------------------------------------
2026-02-10 02:13:35,116 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 02:13:36,632 - INFO - [Train] [45/90] | Loss: 0.3667 | Train Acc: 91.44%
2026-02-10 02:13:37,024 - INFO - [Valid] [45/90] | Loss: 0.5022 | Val Acc: 81.71%
2026-02-10 02:13:37,026 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.8089 | F1: 0.8038
2026-02-10 02:13:37,026 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8242 | F1: 0.8287
2026-02-10 02:13:37,027 - INFO - --------------------------------------------------
2026-02-10 02:13:37,027 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 02:13:38,523 - INFO - [Train] [46/90] | Loss: 0.3681 | Train Acc: 91.37%
2026-02-10 02:13:38,922 - INFO - [Valid] [46/90] | Loss: 0.4935 | Val Acc: 83.19%
2026-02-10 02:13:38,924 - INFO - [Metrics for 'abnormal'] | Precision: 0.7941 | Recall: 0.8599 | F1: 0.8257
2026-02-10 02:13:38,924 - INFO - [Metrics for 'normal'] | Precision: 0.8698 | Recall: 0.8077 | F1: 0.8376
2026-02-10 02:13:38,925 - INFO - --------------------------------------------------
2026-02-10 02:13:38,925 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 02:13:40,433 - INFO - [Train] [47/90] | Loss: 0.3615 | Train Acc: 90.33%
2026-02-10 02:13:40,829 - INFO - [Valid] [47/90] | Loss: 0.5106 | Val Acc: 81.71%
2026-02-10 02:13:40,832 - INFO - [Metrics for 'abnormal'] | Precision: 0.7778 | Recall: 0.8471 | F1: 0.8110
2026-02-10 02:13:40,832 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.7912 | F1: 0.8229
2026-02-10 02:13:40,833 - INFO - --------------------------------------------------
2026-02-10 02:13:40,833 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 02:13:42,271 - INFO - [Train] [48/90] | Loss: 0.3694 | Train Acc: 91.37%
2026-02-10 02:13:42,671 - INFO - [Valid] [48/90] | Loss: 0.4998 | Val Acc: 81.12%
2026-02-10 02:13:42,674 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 02:13:42,674 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 02:13:42,674 - INFO - --------------------------------------------------
2026-02-10 02:13:42,675 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 02:13:44,137 - INFO - [Train] [49/90] | Loss: 0.3570 | Train Acc: 91.22%
2026-02-10 02:13:44,536 - INFO - [Valid] [49/90] | Loss: 0.5092 | Val Acc: 80.83%
2026-02-10 02:13:44,538 - INFO - [Metrics for 'abnormal'] | Precision: 0.7949 | Recall: 0.7898 | F1: 0.7923
2026-02-10 02:13:44,539 - INFO - [Metrics for 'normal'] | Precision: 0.8197 | Recall: 0.8242 | F1: 0.8219
2026-02-10 02:13:44,539 - INFO - --------------------------------------------------
2026-02-10 02:13:44,539 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 02:13:45,935 - INFO - [Train] [50/90] | Loss: 0.3660 | Train Acc: 91.00%
2026-02-10 02:13:46,335 - INFO - [Valid] [50/90] | Loss: 0.4963 | Val Acc: 80.24%
2026-02-10 02:13:46,338 - INFO - [Metrics for 'abnormal'] | Precision: 0.7711 | Recall: 0.8153 | F1: 0.7926
2026-02-10 02:13:46,338 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.7912 | F1: 0.8113
2026-02-10 02:13:46,339 - INFO - --------------------------------------------------
2026-02-10 02:13:46,339 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 02:13:47,804 - INFO - [Train] [51/90] | Loss: 0.3561 | Train Acc: 92.04%
2026-02-10 02:13:48,204 - INFO - [Valid] [51/90] | Loss: 0.4854 | Val Acc: 81.71%
2026-02-10 02:13:48,207 - INFO - [Metrics for 'abnormal'] | Precision: 0.7844 | Recall: 0.8344 | F1: 0.8086
2026-02-10 02:13:48,207 - INFO - [Metrics for 'normal'] | Precision: 0.8488 | Recall: 0.8022 | F1: 0.8249
2026-02-10 02:13:48,208 - INFO - --------------------------------------------------
2026-02-10 02:13:48,208 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 02:13:49,651 - INFO - [Train] [52/90] | Loss: 0.3533 | Train Acc: 92.26%
2026-02-10 02:13:50,053 - INFO - [Valid] [52/90] | Loss: 0.4850 | Val Acc: 82.89%
2026-02-10 02:13:50,055 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8408 | F1: 0.8199
2026-02-10 02:13:50,055 - INFO - [Metrics for 'normal'] | Precision: 0.8563 | Recall: 0.8187 | F1: 0.8371
2026-02-10 02:13:50,056 - INFO - --------------------------------------------------
2026-02-10 02:13:50,056 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 02:13:51,561 - INFO - [Train] [53/90] | Loss: 0.3490 | Train Acc: 92.93%
2026-02-10 02:13:51,959 - INFO - [Valid] [53/90] | Loss: 0.4984 | Val Acc: 83.48%
2026-02-10 02:13:51,961 - INFO - [Metrics for 'abnormal'] | Precision: 0.8061 | Recall: 0.8471 | F1: 0.8261
2026-02-10 02:13:51,961 - INFO - [Metrics for 'normal'] | Precision: 0.8621 | Recall: 0.8242 | F1: 0.8427
2026-02-10 02:13:51,962 - INFO - --------------------------------------------------
2026-02-10 02:13:51,962 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 02:13:53,435 - INFO - [Train] [54/90] | Loss: 0.3520 | Train Acc: 91.74%
2026-02-10 02:13:53,834 - INFO - [Valid] [54/90] | Loss: 0.5141 | Val Acc: 81.42%
2026-02-10 02:13:53,836 - INFO - [Metrics for 'abnormal'] | Precision: 0.7831 | Recall: 0.8280 | F1: 0.8050
2026-02-10 02:13:53,836 - INFO - [Metrics for 'normal'] | Precision: 0.8439 | Recall: 0.8022 | F1: 0.8225
2026-02-10 02:13:53,837 - INFO - --------------------------------------------------
2026-02-10 02:13:53,837 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 02:13:55,346 - INFO - [Train] [55/90] | Loss: 0.3462 | Train Acc: 91.89%
2026-02-10 02:13:55,748 - INFO - [Valid] [55/90] | Loss: 0.4945 | Val Acc: 81.12%
2026-02-10 02:13:55,750 - INFO - [Metrics for 'abnormal'] | Precision: 0.7962 | Recall: 0.7962 | F1: 0.7962
2026-02-10 02:13:55,750 - INFO - [Metrics for 'normal'] | Precision: 0.8242 | Recall: 0.8242 | F1: 0.8242
2026-02-10 02:13:55,751 - INFO - --------------------------------------------------
2026-02-10 02:13:55,751 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 02:13:57,242 - INFO - [Train] [56/90] | Loss: 0.3482 | Train Acc: 92.78%
2026-02-10 02:13:57,631 - INFO - [Valid] [56/90] | Loss: 0.4905 | Val Acc: 82.01%
2026-02-10 02:13:57,633 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 02:13:57,633 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 02:13:57,634 - INFO - --------------------------------------------------
2026-02-10 02:13:57,634 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 02:13:59,078 - INFO - [Train] [57/90] | Loss: 0.3483 | Train Acc: 92.86%
2026-02-10 02:13:59,482 - INFO - [Valid] [57/90] | Loss: 0.4986 | Val Acc: 82.30%
2026-02-10 02:13:59,484 - INFO - [Metrics for 'abnormal'] | Precision: 0.7939 | Recall: 0.8344 | F1: 0.8137
2026-02-10 02:13:59,484 - INFO - [Metrics for 'normal'] | Precision: 0.8506 | Recall: 0.8132 | F1: 0.8315
2026-02-10 02:13:59,485 - INFO - --------------------------------------------------
2026-02-10 02:13:59,485 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 02:14:00,987 - INFO - [Train] [58/90] | Loss: 0.3408 | Train Acc: 93.75%
2026-02-10 02:14:01,387 - INFO - [Valid] [58/90] | Loss: 0.4887 | Val Acc: 81.42%
2026-02-10 02:14:01,389 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-02-10 02:14:01,389 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-02-10 02:14:01,390 - INFO - --------------------------------------------------
2026-02-10 02:14:01,390 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 02:14:02,897 - INFO - [Train] [59/90] | Loss: 0.3436 | Train Acc: 92.93%
2026-02-10 02:14:03,295 - INFO - [Valid] [59/90] | Loss: 0.4916 | Val Acc: 82.01%
2026-02-10 02:14:03,297 - INFO - [Metrics for 'abnormal'] | Precision: 0.8117 | Recall: 0.7962 | F1: 0.8039
2026-02-10 02:14:03,297 - INFO - [Metrics for 'normal'] | Precision: 0.8270 | Recall: 0.8407 | F1: 0.8338
2026-02-10 02:14:03,298 - INFO - --------------------------------------------------
2026-02-10 02:14:03,298 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 02:14:04,719 - INFO - [Train] [60/90] | Loss: 0.3319 | Train Acc: 93.60%
2026-02-10 02:14:05,109 - INFO - [Valid] [60/90] | Loss: 0.4926 | Val Acc: 81.12%
2026-02-10 02:14:05,111 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-02-10 02:14:05,111 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-02-10 02:14:05,112 - INFO - --------------------------------------------------
2026-02-10 02:14:05,112 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 02:14:06,610 - INFO - [Train] [61/90] | Loss: 0.3346 | Train Acc: 93.23%
2026-02-10 02:14:06,998 - INFO - [Valid] [61/90] | Loss: 0.5162 | Val Acc: 81.42%
2026-02-10 02:14:07,001 - INFO - [Metrics for 'abnormal'] | Precision: 0.7640 | Recall: 0.8662 | F1: 0.8119
2026-02-10 02:14:07,002 - INFO - [Metrics for 'normal'] | Precision: 0.8696 | Recall: 0.7692 | F1: 0.8163
2026-02-10 02:14:07,002 - INFO - --------------------------------------------------
2026-02-10 02:14:07,003 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 02:14:08,475 - INFO - [Train] [62/90] | Loss: 0.3299 | Train Acc: 94.35%
2026-02-10 02:14:08,872 - INFO - [Valid] [62/90] | Loss: 0.4870 | Val Acc: 82.01%
2026-02-10 02:14:08,874 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-02-10 02:14:08,874 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-02-10 02:14:08,875 - INFO - --------------------------------------------------
2026-02-10 02:14:08,875 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 02:14:10,321 - INFO - [Train] [63/90] | Loss: 0.3239 | Train Acc: 94.57%
2026-02-10 02:14:10,722 - INFO - [Valid] [63/90] | Loss: 0.5036 | Val Acc: 81.71%
2026-02-10 02:14:10,724 - INFO - [Metrics for 'abnormal'] | Precision: 0.7950 | Recall: 0.8153 | F1: 0.8050
2026-02-10 02:14:10,724 - INFO - [Metrics for 'normal'] | Precision: 0.8371 | Recall: 0.8187 | F1: 0.8278
2026-02-10 02:14:10,725 - INFO - --------------------------------------------------
2026-02-10 02:14:10,725 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 02:14:12,249 - INFO - [Train] [64/90] | Loss: 0.3280 | Train Acc: 94.20%
2026-02-10 02:14:12,647 - INFO - [Valid] [64/90] | Loss: 0.5213 | Val Acc: 81.71%
2026-02-10 02:14:12,649 - INFO - [Metrics for 'abnormal'] | Precision: 0.7811 | Recall: 0.8408 | F1: 0.8098
2026-02-10 02:14:12,649 - INFO - [Metrics for 'normal'] | Precision: 0.8529 | Recall: 0.7967 | F1: 0.8239
2026-02-10 02:14:12,650 - INFO - --------------------------------------------------
2026-02-10 02:14:12,650 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 02:14:14,141 - INFO - [Train] [65/90] | Loss: 0.3234 | Train Acc: 93.97%
2026-02-10 02:14:14,541 - INFO - [Valid] [65/90] | Loss: 0.5043 | Val Acc: 82.30%
2026-02-10 02:14:14,544 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-02-10 02:14:14,544 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-02-10 02:14:14,545 - INFO - --------------------------------------------------
2026-02-10 02:14:14,545 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 02:14:15,970 - INFO - [Train] [66/90] | Loss: 0.3212 | Train Acc: 94.72%
2026-02-10 02:14:16,364 - INFO - [Valid] [66/90] | Loss: 0.4936 | Val Acc: 82.60%
2026-02-10 02:14:16,367 - INFO - [Metrics for 'abnormal'] | Precision: 0.7882 | Recall: 0.8535 | F1: 0.8196
2026-02-10 02:14:16,367 - INFO - [Metrics for 'normal'] | Precision: 0.8639 | Recall: 0.8022 | F1: 0.8319
2026-02-10 02:14:16,368 - INFO - --------------------------------------------------
2026-02-10 02:14:16,368 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 02:14:17,859 - INFO - [Train] [67/90] | Loss: 0.3217 | Train Acc: 94.20%
2026-02-10 02:14:18,259 - INFO - [Valid] [67/90] | Loss: 0.5110 | Val Acc: 82.30%
2026-02-10 02:14:18,261 - INFO - [Metrics for 'abnormal'] | Precision: 0.8050 | Recall: 0.8153 | F1: 0.8101
2026-02-10 02:14:18,261 - INFO - [Metrics for 'normal'] | Precision: 0.8389 | Recall: 0.8297 | F1: 0.8343
2026-02-10 02:14:18,262 - INFO - --------------------------------------------------
2026-02-10 02:14:18,262 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 02:14:19,748 - INFO - [Train] [68/90] | Loss: 0.3182 | Train Acc: 94.35%
2026-02-10 02:14:20,140 - INFO - [Valid] [68/90] | Loss: 0.5031 | Val Acc: 82.60%
2026-02-10 02:14:20,142 - INFO - [Metrics for 'abnormal'] | Precision: 0.7952 | Recall: 0.8408 | F1: 0.8173
2026-02-10 02:14:20,142 - INFO - [Metrics for 'normal'] | Precision: 0.8555 | Recall: 0.8132 | F1: 0.8338
2026-02-10 02:14:20,144 - INFO - --------------------------------------------------
2026-02-10 02:14:20,144 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 02:14:21,654 - INFO - [Train] [69/90] | Loss: 0.3230 | Train Acc: 94.94%
2026-02-10 02:14:22,051 - INFO - [Valid] [69/90] | Loss: 0.5063 | Val Acc: 82.30%
2026-02-10 02:14:22,053 - INFO - [Metrics for 'abnormal'] | Precision: 0.7836 | Recall: 0.8535 | F1: 0.8171
2026-02-10 02:14:22,053 - INFO - [Metrics for 'normal'] | Precision: 0.8631 | Recall: 0.7967 | F1: 0.8286
2026-02-10 02:14:22,054 - INFO - --------------------------------------------------
2026-02-10 02:14:22,054 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 02:14:23,553 - INFO - [Train] [70/90] | Loss: 0.3166 | Train Acc: 95.01%
2026-02-10 02:14:23,959 - INFO - [Valid] [70/90] | Loss: 0.5107 | Val Acc: 81.71%
2026-02-10 02:14:23,961 - INFO - [Metrics for 'abnormal'] | Precision: 0.7879 | Recall: 0.8280 | F1: 0.8075
2026-02-10 02:14:23,961 - INFO - [Metrics for 'normal'] | Precision: 0.8448 | Recall: 0.8077 | F1: 0.8258
2026-02-10 02:14:23,962 - INFO - --------------------------------------------------
2026-02-10 02:14:23,962 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 02:14:25,425 - INFO - [Train] [71/90] | Loss: 0.3183 | Train Acc: 95.09%
2026-02-10 02:14:25,821 - INFO - [Valid] [71/90] | Loss: 0.4985 | Val Acc: 82.60%
2026-02-10 02:14:25,823 - INFO - [Metrics for 'abnormal'] | Precision: 0.7952 | Recall: 0.8408 | F1: 0.8173
2026-02-10 02:14:25,823 - INFO - [Metrics for 'normal'] | Precision: 0.8555 | Recall: 0.8132 | F1: 0.8338
2026-02-10 02:14:25,824 - INFO - --------------------------------------------------
2026-02-10 02:14:25,824 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 02:14:27,303 - INFO - [Train] [72/90] | Loss: 0.3113 | Train Acc: 95.09%
2026-02-10 02:14:27,699 - INFO - [Valid] [72/90] | Loss: 0.5020 | Val Acc: 81.42%
2026-02-10 02:14:27,702 - INFO - [Metrics for 'abnormal'] | Precision: 0.7831 | Recall: 0.8280 | F1: 0.8050
2026-02-10 02:14:27,702 - INFO - [Metrics for 'normal'] | Precision: 0.8439 | Recall: 0.8022 | F1: 0.8225
2026-02-10 02:14:27,702 - INFO - --------------------------------------------------
2026-02-10 02:14:27,702 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 02:14:29,176 - INFO - [Train] [73/90] | Loss: 0.3114 | Train Acc: 95.01%
2026-02-10 02:14:29,574 - INFO - [Valid] [73/90] | Loss: 0.4994 | Val Acc: 82.30%
2026-02-10 02:14:29,576 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-02-10 02:14:29,576 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-02-10 02:14:29,577 - INFO - --------------------------------------------------
2026-02-10 02:14:29,577 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 02:14:31,034 - INFO - [Train] [74/90] | Loss: 0.3103 | Train Acc: 95.01%
2026-02-10 02:14:31,436 - INFO - [Valid] [74/90] | Loss: 0.4918 | Val Acc: 82.89%
2026-02-10 02:14:31,440 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8408 | F1: 0.8199
2026-02-10 02:14:31,440 - INFO - [Metrics for 'normal'] | Precision: 0.8563 | Recall: 0.8187 | F1: 0.8371
2026-02-10 02:14:31,441 - INFO - --------------------------------------------------
2026-02-10 02:14:31,441 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 02:14:32,901 - INFO - [Train] [75/90] | Loss: 0.3049 | Train Acc: 95.09%
2026-02-10 02:14:33,303 - INFO - [Valid] [75/90] | Loss: 0.5035 | Val Acc: 82.01%
2026-02-10 02:14:33,306 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-02-10 02:14:33,306 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-02-10 02:14:33,307 - INFO - --------------------------------------------------
2026-02-10 02:14:33,307 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 02:14:34,823 - INFO - [Train] [76/90] | Loss: 0.3179 | Train Acc: 95.09%
2026-02-10 02:14:35,219 - INFO - [Valid] [76/90] | Loss: 0.4957 | Val Acc: 82.60%
2026-02-10 02:14:35,221 - INFO - [Metrics for 'abnormal'] | Precision: 0.7988 | Recall: 0.8344 | F1: 0.8162
2026-02-10 02:14:35,221 - INFO - [Metrics for 'normal'] | Precision: 0.8514 | Recall: 0.8187 | F1: 0.8347
2026-02-10 02:14:35,222 - INFO - --------------------------------------------------
2026-02-10 02:14:35,222 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 02:14:36,685 - INFO - [Train] [77/90] | Loss: 0.3168 | Train Acc: 94.94%
2026-02-10 02:14:37,082 - INFO - [Valid] [77/90] | Loss: 0.5047 | Val Acc: 82.60%
2026-02-10 02:14:37,084 - INFO - [Metrics for 'abnormal'] | Precision: 0.7917 | Recall: 0.8471 | F1: 0.8185
2026-02-10 02:14:37,084 - INFO - [Metrics for 'normal'] | Precision: 0.8596 | Recall: 0.8077 | F1: 0.8329
2026-02-10 02:14:37,085 - INFO - --------------------------------------------------
2026-02-10 02:14:37,085 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 02:14:38,573 - INFO - [Train] [78/90] | Loss: 0.3106 | Train Acc: 94.49%
2026-02-10 02:14:38,967 - INFO - [Valid] [78/90] | Loss: 0.5047 | Val Acc: 81.42%
2026-02-10 02:14:38,970 - INFO - [Metrics for 'abnormal'] | Precision: 0.7866 | Recall: 0.8217 | F1: 0.8037
2026-02-10 02:14:38,970 - INFO - [Metrics for 'normal'] | Precision: 0.8400 | Recall: 0.8077 | F1: 0.8235
2026-02-10 02:14:38,970 - INFO - --------------------------------------------------
2026-02-10 02:14:38,971 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 02:14:40,461 - INFO - [Train] [79/90] | Loss: 0.3008 | Train Acc: 95.83%
2026-02-10 02:14:40,861 - INFO - [Valid] [79/90] | Loss: 0.5096 | Val Acc: 82.01%
2026-02-10 02:14:40,864 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-02-10 02:14:40,864 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-02-10 02:14:40,865 - INFO - --------------------------------------------------
2026-02-10 02:14:40,865 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 02:14:42,333 - INFO - [Train] [80/90] | Loss: 0.2993 | Train Acc: 95.98%
2026-02-10 02:14:42,731 - INFO - [Valid] [80/90] | Loss: 0.5077 | Val Acc: 82.89%
2026-02-10 02:14:42,734 - INFO - [Metrics for 'abnormal'] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-02-10 02:14:42,734 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-02-10 02:14:42,735 - INFO - --------------------------------------------------
2026-02-10 02:14:42,735 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 02:14:44,251 - INFO - [Train] [81/90] | Loss: 0.3058 | Train Acc: 95.76%
2026-02-10 02:14:44,651 - INFO - [Valid] [81/90] | Loss: 0.5102 | Val Acc: 82.60%
2026-02-10 02:14:44,655 - INFO - [Metrics for 'abnormal'] | Precision: 0.8063 | Recall: 0.8217 | F1: 0.8139
2026-02-10 02:14:44,655 - INFO - [Metrics for 'normal'] | Precision: 0.8436 | Recall: 0.8297 | F1: 0.8366
2026-02-10 02:14:44,655 - INFO - --------------------------------------------------
2026-02-10 02:14:44,656 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 02:14:46,186 - INFO - [Train] [82/90] | Loss: 0.3019 | Train Acc: 95.83%
2026-02-10 02:14:46,584 - INFO - [Valid] [82/90] | Loss: 0.4995 | Val Acc: 83.19%
2026-02-10 02:14:46,587 - INFO - [Metrics for 'abnormal'] | Precision: 0.8086 | Recall: 0.8344 | F1: 0.8213
2026-02-10 02:14:46,587 - INFO - [Metrics for 'normal'] | Precision: 0.8531 | Recall: 0.8297 | F1: 0.8412
2026-02-10 02:14:46,588 - INFO - --------------------------------------------------
2026-02-10 02:14:46,588 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 02:14:48,084 - INFO - [Train] [83/90] | Loss: 0.3048 | Train Acc: 95.16%
2026-02-10 02:14:48,482 - INFO - [Valid] [83/90] | Loss: 0.5139 | Val Acc: 82.01%
2026-02-10 02:14:48,484 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 02:14:48,484 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 02:14:48,485 - INFO - --------------------------------------------------
2026-02-10 02:14:48,485 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 02:14:49,915 - INFO - [Train] [84/90] | Loss: 0.3017 | Train Acc: 95.83%
2026-02-10 02:14:50,310 - INFO - [Valid] [84/90] | Loss: 0.5009 | Val Acc: 81.42%
2026-02-10 02:14:50,313 - INFO - [Metrics for 'abnormal'] | Precision: 0.7937 | Recall: 0.8089 | F1: 0.8013
2026-02-10 02:14:50,313 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8187 | F1: 0.8255
2026-02-10 02:14:50,314 - INFO - --------------------------------------------------
2026-02-10 02:14:50,314 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 02:14:51,778 - INFO - [Train] [85/90] | Loss: 0.2997 | Train Acc: 96.35%
2026-02-10 02:14:52,167 - INFO - [Valid] [85/90] | Loss: 0.5090 | Val Acc: 83.19%
2026-02-10 02:14:52,170 - INFO - [Metrics for 'abnormal'] | Precision: 0.7976 | Recall: 0.8535 | F1: 0.8246
2026-02-10 02:14:52,170 - INFO - [Metrics for 'normal'] | Precision: 0.8655 | Recall: 0.8132 | F1: 0.8385
2026-02-10 02:14:52,170 - INFO - --------------------------------------------------
2026-02-10 02:14:52,170 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 02:14:53,645 - INFO - [Train] [86/90] | Loss: 0.3009 | Train Acc: 95.76%
2026-02-10 02:14:54,046 - INFO - [Valid] [86/90] | Loss: 0.5042 | Val Acc: 81.12%
2026-02-10 02:14:54,048 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 02:14:54,048 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 02:14:54,049 - INFO - --------------------------------------------------
2026-02-10 02:14:54,049 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 02:14:55,505 - INFO - [Train] [87/90] | Loss: 0.2960 | Train Acc: 96.35%
2026-02-10 02:14:55,912 - INFO - [Valid] [87/90] | Loss: 0.5095 | Val Acc: 81.12%
2026-02-10 02:14:55,915 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 02:14:55,915 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 02:14:55,916 - INFO - --------------------------------------------------
2026-02-10 02:14:55,916 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 02:14:57,424 - INFO - [Train] [88/90] | Loss: 0.3020 | Train Acc: 96.28%
2026-02-10 02:14:57,822 - INFO - [Valid] [88/90] | Loss: 0.5045 | Val Acc: 81.71%
2026-02-10 02:14:57,825 - INFO - [Metrics for 'abnormal'] | Precision: 0.7914 | Recall: 0.8217 | F1: 0.8063
2026-02-10 02:14:57,825 - INFO - [Metrics for 'normal'] | Precision: 0.8409 | Recall: 0.8132 | F1: 0.8268
2026-02-10 02:14:57,825 - INFO - --------------------------------------------------
2026-02-10 02:14:57,826 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 02:14:59,307 - INFO - [Train] [89/90] | Loss: 0.2942 | Train Acc: 96.28%
2026-02-10 02:14:59,678 - INFO - [Valid] [89/90] | Loss: 0.5039 | Val Acc: 82.30%
2026-02-10 02:14:59,681 - INFO - [Metrics for 'abnormal'] | Precision: 0.7939 | Recall: 0.8344 | F1: 0.8137
2026-02-10 02:14:59,681 - INFO - [Metrics for 'normal'] | Precision: 0.8506 | Recall: 0.8132 | F1: 0.8315
2026-02-10 02:14:59,682 - INFO - --------------------------------------------------
2026-02-10 02:14:59,682 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 02:15:01,161 - INFO - [Train] [90/90] | Loss: 0.2930 | Train Acc: 96.21%
2026-02-10 02:15:01,533 - INFO - [Valid] [90/90] | Loss: 0.4992 | Val Acc: 82.01%
2026-02-10 02:15:01,536 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 02:15:01,536 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 02:15:01,537 - INFO - ==================================================
2026-02-10 02:15:01,537 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 02:15:01,537 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 02:15:01,537 - INFO - Baseline 모델 'xie2019'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 02:15:01,562 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 02:15:01,562 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:15:01,562 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:15:01,850 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.921357421875)에 맞춰 변경되었습니다.
2026-02-10 02:15:01,850 - INFO - ==================================================
2026-02-10 02:15:01,851 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/best_model.pth'
2026-02-10 02:15:01,851 - INFO - ==================================================
2026-02-10 02:15:01,851 - INFO - Test 모드를 시작합니다.
2026-02-10 02:15:01,868 - INFO - 연산량 (MACs): 0.0927 GMACs per sample
2026-02-10 02:15:01,868 - INFO - 연산량 (FLOPs): 0.1854 GFLOPs per sample
2026-02-10 02:15:01,868 - INFO - ==================================================
2026-02-10 02:15:01,868 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 02:15:02,095 - INFO - 샘플 당 평균 Forward Pass 시간: 0.11ms (std: 0.04ms), FPS: 9165.42 (std: 814.01) (1개 샘플 x 100회 반복)
2026-02-10 02:15:02,095 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 160.22 MB
2026-02-10 02:15:02,095 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 02:15:02,720 - INFO - [Test] Loss: 0.4135 | Test Acc: 82.89%
2026-02-10 02:15:02,723 - INFO - [Metrics for 'abnormal'] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-02-10 02:15:02,723 - INFO - [Metrics for 'normal'] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-02-10 02:15:02,844 - INFO - ==================================================
2026-02-10 02:15:02,844 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/confusion_matrix_20260210_021152.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/confusion_matrix_20260210_021152.pdf'
2026-02-10 02:15:02,844 - INFO - ==================================================
2026-02-10 02:15:02,844 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 02:15:02,862 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/model_fp32_20260210_021152.onnx'에 저장되었습니다. (크기: 0.22 MB)
2026-02-10 02:15:02,996 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 9.00 MB
2026-02-10 02:15:02,996 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 02:15:03,162 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 0.43ms (std: 0.27ms)
2026-02-10 02:15:03,162 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 2458.24 FPS (std: 275.27) (1개 샘플 x 100회 반복)
2026-02-10 02:15:03,162 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 9.00 MB
2026-02-10 02:15:03,162 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 18.25 MB
2026-02-10 02:15:03,742 - INFO - [Test (ONNX)] | Test Acc (ONNX): 82.89%
2026-02-10 02:15:03,745 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8113 | Recall: 0.8217 | F1: 0.8165
2026-02-10 02:15:03,745 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8444 | Recall: 0.8352 | F1: 0.8398
2026-02-10 02:15:03,847 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/val_acc.pdf'
2026-02-10 02:15:03,953 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/train_val_acc.pdf'
2026-02-10 02:15:04,040 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/F1_normal.pdf'
2026-02-10 02:15:04,134 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/val_loss.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/val_loss.pdf'
2026-02-10 02:15:04,224 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/learning_rate.pdf'
2026-02-10 02:15:05,169 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/compile.png' and 'log/Sewer-TAPNEW/baseline_xie2019_l1_20260210_021152/graph_20260210_021152/compile.pdf'
