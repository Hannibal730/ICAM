2026-02-10 01:58:20,510 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/log_20260210_015820.log'에 저장됩니다.
2026-02-10 01:58:20,512 - INFO - ==================================================
2026-02-10 01:58:20,512 - INFO - config.yaml:
2026-02-10 01:58:20,512 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: deit_tiny
  use_l1_pruning: true
  pruning_flops_target: 0.1816

2026-02-10 01:58:20,512 - INFO - ==================================================
2026-02-10 01:58:20,535 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:58:20,536 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:58:20,536 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:58:20,539 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:58:20,539 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:58:20,539 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:58:20,539 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:58:20,669 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:58:20,670 - INFO - ==================================================
2026-02-10 01:58:20,670 - INFO - 모델 파라미터 수:
2026-02-10 01:58:20,670 - INFO -   - 총 파라미터: 5,524,802 개
2026-02-10 01:58:20,670 - INFO -   - 학습 가능한 파라미터: 5,524,802 개
2026-02-10 01:58:20,670 - INFO - ================================================================================
2026-02-10 01:58:20,670 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:58:20,670 - INFO - ================================================================================
2026-02-10 01:58:20,670 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:58:20,671 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:58:20,671 - INFO - ==================================================
2026-02-10 01:58:20,671 - INFO - train 모드를 시작합니다.
2026-02-10 01:58:20,671 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:58:20,671 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:58:20,671 - INFO - --------------------------------------------------
2026-02-10 01:58:20,671 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:58:23,939 - INFO - [Train] [1/10] | Loss: 0.7051 | Train Acc: 60.49%
2026-02-10 01:58:25,228 - INFO - [Valid] [1/10] | Loss: 0.6724 | Val Acc: 59.88%
2026-02-10 01:58:25,234 - INFO - [Metrics for 'abnormal'] | Precision: 0.5488 | Recall: 0.7516 | F1: 0.6344
2026-02-10 01:58:25,235 - INFO - [Metrics for 'normal'] | Precision: 0.6855 | Recall: 0.4670 | F1: 0.5556
2026-02-10 01:58:25,271 - INFO - [Best Model Saved] (val loss: 0.6724) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:58:25,272 - INFO - --------------------------------------------------
2026-02-10 01:58:25,273 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:58:27,121 - INFO - [Train] [2/10] | Loss: 0.6464 | Train Acc: 64.43%
2026-02-10 01:58:27,536 - INFO - [Valid] [2/10] | Loss: 0.6629 | Val Acc: 61.65%
2026-02-10 01:58:27,539 - INFO - [Metrics for 'abnormal'] | Precision: 0.7547 | Recall: 0.2548 | F1: 0.3810
2026-02-10 01:58:27,539 - INFO - [Metrics for 'normal'] | Precision: 0.5909 | Recall: 0.9286 | F1: 0.7222
2026-02-10 01:58:27,571 - INFO - [Best Model Saved] (val loss: 0.6629) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:58:27,571 - INFO - --------------------------------------------------
2026-02-10 01:58:27,572 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:58:29,130 - INFO - [Train] [3/10] | Loss: 0.5956 | Train Acc: 68.90%
2026-02-10 01:58:29,549 - INFO - [Valid] [3/10] | Loss: 0.5660 | Val Acc: 74.93%
2026-02-10 01:58:29,552 - INFO - [Metrics for 'abnormal'] | Precision: 0.7727 | Recall: 0.6497 | F1: 0.7059
2026-02-10 01:58:29,552 - INFO - [Metrics for 'normal'] | Precision: 0.7343 | Recall: 0.8352 | F1: 0.7815
2026-02-10 01:58:29,574 - INFO - [Best Model Saved] (val loss: 0.5660) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:58:29,574 - INFO - --------------------------------------------------
2026-02-10 01:58:29,574 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:58:31,065 - INFO - [Train] [4/10] | Loss: 0.5630 | Train Acc: 75.82%
2026-02-10 01:58:31,486 - INFO - [Valid] [4/10] | Loss: 0.5505 | Val Acc: 74.04%
2026-02-10 01:58:31,489 - INFO - [Metrics for 'abnormal'] | Precision: 0.7117 | Recall: 0.7389 | F1: 0.7250
2026-02-10 01:58:31,489 - INFO - [Metrics for 'normal'] | Precision: 0.7670 | Recall: 0.7418 | F1: 0.7542
2026-02-10 01:58:31,510 - INFO - [Best Model Saved] (val loss: 0.5505) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:58:31,510 - INFO - --------------------------------------------------
2026-02-10 01:58:31,511 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:58:33,007 - INFO - [Train] [5/10] | Loss: 0.5155 | Train Acc: 78.94%
2026-02-10 01:58:33,432 - INFO - [Valid] [5/10] | Loss: 0.5579 | Val Acc: 74.63%
2026-02-10 01:58:33,434 - INFO - [Metrics for 'abnormal'] | Precision: 0.7126 | Recall: 0.7580 | F1: 0.7346
2026-02-10 01:58:33,435 - INFO - [Metrics for 'normal'] | Precision: 0.7791 | Recall: 0.7363 | F1: 0.7571
2026-02-10 01:58:33,435 - INFO - --------------------------------------------------
2026-02-10 01:58:33,436 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:58:34,974 - INFO - [Train] [6/10] | Loss: 0.5177 | Train Acc: 78.50%
2026-02-10 01:58:35,393 - INFO - [Valid] [6/10] | Loss: 0.5393 | Val Acc: 74.63%
2026-02-10 01:58:35,396 - INFO - [Metrics for 'abnormal'] | Precision: 0.6940 | Recall: 0.8089 | F1: 0.7471
2026-02-10 01:58:35,396 - INFO - [Metrics for 'normal'] | Precision: 0.8077 | Recall: 0.6923 | F1: 0.7456
2026-02-10 01:58:35,416 - INFO - [Best Model Saved] (val loss: 0.5393) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:58:35,416 - INFO - --------------------------------------------------
2026-02-10 01:58:35,416 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:58:36,920 - INFO - [Train] [7/10] | Loss: 0.4901 | Train Acc: 81.32%
2026-02-10 01:58:37,341 - INFO - [Valid] [7/10] | Loss: 0.5521 | Val Acc: 76.99%
2026-02-10 01:58:37,344 - INFO - [Metrics for 'abnormal'] | Precision: 0.8264 | Recall: 0.6369 | F1: 0.7194
2026-02-10 01:58:37,344 - INFO - [Metrics for 'normal'] | Precision: 0.7385 | Recall: 0.8846 | F1: 0.8050
2026-02-10 01:58:37,345 - INFO - --------------------------------------------------
2026-02-10 01:58:37,345 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:58:38,910 - INFO - [Train] [8/10] | Loss: 0.4861 | Train Acc: 81.70%
2026-02-10 01:58:39,328 - INFO - [Valid] [8/10] | Loss: 0.5293 | Val Acc: 77.29%
2026-02-10 01:58:39,330 - INFO - [Metrics for 'abnormal'] | Precision: 0.8175 | Recall: 0.6561 | F1: 0.7279
2026-02-10 01:58:39,331 - INFO - [Metrics for 'normal'] | Precision: 0.7465 | Recall: 0.8736 | F1: 0.8051
2026-02-10 01:58:39,364 - INFO - [Best Model Saved] (val loss: 0.5293) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:58:39,364 - INFO - --------------------------------------------------
2026-02-10 01:58:39,364 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:58:40,914 - INFO - [Train] [9/10] | Loss: 0.4830 | Train Acc: 80.88%
2026-02-10 01:58:41,324 - INFO - [Valid] [9/10] | Loss: 0.5372 | Val Acc: 76.40%
2026-02-10 01:58:41,326 - INFO - [Metrics for 'abnormal'] | Precision: 0.7655 | Recall: 0.7070 | F1: 0.7351
2026-02-10 01:58:41,326 - INFO - [Metrics for 'normal'] | Precision: 0.7629 | Recall: 0.8132 | F1: 0.7872
2026-02-10 01:58:41,327 - INFO - --------------------------------------------------
2026-02-10 01:58:41,328 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:58:42,850 - INFO - [Train] [10/10] | Loss: 0.4752 | Train Acc: 82.29%
2026-02-10 01:58:43,276 - INFO - [Valid] [10/10] | Loss: 0.5302 | Val Acc: 76.70%
2026-02-10 01:58:43,279 - INFO - [Metrics for 'abnormal'] | Precision: 0.7378 | Recall: 0.7707 | F1: 0.7539
2026-02-10 01:58:43,279 - INFO - [Metrics for 'normal'] | Precision: 0.7943 | Recall: 0.7637 | F1: 0.7787
2026-02-10 01:58:43,280 - INFO - ================================================================================
2026-02-10 01:58:43,280 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:58:43,280 - INFO - ================================================================================
2026-02-10 01:58:43,303 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:58:43,303 - INFO - ================================================================================
2026-02-10 01:58:43,303 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:58:43,316 - INFO - 원본 모델 FLOPs: 2.1493 GFLOPs
2026-02-10 01:58:43,337 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:43,337 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:43,337 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:43,465 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:58:43,465 - INFO - ==================================================
2026-02-10 01:58:43,479 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.7288 GFLOPs (감소율: 66.09%)
2026-02-10 01:58:43,495 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:43,495 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:43,495 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:43,953 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.7424999999999999)에 맞춰 변경되었습니다.
2026-02-10 01:58:43,953 - INFO - ==================================================
2026-02-10 01:58:43,974 - INFO -   [탐색  2] 희소도: 0.7425 -> FLOPs: 0.2840 GFLOPs (감소율: 86.79%)
2026-02-10 01:58:43,995 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:43,995 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:43,996 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:44,186 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.86625)에 맞춰 변경되었습니다.
2026-02-10 01:58:44,186 - INFO - ==================================================
2026-02-10 01:58:44,206 - INFO -   [탐색  3] 희소도: 0.8662 -> FLOPs: 0.1224 GFLOPs (감소율: 94.30%)
2026-02-10 01:58:44,227 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:44,228 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:44,228 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:44,389 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.804375)에 맞춰 변경되었습니다.
2026-02-10 01:58:44,389 - INFO - ==================================================
2026-02-10 01:58:44,404 - INFO -   [탐색  4] 희소도: 0.8044 -> FLOPs: 0.1980 GFLOPs (감소율: 90.79%)
2026-02-10 01:58:44,423 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:44,423 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:44,423 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:44,583 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8353124999999999)에 맞춰 변경되었습니다.
2026-02-10 01:58:44,583 - INFO - ==================================================
2026-02-10 01:58:44,599 - INFO -   [탐색  5] 희소도: 0.8353 -> FLOPs: 0.1588 GFLOPs (감소율: 92.61%)
2026-02-10 01:58:44,618 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:44,618 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:44,618 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:44,730 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81984375)에 맞춰 변경되었습니다.
2026-02-10 01:58:44,730 - INFO - ==================================================
2026-02-10 01:58:44,741 - INFO -   [탐색  6] 희소도: 0.8198 -> FLOPs: 0.1781 GFLOPs (감소율: 91.72%)
2026-02-10 01:58:44,755 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:44,755 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:44,755 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,048 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8121093749999999)에 맞춰 변경되었습니다.
2026-02-10 01:58:45,048 - INFO - ==================================================
2026-02-10 01:58:45,063 - INFO -   [탐색  7] 희소도: 0.8121 -> FLOPs: 0.1906 GFLOPs (감소율: 91.13%)
2026-02-10 01:58:45,078 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:45,079 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,079 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,208 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8159765625)에 맞춰 변경되었습니다.
2026-02-10 01:58:45,209 - INFO - ==================================================
2026-02-10 01:58:45,220 - INFO -   [탐색  8] 희소도: 0.8160 -> FLOPs: 0.1843 GFLOPs (감소율: 91.43%)
2026-02-10 01:58:45,232 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:45,232 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,232 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,336 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81791015625)에 맞춰 변경되었습니다.
2026-02-10 01:58:45,336 - INFO - ==================================================
2026-02-10 01:58:45,346 - INFO -   [탐색  9] 희소도: 0.8179 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:45,357 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:45,357 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,357 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,460 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-02-10 01:58:45,460 - INFO - ==================================================
2026-02-10 01:58:45,472 - INFO -   [탐색 10] 희소도: 0.8169 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:45,486 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:45,486 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,487 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,599 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8174267578125)에 맞춰 변경되었습니다.
2026-02-10 01:58:45,600 - INFO - ==================================================
2026-02-10 01:58:45,614 - INFO -   [탐색 11] 희소도: 0.8174 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:45,637 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:45,637 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:45,637 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,123 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81766845703125)에 맞춰 변경되었습니다.
2026-02-10 01:58:46,123 - INFO - ==================================================
2026-02-10 01:58:46,144 - INFO -   [탐색 12] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:46,169 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:46,170 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,170 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,398 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.817789306640625)에 맞춰 변경되었습니다.
2026-02-10 01:58:46,398 - INFO - ==================================================
2026-02-10 01:58:46,413 - INFO -   [탐색 13] 희소도: 0.8178 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:46,430 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:46,431 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,431 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,564 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177288818359375)에 맞춰 변경되었습니다.
2026-02-10 01:58:46,564 - INFO - ==================================================
2026-02-10 01:58:46,579 - INFO -   [탐색 14] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:46,594 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:46,595 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,595 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,725 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8176986694335937)에 맞춰 변경되었습니다.
2026-02-10 01:58:46,725 - INFO - ==================================================
2026-02-10 01:58:46,736 - INFO -   [탐색 15] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:46,751 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:46,751 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,751 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,877 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177137756347657)에 맞춰 변경되었습니다.
2026-02-10 01:58:46,877 - INFO - ==================================================
2026-02-10 01:58:46,887 - INFO -   [탐색 16] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:46,898 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:46,898 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:46,898 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:47,304 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177062225341797)에 맞춰 변경되었습니다.
2026-02-10 01:58:47,305 - INFO - ==================================================
2026-02-10 01:58:47,317 - INFO -   [탐색 17] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:47,329 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:47,329 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:47,329 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:47,473 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177099990844727)에 맞춰 변경되었습니다.
2026-02-10 01:58:47,474 - INFO - ==================================================
2026-02-10 01:58:47,493 - INFO -   [탐색 18] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:47,514 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:47,514 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:47,515 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:47,732 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177081108093263)에 맞춰 변경되었습니다.
2026-02-10 01:58:47,732 - INFO - ==================================================
2026-02-10 01:58:47,751 - INFO -   [탐색 19] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:47,772 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:47,772 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:47,772 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:47,992 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177090549468995)에 맞춰 변경되었습니다.
2026-02-10 01:58:47,992 - INFO - ==================================================
2026-02-10 01:58:48,011 - INFO -   [탐색 20] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:48,035 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:48,035 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:48,035 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:48,247 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177085828781129)에 맞춰 변경되었습니다.
2026-02-10 01:58:48,247 - INFO - ==================================================
2026-02-10 01:58:48,266 - INFO -   [탐색 21] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:48,288 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:48,288 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:48,289 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:48,799 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083468437196)에 맞춰 변경되었습니다.
2026-02-10 01:58:48,799 - INFO - ==================================================
2026-02-10 01:58:48,809 - INFO -   [탐색 22] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:48,821 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:48,821 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:48,821 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:48,927 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082288265229)에 맞춰 변경되었습니다.
2026-02-10 01:58:48,927 - INFO - ==================================================
2026-02-10 01:58:48,940 - INFO -   [탐색 23] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:48,961 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:48,961 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:48,962 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:49,115 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177082878351213)에 맞춰 변경되었습니다.
2026-02-10 01:58:49,115 - INFO - ==================================================
2026-02-10 01:58:49,126 - INFO -   [탐색 24] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:49,138 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:49,138 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:49,139 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:49,240 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083173394204)에 맞춰 변경되었습니다.
2026-02-10 01:58:49,240 - INFO - ==================================================
2026-02-10 01:58:49,252 - INFO -   [탐색 25] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:49,264 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:49,264 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:49,264 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:49,366 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833209157)에 맞춰 변경되었습니다.
2026-02-10 01:58:49,366 - INFO - ==================================================
2026-02-10 01:58:49,377 - INFO -   [탐색 26] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:49,391 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:49,391 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:49,391 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:49,845 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083394676448)에 맞춰 변경되었습니다.
2026-02-10 01:58:49,846 - INFO - ==================================================
2026-02-10 01:58:49,863 - INFO -   [탐색 27] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:49,881 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:49,881 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:49,881 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,061 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083357796073)에 맞춰 변경되었습니다.
2026-02-10 01:58:50,062 - INFO - ==================================================
2026-02-10 01:58:50,081 - INFO -   [탐색 28] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:50,104 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:50,105 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,105 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,327 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083339355886)에 맞춰 변경되었습니다.
2026-02-10 01:58:50,327 - INFO - ==================================================
2026-02-10 01:58:50,341 - INFO -   [탐색 29] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:50,356 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:50,356 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,356 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,487 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083330135793)에 맞춰 변경되었습니다.
2026-02-10 01:58:50,487 - INFO - ==================================================
2026-02-10 01:58:50,500 - INFO -   [탐색 30] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:50,516 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:50,517 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,517 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,638 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083334745839)에 맞춰 변경되었습니다.
2026-02-10 01:58:50,638 - INFO - ==================================================
2026-02-10 01:58:50,648 - INFO -   [탐색 31] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:50,661 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:50,661 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,661 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,951 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083332440815)에 맞춰 변경되었습니다.
2026-02-10 01:58:50,951 - INFO - ==================================================
2026-02-10 01:58:50,966 - INFO -   [탐색 32] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:50,988 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:50,988 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:50,988 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:51,139 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333593327)에 맞춰 변경되었습니다.
2026-02-10 01:58:51,139 - INFO - ==================================================
2026-02-10 01:58:51,149 - INFO -   [탐색 33] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:51,162 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:51,162 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:51,162 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:51,266 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333017071)에 맞춰 변경되었습니다.
2026-02-10 01:58:51,266 - INFO - ==================================================
2026-02-10 01:58:51,276 - INFO -   [탐색 34] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:51,290 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:51,290 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:51,291 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:51,418 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.81770833333052)에 맞춰 변경되었습니다.
2026-02-10 01:58:51,418 - INFO - ==================================================
2026-02-10 01:58:51,437 - INFO -   [탐색 35] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:51,458 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:51,458 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:51,458 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:51,626 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333449263)에 맞춰 변경되었습니다.
2026-02-10 01:58:51,626 - INFO - ==================================================
2026-02-10 01:58:51,639 - INFO -   [탐색 36] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:51,653 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:51,653 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:51,653 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,184 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333377231)에 맞춰 변경되었습니다.
2026-02-10 01:58:52,184 - INFO - ==================================================
2026-02-10 01:58:52,205 - INFO -   [탐색 37] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:52,229 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:52,229 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,230 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,434 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333341215)에 맞춰 변경되었습니다.
2026-02-10 01:58:52,434 - INFO - ==================================================
2026-02-10 01:58:52,453 - INFO -   [탐색 38] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:52,475 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:52,475 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,475 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,609 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333323207)에 맞춰 변경되었습니다.
2026-02-10 01:58:52,609 - INFO - ==================================================
2026-02-10 01:58:52,619 - INFO -   [탐색 39] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:52,631 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:52,631 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,632 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,757 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332211)에 맞춰 변경되었습니다.
2026-02-10 01:58:52,757 - INFO - ==================================================
2026-02-10 01:58:52,767 - INFO -   [탐색 40] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:52,780 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:52,780 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,780 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,886 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333336713)에 맞춰 변경되었습니다.
2026-02-10 01:58:52,886 - INFO - ==================================================
2026-02-10 01:58:52,898 - INFO -   [탐색 41] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:52,912 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:52,913 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:52,913 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:53,279 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333334463)에 맞춰 변경되었습니다.
2026-02-10 01:58:53,279 - INFO - ==================================================
2026-02-10 01:58:53,298 - INFO -   [탐색 42] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:53,320 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:53,320 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:53,320 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:53,510 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333337)에 맞춰 변경되었습니다.
2026-02-10 01:58:53,510 - INFO - ==================================================
2026-02-10 01:58:53,532 - INFO -   [탐색 43] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:53,554 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:53,555 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:53,555 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:53,728 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333332774)에 맞춰 변경되었습니다.
2026-02-10 01:58:53,728 - INFO - ==================================================
2026-02-10 01:58:53,745 - INFO -   [탐색 44] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:53,763 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:53,763 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:53,764 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:53,913 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333055)에 맞춰 변경되었습니다.
2026-02-10 01:58:53,914 - INFO - ==================================================
2026-02-10 01:58:53,930 - INFO -   [탐색 45] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:53,948 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:53,948 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:53,949 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:54,077 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333196)에 맞춰 변경되었습니다.
2026-02-10 01:58:54,077 - INFO - ==================================================
2026-02-10 01:58:54,096 - INFO -   [탐색 46] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:54,111 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:54,112 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:54,112 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:54,577 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333266)에 맞춰 변경되었습니다.
2026-02-10 01:58:54,577 - INFO - ==================================================
2026-02-10 01:58:54,588 - INFO -   [탐색 47] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:54,601 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:54,601 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:54,601 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:54,744 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333302)에 맞춰 변경되었습니다.
2026-02-10 01:58:54,744 - INFO - ==================================================
2026-02-10 01:58:54,755 - INFO -   [탐색 48] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:54,766 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:54,766 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:54,766 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:54,876 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333319)에 맞춰 변경되었습니다.
2026-02-10 01:58:54,877 - INFO - ==================================================
2026-02-10 01:58:54,890 - INFO -   [탐색 49] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:54,904 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:54,905 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:54,905 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,019 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333328)에 맞춰 변경되었습니다.
2026-02-10 01:58:55,019 - INFO - ==================================================
2026-02-10 01:58:55,031 - INFO -   [탐색 50] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:55,047 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:55,047 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,047 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,151 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:55,151 - INFO - ==================================================
2026-02-10 01:58:55,162 - INFO -   [탐색 51] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:55,175 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:55,175 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,176 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,532 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333335)에 맞춰 변경되었습니다.
2026-02-10 01:58:55,533 - INFO - ==================================================
2026-02-10 01:58:55,550 - INFO -   [탐색 52] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:55,573 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:55,573 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,573 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,748 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333334)에 맞춰 변경되었습니다.
2026-02-10 01:58:55,748 - INFO - ==================================================
2026-02-10 01:58:55,765 - INFO -   [탐색 53] 희소도: 0.8177 -> FLOPs: 0.1784 GFLOPs (감소율: 91.70%)
2026-02-10 01:58:55,781 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:55,782 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,782 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,954 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:55,955 - INFO - ==================================================
2026-02-10 01:58:55,969 - INFO -   [탐색 54] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:55,984 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:55,984 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:55,984 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:56,135 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:56,135 - INFO - ==================================================
2026-02-10 01:58:56,152 - INFO -   [탐색 55] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:56,171 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:56,172 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:56,172 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:56,326 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:56,326 - INFO - ==================================================
2026-02-10 01:58:56,346 - INFO -   [탐색 56] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:56,367 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:56,368 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:56,368 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:56,756 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:56,756 - INFO - ==================================================
2026-02-10 01:58:56,766 - INFO -   [탐색 57] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:56,778 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:56,779 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:56,779 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:56,886 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:56,886 - INFO - ==================================================
2026-02-10 01:58:56,900 - INFO -   [탐색 58] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:56,916 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:56,916 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:56,916 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:57,029 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:57,030 - INFO - ==================================================
2026-02-10 01:58:57,043 - INFO -   [탐색 59] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:57,055 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:57,055 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:57,056 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:57,156 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:57,156 - INFO - ==================================================
2026-02-10 01:58:57,167 - INFO -   [탐색 60] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:57,181 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:57,181 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:57,181 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:57,282 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:57,282 - INFO - ==================================================
2026-02-10 01:58:57,294 - INFO -   [탐색 61] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:57,309 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:57,309 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:57,309 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:57,813 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:57,813 - INFO - ==================================================
2026-02-10 01:58:57,826 - INFO -   [탐색 62] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:57,841 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:57,841 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:57,841 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,000 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:58,001 - INFO - ==================================================
2026-02-10 01:58:58,014 - INFO -   [탐색 63] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:58,030 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:58,031 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,031 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,191 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:58,192 - INFO - ==================================================
2026-02-10 01:58:58,209 - INFO -   [탐색 64] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:58,230 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:58,230 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,231 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,382 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:58,382 - INFO - ==================================================
2026-02-10 01:58:58,400 - INFO -   [탐색 65] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:58,420 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:58,421 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,421 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,572 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:58,572 - INFO - ==================================================
2026-02-10 01:58:58,583 - INFO -   [탐색 66] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:58,596 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:58,596 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,596 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,943 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:58,943 - INFO - ==================================================
2026-02-10 01:58:58,959 - INFO -   [탐색 67] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:58,975 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:58,975 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:58,976 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:59,079 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:59,079 - INFO - ==================================================
2026-02-10 01:58:59,090 - INFO -   [탐색 68] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:59,103 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:59,103 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:59,103 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:59,250 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:59,250 - INFO - ==================================================
2026-02-10 01:58:59,268 - INFO -   [탐색 69] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:59,289 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:59,290 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:59,290 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:59,465 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:59,466 - INFO - ==================================================
2026-02-10 01:58:59,479 - INFO -   [탐색 70] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:59,493 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:59,494 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:59,494 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:58:59,642 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:58:59,643 - INFO - ==================================================
2026-02-10 01:58:59,656 - INFO -   [탐색 71] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:58:59,675 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:58:59,675 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:58:59,675 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,162 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:00,162 - INFO - ==================================================
2026-02-10 01:59:00,180 - INFO -   [탐색 72] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:00,196 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:00,196 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,196 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,337 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:00,337 - INFO - ==================================================
2026-02-10 01:59:00,350 - INFO -   [탐색 73] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:00,365 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:00,365 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,366 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,500 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:00,500 - INFO - ==================================================
2026-02-10 01:59:00,512 - INFO -   [탐색 74] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:00,524 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:00,524 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,525 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,651 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:00,651 - INFO - ==================================================
2026-02-10 01:59:00,660 - INFO -   [탐색 75] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:00,671 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:00,672 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,672 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,772 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:00,772 - INFO - ==================================================
2026-02-10 01:59:00,784 - INFO -   [탐색 76] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:00,800 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:00,800 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:00,800 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:01,164 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:01,165 - INFO - ==================================================
2026-02-10 01:59:01,178 - INFO -   [탐색 77] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:01,193 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:01,194 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:01,194 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:01,308 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:01,308 - INFO - ==================================================
2026-02-10 01:59:01,322 - INFO -   [탐색 78] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:01,338 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:01,338 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:01,338 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:01,560 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:01,560 - INFO - ==================================================
2026-02-10 01:59:01,580 - INFO -   [탐색 79] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:01,601 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:01,602 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:01,602 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:01,816 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:01,816 - INFO - ==================================================
2026-02-10 01:59:01,829 - INFO -   [탐색 80] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:01,846 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:01,846 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:01,847 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:01,987 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:01,987 - INFO - ==================================================
2026-02-10 01:59:02,000 - INFO -   [탐색 81] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:02,015 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:02,015 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:02,015 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:02,476 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:02,477 - INFO - ==================================================
2026-02-10 01:59:02,488 - INFO -   [탐색 82] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:02,501 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:02,501 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:02,502 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:02,604 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:02,604 - INFO - ==================================================
2026-02-10 01:59:02,614 - INFO -   [탐색 83] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:02,625 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:02,625 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:02,625 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:02,729 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:02,729 - INFO - ==================================================
2026-02-10 01:59:02,742 - INFO -   [탐색 84] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:02,756 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:02,756 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:02,756 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:02,884 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:02,884 - INFO - ==================================================
2026-02-10 01:59:02,896 - INFO -   [탐색 85] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:02,908 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:02,908 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:02,909 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:03,010 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:03,010 - INFO - ==================================================
2026-02-10 01:59:03,020 - INFO -   [탐색 86] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:03,033 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:03,034 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:03,034 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:03,508 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:03,509 - INFO - ==================================================
2026-02-10 01:59:03,528 - INFO -   [탐색 87] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:03,544 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:03,544 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:03,544 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:03,701 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:03,701 - INFO - ==================================================
2026-02-10 01:59:03,719 - INFO -   [탐색 88] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:03,741 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:03,741 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:03,741 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:03,946 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:03,946 - INFO - ==================================================
2026-02-10 01:59:03,966 - INFO -   [탐색 89] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:03,989 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:03,989 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:03,990 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:04,210 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:04,211 - INFO - ==================================================
2026-02-10 01:59:04,229 - INFO -   [탐색 90] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:04,251 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:04,251 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:04,251 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:04,386 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:04,386 - INFO - ==================================================
2026-02-10 01:59:04,398 - INFO -   [탐색 91] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:04,411 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:04,412 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:04,412 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:04,816 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:04,816 - INFO - ==================================================
2026-02-10 01:59:04,828 - INFO -   [탐색 92] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:04,841 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:04,841 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:04,842 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:04,948 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:04,949 - INFO - ==================================================
2026-02-10 01:59:04,959 - INFO -   [탐색 93] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:04,971 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:04,971 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:04,971 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:05,081 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:05,081 - INFO - ==================================================
2026-02-10 01:59:05,094 - INFO -   [탐색 94] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:05,108 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:05,109 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:05,109 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:05,222 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:05,222 - INFO - ==================================================
2026-02-10 01:59:05,236 - INFO -   [탐색 95] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:05,257 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:05,257 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:05,257 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:05,406 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:05,406 - INFO - ==================================================
2026-02-10 01:59:05,419 - INFO -   [탐색 96] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:05,434 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:05,434 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:05,435 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:05,988 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:05,988 - INFO - ==================================================
2026-02-10 01:59:06,002 - INFO -   [탐색 97] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:06,018 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:06,018 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:06,018 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:06,159 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:06,159 - INFO - ==================================================
2026-02-10 01:59:06,172 - INFO -   [탐색 98] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:06,191 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:06,191 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:06,192 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:06,317 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:06,317 - INFO - ==================================================
2026-02-10 01:59:06,329 - INFO -   [탐색 99] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:06,343 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:06,343 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:06,344 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:06,473 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.8177083333333333)에 맞춰 변경되었습니다.
2026-02-10 01:59:06,473 - INFO - ==================================================
2026-02-10 01:59:06,484 - INFO -   [탐색 100] 희소도: 0.8177 -> FLOPs: 0.1840 GFLOPs (감소율: 91.44%)
2026-02-10 01:59:06,484 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.8169 입니다.
2026-02-10 01:59:06,484 - INFO - ================================================================================
2026-02-10 01:59:06,485 - INFO - 계산된 Pruning 정보(희소도: 0.8169)를 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:59:06,495 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:59:06,515 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:59:06,515 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:59:06,515 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:59:06,629 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-02-10 01:59:06,630 - INFO - ==================================================
2026-02-10 01:59:06,630 - INFO - ==================================================
2026-02-10 01:59:06,630 - INFO - 모델 파라미터 수:
2026-02-10 01:59:06,630 - INFO -   - 총 파라미터: 485,259 개
2026-02-10 01:59:06,630 - INFO -   - 학습 가능한 파라미터: 485,259 개
2026-02-10 01:59:06,643 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:59:06,672 - INFO - FLOPs가 2.1493 GFLOPs에서 0.1840 GFLOPs로 감소했습니다 (감소율: 91.44%).
2026-02-10 01:59:06,672 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:59:06,672 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:59:06,673 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:59:06,673 - INFO - ==================================================
2026-02-10 01:59:06,673 - INFO - train 모드를 시작합니다.
2026-02-10 01:59:06,673 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:59:06,673 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:59:06,673 - INFO - --------------------------------------------------
2026-02-10 01:59:06,673 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:59:09,347 - INFO - [Train] [11/90] | Loss: 0.5270 | Train Acc: 78.05%
2026-02-10 01:59:09,881 - INFO - [Valid] [11/90] | Loss: 0.5604 | Val Acc: 75.22%
2026-02-10 01:59:09,883 - INFO - [Metrics for 'abnormal'] | Precision: 0.7744 | Recall: 0.6561 | F1: 0.7103
2026-02-10 01:59:09,883 - INFO - [Metrics for 'normal'] | Precision: 0.7379 | Recall: 0.8352 | F1: 0.7835
2026-02-10 01:59:09,904 - INFO - [Best Model Saved] (val loss: 0.5604) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:59:09,904 - INFO - --------------------------------------------------
2026-02-10 01:59:09,905 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:59:12,581 - INFO - [Train] [12/90] | Loss: 0.4988 | Train Acc: 80.65%
2026-02-10 01:59:13,151 - INFO - [Valid] [12/90] | Loss: 0.5585 | Val Acc: 75.22%
2026-02-10 01:59:13,155 - INFO - [Metrics for 'abnormal'] | Precision: 0.7786 | Recall: 0.6497 | F1: 0.7083
2026-02-10 01:59:13,155 - INFO - [Metrics for 'normal'] | Precision: 0.7356 | Recall: 0.8407 | F1: 0.7846
2026-02-10 01:59:13,169 - INFO - [Best Model Saved] (val loss: 0.5585) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:59:13,169 - INFO - --------------------------------------------------
2026-02-10 01:59:13,170 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:59:15,773 - INFO - [Train] [13/90] | Loss: 0.4966 | Train Acc: 80.28%
2026-02-10 01:59:16,359 - INFO - [Valid] [13/90] | Loss: 0.5398 | Val Acc: 76.70%
2026-02-10 01:59:16,363 - INFO - [Metrics for 'abnormal'] | Precision: 0.7600 | Recall: 0.7261 | F1: 0.7427
2026-02-10 01:59:16,363 - INFO - [Metrics for 'normal'] | Precision: 0.7725 | Recall: 0.8022 | F1: 0.7871
2026-02-10 01:59:16,375 - INFO - [Best Model Saved] (val loss: 0.5398) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:59:16,375 - INFO - --------------------------------------------------
2026-02-10 01:59:16,376 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:59:18,908 - INFO - [Train] [14/90] | Loss: 0.4900 | Train Acc: 80.58%
2026-02-10 01:59:19,670 - INFO - [Valid] [14/90] | Loss: 0.5472 | Val Acc: 76.40%
2026-02-10 01:59:19,675 - INFO - [Metrics for 'abnormal'] | Precision: 0.7278 | Recall: 0.7834 | F1: 0.7546
2026-02-10 01:59:19,675 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.7473 | F1: 0.7727
2026-02-10 01:59:19,676 - INFO - --------------------------------------------------
2026-02-10 01:59:19,678 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:59:22,162 - INFO - [Train] [15/90] | Loss: 0.4857 | Train Acc: 80.36%
2026-02-10 01:59:22,927 - INFO - [Valid] [15/90] | Loss: 0.5543 | Val Acc: 75.22%
2026-02-10 01:59:22,931 - INFO - [Metrics for 'abnormal'] | Precision: 0.7267 | Recall: 0.7452 | F1: 0.7358
2026-02-10 01:59:22,931 - INFO - [Metrics for 'normal'] | Precision: 0.7753 | Recall: 0.7582 | F1: 0.7667
2026-02-10 01:59:22,933 - INFO - --------------------------------------------------
2026-02-10 01:59:22,934 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:59:25,400 - INFO - [Train] [16/90] | Loss: 0.4841 | Train Acc: 81.99%
2026-02-10 01:59:26,133 - INFO - [Valid] [16/90] | Loss: 0.5387 | Val Acc: 76.40%
2026-02-10 01:59:26,137 - INFO - [Metrics for 'abnormal'] | Precision: 0.7200 | Recall: 0.8025 | F1: 0.7590
2026-02-10 01:59:26,137 - INFO - [Metrics for 'normal'] | Precision: 0.8110 | Recall: 0.7308 | F1: 0.7688
2026-02-10 01:59:26,162 - INFO - [Best Model Saved] (val loss: 0.5387) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:59:26,162 - INFO - --------------------------------------------------
2026-02-10 01:59:26,163 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:59:28,645 - INFO - [Train] [17/90] | Loss: 0.4706 | Train Acc: 82.51%
2026-02-10 01:59:29,263 - INFO - [Valid] [17/90] | Loss: 0.5727 | Val Acc: 73.45%
2026-02-10 01:59:29,266 - INFO - [Metrics for 'abnormal'] | Precision: 0.6791 | Recall: 0.8089 | F1: 0.7384
2026-02-10 01:59:29,266 - INFO - [Metrics for 'normal'] | Precision: 0.8026 | Recall: 0.6703 | F1: 0.7305
2026-02-10 01:59:29,267 - INFO - --------------------------------------------------
2026-02-10 01:59:29,267 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:59:32,007 - INFO - [Train] [18/90] | Loss: 0.4741 | Train Acc: 82.59%
2026-02-10 01:59:32,555 - INFO - [Valid] [18/90] | Loss: 0.5377 | Val Acc: 76.11%
2026-02-10 01:59:32,558 - INFO - [Metrics for 'abnormal'] | Precision: 0.7235 | Recall: 0.7834 | F1: 0.7523
2026-02-10 01:59:32,558 - INFO - [Metrics for 'normal'] | Precision: 0.7988 | Recall: 0.7418 | F1: 0.7692
2026-02-10 01:59:32,566 - INFO - [Best Model Saved] (val loss: 0.5377) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:59:32,566 - INFO - --------------------------------------------------
2026-02-10 01:59:32,567 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:59:35,277 - INFO - [Train] [19/90] | Loss: 0.4653 | Train Acc: 82.81%
2026-02-10 01:59:35,824 - INFO - [Valid] [19/90] | Loss: 0.5309 | Val Acc: 76.40%
2026-02-10 01:59:35,828 - INFO - [Metrics for 'abnormal'] | Precision: 0.7362 | Recall: 0.7643 | F1: 0.7500
2026-02-10 01:59:35,829 - INFO - [Metrics for 'normal'] | Precision: 0.7898 | Recall: 0.7637 | F1: 0.7765
2026-02-10 01:59:35,844 - INFO - [Best Model Saved] (val loss: 0.5309) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:59:35,844 - INFO - --------------------------------------------------
2026-02-10 01:59:35,845 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:59:38,427 - INFO - [Train] [20/90] | Loss: 0.4725 | Train Acc: 82.74%
2026-02-10 01:59:39,039 - INFO - [Valid] [20/90] | Loss: 0.5347 | Val Acc: 77.58%
2026-02-10 01:59:39,043 - INFO - [Metrics for 'abnormal'] | Precision: 0.7455 | Recall: 0.7834 | F1: 0.7640
2026-02-10 01:59:39,044 - INFO - [Metrics for 'normal'] | Precision: 0.8046 | Recall: 0.7692 | F1: 0.7865
2026-02-10 01:59:39,045 - INFO - --------------------------------------------------
2026-02-10 01:59:39,046 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:59:41,580 - INFO - [Train] [21/90] | Loss: 0.4630 | Train Acc: 82.51%
2026-02-10 01:59:42,376 - INFO - [Valid] [21/90] | Loss: 0.5429 | Val Acc: 76.99%
2026-02-10 01:59:42,380 - INFO - [Metrics for 'abnormal'] | Precision: 0.7423 | Recall: 0.7707 | F1: 0.7562
2026-02-10 01:59:42,380 - INFO - [Metrics for 'normal'] | Precision: 0.7955 | Recall: 0.7692 | F1: 0.7821
2026-02-10 01:59:42,382 - INFO - --------------------------------------------------
2026-02-10 01:59:42,383 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:59:44,963 - INFO - [Train] [22/90] | Loss: 0.4664 | Train Acc: 82.89%
2026-02-10 01:59:45,701 - INFO - [Valid] [22/90] | Loss: 0.5230 | Val Acc: 78.76%
2026-02-10 01:59:45,706 - INFO - [Metrics for 'abnormal'] | Precision: 0.7742 | Recall: 0.7643 | F1: 0.7692
2026-02-10 01:59:45,706 - INFO - [Metrics for 'normal'] | Precision: 0.7989 | Recall: 0.8077 | F1: 0.8033
2026-02-10 01:59:45,720 - INFO - [Best Model Saved] (val loss: 0.5230) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:59:45,720 - INFO - --------------------------------------------------
2026-02-10 01:59:45,721 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:59:48,196 - INFO - [Train] [23/90] | Loss: 0.4679 | Train Acc: 82.81%
2026-02-10 01:59:48,903 - INFO - [Valid] [23/90] | Loss: 0.5491 | Val Acc: 76.11%
2026-02-10 01:59:48,906 - INFO - [Metrics for 'abnormal'] | Precision: 0.7676 | Recall: 0.6943 | F1: 0.7291
2026-02-10 01:59:48,906 - INFO - [Metrics for 'normal'] | Precision: 0.7563 | Recall: 0.8187 | F1: 0.7863
2026-02-10 01:59:48,907 - INFO - --------------------------------------------------
2026-02-10 01:59:48,907 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:59:51,570 - INFO - [Train] [24/90] | Loss: 0.4645 | Train Acc: 82.51%
2026-02-10 01:59:52,073 - INFO - [Valid] [24/90] | Loss: 0.5049 | Val Acc: 78.17%
2026-02-10 01:59:52,078 - INFO - [Metrics for 'abnormal'] | Precision: 0.7677 | Recall: 0.7580 | F1: 0.7628
2026-02-10 01:59:52,078 - INFO - [Metrics for 'normal'] | Precision: 0.7935 | Recall: 0.8022 | F1: 0.7978
2026-02-10 01:59:52,102 - INFO - [Best Model Saved] (val loss: 0.5049) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 01:59:52,102 - INFO - --------------------------------------------------
2026-02-10 01:59:52,103 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:59:54,767 - INFO - [Train] [25/90] | Loss: 0.4525 | Train Acc: 83.78%
2026-02-10 01:59:55,381 - INFO - [Valid] [25/90] | Loss: 0.5153 | Val Acc: 76.99%
2026-02-10 01:59:55,384 - INFO - [Metrics for 'abnormal'] | Precision: 0.7926 | Recall: 0.6815 | F1: 0.7329
2026-02-10 01:59:55,384 - INFO - [Metrics for 'normal'] | Precision: 0.7549 | Recall: 0.8462 | F1: 0.7979
2026-02-10 01:59:55,385 - INFO - --------------------------------------------------
2026-02-10 01:59:55,385 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:59:58,036 - INFO - [Train] [26/90] | Loss: 0.4508 | Train Acc: 84.82%
2026-02-10 01:59:58,593 - INFO - [Valid] [26/90] | Loss: 0.5217 | Val Acc: 76.99%
2026-02-10 01:59:58,597 - INFO - [Metrics for 'abnormal'] | Precision: 0.7365 | Recall: 0.7834 | F1: 0.7593
2026-02-10 01:59:58,598 - INFO - [Metrics for 'normal'] | Precision: 0.8023 | Recall: 0.7582 | F1: 0.7797
2026-02-10 01:59:58,599 - INFO - --------------------------------------------------
2026-02-10 01:59:58,600 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 02:00:01,166 - INFO - [Train] [27/90] | Loss: 0.4679 | Train Acc: 81.62%
2026-02-10 02:00:01,779 - INFO - [Valid] [27/90] | Loss: 0.5422 | Val Acc: 76.70%
2026-02-10 02:00:01,783 - INFO - [Metrics for 'abnormal'] | Precision: 0.7010 | Recall: 0.8662 | F1: 0.7749
2026-02-10 02:00:01,783 - INFO - [Metrics for 'normal'] | Precision: 0.8552 | Recall: 0.6813 | F1: 0.7584
2026-02-10 02:00:01,785 - INFO - --------------------------------------------------
2026-02-10 02:00:01,786 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 02:00:04,247 - INFO - [Train] [28/90] | Loss: 0.4424 | Train Acc: 84.08%
2026-02-10 02:00:04,830 - INFO - [Valid] [28/90] | Loss: 0.5269 | Val Acc: 78.17%
2026-02-10 02:00:04,834 - INFO - [Metrics for 'abnormal'] | Precision: 0.7456 | Recall: 0.8025 | F1: 0.7730
2026-02-10 02:00:04,835 - INFO - [Metrics for 'normal'] | Precision: 0.8176 | Recall: 0.7637 | F1: 0.7898
2026-02-10 02:00:04,836 - INFO - --------------------------------------------------
2026-02-10 02:00:04,837 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 02:00:07,494 - INFO - [Train] [29/90] | Loss: 0.4616 | Train Acc: 82.89%
2026-02-10 02:00:08,221 - INFO - [Valid] [29/90] | Loss: 0.5236 | Val Acc: 76.11%
2026-02-10 02:00:08,225 - INFO - [Metrics for 'abnormal'] | Precision: 0.7754 | Recall: 0.6815 | F1: 0.7254
2026-02-10 02:00:08,225 - INFO - [Metrics for 'normal'] | Precision: 0.7512 | Recall: 0.8297 | F1: 0.7885
2026-02-10 02:00:08,227 - INFO - --------------------------------------------------
2026-02-10 02:00:08,228 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 02:00:10,806 - INFO - [Train] [30/90] | Loss: 0.4599 | Train Acc: 83.63%
2026-02-10 02:00:11,534 - INFO - [Valid] [30/90] | Loss: 0.5179 | Val Acc: 75.52%
2026-02-10 02:00:11,537 - INFO - [Metrics for 'abnormal'] | Precision: 0.7643 | Recall: 0.6815 | F1: 0.7205
2026-02-10 02:00:11,537 - INFO - [Metrics for 'normal'] | Precision: 0.7487 | Recall: 0.8187 | F1: 0.7822
2026-02-10 02:00:11,538 - INFO - --------------------------------------------------
2026-02-10 02:00:11,539 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 02:00:14,220 - INFO - [Train] [31/90] | Loss: 0.4427 | Train Acc: 84.75%
2026-02-10 02:00:14,847 - INFO - [Valid] [31/90] | Loss: 0.5033 | Val Acc: 78.47%
2026-02-10 02:00:14,852 - INFO - [Metrics for 'abnormal'] | Precision: 0.7561 | Recall: 0.7898 | F1: 0.7726
2026-02-10 02:00:14,852 - INFO - [Metrics for 'normal'] | Precision: 0.8114 | Recall: 0.7802 | F1: 0.7955
2026-02-10 02:00:14,866 - INFO - [Best Model Saved] (val loss: 0.5033) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:00:14,866 - INFO - --------------------------------------------------
2026-02-10 02:00:14,867 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 02:00:17,544 - INFO - [Train] [32/90] | Loss: 0.4436 | Train Acc: 83.63%
2026-02-10 02:00:18,110 - INFO - [Valid] [32/90] | Loss: 0.5014 | Val Acc: 79.94%
2026-02-10 02:00:18,113 - INFO - [Metrics for 'abnormal'] | Precision: 0.7459 | Recall: 0.8599 | F1: 0.7988
2026-02-10 02:00:18,113 - INFO - [Metrics for 'normal'] | Precision: 0.8608 | Recall: 0.7473 | F1: 0.8000
2026-02-10 02:00:18,120 - INFO - [Best Model Saved] (val loss: 0.5014) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:00:18,121 - INFO - --------------------------------------------------
2026-02-10 02:00:18,121 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 02:00:20,739 - INFO - [Train] [33/90] | Loss: 0.4396 | Train Acc: 84.38%
2026-02-10 02:00:21,319 - INFO - [Valid] [33/90] | Loss: 0.4922 | Val Acc: 79.06%
2026-02-10 02:00:21,323 - INFO - [Metrics for 'abnormal'] | Precision: 0.7792 | Recall: 0.7643 | F1: 0.7717
2026-02-10 02:00:21,323 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.8132 | F1: 0.8065
2026-02-10 02:00:21,332 - INFO - [Best Model Saved] (val loss: 0.4922) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:00:21,332 - INFO - --------------------------------------------------
2026-02-10 02:00:21,332 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 02:00:23,939 - INFO - [Train] [34/90] | Loss: 0.4359 | Train Acc: 83.93%
2026-02-10 02:00:24,500 - INFO - [Valid] [34/90] | Loss: 0.5118 | Val Acc: 79.65%
2026-02-10 02:00:24,505 - INFO - [Metrics for 'abnormal'] | Precision: 0.7558 | Recall: 0.8280 | F1: 0.7903
2026-02-10 02:00:24,505 - INFO - [Metrics for 'normal'] | Precision: 0.8383 | Recall: 0.7692 | F1: 0.8023
2026-02-10 02:00:24,506 - INFO - --------------------------------------------------
2026-02-10 02:00:24,508 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 02:00:27,091 - INFO - [Train] [35/90] | Loss: 0.4506 | Train Acc: 83.48%
2026-02-10 02:00:27,849 - INFO - [Valid] [35/90] | Loss: 0.4918 | Val Acc: 80.53%
2026-02-10 02:00:27,854 - INFO - [Metrics for 'abnormal'] | Precision: 0.7758 | Recall: 0.8153 | F1: 0.7950
2026-02-10 02:00:27,854 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.7967 | F1: 0.8146
2026-02-10 02:00:27,868 - INFO - [Best Model Saved] (val loss: 0.4918) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:00:27,868 - INFO - --------------------------------------------------
2026-02-10 02:00:27,869 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 02:00:30,339 - INFO - [Train] [36/90] | Loss: 0.4325 | Train Acc: 84.75%
2026-02-10 02:00:31,050 - INFO - [Valid] [36/90] | Loss: 0.5074 | Val Acc: 79.94%
2026-02-10 02:00:31,054 - INFO - [Metrics for 'abnormal'] | Precision: 0.7730 | Recall: 0.8025 | F1: 0.7875
2026-02-10 02:00:31,055 - INFO - [Metrics for 'normal'] | Precision: 0.8239 | Recall: 0.7967 | F1: 0.8101
2026-02-10 02:00:31,056 - INFO - --------------------------------------------------
2026-02-10 02:00:31,057 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 02:00:33,748 - INFO - [Train] [37/90] | Loss: 0.4275 | Train Acc: 85.79%
2026-02-10 02:00:34,470 - INFO - [Valid] [37/90] | Loss: 0.4906 | Val Acc: 80.53%
2026-02-10 02:00:34,474 - INFO - [Metrics for 'abnormal'] | Precision: 0.7630 | Recall: 0.8408 | F1: 0.8000
2026-02-10 02:00:34,474 - INFO - [Metrics for 'normal'] | Precision: 0.8494 | Recall: 0.7747 | F1: 0.8103
2026-02-10 02:00:34,482 - INFO - [Best Model Saved] (val loss: 0.4906) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:00:34,482 - INFO - --------------------------------------------------
2026-02-10 02:00:34,483 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 02:00:37,241 - INFO - [Train] [38/90] | Loss: 0.4241 | Train Acc: 85.34%
2026-02-10 02:00:37,766 - INFO - [Valid] [38/90] | Loss: 0.5025 | Val Acc: 79.94%
2026-02-10 02:00:37,769 - INFO - [Metrics for 'abnormal'] | Precision: 0.7514 | Recall: 0.8471 | F1: 0.7964
2026-02-10 02:00:37,769 - INFO - [Metrics for 'normal'] | Precision: 0.8519 | Recall: 0.7582 | F1: 0.8023
2026-02-10 02:00:37,770 - INFO - --------------------------------------------------
2026-02-10 02:00:37,770 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 02:00:40,377 - INFO - [Train] [39/90] | Loss: 0.4239 | Train Acc: 85.19%
2026-02-10 02:00:40,907 - INFO - [Valid] [39/90] | Loss: 0.5133 | Val Acc: 79.94%
2026-02-10 02:00:40,910 - INFO - [Metrics for 'abnormal'] | Precision: 0.7633 | Recall: 0.8217 | F1: 0.7914
2026-02-10 02:00:40,910 - INFO - [Metrics for 'normal'] | Precision: 0.8353 | Recall: 0.7802 | F1: 0.8068
2026-02-10 02:00:40,911 - INFO - --------------------------------------------------
2026-02-10 02:00:40,912 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 02:00:43,565 - INFO - [Train] [40/90] | Loss: 0.4176 | Train Acc: 85.94%
2026-02-10 02:00:44,146 - INFO - [Valid] [40/90] | Loss: 0.4852 | Val Acc: 79.06%
2026-02-10 02:00:44,150 - INFO - [Metrics for 'abnormal'] | Precision: 0.7905 | Recall: 0.7452 | F1: 0.7672
2026-02-10 02:00:44,150 - INFO - [Metrics for 'normal'] | Precision: 0.7906 | Recall: 0.8297 | F1: 0.8097
2026-02-10 02:00:44,163 - INFO - [Best Model Saved] (val loss: 0.4852) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:00:44,163 - INFO - --------------------------------------------------
2026-02-10 02:00:44,164 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 02:00:46,765 - INFO - [Train] [41/90] | Loss: 0.4078 | Train Acc: 86.09%
2026-02-10 02:00:47,352 - INFO - [Valid] [41/90] | Loss: 0.4838 | Val Acc: 79.65%
2026-02-10 02:00:47,356 - INFO - [Metrics for 'abnormal'] | Precision: 0.7716 | Recall: 0.7962 | F1: 0.7837
2026-02-10 02:00:47,356 - INFO - [Metrics for 'normal'] | Precision: 0.8192 | Recall: 0.7967 | F1: 0.8078
2026-02-10 02:00:47,369 - INFO - [Best Model Saved] (val loss: 0.4838) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:00:47,369 - INFO - --------------------------------------------------
2026-02-10 02:00:47,370 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 02:00:50,037 - INFO - [Train] [42/90] | Loss: 0.4114 | Train Acc: 86.16%
2026-02-10 02:00:50,760 - INFO - [Valid] [42/90] | Loss: 0.4839 | Val Acc: 78.17%
2026-02-10 02:00:50,764 - INFO - [Metrics for 'abnormal'] | Precision: 0.8120 | Recall: 0.6879 | F1: 0.7448
2026-02-10 02:00:50,764 - INFO - [Metrics for 'normal'] | Precision: 0.7621 | Recall: 0.8626 | F1: 0.8093
2026-02-10 02:00:50,766 - INFO - --------------------------------------------------
2026-02-10 02:00:50,767 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 02:00:53,188 - INFO - [Train] [43/90] | Loss: 0.4090 | Train Acc: 86.16%
2026-02-10 02:00:53,942 - INFO - [Valid] [43/90] | Loss: 0.4852 | Val Acc: 79.94%
2026-02-10 02:00:53,947 - INFO - [Metrics for 'abnormal'] | Precision: 0.7697 | Recall: 0.8089 | F1: 0.7888
2026-02-10 02:00:53,947 - INFO - [Metrics for 'normal'] | Precision: 0.8276 | Recall: 0.7912 | F1: 0.8090
2026-02-10 02:00:53,948 - INFO - --------------------------------------------------
2026-02-10 02:00:53,949 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 02:00:56,537 - INFO - [Train] [44/90] | Loss: 0.4174 | Train Acc: 85.57%
2026-02-10 02:00:57,222 - INFO - [Valid] [44/90] | Loss: 0.5726 | Val Acc: 76.70%
2026-02-10 02:00:57,225 - INFO - [Metrics for 'abnormal'] | Precision: 0.6840 | Recall: 0.9236 | F1: 0.7859
2026-02-10 02:00:57,225 - INFO - [Metrics for 'normal'] | Precision: 0.9055 | Recall: 0.6319 | F1: 0.7443
2026-02-10 02:00:57,226 - INFO - --------------------------------------------------
2026-02-10 02:00:57,227 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 02:00:59,893 - INFO - [Train] [45/90] | Loss: 0.4121 | Train Acc: 85.42%
2026-02-10 02:01:00,411 - INFO - [Valid] [45/90] | Loss: 0.5044 | Val Acc: 79.65%
2026-02-10 02:01:00,414 - INFO - [Metrics for 'abnormal'] | Precision: 0.7500 | Recall: 0.8408 | F1: 0.7928
2026-02-10 02:01:00,414 - INFO - [Metrics for 'normal'] | Precision: 0.8466 | Recall: 0.7582 | F1: 0.8000
2026-02-10 02:01:00,415 - INFO - --------------------------------------------------
2026-02-10 02:01:00,416 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 02:01:03,077 - INFO - [Train] [46/90] | Loss: 0.4063 | Train Acc: 86.61%
2026-02-10 02:01:03,681 - INFO - [Valid] [46/90] | Loss: 0.4926 | Val Acc: 79.06%
2026-02-10 02:01:03,685 - INFO - [Metrics for 'abnormal'] | Precision: 0.7312 | Recall: 0.8662 | F1: 0.7930
2026-02-10 02:01:03,685 - INFO - [Metrics for 'normal'] | Precision: 0.8627 | Recall: 0.7253 | F1: 0.7881
2026-02-10 02:01:03,687 - INFO - --------------------------------------------------
2026-02-10 02:01:03,688 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 02:01:06,446 - INFO - [Train] [47/90] | Loss: 0.4003 | Train Acc: 86.90%
2026-02-10 02:01:07,128 - INFO - [Valid] [47/90] | Loss: 0.4752 | Val Acc: 79.94%
2026-02-10 02:01:07,131 - INFO - [Metrics for 'abnormal'] | Precision: 0.7665 | Recall: 0.8153 | F1: 0.7901
2026-02-10 02:01:07,131 - INFO - [Metrics for 'normal'] | Precision: 0.8314 | Recall: 0.7857 | F1: 0.8079
2026-02-10 02:01:07,143 - INFO - [Best Model Saved] (val loss: 0.4752) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:01:07,143 - INFO - --------------------------------------------------
2026-02-10 02:01:07,144 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 02:01:09,735 - INFO - [Train] [48/90] | Loss: 0.4046 | Train Acc: 86.24%
2026-02-10 02:01:10,476 - INFO - [Valid] [48/90] | Loss: 0.4867 | Val Acc: 80.83%
2026-02-10 02:01:10,480 - INFO - [Metrics for 'abnormal'] | Precision: 0.7738 | Recall: 0.8280 | F1: 0.8000
2026-02-10 02:01:10,480 - INFO - [Metrics for 'normal'] | Precision: 0.8421 | Recall: 0.7912 | F1: 0.8159
2026-02-10 02:01:10,481 - INFO - --------------------------------------------------
2026-02-10 02:01:10,482 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 02:01:12,986 - INFO - [Train] [49/90] | Loss: 0.3928 | Train Acc: 87.43%
2026-02-10 02:01:13,728 - INFO - [Valid] [49/90] | Loss: 0.4852 | Val Acc: 79.06%
2026-02-10 02:01:13,732 - INFO - [Metrics for 'abnormal'] | Precision: 0.7654 | Recall: 0.7898 | F1: 0.7774
2026-02-10 02:01:13,732 - INFO - [Metrics for 'normal'] | Precision: 0.8136 | Recall: 0.7912 | F1: 0.8022
2026-02-10 02:01:13,734 - INFO - --------------------------------------------------
2026-02-10 02:01:13,735 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 02:01:16,174 - INFO - [Train] [50/90] | Loss: 0.3861 | Train Acc: 88.32%
2026-02-10 02:01:16,853 - INFO - [Valid] [50/90] | Loss: 0.4877 | Val Acc: 79.65%
2026-02-10 02:01:16,856 - INFO - [Metrics for 'abnormal'] | Precision: 0.7529 | Recall: 0.8344 | F1: 0.7915
2026-02-10 02:01:16,857 - INFO - [Metrics for 'normal'] | Precision: 0.8424 | Recall: 0.7637 | F1: 0.8012
2026-02-10 02:01:16,857 - INFO - --------------------------------------------------
2026-02-10 02:01:16,858 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 02:01:19,561 - INFO - [Train] [51/90] | Loss: 0.3915 | Train Acc: 87.57%
2026-02-10 02:01:20,065 - INFO - [Valid] [51/90] | Loss: 0.5392 | Val Acc: 79.06%
2026-02-10 02:01:20,071 - INFO - [Metrics for 'abnormal'] | Precision: 0.7389 | Recall: 0.8471 | F1: 0.7893
2026-02-10 02:01:20,071 - INFO - [Metrics for 'normal'] | Precision: 0.8491 | Recall: 0.7418 | F1: 0.7918
2026-02-10 02:01:20,072 - INFO - --------------------------------------------------
2026-02-10 02:01:20,073 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 02:01:22,761 - INFO - [Train] [52/90] | Loss: 0.3899 | Train Acc: 87.57%
2026-02-10 02:01:23,365 - INFO - [Valid] [52/90] | Loss: 0.4762 | Val Acc: 79.65%
2026-02-10 02:01:23,369 - INFO - [Metrics for 'abnormal'] | Precision: 0.7933 | Recall: 0.7580 | F1: 0.7752
2026-02-10 02:01:23,369 - INFO - [Metrics for 'normal'] | Precision: 0.7989 | Recall: 0.8297 | F1: 0.8140
2026-02-10 02:01:23,370 - INFO - --------------------------------------------------
2026-02-10 02:01:23,371 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 02:01:25,960 - INFO - [Train] [53/90] | Loss: 0.3870 | Train Acc: 87.72%
2026-02-10 02:01:26,528 - INFO - [Valid] [53/90] | Loss: 0.4666 | Val Acc: 82.01%
2026-02-10 02:01:26,533 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 02:01:26,533 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 02:01:26,547 - INFO - [Best Model Saved] (val loss: 0.4666) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:01:26,547 - INFO - --------------------------------------------------
2026-02-10 02:01:26,548 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 02:01:29,125 - INFO - [Train] [54/90] | Loss: 0.3809 | Train Acc: 87.80%
2026-02-10 02:01:29,874 - INFO - [Valid] [54/90] | Loss: 0.4795 | Val Acc: 80.83%
2026-02-10 02:01:29,878 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-02-10 02:01:29,879 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-02-10 02:01:29,880 - INFO - --------------------------------------------------
2026-02-10 02:01:29,881 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 02:01:32,583 - INFO - [Train] [55/90] | Loss: 0.3828 | Train Acc: 88.32%
2026-02-10 02:01:33,357 - INFO - [Valid] [55/90] | Loss: 0.4726 | Val Acc: 80.83%
2026-02-10 02:01:33,362 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.7834 | F1: 0.7910
2026-02-10 02:01:33,362 - INFO - [Metrics for 'normal'] | Precision: 0.8162 | Recall: 0.8297 | F1: 0.8229
2026-02-10 02:01:33,363 - INFO - --------------------------------------------------
2026-02-10 02:01:33,364 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 02:01:36,034 - INFO - [Train] [56/90] | Loss: 0.3680 | Train Acc: 89.21%
2026-02-10 02:01:36,612 - INFO - [Valid] [56/90] | Loss: 0.4853 | Val Acc: 81.12%
2026-02-10 02:01:36,617 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-02-10 02:01:36,617 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-02-10 02:01:36,618 - INFO - --------------------------------------------------
2026-02-10 02:01:36,619 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 02:01:39,291 - INFO - [Train] [57/90] | Loss: 0.3718 | Train Acc: 88.54%
2026-02-10 02:01:39,816 - INFO - [Valid] [57/90] | Loss: 0.4893 | Val Acc: 81.42%
2026-02-10 02:01:39,819 - INFO - [Metrics for 'abnormal'] | Precision: 0.7798 | Recall: 0.8344 | F1: 0.8062
2026-02-10 02:01:39,819 - INFO - [Metrics for 'normal'] | Precision: 0.8480 | Recall: 0.7967 | F1: 0.8215
2026-02-10 02:01:39,820 - INFO - --------------------------------------------------
2026-02-10 02:01:39,820 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 02:01:42,517 - INFO - [Train] [58/90] | Loss: 0.3768 | Train Acc: 88.91%
2026-02-10 02:01:43,050 - INFO - [Valid] [58/90] | Loss: 0.4877 | Val Acc: 80.24%
2026-02-10 02:01:43,055 - INFO - [Metrics for 'abnormal'] | Precision: 0.7616 | Recall: 0.8344 | F1: 0.7964
2026-02-10 02:01:43,055 - INFO - [Metrics for 'normal'] | Precision: 0.8443 | Recall: 0.7747 | F1: 0.8080
2026-02-10 02:01:43,056 - INFO - --------------------------------------------------
2026-02-10 02:01:43,057 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 02:01:45,688 - INFO - [Train] [59/90] | Loss: 0.3657 | Train Acc: 89.29%
2026-02-10 02:01:46,353 - INFO - [Valid] [59/90] | Loss: 0.4879 | Val Acc: 79.35%
2026-02-10 02:01:46,357 - INFO - [Metrics for 'abnormal'] | Precision: 0.7669 | Recall: 0.7962 | F1: 0.7812
2026-02-10 02:01:46,357 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.7912 | F1: 0.8045
2026-02-10 02:01:46,358 - INFO - --------------------------------------------------
2026-02-10 02:01:46,359 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 02:01:48,923 - INFO - [Train] [60/90] | Loss: 0.3657 | Train Acc: 89.36%
2026-02-10 02:01:49,641 - INFO - [Valid] [60/90] | Loss: 0.4820 | Val Acc: 82.30%
2026-02-10 02:01:49,646 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8662 | F1: 0.8193
2026-02-10 02:01:49,646 - INFO - [Metrics for 'normal'] | Precision: 0.8720 | Recall: 0.7857 | F1: 0.8266
2026-02-10 02:01:49,647 - INFO - --------------------------------------------------
2026-02-10 02:01:49,648 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 02:01:52,273 - INFO - [Train] [61/90] | Loss: 0.3658 | Train Acc: 89.06%
2026-02-10 02:01:53,008 - INFO - [Valid] [61/90] | Loss: 0.4646 | Val Acc: 80.24%
2026-02-10 02:01:53,012 - INFO - [Metrics for 'abnormal'] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-02-10 02:01:53,013 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-02-10 02:01:53,026 - INFO - [Best Model Saved] (val loss: 0.4646) -> 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:01:53,026 - INFO - --------------------------------------------------
2026-02-10 02:01:53,027 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 02:01:55,640 - INFO - [Train] [62/90] | Loss: 0.3620 | Train Acc: 90.03%
2026-02-10 02:01:56,257 - INFO - [Valid] [62/90] | Loss: 0.4946 | Val Acc: 80.53%
2026-02-10 02:01:56,262 - INFO - [Metrics for 'abnormal'] | Precision: 0.7791 | Recall: 0.8089 | F1: 0.7937
2026-02-10 02:01:56,262 - INFO - [Metrics for 'normal'] | Precision: 0.8295 | Recall: 0.8022 | F1: 0.8156
2026-02-10 02:01:56,264 - INFO - --------------------------------------------------
2026-02-10 02:01:56,265 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 02:01:58,908 - INFO - [Train] [63/90] | Loss: 0.3530 | Train Acc: 90.03%
2026-02-10 02:01:59,433 - INFO - [Valid] [63/90] | Loss: 0.4857 | Val Acc: 79.94%
2026-02-10 02:01:59,435 - INFO - [Metrics for 'abnormal'] | Precision: 0.7633 | Recall: 0.8217 | F1: 0.7914
2026-02-10 02:01:59,436 - INFO - [Metrics for 'normal'] | Precision: 0.8353 | Recall: 0.7802 | F1: 0.8068
2026-02-10 02:01:59,436 - INFO - --------------------------------------------------
2026-02-10 02:01:59,437 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 02:02:02,080 - INFO - [Train] [64/90] | Loss: 0.3929 | Train Acc: 87.43%
2026-02-10 02:02:02,665 - INFO - [Valid] [64/90] | Loss: 0.4976 | Val Acc: 78.76%
2026-02-10 02:02:02,669 - INFO - [Metrics for 'abnormal'] | Precision: 0.7673 | Recall: 0.7771 | F1: 0.7722
2026-02-10 02:02:02,669 - INFO - [Metrics for 'normal'] | Precision: 0.8056 | Recall: 0.7967 | F1: 0.8011
2026-02-10 02:02:02,671 - INFO - --------------------------------------------------
2026-02-10 02:02:02,671 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 02:02:05,368 - INFO - [Train] [65/90] | Loss: 0.3613 | Train Acc: 89.58%
2026-02-10 02:02:05,952 - INFO - [Valid] [65/90] | Loss: 0.4807 | Val Acc: 81.42%
2026-02-10 02:02:05,957 - INFO - [Metrics for 'abnormal'] | Precision: 0.7798 | Recall: 0.8344 | F1: 0.8062
2026-02-10 02:02:05,957 - INFO - [Metrics for 'normal'] | Precision: 0.8480 | Recall: 0.7967 | F1: 0.8215
2026-02-10 02:02:05,958 - INFO - --------------------------------------------------
2026-02-10 02:02:05,959 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 02:02:08,571 - INFO - [Train] [66/90] | Loss: 0.3613 | Train Acc: 89.29%
2026-02-10 02:02:09,297 - INFO - [Valid] [66/90] | Loss: 0.4978 | Val Acc: 79.65%
2026-02-10 02:02:09,301 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.7707 | F1: 0.7781
2026-02-10 02:02:09,301 - INFO - [Metrics for 'normal'] | Precision: 0.8054 | Recall: 0.8187 | F1: 0.8120
2026-02-10 02:02:09,303 - INFO - --------------------------------------------------
2026-02-10 02:02:09,304 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 02:02:11,725 - INFO - [Train] [67/90] | Loss: 0.3386 | Train Acc: 91.29%
2026-02-10 02:02:12,431 - INFO - [Valid] [67/90] | Loss: 0.5062 | Val Acc: 80.53%
2026-02-10 02:02:12,436 - INFO - [Metrics for 'abnormal'] | Precision: 0.7692 | Recall: 0.8280 | F1: 0.7975
2026-02-10 02:02:12,436 - INFO - [Metrics for 'normal'] | Precision: 0.8412 | Recall: 0.7857 | F1: 0.8125
2026-02-10 02:02:12,437 - INFO - --------------------------------------------------
2026-02-10 02:02:12,438 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 02:02:14,988 - INFO - [Train] [68/90] | Loss: 0.3528 | Train Acc: 90.70%
2026-02-10 02:02:15,710 - INFO - [Valid] [68/90] | Loss: 0.4737 | Val Acc: 82.30%
2026-02-10 02:02:15,715 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-02-10 02:02:15,715 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-02-10 02:02:15,717 - INFO - --------------------------------------------------
2026-02-10 02:02:15,718 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 02:02:18,353 - INFO - [Train] [69/90] | Loss: 0.3429 | Train Acc: 90.92%
2026-02-10 02:02:18,955 - INFO - [Valid] [69/90] | Loss: 0.4879 | Val Acc: 80.24%
2026-02-10 02:02:18,959 - INFO - [Metrics for 'abnormal'] | Precision: 0.7885 | Recall: 0.7834 | F1: 0.7859
2026-02-10 02:02:18,960 - INFO - [Metrics for 'normal'] | Precision: 0.8142 | Recall: 0.8187 | F1: 0.8164
2026-02-10 02:02:18,961 - INFO - --------------------------------------------------
2026-02-10 02:02:18,962 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 02:02:20,617 - INFO - [Train] [70/90] | Loss: 0.3410 | Train Acc: 90.48%
2026-02-10 02:02:21,057 - INFO - [Valid] [70/90] | Loss: 0.4938 | Val Acc: 80.83%
2026-02-10 02:02:21,062 - INFO - [Metrics for 'abnormal'] | Precision: 0.8026 | Recall: 0.7771 | F1: 0.7896
2026-02-10 02:02:21,062 - INFO - [Metrics for 'normal'] | Precision: 0.8128 | Recall: 0.8352 | F1: 0.8238
2026-02-10 02:02:21,063 - INFO - --------------------------------------------------
2026-02-10 02:02:21,064 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 02:02:22,858 - INFO - [Train] [71/90] | Loss: 0.3376 | Train Acc: 90.77%
2026-02-10 02:02:23,501 - INFO - [Valid] [71/90] | Loss: 0.4785 | Val Acc: 80.83%
2026-02-10 02:02:23,508 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-02-10 02:02:23,508 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-02-10 02:02:23,509 - INFO - --------------------------------------------------
2026-02-10 02:02:23,509 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 02:02:25,593 - INFO - [Train] [72/90] | Loss: 0.3352 | Train Acc: 91.15%
2026-02-10 02:02:26,025 - INFO - [Valid] [72/90] | Loss: 0.5390 | Val Acc: 79.94%
2026-02-10 02:02:26,028 - INFO - [Metrics for 'abnormal'] | Precision: 0.7330 | Recall: 0.8917 | F1: 0.8046
2026-02-10 02:02:26,028 - INFO - [Metrics for 'normal'] | Precision: 0.8851 | Recall: 0.7198 | F1: 0.7939
2026-02-10 02:02:26,029 - INFO - --------------------------------------------------
2026-02-10 02:02:26,029 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 02:02:27,704 - INFO - [Train] [73/90] | Loss: 0.3269 | Train Acc: 91.89%
2026-02-10 02:02:28,104 - INFO - [Valid] [73/90] | Loss: 0.5254 | Val Acc: 81.12%
2026-02-10 02:02:28,107 - INFO - [Metrics for 'abnormal'] | Precision: 0.7657 | Recall: 0.8535 | F1: 0.8072
2026-02-10 02:02:28,107 - INFO - [Metrics for 'normal'] | Precision: 0.8598 | Recall: 0.7747 | F1: 0.8150
2026-02-10 02:02:28,108 - INFO - --------------------------------------------------
2026-02-10 02:02:28,108 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 02:02:29,589 - INFO - [Train] [74/90] | Loss: 0.3308 | Train Acc: 92.11%
2026-02-10 02:02:29,997 - INFO - [Valid] [74/90] | Loss: 0.4983 | Val Acc: 82.89%
2026-02-10 02:02:30,000 - INFO - [Metrics for 'abnormal'] | Precision: 0.7964 | Recall: 0.8471 | F1: 0.8210
2026-02-10 02:02:30,000 - INFO - [Metrics for 'normal'] | Precision: 0.8605 | Recall: 0.8132 | F1: 0.8362
2026-02-10 02:02:30,001 - INFO - --------------------------------------------------
2026-02-10 02:02:30,001 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 02:02:31,500 - INFO - [Train] [75/90] | Loss: 0.3172 | Train Acc: 92.63%
2026-02-10 02:02:31,903 - INFO - [Valid] [75/90] | Loss: 0.5271 | Val Acc: 81.71%
2026-02-10 02:02:31,905 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8025 | F1: 0.8025
2026-02-10 02:02:31,905 - INFO - [Metrics for 'normal'] | Precision: 0.8297 | Recall: 0.8297 | F1: 0.8297
2026-02-10 02:02:31,906 - INFO - --------------------------------------------------
2026-02-10 02:02:31,907 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 02:02:33,407 - INFO - [Train] [76/90] | Loss: 0.3243 | Train Acc: 92.41%
2026-02-10 02:02:33,816 - INFO - [Valid] [76/90] | Loss: 0.4972 | Val Acc: 81.42%
2026-02-10 02:02:33,819 - INFO - [Metrics for 'abnormal'] | Precision: 0.7798 | Recall: 0.8344 | F1: 0.8062
2026-02-10 02:02:33,819 - INFO - [Metrics for 'normal'] | Precision: 0.8480 | Recall: 0.7967 | F1: 0.8215
2026-02-10 02:02:33,820 - INFO - --------------------------------------------------
2026-02-10 02:02:33,820 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 02:02:35,321 - INFO - [Train] [77/90] | Loss: 0.3236 | Train Acc: 92.56%
2026-02-10 02:02:35,731 - INFO - [Valid] [77/90] | Loss: 0.5106 | Val Acc: 80.83%
2026-02-10 02:02:35,734 - INFO - [Metrics for 'abnormal'] | Precision: 0.7805 | Recall: 0.8153 | F1: 0.7975
2026-02-10 02:02:35,734 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8022 | F1: 0.8179
2026-02-10 02:02:35,735 - INFO - --------------------------------------------------
2026-02-10 02:02:35,735 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 02:02:37,237 - INFO - [Train] [78/90] | Loss: 0.3190 | Train Acc: 93.30%
2026-02-10 02:02:37,649 - INFO - [Valid] [78/90] | Loss: 0.5014 | Val Acc: 81.71%
2026-02-10 02:02:37,651 - INFO - [Metrics for 'abnormal'] | Precision: 0.7879 | Recall: 0.8280 | F1: 0.8075
2026-02-10 02:02:37,652 - INFO - [Metrics for 'normal'] | Precision: 0.8448 | Recall: 0.8077 | F1: 0.8258
2026-02-10 02:02:37,652 - INFO - --------------------------------------------------
2026-02-10 02:02:37,653 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 02:02:39,199 - INFO - [Train] [79/90] | Loss: 0.3347 | Train Acc: 91.59%
2026-02-10 02:02:39,606 - INFO - [Valid] [79/90] | Loss: 0.5024 | Val Acc: 81.12%
2026-02-10 02:02:39,609 - INFO - [Metrics for 'abnormal'] | Precision: 0.7657 | Recall: 0.8535 | F1: 0.8072
2026-02-10 02:02:39,609 - INFO - [Metrics for 'normal'] | Precision: 0.8598 | Recall: 0.7747 | F1: 0.8150
2026-02-10 02:02:39,609 - INFO - --------------------------------------------------
2026-02-10 02:02:39,610 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 02:02:41,130 - INFO - [Train] [80/90] | Loss: 0.3168 | Train Acc: 93.15%
2026-02-10 02:02:41,536 - INFO - [Valid] [80/90] | Loss: 0.5491 | Val Acc: 81.12%
2026-02-10 02:02:41,539 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7898 | F1: 0.7949
2026-02-10 02:02:41,539 - INFO - [Metrics for 'normal'] | Precision: 0.8207 | Recall: 0.8297 | F1: 0.8251
2026-02-10 02:02:41,540 - INFO - --------------------------------------------------
2026-02-10 02:02:41,540 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 02:02:43,054 - INFO - [Train] [81/90] | Loss: 0.3272 | Train Acc: 92.86%
2026-02-10 02:02:43,468 - INFO - [Valid] [81/90] | Loss: 0.5004 | Val Acc: 81.12%
2026-02-10 02:02:43,470 - INFO - [Metrics for 'abnormal'] | Precision: 0.7962 | Recall: 0.7962 | F1: 0.7962
2026-02-10 02:02:43,470 - INFO - [Metrics for 'normal'] | Precision: 0.8242 | Recall: 0.8242 | F1: 0.8242
2026-02-10 02:02:43,471 - INFO - --------------------------------------------------
2026-02-10 02:02:43,471 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 02:02:44,966 - INFO - [Train] [82/90] | Loss: 0.3102 | Train Acc: 93.23%
2026-02-10 02:02:45,375 - INFO - [Valid] [82/90] | Loss: 0.5063 | Val Acc: 82.01%
2026-02-10 02:02:45,378 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 02:02:45,378 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 02:02:45,379 - INFO - --------------------------------------------------
2026-02-10 02:02:45,379 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 02:02:46,884 - INFO - [Train] [83/90] | Loss: 0.3136 | Train Acc: 92.63%
2026-02-10 02:02:47,289 - INFO - [Valid] [83/90] | Loss: 0.5035 | Val Acc: 81.71%
2026-02-10 02:02:47,292 - INFO - [Metrics for 'abnormal'] | Precision: 0.7879 | Recall: 0.8280 | F1: 0.8075
2026-02-10 02:02:47,292 - INFO - [Metrics for 'normal'] | Precision: 0.8448 | Recall: 0.8077 | F1: 0.8258
2026-02-10 02:02:47,293 - INFO - --------------------------------------------------
2026-02-10 02:02:47,293 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 02:02:48,807 - INFO - [Train] [84/90] | Loss: 0.3019 | Train Acc: 93.68%
2026-02-10 02:02:49,219 - INFO - [Valid] [84/90] | Loss: 0.5065 | Val Acc: 81.12%
2026-02-10 02:02:49,222 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 02:02:49,222 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 02:02:49,223 - INFO - --------------------------------------------------
2026-02-10 02:02:49,223 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 02:02:50,709 - INFO - [Train] [85/90] | Loss: 0.3009 | Train Acc: 93.82%
2026-02-10 02:02:51,126 - INFO - [Valid] [85/90] | Loss: 0.4975 | Val Acc: 82.30%
2026-02-10 02:02:51,128 - INFO - [Metrics for 'abnormal'] | Precision: 0.8050 | Recall: 0.8153 | F1: 0.8101
2026-02-10 02:02:51,128 - INFO - [Metrics for 'normal'] | Precision: 0.8389 | Recall: 0.8297 | F1: 0.8343
2026-02-10 02:02:51,129 - INFO - --------------------------------------------------
2026-02-10 02:02:51,129 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 02:02:52,627 - INFO - [Train] [86/90] | Loss: 0.3055 | Train Acc: 93.82%
2026-02-10 02:02:53,039 - INFO - [Valid] [86/90] | Loss: 0.5204 | Val Acc: 81.12%
2026-02-10 02:02:53,042 - INFO - [Metrics for 'abnormal'] | Precision: 0.7925 | Recall: 0.8025 | F1: 0.7975
2026-02-10 02:02:53,042 - INFO - [Metrics for 'normal'] | Precision: 0.8278 | Recall: 0.8187 | F1: 0.8232
2026-02-10 02:02:53,043 - INFO - --------------------------------------------------
2026-02-10 02:02:53,043 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 02:02:54,533 - INFO - [Train] [87/90] | Loss: 0.3058 | Train Acc: 94.20%
2026-02-10 02:02:54,946 - INFO - [Valid] [87/90] | Loss: 0.5105 | Val Acc: 81.42%
2026-02-10 02:02:54,948 - INFO - [Metrics for 'abnormal'] | Precision: 0.8052 | Recall: 0.7898 | F1: 0.7974
2026-02-10 02:02:54,949 - INFO - [Metrics for 'normal'] | Precision: 0.8216 | Recall: 0.8352 | F1: 0.8283
2026-02-10 02:02:54,949 - INFO - --------------------------------------------------
2026-02-10 02:02:54,950 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 02:02:56,442 - INFO - [Train] [88/90] | Loss: 0.3043 | Train Acc: 93.90%
2026-02-10 02:02:56,847 - INFO - [Valid] [88/90] | Loss: 0.4943 | Val Acc: 81.42%
2026-02-10 02:02:56,850 - INFO - [Metrics for 'abnormal'] | Precision: 0.8092 | Recall: 0.7834 | F1: 0.7961
2026-02-10 02:02:56,850 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8407 | F1: 0.8293
2026-02-10 02:02:56,851 - INFO - --------------------------------------------------
2026-02-10 02:02:56,851 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 02:02:58,315 - INFO - [Train] [89/90] | Loss: 0.2962 | Train Acc: 94.94%
2026-02-10 02:02:58,722 - INFO - [Valid] [89/90] | Loss: 0.5153 | Val Acc: 82.30%
2026-02-10 02:02:58,725 - INFO - [Metrics for 'abnormal'] | Precision: 0.8050 | Recall: 0.8153 | F1: 0.8101
2026-02-10 02:02:58,725 - INFO - [Metrics for 'normal'] | Precision: 0.8389 | Recall: 0.8297 | F1: 0.8343
2026-02-10 02:02:58,726 - INFO - --------------------------------------------------
2026-02-10 02:02:58,726 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 02:03:00,175 - INFO - [Train] [90/90] | Loss: 0.2992 | Train Acc: 94.42%
2026-02-10 02:03:00,587 - INFO - [Valid] [90/90] | Loss: 0.5324 | Val Acc: 80.83%
2026-02-10 02:03:00,590 - INFO - [Metrics for 'abnormal'] | Precision: 0.7644 | Recall: 0.8471 | F1: 0.8036
2026-02-10 02:03:00,590 - INFO - [Metrics for 'normal'] | Precision: 0.8545 | Recall: 0.7747 | F1: 0.8127
2026-02-10 02:03:00,591 - INFO - ==================================================
2026-02-10 02:03:00,591 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 02:03:00,592 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 02:03:00,592 - INFO - Baseline 모델 'deit_tiny'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 02:03:00,624 - INFO - timm 모델(deit_tiny)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 02:03:00,625 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 02:03:00,625 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 02:03:00,625 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 02:03:00,625 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 02:03:00,861 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.816943359375)에 맞춰 변경되었습니다.
2026-02-10 02:03:00,861 - INFO - ==================================================
2026-02-10 02:03:00,871 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/best_model.pth'
2026-02-10 02:03:00,871 - INFO - ==================================================
2026-02-10 02:03:00,871 - INFO - Test 모드를 시작합니다.
2026-02-10 02:03:00,896 - INFO - 연산량 (MACs): 0.0920 GMACs per sample
2026-02-10 02:03:00,896 - INFO - 연산량 (FLOPs): 0.1840 GFLOPs per sample
2026-02-10 02:03:00,896 - INFO - ==================================================
2026-02-10 02:03:00,896 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 02:03:01,315 - INFO - 샘플 당 평균 Forward Pass 시간: 1.56ms (std: 0.07ms), FPS: 640.77 (std: 19.82) (1개 샘플 x 100회 반복)
2026-02-10 02:03:01,315 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 112.93 MB
2026-02-10 02:03:01,315 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 02:03:01,969 - INFO - [Test] Loss: 0.3962 | Test Acc: 80.24%
2026-02-10 02:03:01,972 - INFO - [Metrics for 'abnormal'] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-02-10 02:03:01,972 - INFO - [Metrics for 'normal'] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-02-10 02:03:02,097 - INFO - ==================================================
2026-02-10 02:03:02,097 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/confusion_matrix_20260210_015820.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/confusion_matrix_20260210_015820.pdf'
2026-02-10 02:03:02,097 - INFO - ==================================================
2026-02-10 02:03:02,097 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 02:03:02,231 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/model_fp32_20260210_015820.onnx'에 저장되었습니다. (크기: 1.96 MB)
2026-02-10 02:03:02,402 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 13.31 MB
2026-02-10 02:03:02,402 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 02:03:02,819 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 2.68ms (std: 0.04ms)
2026-02-10 02:03:02,819 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 373.82 FPS (std: 5.69) (1개 샘플 x 100회 반복)
2026-02-10 02:03:02,819 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 5.06 MB
2026-02-10 02:03:02,819 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 16.23 MB
2026-02-10 02:03:03,809 - INFO - [Test (ONNX)] | Test Acc (ONNX): 80.24%
2026-02-10 02:03:03,812 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.7848 | Recall: 0.7898 | F1: 0.7873
2026-02-10 02:03:03,812 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8177 | Recall: 0.8132 | F1: 0.8154
2026-02-10 02:03:03,913 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/val_acc.pdf'
2026-02-10 02:03:04,021 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/train_val_acc.pdf'
2026-02-10 02:03:04,111 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/F1_normal.pdf'
2026-02-10 02:03:04,200 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/val_loss.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/val_loss.pdf'
2026-02-10 02:03:04,291 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/learning_rate.pdf'
2026-02-10 02:03:05,261 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/compile.png' and 'log/Sewer-TAPNEW/baseline_deit_tiny_l1_20260210_015820/graph_20260210_015820/compile.pdf'
