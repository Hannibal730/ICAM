2026-02-10 10:05:28,924 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/log_20260210_100528.log'에 저장됩니다.
2026-02-10 10:05:28,926 - INFO - ==================================================
2026-02-10 10:05:28,926 - INFO - config.yaml:
2026-02-10 10:05:28,926 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: mobilenet_v4_s
  use_l1_pruning: true
  pruning_flops_target: 0.1816

2026-02-10 10:05:28,926 - INFO - ==================================================
2026-02-10 10:05:29,067 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 10:05:29,067 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 10:05:29,078 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 10:05:29,081 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 10:05:29,081 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 10:05:29,081 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 10:05:29,081 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 10:05:29,183 - INFO - ==================================================
2026-02-10 10:05:29,183 - INFO - 모델 파라미터 수:
2026-02-10 10:05:29,183 - INFO -   - 총 파라미터: 2,495,586 개
2026-02-10 10:05:29,183 - INFO -   - 학습 가능한 파라미터: 2,495,586 개
2026-02-10 10:05:29,183 - INFO - ================================================================================
2026-02-10 10:05:29,183 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 10:05:29,183 - INFO - ================================================================================
2026-02-10 10:05:29,183 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 10:05:29,184 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 10:05:29,184 - INFO - ==================================================
2026-02-10 10:05:29,184 - INFO - train 모드를 시작합니다.
2026-02-10 10:05:29,184 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 10:05:29,184 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 10:05:29,184 - INFO - --------------------------------------------------
2026-02-10 10:05:29,184 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 10:05:30,943 - INFO - [Train] [1/10] | Loss: 2.7743 | Train Acc: 66.52%
2026-02-10 10:05:31,615 - INFO - [Valid] [1/10] | Loss: 2.0253 | Val Acc: 63.13%
2026-02-10 10:05:31,619 - INFO - [Metrics for 'abnormal'] | Precision: 0.6111 | Recall: 0.5605 | F1: 0.5847
2026-02-10 10:05:31,619 - INFO - [Metrics for 'normal'] | Precision: 0.6462 | Recall: 0.6923 | F1: 0.6684
2026-02-10 10:05:31,633 - INFO - [Best Model Saved] (val loss: 2.0253) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:05:31,633 - INFO - --------------------------------------------------
2026-02-10 10:05:31,633 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 10:05:33,097 - INFO - [Train] [2/10] | Loss: 0.8168 | Train Acc: 69.42%
2026-02-10 10:05:33,501 - INFO - [Valid] [2/10] | Loss: 1.0119 | Val Acc: 72.27%
2026-02-10 10:05:33,504 - INFO - [Metrics for 'abnormal'] | Precision: 0.7299 | Recall: 0.6369 | F1: 0.6803
2026-02-10 10:05:33,504 - INFO - [Metrics for 'normal'] | Precision: 0.7178 | Recall: 0.7967 | F1: 0.7552
2026-02-10 10:05:33,518 - INFO - [Best Model Saved] (val loss: 1.0119) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:05:33,519 - INFO - --------------------------------------------------
2026-02-10 10:05:33,519 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 10:05:35,496 - INFO - [Train] [3/10] | Loss: 0.6917 | Train Acc: 72.40%
2026-02-10 10:05:36,170 - INFO - [Valid] [3/10] | Loss: 0.8621 | Val Acc: 71.68%
2026-02-10 10:05:36,175 - INFO - [Metrics for 'abnormal'] | Precision: 0.7293 | Recall: 0.6178 | F1: 0.6690
2026-02-10 10:05:36,175 - INFO - [Metrics for 'normal'] | Precision: 0.7087 | Recall: 0.8022 | F1: 0.7526
2026-02-10 10:05:36,203 - INFO - [Best Model Saved] (val loss: 0.8621) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:05:36,204 - INFO - --------------------------------------------------
2026-02-10 10:05:36,205 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 10:05:38,179 - INFO - [Train] [4/10] | Loss: 0.6041 | Train Acc: 75.07%
2026-02-10 10:05:38,833 - INFO - [Valid] [4/10] | Loss: 0.9130 | Val Acc: 58.70%
2026-02-10 10:05:38,837 - INFO - [Metrics for 'abnormal'] | Precision: 0.5326 | Recall: 0.8854 | F1: 0.6651
2026-02-10 10:05:38,837 - INFO - [Metrics for 'normal'] | Precision: 0.7692 | Recall: 0.3297 | F1: 0.4615
2026-02-10 10:05:38,839 - INFO - --------------------------------------------------
2026-02-10 10:05:38,840 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 10:05:40,970 - INFO - [Train] [5/10] | Loss: 0.5820 | Train Acc: 76.71%
2026-02-10 10:05:41,770 - INFO - [Valid] [5/10] | Loss: 1.4230 | Val Acc: 63.13%
2026-02-10 10:05:41,776 - INFO - [Metrics for 'abnormal'] | Precision: 0.5860 | Recall: 0.6943 | F1: 0.6356
2026-02-10 10:05:41,776 - INFO - [Metrics for 'normal'] | Precision: 0.6863 | Recall: 0.5769 | F1: 0.6269
2026-02-10 10:05:41,777 - INFO - --------------------------------------------------
2026-02-10 10:05:41,779 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 10:05:43,999 - INFO - [Train] [6/10] | Loss: 0.5624 | Train Acc: 77.31%
2026-02-10 10:05:44,764 - INFO - [Valid] [6/10] | Loss: 0.7661 | Val Acc: 71.68%
2026-02-10 10:05:44,769 - INFO - [Metrics for 'abnormal'] | Precision: 0.8427 | Recall: 0.4777 | F1: 0.6098
2026-02-10 10:05:44,770 - INFO - [Metrics for 'normal'] | Precision: 0.6720 | Recall: 0.9231 | F1: 0.7778
2026-02-10 10:05:44,799 - INFO - [Best Model Saved] (val loss: 0.7661) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:05:44,800 - INFO - --------------------------------------------------
2026-02-10 10:05:44,801 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 10:05:46,950 - INFO - [Train] [7/10] | Loss: 0.5240 | Train Acc: 77.75%
2026-02-10 10:05:47,639 - INFO - [Valid] [7/10] | Loss: 0.6184 | Val Acc: 77.58%
2026-02-10 10:05:47,643 - INFO - [Metrics for 'abnormal'] | Precision: 0.7914 | Recall: 0.7006 | F1: 0.7432
2026-02-10 10:05:47,643 - INFO - [Metrics for 'normal'] | Precision: 0.7650 | Recall: 0.8407 | F1: 0.8010
2026-02-10 10:05:47,682 - INFO - [Best Model Saved] (val loss: 0.6184) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:05:47,682 - INFO - --------------------------------------------------
2026-02-10 10:05:47,683 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 10:05:49,828 - INFO - [Train] [8/10] | Loss: 0.5048 | Train Acc: 80.80%
2026-02-10 10:05:50,576 - INFO - [Valid] [8/10] | Loss: 0.5576 | Val Acc: 79.94%
2026-02-10 10:05:50,580 - INFO - [Metrics for 'abnormal'] | Precision: 0.8201 | Recall: 0.7261 | F1: 0.7703
2026-02-10 10:05:50,580 - INFO - [Metrics for 'normal'] | Precision: 0.7850 | Recall: 0.8626 | F1: 0.8220
2026-02-10 10:05:50,608 - INFO - [Best Model Saved] (val loss: 0.5576) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:05:50,609 - INFO - --------------------------------------------------
2026-02-10 10:05:50,610 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 10:05:52,728 - INFO - [Train] [9/10] | Loss: 0.4824 | Train Acc: 82.14%
2026-02-10 10:05:53,516 - INFO - [Valid] [9/10] | Loss: 0.5508 | Val Acc: 78.47%
2026-02-10 10:05:53,521 - INFO - [Metrics for 'abnormal'] | Precision: 0.7283 | Recall: 0.8535 | F1: 0.7859
2026-02-10 10:05:53,521 - INFO - [Metrics for 'normal'] | Precision: 0.8516 | Recall: 0.7253 | F1: 0.7834
2026-02-10 10:05:53,558 - INFO - [Best Model Saved] (val loss: 0.5508) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:05:53,558 - INFO - --------------------------------------------------
2026-02-10 10:05:53,559 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 10:05:55,785 - INFO - [Train] [10/10] | Loss: 0.4585 | Train Acc: 82.89%
2026-02-10 10:05:56,490 - INFO - [Valid] [10/10] | Loss: 0.5298 | Val Acc: 79.65%
2026-02-10 10:05:56,495 - INFO - [Metrics for 'abnormal'] | Precision: 0.7821 | Recall: 0.7771 | F1: 0.7796
2026-02-10 10:05:56,495 - INFO - [Metrics for 'normal'] | Precision: 0.8087 | Recall: 0.8132 | F1: 0.8110
2026-02-10 10:05:56,519 - INFO - [Best Model Saved] (val loss: 0.5298) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:05:56,520 - INFO - ================================================================================
2026-02-10 10:05:56,520 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 10:05:56,520 - INFO - ================================================================================
2026-02-10 10:05:56,572 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'을(를) 불러왔습니다.
2026-02-10 10:05:56,573 - INFO - ================================================================================
2026-02-10 10:05:56,573 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 10:05:56,605 - INFO - 원본 모델 FLOPs: 0.3853 GFLOPs
2026-02-10 10:05:56,657 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:56,657 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:56,805 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 10:05:56,805 - INFO - ==================================================
2026-02-10 10:05:56,842 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.1092 GFLOPs (감소율: 71.66%)
2026-02-10 10:05:56,869 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:56,869 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:56,935 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.2475)에 맞춰 변경되었습니다.
2026-02-10 10:05:56,935 - INFO - ==================================================
2026-02-10 10:05:56,974 - INFO -   [탐색  2] 희소도: 0.2475 -> FLOPs: 0.2263 GFLOPs (감소율: 41.26%)
2026-02-10 10:05:57,002 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:57,002 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:57,114 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.37124999999999997)에 맞춰 변경되었습니다.
2026-02-10 10:05:57,114 - INFO - ==================================================
2026-02-10 10:05:57,137 - INFO -   [탐색  3] 희소도: 0.3712 -> FLOPs: 0.1626 GFLOPs (감소율: 57.81%)
2026-02-10 10:05:57,156 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:57,156 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:57,412 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.30937499999999996)에 맞춰 변경되었습니다.
2026-02-10 10:05:57,412 - INFO - ==================================================
2026-02-10 10:05:57,438 - INFO -   [탐색  4] 희소도: 0.3094 -> FLOPs: 0.1932 GFLOPs (감소율: 49.86%)
2026-02-10 10:05:57,456 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:57,456 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:57,501 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.34031249999999996)에 맞춰 변경되었습니다.
2026-02-10 10:05:57,501 - INFO - ==================================================
2026-02-10 10:05:57,527 - INFO -   [탐색  5] 희소도: 0.3403 -> FLOPs: 0.1775 GFLOPs (감소율: 53.92%)
2026-02-10 10:05:57,546 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:57,546 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:57,593 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32484375)에 맞춰 변경되었습니다.
2026-02-10 10:05:57,593 - INFO - ==================================================
2026-02-10 10:05:57,620 - INFO -   [탐색  6] 희소도: 0.3248 -> FLOPs: 0.1825 GFLOPs (감소율: 52.64%)
2026-02-10 10:05:57,636 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:57,636 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:57,673 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.33257812499999995)에 맞춰 변경되었습니다.
2026-02-10 10:05:57,673 - INFO - ==================================================
2026-02-10 10:05:57,694 - INFO -   [탐색  7] 희소도: 0.3326 -> FLOPs: 0.1806 GFLOPs (감소율: 53.13%)
2026-02-10 10:05:57,709 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:57,709 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:57,748 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32871093749999997)에 맞춰 변경되었습니다.
2026-02-10 10:05:57,748 - INFO - ==================================================
2026-02-10 10:05:57,764 - INFO -   [탐색  8] 희소도: 0.3287 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:05:57,778 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:57,778 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:57,818 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32677734375)에 맞춰 변경되었습니다.
2026-02-10 10:05:57,818 - INFO - ==================================================
2026-02-10 10:05:57,850 - INFO -   [탐색  9] 희소도: 0.3268 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:57,876 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:57,876 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:57,940 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.327744140625)에 맞춰 변경되었습니다.
2026-02-10 10:05:57,940 - INFO - ==================================================
2026-02-10 10:05:57,966 - INFO -   [탐색 10] 희소도: 0.3277 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:57,993 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:57,993 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:58,063 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3282275390625)에 맞춰 변경되었습니다.
2026-02-10 10:05:58,064 - INFO - ==================================================
2026-02-10 10:05:58,092 - INFO -   [탐색 11] 희소도: 0.3282 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:05:58,123 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:58,123 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:58,245 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32798583984375)에 맞춰 변경되었습니다.
2026-02-10 10:05:58,245 - INFO - ==================================================
2026-02-10 10:05:58,273 - INFO -   [탐색 12] 희소도: 0.3280 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:58,302 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:58,303 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:58,370 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32810668945312504)에 맞춰 변경되었습니다.
2026-02-10 10:05:58,370 - INFO - ==================================================
2026-02-10 10:05:58,396 - INFO -   [탐색 13] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:58,423 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:58,423 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:58,547 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32816711425781253)에 맞춰 변경되었습니다.
2026-02-10 10:05:58,547 - INFO - ==================================================
2026-02-10 10:05:58,575 - INFO -   [탐색 14] 희소도: 0.3282 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:05:58,603 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:58,603 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:58,681 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281369018554688)에 맞춰 변경되었습니다.
2026-02-10 10:05:58,681 - INFO - ==================================================
2026-02-10 10:05:58,708 - INFO -   [탐색 15] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:05:59,052 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,053 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,108 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281217956542969)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,108 - INFO - ==================================================
2026-02-10 10:05:59,123 - INFO -   [탐색 16] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:59,137 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,138 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,251 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812934875488287)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,251 - INFO - ==================================================
2026-02-10 10:05:59,265 - INFO -   [탐색 17] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:05:59,280 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,280 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,315 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281255722045899)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,315 - INFO - ==================================================
2026-02-10 10:05:59,327 - INFO -   [탐색 18] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:05:59,340 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,340 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,390 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812368392944335)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,390 - INFO - ==================================================
2026-02-10 10:05:59,407 - INFO -   [탐색 19] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:59,426 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,426 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,468 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281246280670166)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,468 - INFO - ==================================================
2026-02-10 10:05:59,486 - INFO -   [탐색 20] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:59,503 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,504 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,546 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281251001358032)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,547 - INFO - ==================================================
2026-02-10 10:05:59,566 - INFO -   [탐색 21] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:05:59,585 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,585 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,650 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281248641014099)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,650 - INFO - ==================================================
2026-02-10 10:05:59,665 - INFO -   [탐색 22] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:59,681 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,681 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,718 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249821186065)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,718 - INFO - ==================================================
2026-02-10 10:05:59,734 - INFO -   [탐색 23] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:59,748 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,748 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,791 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812504112720486)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,791 - INFO - ==================================================
2026-02-10 10:05:59,807 - INFO -   [탐색 24] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:05:59,825 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,825 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,865 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250116229057)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,865 - INFO - ==================================================
2026-02-10 10:05:59,882 - INFO -   [탐색 25] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:05:59,899 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,900 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:05:59,940 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499687075614)에 맞춰 변경되었습니다.
2026-02-10 10:05:59,940 - INFO - ==================================================
2026-02-10 10:05:59,957 - INFO -   [탐색 26] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:05:59,974 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:05:59,974 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:00,017 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250042468309)에 맞춰 변경되었습니다.
2026-02-10 10:06:00,017 - INFO - ==================================================
2026-02-10 10:06:00,035 - INFO -   [탐색 27] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:00,409 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:00,409 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:00,495 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250005587935)에 맞춰 변경되었습니다.
2026-02-10 10:06:00,495 - INFO - ==================================================
2026-02-10 10:06:00,515 - INFO -   [탐색 28] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:00,535 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:00,535 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:00,588 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499871477485)에 맞춰 변경되었습니다.
2026-02-10 10:06:00,588 - INFO - ==================================================
2026-02-10 10:06:00,606 - INFO -   [탐색 29] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:00,625 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:00,626 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:00,728 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249996367842)에 맞춰 변경되었습니다.
2026-02-10 10:06:00,728 - INFO - ==================================================
2026-02-10 10:06:00,751 - INFO -   [탐색 30] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:00,774 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:00,774 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:00,846 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000977889)에 맞춰 변경되었습니다.
2026-02-10 10:06:00,847 - INFO - ==================================================
2026-02-10 10:06:00,864 - INFO -   [탐색 31] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:00,884 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:00,884 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:00,986 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499986728655)에 맞춰 변경되었습니다.
2026-02-10 10:06:00,986 - INFO - ==================================================
2026-02-10 10:06:01,003 - INFO -   [탐색 32] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:01,025 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:01,025 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:01,084 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499998253775)에 맞춰 변경되었습니다.
2026-02-10 10:06:01,084 - INFO - ==================================================
2026-02-10 10:06:01,100 - INFO -   [탐색 33] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:01,115 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:01,115 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:01,207 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000401633)에 맞춰 변경되었습니다.
2026-02-10 10:06:01,207 - INFO - ==================================================
2026-02-10 10:06:01,220 - INFO -   [탐색 34] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:01,235 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:01,236 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:01,285 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500001135053)에 맞춰 변경되었습니다.
2026-02-10 10:06:01,285 - INFO - ==================================================
2026-02-10 10:06:01,298 - INFO -   [탐색 35] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:01,311 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:01,311 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:01,349 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999969441)에 맞춰 변경되었습니다.
2026-02-10 10:06:01,349 - INFO - ==================================================
2026-02-10 10:06:01,365 - INFO -   [탐색 36] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:01,382 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:01,382 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:01,421 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000414735)에 맞춰 변경되었습니다.
2026-02-10 10:06:01,422 - INFO - ==================================================
2026-02-10 10:06:01,438 - INFO -   [탐색 37] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:01,456 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:01,456 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:01,499 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000054573)에 맞춰 변경되었습니다.
2026-02-10 10:06:01,500 - INFO - ==================================================
2026-02-10 10:06:01,518 - INFO -   [탐색 38] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:01,537 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:01,538 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:01,604 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999987449)에 맞춰 변경되었습니다.
2026-02-10 10:06:01,604 - INFO - ==================================================
2026-02-10 10:06:01,619 - INFO -   [탐색 39] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:01,635 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:01,635 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:01,896 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999996453)에 맞춰 변경되었습니다.
2026-02-10 10:06:01,896 - INFO - ==================================================
2026-02-10 10:06:01,909 - INFO -   [탐색 40] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:01,922 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:01,922 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:01,995 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000955)에 맞춰 변경되었습니다.
2026-02-10 10:06:01,995 - INFO - ==================================================
2026-02-10 10:06:02,007 - INFO -   [탐색 41] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:02,020 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,020 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,078 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999998704)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,078 - INFO - ==================================================
2026-02-10 10:06:02,090 - INFO -   [탐색 42] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:02,103 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,103 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,142 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999999829)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,142 - INFO - ==================================================
2026-02-10 10:06:02,157 - INFO -   [탐색 43] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:02,171 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,171 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,206 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000392)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,206 - INFO - ==================================================
2026-02-10 10:06:02,219 - INFO -   [탐색 44] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:02,232 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,232 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,265 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000001105)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,265 - INFO - ==================================================
2026-02-10 10:06:02,278 - INFO -   [탐색 45] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:02,290 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,290 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,354 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.328124999999997)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,354 - INFO - ==================================================
2026-02-10 10:06:02,366 - INFO -   [탐색 46] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:02,379 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,379 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,444 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.328125000000004)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,444 - INFO - ==================================================
2026-02-10 10:06:02,457 - INFO -   [탐색 47] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:02,469 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,469 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,532 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000005)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,532 - INFO - ==================================================
2026-02-10 10:06:02,544 - INFO -   [탐색 48] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:02,557 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,557 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,620 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999999988)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,620 - INFO - ==================================================
2026-02-10 10:06:02,632 - INFO -   [탐색 49] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:02,645 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,645 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,708 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812499999999967)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,708 - INFO - ==================================================
2026-02-10 10:06:02,720 - INFO -   [탐색 50] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:02,733 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,733 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,795 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,795 - INFO - ==================================================
2026-02-10 10:06:02,807 - INFO -   [탐색 51] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:02,820 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,820 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:02,883 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281249999999999)에 맞춰 변경되었습니다.
2026-02-10 10:06:02,883 - INFO - ==================================================
2026-02-10 10:06:02,895 - INFO -   [탐색 52] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:02,907 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:02,907 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,149 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.328125)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,149 - INFO - ==================================================
2026-02-10 10:06:03,161 - INFO -   [탐색 53] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:03,174 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,174 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,208 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.32812500000000006)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,208 - INFO - ==================================================
2026-02-10 10:06:03,222 - INFO -   [탐색 54] 희소도: 0.3281 -> FLOPs: 0.1822 GFLOPs (감소율: 52.70%)
2026-02-10 10:06:03,235 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,235 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,269 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,269 - INFO - ==================================================
2026-02-10 10:06:03,282 - INFO -   [탐색 55] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:03,295 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,295 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,329 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,329 - INFO - ==================================================
2026-02-10 10:06:03,341 - INFO -   [탐색 56] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:03,354 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,354 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,418 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,418 - INFO - ==================================================
2026-02-10 10:06:03,430 - INFO -   [탐색 57] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:03,443 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,443 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,502 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,502 - INFO - ==================================================
2026-02-10 10:06:03,514 - INFO -   [탐색 58] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:03,527 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,527 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,580 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,580 - INFO - ==================================================
2026-02-10 10:06:03,592 - INFO -   [탐색 59] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:03,605 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,605 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,656 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,656 - INFO - ==================================================
2026-02-10 10:06:03,668 - INFO -   [탐색 60] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:03,681 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,681 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,732 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,732 - INFO - ==================================================
2026-02-10 10:06:03,744 - INFO -   [탐색 61] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:03,757 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,757 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,808 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,808 - INFO - ==================================================
2026-02-10 10:06:03,820 - INFO -   [탐색 62] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:03,832 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:03,832 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:03,884 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:03,884 - INFO - ==================================================
2026-02-10 10:06:03,895 - INFO -   [탐색 63] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,067 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,067 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,141 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,141 - INFO - ==================================================
2026-02-10 10:06:04,154 - INFO -   [탐색 64] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,167 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,167 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,200 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,201 - INFO - ==================================================
2026-02-10 10:06:04,213 - INFO -   [탐색 65] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,227 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,227 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,261 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,261 - INFO - ==================================================
2026-02-10 10:06:04,274 - INFO -   [탐색 66] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,287 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,287 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,321 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,321 - INFO - ==================================================
2026-02-10 10:06:04,333 - INFO -   [탐색 67] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,346 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,346 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,409 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,409 - INFO - ==================================================
2026-02-10 10:06:04,421 - INFO -   [탐색 68] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,434 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,434 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,487 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,487 - INFO - ==================================================
2026-02-10 10:06:04,499 - INFO -   [탐색 69] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,511 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,511 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,563 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,563 - INFO - ==================================================
2026-02-10 10:06:04,575 - INFO -   [탐색 70] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,588 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,588 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,639 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,639 - INFO - ==================================================
2026-02-10 10:06:04,651 - INFO -   [탐색 71] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,664 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,664 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,716 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,716 - INFO - ==================================================
2026-02-10 10:06:04,728 - INFO -   [탐색 72] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,741 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,741 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,793 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,793 - INFO - ==================================================
2026-02-10 10:06:04,804 - INFO -   [탐색 73] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:04,817 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:04,817 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:04,869 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:04,869 - INFO - ==================================================
2026-02-10 10:06:04,881 - INFO -   [탐색 74] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,054 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,054 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,124 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,125 - INFO - ==================================================
2026-02-10 10:06:05,137 - INFO -   [탐색 75] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,150 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,150 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,215 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,215 - INFO - ==================================================
2026-02-10 10:06:05,227 - INFO -   [탐색 76] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,241 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,241 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,275 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,275 - INFO - ==================================================
2026-02-10 10:06:05,288 - INFO -   [탐색 77] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,302 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,302 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,336 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,336 - INFO - ==================================================
2026-02-10 10:06:05,349 - INFO -   [탐색 78] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,362 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,362 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,396 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,396 - INFO - ==================================================
2026-02-10 10:06:05,409 - INFO -   [탐색 79] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,422 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,422 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,469 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,469 - INFO - ==================================================
2026-02-10 10:06:05,481 - INFO -   [탐색 80] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,494 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,494 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,541 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,541 - INFO - ==================================================
2026-02-10 10:06:05,553 - INFO -   [탐색 81] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,565 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,565 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,612 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,612 - INFO - ==================================================
2026-02-10 10:06:05,623 - INFO -   [탐색 82] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,636 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,636 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,686 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,686 - INFO - ==================================================
2026-02-10 10:06:05,698 - INFO -   [탐색 83] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,711 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,711 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,762 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,762 - INFO - ==================================================
2026-02-10 10:06:05,774 - INFO -   [탐색 84] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,786 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,786 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:05,837 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:05,837 - INFO - ==================================================
2026-02-10 10:06:05,849 - INFO -   [탐색 85] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:05,861 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:05,861 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,093 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,093 - INFO - ==================================================
2026-02-10 10:06:06,105 - INFO -   [탐색 86] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,119 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,119 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,184 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,184 - INFO - ==================================================
2026-02-10 10:06:06,196 - INFO -   [탐색 87] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,209 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,209 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,276 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,277 - INFO - ==================================================
2026-02-10 10:06:06,288 - INFO -   [탐색 88] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,301 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,301 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,359 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,359 - INFO - ==================================================
2026-02-10 10:06:06,371 - INFO -   [탐색 89] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,385 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,385 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,419 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,419 - INFO - ==================================================
2026-02-10 10:06:06,432 - INFO -   [탐색 90] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,445 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,445 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,479 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,479 - INFO - ==================================================
2026-02-10 10:06:06,492 - INFO -   [탐색 91] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,505 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,505 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,562 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,562 - INFO - ==================================================
2026-02-10 10:06:06,575 - INFO -   [탐색 92] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,587 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,588 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,644 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,644 - INFO - ==================================================
2026-02-10 10:06:06,656 - INFO -   [탐색 93] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,669 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,669 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,724 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,724 - INFO - ==================================================
2026-02-10 10:06:06,736 - INFO -   [탐색 94] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,748 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,748 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,797 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,797 - INFO - ==================================================
2026-02-10 10:06:06,809 - INFO -   [탐색 95] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,822 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,822 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,871 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,871 - INFO - ==================================================
2026-02-10 10:06:06,883 - INFO -   [탐색 96] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,895 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,895 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:06,946 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:06,946 - INFO - ==================================================
2026-02-10 10:06:06,958 - INFO -   [탐색 97] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:06,971 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:06,971 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:07,202 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:07,202 - INFO - ==================================================
2026-02-10 10:06:07,215 - INFO -   [탐색 98] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:07,227 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:07,227 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:07,289 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:07,289 - INFO - ==================================================
2026-02-10 10:06:07,301 - INFO -   [탐색 99] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:07,314 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:07,314 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:07,366 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.3281250000000001)에 맞춰 변경되었습니다.
2026-02-10 10:06:07,366 - INFO - ==================================================
2026-02-10 10:06:07,378 - INFO -   [탐색 100] 희소도: 0.3281 -> FLOPs: 0.1809 GFLOPs (감소율: 53.04%)
2026-02-10 10:06:07,379 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.3277 입니다.
2026-02-10 10:06:07,379 - INFO - ================================================================================
2026-02-10 10:06:07,380 - INFO - 계산된 Pruning 정보(희소도: 0.3277)를 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/pruning_info.yaml'에 저장했습니다.
2026-02-10 10:06:07,392 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 10:06:07,418 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:06:07,418 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:06:07,456 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.327744140625)에 맞춰 변경되었습니다.
2026-02-10 10:06:07,456 - INFO - ==================================================
2026-02-10 10:06:07,457 - INFO - ==================================================
2026-02-10 10:06:07,457 - INFO - 모델 파라미터 수:
2026-02-10 10:06:07,457 - INFO -   - 총 파라미터: 1,146,105 개
2026-02-10 10:06:07,457 - INFO -   - 학습 가능한 파라미터: 1,146,105 개
2026-02-10 10:06:07,469 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 10:06:07,494 - INFO - FLOPs가 0.3853 GFLOPs에서 0.1822 GFLOPs로 감소했습니다 (감소율: 52.70%).
2026-02-10 10:06:07,494 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 10:06:07,494 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 10:06:07,494 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 10:06:07,494 - INFO - ==================================================
2026-02-10 10:06:07,494 - INFO - train 모드를 시작합니다.
2026-02-10 10:06:07,494 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 10:06:07,494 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 10:06:07,494 - INFO - --------------------------------------------------
2026-02-10 10:06:07,495 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 10:06:08,996 - INFO - [Train] [11/90] | Loss: 0.7735 | Train Acc: 69.12%
2026-02-10 10:06:09,406 - INFO - [Valid] [11/90] | Loss: 0.6029 | Val Acc: 76.11%
2026-02-10 10:06:09,408 - INFO - [Metrics for 'abnormal'] | Precision: 0.7065 | Recall: 0.8280 | F1: 0.7625
2026-02-10 10:06:09,408 - INFO - [Metrics for 'normal'] | Precision: 0.8258 | Recall: 0.7033 | F1: 0.7596
2026-02-10 10:06:09,423 - INFO - [Best Model Saved] (val loss: 0.6029) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:06:09,423 - INFO - --------------------------------------------------
2026-02-10 10:06:09,424 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 10:06:10,940 - INFO - [Train] [12/90] | Loss: 0.6053 | Train Acc: 72.10%
2026-02-10 10:06:11,343 - INFO - [Valid] [12/90] | Loss: 0.5575 | Val Acc: 75.52%
2026-02-10 10:06:11,346 - INFO - [Metrics for 'abnormal'] | Precision: 0.7102 | Recall: 0.7962 | F1: 0.7508
2026-02-10 10:06:11,346 - INFO - [Metrics for 'normal'] | Precision: 0.8037 | Recall: 0.7198 | F1: 0.7594
2026-02-10 10:06:11,359 - INFO - [Best Model Saved] (val loss: 0.5575) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:06:11,359 - INFO - --------------------------------------------------
2026-02-10 10:06:11,359 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 10:06:13,070 - INFO - [Train] [13/90] | Loss: 0.5442 | Train Acc: 77.16%
2026-02-10 10:06:13,761 - INFO - [Valid] [13/90] | Loss: 0.6205 | Val Acc: 73.16%
2026-02-10 10:06:13,765 - INFO - [Metrics for 'abnormal'] | Precision: 0.6618 | Recall: 0.8599 | F1: 0.7479
2026-02-10 10:06:13,765 - INFO - [Metrics for 'normal'] | Precision: 0.8370 | Recall: 0.6209 | F1: 0.7129
2026-02-10 10:06:13,767 - INFO - --------------------------------------------------
2026-02-10 10:06:13,768 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 10:06:15,871 - INFO - [Train] [14/90] | Loss: 0.5524 | Train Acc: 77.46%
2026-02-10 10:06:16,606 - INFO - [Valid] [14/90] | Loss: 0.7207 | Val Acc: 73.75%
2026-02-10 10:06:16,610 - INFO - [Metrics for 'abnormal'] | Precision: 0.7024 | Recall: 0.7516 | F1: 0.7262
2026-02-10 10:06:16,610 - INFO - [Metrics for 'normal'] | Precision: 0.7719 | Recall: 0.7253 | F1: 0.7479
2026-02-10 10:06:16,612 - INFO - --------------------------------------------------
2026-02-10 10:06:16,617 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 10:06:18,917 - INFO - [Train] [15/90] | Loss: 0.5603 | Train Acc: 78.57%
2026-02-10 10:06:19,607 - INFO - [Valid] [15/90] | Loss: 0.8900 | Val Acc: 66.67%
2026-02-10 10:06:19,610 - INFO - [Metrics for 'abnormal'] | Precision: 0.5840 | Recall: 0.9745 | F1: 0.7303
2026-02-10 10:06:19,610 - INFO - [Metrics for 'normal'] | Precision: 0.9481 | Recall: 0.4011 | F1: 0.5637
2026-02-10 10:06:19,611 - INFO - --------------------------------------------------
2026-02-10 10:06:19,612 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 10:06:21,822 - INFO - [Train] [16/90] | Loss: 0.5382 | Train Acc: 79.46%
2026-02-10 10:06:22,535 - INFO - [Valid] [16/90] | Loss: 0.5907 | Val Acc: 73.75%
2026-02-10 10:06:22,540 - INFO - [Metrics for 'abnormal'] | Precision: 0.6683 | Recall: 0.8599 | F1: 0.7521
2026-02-10 10:06:22,540 - INFO - [Metrics for 'normal'] | Precision: 0.8394 | Recall: 0.6319 | F1: 0.7210
2026-02-10 10:06:22,541 - INFO - --------------------------------------------------
2026-02-10 10:06:22,543 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 10:06:24,775 - INFO - [Train] [17/90] | Loss: 0.4976 | Train Acc: 80.88%
2026-02-10 10:06:25,519 - INFO - [Valid] [17/90] | Loss: 0.8021 | Val Acc: 72.57%
2026-02-10 10:06:25,524 - INFO - [Metrics for 'abnormal'] | Precision: 0.6649 | Recall: 0.8217 | F1: 0.7350
2026-02-10 10:06:25,524 - INFO - [Metrics for 'normal'] | Precision: 0.8069 | Recall: 0.6429 | F1: 0.7156
2026-02-10 10:06:25,525 - INFO - --------------------------------------------------
2026-02-10 10:06:25,526 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 10:06:27,712 - INFO - [Train] [18/90] | Loss: 0.5225 | Train Acc: 79.91%
2026-02-10 10:06:28,406 - INFO - [Valid] [18/90] | Loss: 0.5443 | Val Acc: 77.88%
2026-02-10 10:06:28,411 - INFO - [Metrics for 'abnormal'] | Precision: 0.8154 | Recall: 0.6752 | F1: 0.7387
2026-02-10 10:06:28,411 - INFO - [Metrics for 'normal'] | Precision: 0.7560 | Recall: 0.8681 | F1: 0.8082
2026-02-10 10:06:28,434 - INFO - [Best Model Saved] (val loss: 0.5443) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:06:28,434 - INFO - --------------------------------------------------
2026-02-10 10:06:28,435 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 10:06:30,669 - INFO - [Train] [19/90] | Loss: 0.5153 | Train Acc: 80.80%
2026-02-10 10:06:31,330 - INFO - [Valid] [19/90] | Loss: 0.5637 | Val Acc: 75.81%
2026-02-10 10:06:31,334 - INFO - [Metrics for 'abnormal'] | Precision: 0.7072 | Recall: 0.8153 | F1: 0.7574
2026-02-10 10:06:31,334 - INFO - [Metrics for 'normal'] | Precision: 0.8165 | Recall: 0.7088 | F1: 0.7588
2026-02-10 10:06:31,336 - INFO - --------------------------------------------------
2026-02-10 10:06:31,337 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 10:06:33,350 - INFO - [Train] [20/90] | Loss: 0.4891 | Train Acc: 82.44%
2026-02-10 10:06:34,132 - INFO - [Valid] [20/90] | Loss: 0.5746 | Val Acc: 75.81%
2026-02-10 10:06:34,137 - INFO - [Metrics for 'abnormal'] | Precision: 0.8505 | Recall: 0.5796 | F1: 0.6894
2026-02-10 10:06:34,137 - INFO - [Metrics for 'normal'] | Precision: 0.7155 | Recall: 0.9121 | F1: 0.8019
2026-02-10 10:06:34,139 - INFO - --------------------------------------------------
2026-02-10 10:06:34,140 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 10:06:36,361 - INFO - [Train] [21/90] | Loss: 0.4776 | Train Acc: 82.74%
2026-02-10 10:06:37,183 - INFO - [Valid] [21/90] | Loss: 0.5683 | Val Acc: 76.11%
2026-02-10 10:06:37,187 - INFO - [Metrics for 'abnormal'] | Precision: 0.7794 | Recall: 0.6752 | F1: 0.7235
2026-02-10 10:06:37,187 - INFO - [Metrics for 'normal'] | Precision: 0.7488 | Recall: 0.8352 | F1: 0.7896
2026-02-10 10:06:37,188 - INFO - --------------------------------------------------
2026-02-10 10:06:37,189 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 10:06:39,291 - INFO - [Train] [22/90] | Loss: 0.4609 | Train Acc: 83.33%
2026-02-10 10:06:40,021 - INFO - [Valid] [22/90] | Loss: 0.6185 | Val Acc: 79.35%
2026-02-10 10:06:40,025 - INFO - [Metrics for 'abnormal'] | Precision: 0.7351 | Recall: 0.8662 | F1: 0.7953
2026-02-10 10:06:40,025 - INFO - [Metrics for 'normal'] | Precision: 0.8636 | Recall: 0.7308 | F1: 0.7917
2026-02-10 10:06:40,027 - INFO - --------------------------------------------------
2026-02-10 10:06:40,028 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 10:06:42,167 - INFO - [Train] [23/90] | Loss: 0.4616 | Train Acc: 85.27%
2026-02-10 10:06:42,725 - INFO - [Valid] [23/90] | Loss: 0.5539 | Val Acc: 78.76%
2026-02-10 10:06:42,730 - INFO - [Metrics for 'abnormal'] | Precision: 0.7114 | Recall: 0.9108 | F1: 0.7989
2026-02-10 10:06:42,730 - INFO - [Metrics for 'normal'] | Precision: 0.8986 | Recall: 0.6813 | F1: 0.7750
2026-02-10 10:06:42,731 - INFO - --------------------------------------------------
2026-02-10 10:06:42,732 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 10:06:45,008 - INFO - [Train] [24/90] | Loss: 0.4781 | Train Acc: 83.56%
2026-02-10 10:06:45,575 - INFO - [Valid] [24/90] | Loss: 0.5215 | Val Acc: 82.01%
2026-02-10 10:06:45,580 - INFO - [Metrics for 'abnormal'] | Precision: 0.8582 | Recall: 0.7325 | F1: 0.7904
2026-02-10 10:06:45,580 - INFO - [Metrics for 'normal'] | Precision: 0.7951 | Recall: 0.8956 | F1: 0.8424
2026-02-10 10:06:45,603 - INFO - [Best Model Saved] (val loss: 0.5215) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:06:45,603 - INFO - --------------------------------------------------
2026-02-10 10:06:45,604 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 10:06:47,971 - INFO - [Train] [25/90] | Loss: 0.4193 | Train Acc: 87.13%
2026-02-10 10:06:48,500 - INFO - [Valid] [25/90] | Loss: 0.4840 | Val Acc: 80.24%
2026-02-10 10:06:48,504 - INFO - [Metrics for 'abnormal'] | Precision: 0.7368 | Recall: 0.8917 | F1: 0.8069
2026-02-10 10:06:48,505 - INFO - [Metrics for 'normal'] | Precision: 0.8859 | Recall: 0.7253 | F1: 0.7976
2026-02-10 10:06:48,528 - INFO - [Best Model Saved] (val loss: 0.4840) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:06:48,528 - INFO - --------------------------------------------------
2026-02-10 10:06:48,529 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 10:06:50,912 - INFO - [Train] [26/90] | Loss: 0.4502 | Train Acc: 85.71%
2026-02-10 10:06:51,489 - INFO - [Valid] [26/90] | Loss: 0.5876 | Val Acc: 77.58%
2026-02-10 10:06:51,493 - INFO - [Metrics for 'abnormal'] | Precision: 0.7098 | Recall: 0.8726 | F1: 0.7829
2026-02-10 10:06:51,493 - INFO - [Metrics for 'normal'] | Precision: 0.8630 | Recall: 0.6923 | F1: 0.7683
2026-02-10 10:06:51,494 - INFO - --------------------------------------------------
2026-02-10 10:06:51,495 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 10:06:53,878 - INFO - [Train] [27/90] | Loss: 0.4886 | Train Acc: 82.81%
2026-02-10 10:06:54,490 - INFO - [Valid] [27/90] | Loss: 0.5528 | Val Acc: 81.12%
2026-02-10 10:06:54,494 - INFO - [Metrics for 'abnormal'] | Precision: 0.7688 | Recall: 0.8471 | F1: 0.8061
2026-02-10 10:06:54,494 - INFO - [Metrics for 'normal'] | Precision: 0.8554 | Recall: 0.7802 | F1: 0.8161
2026-02-10 10:06:54,496 - INFO - --------------------------------------------------
2026-02-10 10:06:54,497 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 10:06:56,976 - INFO - [Train] [28/90] | Loss: 0.4659 | Train Acc: 84.45%
2026-02-10 10:06:57,501 - INFO - [Valid] [28/90] | Loss: 0.5644 | Val Acc: 77.58%
2026-02-10 10:06:57,506 - INFO - [Metrics for 'abnormal'] | Precision: 0.7368 | Recall: 0.8025 | F1: 0.7683
2026-02-10 10:06:57,506 - INFO - [Metrics for 'normal'] | Precision: 0.8155 | Recall: 0.7527 | F1: 0.7829
2026-02-10 10:06:57,507 - INFO - --------------------------------------------------
2026-02-10 10:06:57,508 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 10:06:59,914 - INFO - [Train] [29/90] | Loss: 0.4439 | Train Acc: 85.34%
2026-02-10 10:07:00,423 - INFO - [Valid] [29/90] | Loss: 0.5660 | Val Acc: 81.12%
2026-02-10 10:07:00,427 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.7707 | F1: 0.7908
2026-02-10 10:07:00,427 - INFO - [Metrics for 'normal'] | Precision: 0.8105 | Recall: 0.8462 | F1: 0.8280
2026-02-10 10:07:00,429 - INFO - --------------------------------------------------
2026-02-10 10:07:00,430 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 10:07:02,888 - INFO - [Train] [30/90] | Loss: 0.4142 | Train Acc: 88.24%
2026-02-10 10:07:03,431 - INFO - [Valid] [30/90] | Loss: 0.6019 | Val Acc: 81.71%
2026-02-10 10:07:03,436 - INFO - [Metrics for 'abnormal'] | Precision: 0.7811 | Recall: 0.8408 | F1: 0.8098
2026-02-10 10:07:03,436 - INFO - [Metrics for 'normal'] | Precision: 0.8529 | Recall: 0.7967 | F1: 0.8239
2026-02-10 10:07:03,438 - INFO - --------------------------------------------------
2026-02-10 10:07:03,439 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 10:07:05,810 - INFO - [Train] [31/90] | Loss: 0.4121 | Train Acc: 87.65%
2026-02-10 10:07:06,456 - INFO - [Valid] [31/90] | Loss: 0.5147 | Val Acc: 80.24%
2026-02-10 10:07:06,460 - INFO - [Metrics for 'abnormal'] | Precision: 0.7711 | Recall: 0.8153 | F1: 0.7926
2026-02-10 10:07:06,460 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.7912 | F1: 0.8113
2026-02-10 10:07:06,462 - INFO - --------------------------------------------------
2026-02-10 10:07:06,463 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 10:07:08,885 - INFO - [Train] [32/90] | Loss: 0.4235 | Train Acc: 86.61%
2026-02-10 10:07:09,466 - INFO - [Valid] [32/90] | Loss: 0.5653 | Val Acc: 83.19%
2026-02-10 10:07:09,470 - INFO - [Metrics for 'abnormal'] | Precision: 0.7747 | Recall: 0.8981 | F1: 0.8319
2026-02-10 10:07:09,471 - INFO - [Metrics for 'normal'] | Precision: 0.8981 | Recall: 0.7747 | F1: 0.8319
2026-02-10 10:07:09,472 - INFO - --------------------------------------------------
2026-02-10 10:07:09,473 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 10:07:11,956 - INFO - [Train] [33/90] | Loss: 0.3975 | Train Acc: 87.95%
2026-02-10 10:07:12,566 - INFO - [Valid] [33/90] | Loss: 0.5274 | Val Acc: 83.48%
2026-02-10 10:07:12,570 - INFO - [Metrics for 'abnormal'] | Precision: 0.8137 | Recall: 0.8344 | F1: 0.8239
2026-02-10 10:07:12,570 - INFO - [Metrics for 'normal'] | Precision: 0.8539 | Recall: 0.8352 | F1: 0.8444
2026-02-10 10:07:12,571 - INFO - --------------------------------------------------
2026-02-10 10:07:12,572 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 10:07:14,981 - INFO - [Train] [34/90] | Loss: 0.3587 | Train Acc: 91.96%
2026-02-10 10:07:15,569 - INFO - [Valid] [34/90] | Loss: 0.4952 | Val Acc: 82.89%
2026-02-10 10:07:15,573 - INFO - [Metrics for 'abnormal'] | Precision: 0.8194 | Recall: 0.8089 | F1: 0.8141
2026-02-10 10:07:15,574 - INFO - [Metrics for 'normal'] | Precision: 0.8370 | Recall: 0.8462 | F1: 0.8415
2026-02-10 10:07:15,575 - INFO - --------------------------------------------------
2026-02-10 10:07:15,576 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 10:07:18,054 - INFO - [Train] [35/90] | Loss: 0.3889 | Train Acc: 89.29%
2026-02-10 10:07:18,616 - INFO - [Valid] [35/90] | Loss: 0.5194 | Val Acc: 80.24%
2026-02-10 10:07:18,620 - INFO - [Metrics for 'abnormal'] | Precision: 0.8571 | Recall: 0.6879 | F1: 0.7633
2026-02-10 10:07:18,620 - INFO - [Metrics for 'normal'] | Precision: 0.7700 | Recall: 0.9011 | F1: 0.8304
2026-02-10 10:07:18,622 - INFO - --------------------------------------------------
2026-02-10 10:07:18,623 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 10:07:21,018 - INFO - [Train] [36/90] | Loss: 0.3858 | Train Acc: 90.55%
2026-02-10 10:07:21,617 - INFO - [Valid] [36/90] | Loss: 0.5836 | Val Acc: 81.42%
2026-02-10 10:07:21,621 - INFO - [Metrics for 'abnormal'] | Precision: 0.8176 | Recall: 0.7707 | F1: 0.7934
2026-02-10 10:07:21,621 - INFO - [Metrics for 'normal'] | Precision: 0.8115 | Recall: 0.8516 | F1: 0.8311
2026-02-10 10:07:21,623 - INFO - --------------------------------------------------
2026-02-10 10:07:21,624 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 10:07:24,082 - INFO - [Train] [37/90] | Loss: 0.3559 | Train Acc: 91.07%
2026-02-10 10:07:24,689 - INFO - [Valid] [37/90] | Loss: 0.5743 | Val Acc: 80.53%
2026-02-10 10:07:24,694 - INFO - [Metrics for 'abnormal'] | Precision: 0.8473 | Recall: 0.7070 | F1: 0.7708
2026-02-10 10:07:24,694 - INFO - [Metrics for 'normal'] | Precision: 0.7788 | Recall: 0.8901 | F1: 0.8308
2026-02-10 10:07:24,695 - INFO - --------------------------------------------------
2026-02-10 10:07:24,696 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 10:07:27,131 - INFO - [Train] [38/90] | Loss: 0.3582 | Train Acc: 90.85%
2026-02-10 10:07:27,805 - INFO - [Valid] [38/90] | Loss: 0.5373 | Val Acc: 82.89%
2026-02-10 10:07:27,810 - INFO - [Metrics for 'abnormal'] | Precision: 0.8462 | Recall: 0.7707 | F1: 0.8067
2026-02-10 10:07:27,810 - INFO - [Metrics for 'normal'] | Precision: 0.8163 | Recall: 0.8791 | F1: 0.8466
2026-02-10 10:07:27,811 - INFO - --------------------------------------------------
2026-02-10 10:07:27,813 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 10:07:30,184 - INFO - [Train] [39/90] | Loss: 0.3551 | Train Acc: 91.29%
2026-02-10 10:07:30,782 - INFO - [Valid] [39/90] | Loss: 0.5961 | Val Acc: 82.30%
2026-02-10 10:07:30,787 - INFO - [Metrics for 'abnormal'] | Precision: 0.8089 | Recall: 0.8089 | F1: 0.8089
2026-02-10 10:07:30,787 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8352 | F1: 0.8352
2026-02-10 10:07:30,788 - INFO - --------------------------------------------------
2026-02-10 10:07:30,789 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 10:07:33,169 - INFO - [Train] [40/90] | Loss: 0.3661 | Train Acc: 89.96%
2026-02-10 10:07:33,799 - INFO - [Valid] [40/90] | Loss: 0.7494 | Val Acc: 78.76%
2026-02-10 10:07:33,803 - INFO - [Metrics for 'abnormal'] | Precision: 0.8696 | Recall: 0.6369 | F1: 0.7353
2026-02-10 10:07:33,803 - INFO - [Metrics for 'normal'] | Precision: 0.7455 | Recall: 0.9176 | F1: 0.8227
2026-02-10 10:07:33,804 - INFO - --------------------------------------------------
2026-02-10 10:07:33,805 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 10:07:36,161 - INFO - [Train] [41/90] | Loss: 0.3602 | Train Acc: 90.40%
2026-02-10 10:07:36,783 - INFO - [Valid] [41/90] | Loss: 0.4446 | Val Acc: 83.78%
2026-02-10 10:07:36,785 - INFO - [Metrics for 'abnormal'] | Precision: 0.8269 | Recall: 0.8217 | F1: 0.8243
2026-02-10 10:07:36,785 - INFO - [Metrics for 'normal'] | Precision: 0.8470 | Recall: 0.8516 | F1: 0.8493
2026-02-10 10:07:36,812 - INFO - [Best Model Saved] (val loss: 0.4446) -> 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:07:36,812 - INFO - --------------------------------------------------
2026-02-10 10:07:36,813 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 10:07:39,256 - INFO - [Train] [42/90] | Loss: 0.3572 | Train Acc: 91.29%
2026-02-10 10:07:39,824 - INFO - [Valid] [42/90] | Loss: 0.5083 | Val Acc: 82.01%
2026-02-10 10:07:39,828 - INFO - [Metrics for 'abnormal'] | Precision: 0.8529 | Recall: 0.7389 | F1: 0.7918
2026-02-10 10:07:39,828 - INFO - [Metrics for 'normal'] | Precision: 0.7980 | Recall: 0.8901 | F1: 0.8416
2026-02-10 10:07:39,830 - INFO - --------------------------------------------------
2026-02-10 10:07:39,831 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 10:07:42,240 - INFO - [Train] [43/90] | Loss: 0.3069 | Train Acc: 94.72%
2026-02-10 10:07:42,772 - INFO - [Valid] [43/90] | Loss: 0.5613 | Val Acc: 83.78%
2026-02-10 10:07:42,775 - INFO - [Metrics for 'abnormal'] | Precision: 0.7742 | Recall: 0.9172 | F1: 0.8397
2026-02-10 10:07:42,775 - INFO - [Metrics for 'normal'] | Precision: 0.9150 | Recall: 0.7692 | F1: 0.8358
2026-02-10 10:07:42,776 - INFO - --------------------------------------------------
2026-02-10 10:07:42,777 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 10:07:45,178 - INFO - [Train] [44/90] | Loss: 0.2981 | Train Acc: 96.06%
2026-02-10 10:07:45,770 - INFO - [Valid] [44/90] | Loss: 0.5039 | Val Acc: 82.60%
2026-02-10 10:07:45,775 - INFO - [Metrics for 'abnormal'] | Precision: 0.8500 | Recall: 0.7580 | F1: 0.8013
2026-02-10 10:07:45,775 - INFO - [Metrics for 'normal'] | Precision: 0.8090 | Recall: 0.8846 | F1: 0.8451
2026-02-10 10:07:45,776 - INFO - --------------------------------------------------
2026-02-10 10:07:45,777 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 10:07:48,158 - INFO - [Train] [45/90] | Loss: 0.2861 | Train Acc: 96.06%
2026-02-10 10:07:48,775 - INFO - [Valid] [45/90] | Loss: 0.4800 | Val Acc: 82.60%
2026-02-10 10:07:48,779 - INFO - [Metrics for 'abnormal'] | Precision: 0.8311 | Recall: 0.7834 | F1: 0.8066
2026-02-10 10:07:48,779 - INFO - [Metrics for 'normal'] | Precision: 0.8220 | Recall: 0.8626 | F1: 0.8418
2026-02-10 10:07:48,781 - INFO - --------------------------------------------------
2026-02-10 10:07:48,782 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 10:07:51,143 - INFO - [Train] [46/90] | Loss: 0.3007 | Train Acc: 95.16%
2026-02-10 10:07:51,771 - INFO - [Valid] [46/90] | Loss: 0.5326 | Val Acc: 83.48%
2026-02-10 10:07:51,775 - INFO - [Metrics for 'abnormal'] | Precision: 0.8582 | Recall: 0.7707 | F1: 0.8121
2026-02-10 10:07:51,775 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.8901 | F1: 0.8526
2026-02-10 10:07:51,777 - INFO - --------------------------------------------------
2026-02-10 10:07:51,778 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 10:07:54,188 - INFO - [Train] [47/90] | Loss: 0.2862 | Train Acc: 95.68%
2026-02-10 10:07:54,711 - INFO - [Valid] [47/90] | Loss: 0.5308 | Val Acc: 80.24%
2026-02-10 10:07:54,713 - INFO - [Metrics for 'abnormal'] | Precision: 0.8516 | Recall: 0.6943 | F1: 0.7649
2026-02-10 10:07:54,714 - INFO - [Metrics for 'normal'] | Precision: 0.7725 | Recall: 0.8956 | F1: 0.8295
2026-02-10 10:07:54,714 - INFO - --------------------------------------------------
2026-02-10 10:07:54,715 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 10:07:57,238 - INFO - [Train] [48/90] | Loss: 0.2727 | Train Acc: 96.80%
2026-02-10 10:07:57,769 - INFO - [Valid] [48/90] | Loss: 0.5063 | Val Acc: 81.71%
2026-02-10 10:07:57,773 - INFO - [Metrics for 'abnormal'] | Precision: 0.7714 | Recall: 0.8599 | F1: 0.8133
2026-02-10 10:07:57,773 - INFO - [Metrics for 'normal'] | Precision: 0.8659 | Recall: 0.7802 | F1: 0.8208
2026-02-10 10:07:57,774 - INFO - --------------------------------------------------
2026-02-10 10:07:57,775 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 10:08:00,210 - INFO - [Train] [49/90] | Loss: 0.2712 | Train Acc: 97.02%
2026-02-10 10:08:00,737 - INFO - [Valid] [49/90] | Loss: 0.5249 | Val Acc: 83.78%
2026-02-10 10:08:00,740 - INFO - [Metrics for 'abnormal'] | Precision: 0.8312 | Recall: 0.8153 | F1: 0.8232
2026-02-10 10:08:00,740 - INFO - [Metrics for 'normal'] | Precision: 0.8432 | Recall: 0.8571 | F1: 0.8501
2026-02-10 10:08:00,740 - INFO - --------------------------------------------------
2026-02-10 10:08:00,741 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 10:08:03,165 - INFO - [Train] [50/90] | Loss: 0.2788 | Train Acc: 96.50%
2026-02-10 10:08:03,683 - INFO - [Valid] [50/90] | Loss: 0.5249 | Val Acc: 82.30%
2026-02-10 10:08:03,686 - INFO - [Metrics for 'abnormal'] | Precision: 0.8440 | Recall: 0.7580 | F1: 0.7987
2026-02-10 10:08:03,686 - INFO - [Metrics for 'normal'] | Precision: 0.8081 | Recall: 0.8791 | F1: 0.8421
2026-02-10 10:08:03,687 - INFO - --------------------------------------------------
2026-02-10 10:08:03,687 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 10:08:06,136 - INFO - [Train] [51/90] | Loss: 0.2635 | Train Acc: 97.40%
2026-02-10 10:08:06,655 - INFO - [Valid] [51/90] | Loss: 0.5575 | Val Acc: 82.30%
2026-02-10 10:08:06,658 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-02-10 10:08:06,658 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-02-10 10:08:06,659 - INFO - --------------------------------------------------
2026-02-10 10:08:06,660 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 10:08:09,165 - INFO - [Train] [52/90] | Loss: 0.2594 | Train Acc: 97.84%
2026-02-10 10:08:09,708 - INFO - [Valid] [52/90] | Loss: 0.5492 | Val Acc: 80.53%
2026-02-10 10:08:09,711 - INFO - [Metrics for 'abnormal'] | Precision: 0.8054 | Recall: 0.7643 | F1: 0.7843
2026-02-10 10:08:09,711 - INFO - [Metrics for 'normal'] | Precision: 0.8053 | Recall: 0.8407 | F1: 0.8226
2026-02-10 10:08:09,712 - INFO - --------------------------------------------------
2026-02-10 10:08:09,712 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 10:08:12,226 - INFO - [Train] [53/90] | Loss: 0.2645 | Train Acc: 97.84%
2026-02-10 10:08:12,857 - INFO - [Valid] [53/90] | Loss: 0.5438 | Val Acc: 82.60%
2026-02-10 10:08:12,862 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8726 | F1: 0.8228
2026-02-10 10:08:12,862 - INFO - [Metrics for 'normal'] | Precision: 0.8773 | Recall: 0.7857 | F1: 0.8290
2026-02-10 10:08:12,863 - INFO - --------------------------------------------------
2026-02-10 10:08:12,864 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 10:08:15,332 - INFO - [Train] [54/90] | Loss: 0.2514 | Train Acc: 97.69%
2026-02-10 10:08:15,921 - INFO - [Valid] [54/90] | Loss: 0.5063 | Val Acc: 82.89%
2026-02-10 10:08:15,925 - INFO - [Metrics for 'abnormal'] | Precision: 0.8075 | Recall: 0.8280 | F1: 0.8176
2026-02-10 10:08:15,926 - INFO - [Metrics for 'normal'] | Precision: 0.8483 | Recall: 0.8297 | F1: 0.8389
2026-02-10 10:08:15,927 - INFO - --------------------------------------------------
2026-02-10 10:08:15,928 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 10:08:18,363 - INFO - [Train] [55/90] | Loss: 0.2532 | Train Acc: 98.21%
2026-02-10 10:08:18,985 - INFO - [Valid] [55/90] | Loss: 0.5143 | Val Acc: 83.19%
2026-02-10 10:08:18,989 - INFO - [Metrics for 'abnormal'] | Precision: 0.8086 | Recall: 0.8344 | F1: 0.8213
2026-02-10 10:08:18,989 - INFO - [Metrics for 'normal'] | Precision: 0.8531 | Recall: 0.8297 | F1: 0.8412
2026-02-10 10:08:18,990 - INFO - --------------------------------------------------
2026-02-10 10:08:18,991 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 10:08:21,329 - INFO - [Train] [56/90] | Loss: 0.2546 | Train Acc: 98.07%
2026-02-10 10:08:21,974 - INFO - [Valid] [56/90] | Loss: 0.5354 | Val Acc: 82.89%
2026-02-10 10:08:21,978 - INFO - [Metrics for 'abnormal'] | Precision: 0.8194 | Recall: 0.8089 | F1: 0.8141
2026-02-10 10:08:21,978 - INFO - [Metrics for 'normal'] | Precision: 0.8370 | Recall: 0.8462 | F1: 0.8415
2026-02-10 10:08:21,980 - INFO - --------------------------------------------------
2026-02-10 10:08:21,981 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 10:08:24,406 - INFO - [Train] [57/90] | Loss: 0.2600 | Train Acc: 97.84%
2026-02-10 10:08:25,000 - INFO - [Valid] [57/90] | Loss: 0.5430 | Val Acc: 80.24%
2026-02-10 10:08:25,003 - INFO - [Metrics for 'abnormal'] | Precision: 0.7394 | Recall: 0.8854 | F1: 0.8058
2026-02-10 10:08:25,004 - INFO - [Metrics for 'normal'] | Precision: 0.8808 | Recall: 0.7308 | F1: 0.7988
2026-02-10 10:08:25,005 - INFO - --------------------------------------------------
2026-02-10 10:08:25,006 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 10:08:27,516 - INFO - [Train] [58/90] | Loss: 0.2509 | Train Acc: 97.54%
2026-02-10 10:08:28,069 - INFO - [Valid] [58/90] | Loss: 0.5139 | Val Acc: 83.19%
2026-02-10 10:08:28,072 - INFO - [Metrics for 'abnormal'] | Precision: 0.8165 | Recall: 0.8217 | F1: 0.8190
2026-02-10 10:08:28,072 - INFO - [Metrics for 'normal'] | Precision: 0.8453 | Recall: 0.8407 | F1: 0.8430
2026-02-10 10:08:28,073 - INFO - --------------------------------------------------
2026-02-10 10:08:28,074 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 10:08:30,471 - INFO - [Train] [59/90] | Loss: 0.2530 | Train Acc: 98.36%
2026-02-10 10:08:31,031 - INFO - [Valid] [59/90] | Loss: 0.5309 | Val Acc: 83.78%
2026-02-10 10:08:31,035 - INFO - [Metrics for 'abnormal'] | Precision: 0.8542 | Recall: 0.7834 | F1: 0.8173
2026-02-10 10:08:31,035 - INFO - [Metrics for 'normal'] | Precision: 0.8256 | Recall: 0.8846 | F1: 0.8541
2026-02-10 10:08:31,037 - INFO - --------------------------------------------------
2026-02-10 10:08:31,038 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 10:08:33,419 - INFO - [Train] [60/90] | Loss: 0.2394 | Train Acc: 98.81%
2026-02-10 10:08:34,009 - INFO - [Valid] [60/90] | Loss: 0.5712 | Val Acc: 81.71%
2026-02-10 10:08:34,012 - INFO - [Metrics for 'abnormal'] | Precision: 0.8188 | Recall: 0.7771 | F1: 0.7974
2026-02-10 10:08:34,012 - INFO - [Metrics for 'normal'] | Precision: 0.8158 | Recall: 0.8516 | F1: 0.8333
2026-02-10 10:08:34,013 - INFO - --------------------------------------------------
2026-02-10 10:08:34,013 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 10:08:36,462 - INFO - [Train] [61/90] | Loss: 0.2537 | Train Acc: 97.92%
2026-02-10 10:08:37,088 - INFO - [Valid] [61/90] | Loss: 0.6534 | Val Acc: 74.93%
2026-02-10 10:08:37,093 - INFO - [Metrics for 'abnormal'] | Precision: 0.7571 | Recall: 0.6752 | F1: 0.7138
2026-02-10 10:08:37,093 - INFO - [Metrics for 'normal'] | Precision: 0.7437 | Recall: 0.8132 | F1: 0.7769
2026-02-10 10:08:37,095 - INFO - --------------------------------------------------
2026-02-10 10:08:37,096 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 10:08:39,504 - INFO - [Train] [62/90] | Loss: 0.2505 | Train Acc: 98.14%
2026-02-10 10:08:40,080 - INFO - [Valid] [62/90] | Loss: 0.5553 | Val Acc: 82.60%
2026-02-10 10:08:40,084 - INFO - [Metrics for 'abnormal'] | Precision: 0.8267 | Recall: 0.7898 | F1: 0.8078
2026-02-10 10:08:40,084 - INFO - [Metrics for 'normal'] | Precision: 0.8254 | Recall: 0.8571 | F1: 0.8410
2026-02-10 10:08:40,085 - INFO - --------------------------------------------------
2026-02-10 10:08:40,086 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 10:08:42,483 - INFO - [Train] [63/90] | Loss: 0.2401 | Train Acc: 98.59%
2026-02-10 10:08:43,071 - INFO - [Valid] [63/90] | Loss: 0.5731 | Val Acc: 81.12%
2026-02-10 10:08:43,075 - INFO - [Metrics for 'abnormal'] | Precision: 0.8252 | Recall: 0.7516 | F1: 0.7867
2026-02-10 10:08:43,075 - INFO - [Metrics for 'normal'] | Precision: 0.8010 | Recall: 0.8626 | F1: 0.8307
2026-02-10 10:08:43,077 - INFO - --------------------------------------------------
2026-02-10 10:08:43,077 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 10:08:45,460 - INFO - [Train] [64/90] | Loss: 0.2345 | Train Acc: 98.59%
2026-02-10 10:08:46,058 - INFO - [Valid] [64/90] | Loss: 0.5520 | Val Acc: 82.89%
2026-02-10 10:08:46,062 - INFO - [Metrics for 'abnormal'] | Precision: 0.8462 | Recall: 0.7707 | F1: 0.8067
2026-02-10 10:08:46,062 - INFO - [Metrics for 'normal'] | Precision: 0.8163 | Recall: 0.8791 | F1: 0.8466
2026-02-10 10:08:46,063 - INFO - --------------------------------------------------
2026-02-10 10:08:46,064 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 10:08:48,484 - INFO - [Train] [65/90] | Loss: 0.2278 | Train Acc: 99.26%
2026-02-10 10:08:49,051 - INFO - [Valid] [65/90] | Loss: 0.5221 | Val Acc: 84.07%
2026-02-10 10:08:49,055 - INFO - [Metrics for 'abnormal'] | Precision: 0.8552 | Recall: 0.7898 | F1: 0.8212
2026-02-10 10:08:49,056 - INFO - [Metrics for 'normal'] | Precision: 0.8299 | Recall: 0.8846 | F1: 0.8564
2026-02-10 10:08:49,057 - INFO - --------------------------------------------------
2026-02-10 10:08:49,058 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 10:08:51,439 - INFO - [Train] [66/90] | Loss: 0.2426 | Train Acc: 98.51%
2026-02-10 10:08:52,039 - INFO - [Valid] [66/90] | Loss: 0.5249 | Val Acc: 84.96%
2026-02-10 10:08:52,044 - INFO - [Metrics for 'abnormal'] | Precision: 0.8232 | Recall: 0.8599 | F1: 0.8411
2026-02-10 10:08:52,051 - INFO - [Metrics for 'normal'] | Precision: 0.8743 | Recall: 0.8407 | F1: 0.8571
2026-02-10 10:08:52,053 - INFO - --------------------------------------------------
2026-02-10 10:08:52,054 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 10:08:54,479 - INFO - [Train] [67/90] | Loss: 0.2349 | Train Acc: 99.03%
2026-02-10 10:08:55,109 - INFO - [Valid] [67/90] | Loss: 0.5272 | Val Acc: 84.66%
2026-02-10 10:08:55,113 - INFO - [Metrics for 'abnormal'] | Precision: 0.8344 | Recall: 0.8344 | F1: 0.8344
2026-02-10 10:08:55,113 - INFO - [Metrics for 'normal'] | Precision: 0.8571 | Recall: 0.8571 | F1: 0.8571
2026-02-10 10:08:55,115 - INFO - --------------------------------------------------
2026-02-10 10:08:55,116 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 10:08:57,575 - INFO - [Train] [68/90] | Loss: 0.2341 | Train Acc: 98.74%
2026-02-10 10:08:58,132 - INFO - [Valid] [68/90] | Loss: 0.5802 | Val Acc: 80.53%
2026-02-10 10:08:58,137 - INFO - [Metrics for 'abnormal'] | Precision: 0.8138 | Recall: 0.7516 | F1: 0.7815
2026-02-10 10:08:58,137 - INFO - [Metrics for 'normal'] | Precision: 0.7990 | Recall: 0.8516 | F1: 0.8245
2026-02-10 10:08:58,138 - INFO - --------------------------------------------------
2026-02-10 10:08:58,139 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 10:09:00,572 - INFO - [Train] [69/90] | Loss: 0.2331 | Train Acc: 99.03%
2026-02-10 10:09:01,095 - INFO - [Valid] [69/90] | Loss: 0.5383 | Val Acc: 83.48%
2026-02-10 10:09:01,099 - INFO - [Metrics for 'abnormal'] | Precision: 0.8301 | Recall: 0.8089 | F1: 0.8194
2026-02-10 10:09:01,099 - INFO - [Metrics for 'normal'] | Precision: 0.8387 | Recall: 0.8571 | F1: 0.8478
2026-02-10 10:09:01,101 - INFO - --------------------------------------------------
2026-02-10 10:09:01,102 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 10:09:03,476 - INFO - [Train] [70/90] | Loss: 0.2391 | Train Acc: 98.66%
2026-02-10 10:09:04,056 - INFO - [Valid] [70/90] | Loss: 0.6265 | Val Acc: 83.19%
2026-02-10 10:09:04,061 - INFO - [Metrics for 'abnormal'] | Precision: 0.8623 | Recall: 0.7580 | F1: 0.8068
2026-02-10 10:09:04,061 - INFO - [Metrics for 'normal'] | Precision: 0.8109 | Recall: 0.8956 | F1: 0.8512
2026-02-10 10:09:04,063 - INFO - --------------------------------------------------
2026-02-10 10:09:04,064 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 10:09:06,448 - INFO - [Train] [71/90] | Loss: 0.2429 | Train Acc: 97.99%
2026-02-10 10:09:07,028 - INFO - [Valid] [71/90] | Loss: 0.5374 | Val Acc: 82.60%
2026-02-10 10:09:07,032 - INFO - [Metrics for 'abnormal'] | Precision: 0.8311 | Recall: 0.7834 | F1: 0.8066
2026-02-10 10:09:07,033 - INFO - [Metrics for 'normal'] | Precision: 0.8220 | Recall: 0.8626 | F1: 0.8418
2026-02-10 10:09:07,034 - INFO - --------------------------------------------------
2026-02-10 10:09:07,035 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 10:09:09,408 - INFO - [Train] [72/90] | Loss: 0.2252 | Train Acc: 99.26%
2026-02-10 10:09:09,992 - INFO - [Valid] [72/90] | Loss: 0.5847 | Val Acc: 82.89%
2026-02-10 10:09:09,996 - INFO - [Metrics for 'abnormal'] | Precision: 0.7964 | Recall: 0.8471 | F1: 0.8210
2026-02-10 10:09:09,996 - INFO - [Metrics for 'normal'] | Precision: 0.8605 | Recall: 0.8132 | F1: 0.8362
2026-02-10 10:09:09,998 - INFO - --------------------------------------------------
2026-02-10 10:09:09,999 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 10:09:12,486 - INFO - [Train] [73/90] | Loss: 0.2269 | Train Acc: 99.03%
2026-02-10 10:09:13,072 - INFO - [Valid] [73/90] | Loss: 0.5776 | Val Acc: 82.30%
2026-02-10 10:09:13,076 - INFO - [Metrics for 'abnormal'] | Precision: 0.7904 | Recall: 0.8408 | F1: 0.8148
2026-02-10 10:09:13,077 - INFO - [Metrics for 'normal'] | Precision: 0.8547 | Recall: 0.8077 | F1: 0.8305
2026-02-10 10:09:13,078 - INFO - --------------------------------------------------
2026-02-10 10:09:13,080 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 10:09:15,474 - INFO - [Train] [74/90] | Loss: 0.2291 | Train Acc: 99.11%
2026-02-10 10:09:16,004 - INFO - [Valid] [74/90] | Loss: 0.5253 | Val Acc: 84.07%
2026-02-10 10:09:16,008 - INFO - [Metrics for 'abnormal'] | Precision: 0.8239 | Recall: 0.8344 | F1: 0.8291
2026-02-10 10:09:16,008 - INFO - [Metrics for 'normal'] | Precision: 0.8556 | Recall: 0.8462 | F1: 0.8508
2026-02-10 10:09:16,009 - INFO - --------------------------------------------------
2026-02-10 10:09:16,010 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 10:09:18,416 - INFO - [Train] [75/90] | Loss: 0.2224 | Train Acc: 99.26%
2026-02-10 10:09:18,951 - INFO - [Valid] [75/90] | Loss: 0.5256 | Val Acc: 84.37%
2026-02-10 10:09:18,954 - INFO - [Metrics for 'abnormal'] | Precision: 0.8291 | Recall: 0.8344 | F1: 0.8317
2026-02-10 10:09:18,954 - INFO - [Metrics for 'normal'] | Precision: 0.8564 | Recall: 0.8516 | F1: 0.8540
2026-02-10 10:09:18,955 - INFO - --------------------------------------------------
2026-02-10 10:09:18,956 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 10:09:21,431 - INFO - [Train] [76/90] | Loss: 0.2191 | Train Acc: 99.55%
2026-02-10 10:09:21,963 - INFO - [Valid] [76/90] | Loss: 0.5327 | Val Acc: 84.66%
2026-02-10 10:09:21,967 - INFO - [Metrics for 'abnormal'] | Precision: 0.8431 | Recall: 0.8217 | F1: 0.8323
2026-02-10 10:09:21,967 - INFO - [Metrics for 'normal'] | Precision: 0.8495 | Recall: 0.8681 | F1: 0.8587
2026-02-10 10:09:21,969 - INFO - --------------------------------------------------
2026-02-10 10:09:21,970 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 10:09:24,394 - INFO - [Train] [77/90] | Loss: 0.2145 | Train Acc: 99.85%
2026-02-10 10:09:24,932 - INFO - [Valid] [77/90] | Loss: 0.5470 | Val Acc: 84.37%
2026-02-10 10:09:24,937 - INFO - [Metrics for 'abnormal'] | Precision: 0.8421 | Recall: 0.8153 | F1: 0.8285
2026-02-10 10:09:24,937 - INFO - [Metrics for 'normal'] | Precision: 0.8449 | Recall: 0.8681 | F1: 0.8564
2026-02-10 10:09:24,938 - INFO - --------------------------------------------------
2026-02-10 10:09:24,939 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 10:09:27,354 - INFO - [Train] [78/90] | Loss: 0.2166 | Train Acc: 99.48%
2026-02-10 10:09:27,920 - INFO - [Valid] [78/90] | Loss: 0.5542 | Val Acc: 82.89%
2026-02-10 10:09:27,925 - INFO - [Metrics for 'abnormal'] | Precision: 0.8322 | Recall: 0.7898 | F1: 0.8105
2026-02-10 10:09:27,925 - INFO - [Metrics for 'normal'] | Precision: 0.8263 | Recall: 0.8626 | F1: 0.8441
2026-02-10 10:09:27,926 - INFO - --------------------------------------------------
2026-02-10 10:09:27,928 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 10:09:30,420 - INFO - [Train] [79/90] | Loss: 0.2132 | Train Acc: 99.70%
2026-02-10 10:09:30,939 - INFO - [Valid] [79/90] | Loss: 0.5897 | Val Acc: 81.42%
2026-02-10 10:09:30,943 - INFO - [Metrics for 'abnormal'] | Precision: 0.7765 | Recall: 0.8408 | F1: 0.8073
2026-02-10 10:09:30,943 - INFO - [Metrics for 'normal'] | Precision: 0.8521 | Recall: 0.7912 | F1: 0.8205
2026-02-10 10:09:30,944 - INFO - --------------------------------------------------
2026-02-10 10:09:30,946 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 10:09:33,351 - INFO - [Train] [80/90] | Loss: 0.2156 | Train Acc: 99.63%
2026-02-10 10:09:33,886 - INFO - [Valid] [80/90] | Loss: 0.5479 | Val Acc: 83.78%
2026-02-10 10:09:33,890 - INFO - [Metrics for 'abnormal'] | Precision: 0.8312 | Recall: 0.8153 | F1: 0.8232
2026-02-10 10:09:33,890 - INFO - [Metrics for 'normal'] | Precision: 0.8432 | Recall: 0.8571 | F1: 0.8501
2026-02-10 10:09:33,892 - INFO - --------------------------------------------------
2026-02-10 10:09:33,893 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 10:09:36,262 - INFO - [Train] [81/90] | Loss: 0.2093 | Train Acc: 99.78%
2026-02-10 10:09:36,818 - INFO - [Valid] [81/90] | Loss: 0.6185 | Val Acc: 82.60%
2026-02-10 10:09:36,822 - INFO - [Metrics for 'abnormal'] | Precision: 0.8403 | Recall: 0.7707 | F1: 0.8040
2026-02-10 10:09:36,822 - INFO - [Metrics for 'normal'] | Precision: 0.8154 | Recall: 0.8736 | F1: 0.8435
2026-02-10 10:09:36,824 - INFO - --------------------------------------------------
2026-02-10 10:09:36,825 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 10:09:39,210 - INFO - [Train] [82/90] | Loss: 0.2067 | Train Acc: 99.93%
2026-02-10 10:09:39,796 - INFO - [Valid] [82/90] | Loss: 0.5856 | Val Acc: 84.07%
2026-02-10 10:09:39,799 - INFO - [Metrics for 'abnormal'] | Precision: 0.8239 | Recall: 0.8344 | F1: 0.8291
2026-02-10 10:09:39,800 - INFO - [Metrics for 'normal'] | Precision: 0.8556 | Recall: 0.8462 | F1: 0.8508
2026-02-10 10:09:39,800 - INFO - --------------------------------------------------
2026-02-10 10:09:39,801 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 10:09:42,356 - INFO - [Train] [83/90] | Loss: 0.2079 | Train Acc: 99.85%
2026-02-10 10:09:42,873 - INFO - [Valid] [83/90] | Loss: 0.5467 | Val Acc: 84.37%
2026-02-10 10:09:42,877 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.8280 | F1: 0.8307
2026-02-10 10:09:42,878 - INFO - [Metrics for 'normal'] | Precision: 0.8525 | Recall: 0.8571 | F1: 0.8548
2026-02-10 10:09:42,879 - INFO - --------------------------------------------------
2026-02-10 10:09:42,880 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 10:09:45,253 - INFO - [Train] [84/90] | Loss: 0.2101 | Train Acc: 99.63%
2026-02-10 10:09:45,794 - INFO - [Valid] [84/90] | Loss: 0.5716 | Val Acc: 83.19%
2026-02-10 10:09:45,798 - INFO - [Metrics for 'abnormal'] | Precision: 0.8289 | Recall: 0.8025 | F1: 0.8155
2026-02-10 10:09:45,798 - INFO - [Metrics for 'normal'] | Precision: 0.8342 | Recall: 0.8571 | F1: 0.8455
2026-02-10 10:09:45,799 - INFO - --------------------------------------------------
2026-02-10 10:09:45,800 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 10:09:48,223 - INFO - [Train] [85/90] | Loss: 0.2155 | Train Acc: 99.48%
2026-02-10 10:09:48,800 - INFO - [Valid] [85/90] | Loss: 0.5285 | Val Acc: 83.78%
2026-02-10 10:09:48,804 - INFO - [Metrics for 'abnormal'] | Precision: 0.8493 | Recall: 0.7898 | F1: 0.8185
2026-02-10 10:09:48,805 - INFO - [Metrics for 'normal'] | Precision: 0.8290 | Recall: 0.8791 | F1: 0.8533
2026-02-10 10:09:48,806 - INFO - --------------------------------------------------
2026-02-10 10:09:48,807 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 10:09:51,210 - INFO - [Train] [86/90] | Loss: 0.2128 | Train Acc: 99.70%
2026-02-10 10:09:51,801 - INFO - [Valid] [86/90] | Loss: 0.5324 | Val Acc: 83.19%
2026-02-10 10:09:51,805 - INFO - [Metrics for 'abnormal'] | Precision: 0.8378 | Recall: 0.7898 | F1: 0.8131
2026-02-10 10:09:51,805 - INFO - [Metrics for 'normal'] | Precision: 0.8272 | Recall: 0.8681 | F1: 0.8472
2026-02-10 10:09:51,806 - INFO - --------------------------------------------------
2026-02-10 10:09:51,807 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 10:09:54,185 - INFO - [Train] [87/90] | Loss: 0.2080 | Train Acc: 99.78%
2026-02-10 10:09:54,813 - INFO - [Valid] [87/90] | Loss: 0.5736 | Val Acc: 83.78%
2026-02-10 10:09:54,818 - INFO - [Metrics for 'abnormal'] | Precision: 0.8446 | Recall: 0.7962 | F1: 0.8197
2026-02-10 10:09:54,818 - INFO - [Metrics for 'normal'] | Precision: 0.8325 | Recall: 0.8736 | F1: 0.8525
2026-02-10 10:09:54,819 - INFO - --------------------------------------------------
2026-02-10 10:09:54,820 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 10:09:57,222 - INFO - [Train] [88/90] | Loss: 0.2103 | Train Acc: 99.63%
2026-02-10 10:09:57,816 - INFO - [Valid] [88/90] | Loss: 0.5748 | Val Acc: 83.48%
2026-02-10 10:09:57,820 - INFO - [Metrics for 'abnormal'] | Precision: 0.8389 | Recall: 0.7962 | F1: 0.8170
2026-02-10 10:09:57,820 - INFO - [Metrics for 'normal'] | Precision: 0.8316 | Recall: 0.8681 | F1: 0.8495
2026-02-10 10:09:57,822 - INFO - --------------------------------------------------
2026-02-10 10:09:57,823 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 10:10:00,340 - INFO - [Train] [89/90] | Loss: 0.2149 | Train Acc: 99.55%
2026-02-10 10:10:00,860 - INFO - [Valid] [89/90] | Loss: 0.6362 | Val Acc: 82.30%
2026-02-10 10:10:00,865 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8662 | F1: 0.8193
2026-02-10 10:10:00,865 - INFO - [Metrics for 'normal'] | Precision: 0.8720 | Recall: 0.7857 | F1: 0.8266
2026-02-10 10:10:00,867 - INFO - --------------------------------------------------
2026-02-10 10:10:00,868 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 10:10:03,198 - INFO - [Train] [90/90] | Loss: 0.2167 | Train Acc: 99.18%
2026-02-10 10:10:03,822 - INFO - [Valid] [90/90] | Loss: 0.5816 | Val Acc: 82.89%
2026-02-10 10:10:03,826 - INFO - [Metrics for 'abnormal'] | Precision: 0.8322 | Recall: 0.7898 | F1: 0.8105
2026-02-10 10:10:03,826 - INFO - [Metrics for 'normal'] | Precision: 0.8263 | Recall: 0.8626 | F1: 0.8441
2026-02-10 10:10:03,828 - INFO - ==================================================
2026-02-10 10:10:03,829 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 10:10:03,829 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 10:10:03,829 - INFO - Baseline 모델 'mobilenet_v4_s'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 10:10:03,878 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 10:10:03,879 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 10:10:03,879 - INFO - 분류 레이어 'Linear'을(를) Pruning 대상에서 제외합니다.
2026-02-10 10:10:03,949 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.327744140625)에 맞춰 변경되었습니다.
2026-02-10 10:10:03,950 - INFO - ==================================================
2026-02-10 10:10:03,993 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/best_model.pth'
2026-02-10 10:10:03,993 - INFO - ==================================================
2026-02-10 10:10:03,993 - INFO - Test 모드를 시작합니다.
2026-02-10 10:10:04,088 - INFO - 연산량 (MACs): 0.0911 GMACs per sample
2026-02-10 10:10:04,089 - INFO - 연산량 (FLOPs): 0.1822 GFLOPs per sample
2026-02-10 10:10:04,089 - INFO - ==================================================
2026-02-10 10:10:04,089 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 10:10:04,874 - INFO - 샘플 당 평균 Forward Pass 시간: 1.76ms (std: 0.51ms), FPS: 628.83 (std: 240.48) (1개 샘플 x 100회 반복)
2026-02-10 10:10:04,874 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 81.16 MB
2026-02-10 10:10:04,874 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 10:10:05,908 - INFO - [Test] Loss: 0.3637 | Test Acc: 83.78%
2026-02-10 10:10:05,913 - INFO - [Metrics for 'abnormal'] | Precision: 0.8269 | Recall: 0.8217 | F1: 0.8243
2026-02-10 10:10:05,914 - INFO - [Metrics for 'normal'] | Precision: 0.8470 | Recall: 0.8516 | F1: 0.8493
2026-02-10 10:10:06,166 - INFO - ==================================================
2026-02-10 10:10:06,166 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/confusion_matrix_20260210_100528.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/confusion_matrix_20260210_100528.pdf'
2026-02-10 10:10:06,167 - INFO - ==================================================
2026-02-10 10:10:06,167 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 10:10:07,179 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/model_fp32_20260210_100528.onnx'에 저장되었습니다. (크기: 4.36 MB)
2026-02-10 10:10:07,378 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 10.88 MB
2026-02-10 10:10:07,378 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 10:10:07,964 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 3.12ms (std: 4.13ms)
2026-02-10 10:10:07,964 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 633.98 FPS (std: 340.15) (1개 샘플 x 100회 반복)
2026-02-10 10:10:07,964 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 4.12 MB
2026-02-10 10:10:07,965 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 19.11 MB
2026-02-10 10:10:09,822 - INFO - [Test (ONNX)] | Test Acc (ONNX): 83.78%
2026-02-10 10:10:09,828 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8269 | Recall: 0.8217 | F1: 0.8243
2026-02-10 10:10:09,828 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8470 | Recall: 0.8516 | F1: 0.8493
2026-02-10 10:10:09,983 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/val_acc.pdf'
2026-02-10 10:10:10,109 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/train_val_acc.pdf'
2026-02-10 10:10:10,239 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/F1_normal.pdf'
2026-02-10 10:10:10,422 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/val_loss.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/val_loss.pdf'
2026-02-10 10:10:10,527 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/learning_rate.pdf'
2026-02-10 10:10:11,573 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/compile.png' and 'log/Sewer-TAPNEW/baseline_mobilenet_v4_s_l1_20260210_100528/graph_20260210_100528/compile.pdf'
