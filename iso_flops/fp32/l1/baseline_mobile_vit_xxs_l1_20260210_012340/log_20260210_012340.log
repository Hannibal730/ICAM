2026-02-10 01:23:40,422 - INFO - 로그 파일이 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/log_20260210_012340.log'에 저장됩니다.
2026-02-10 01:23:40,424 - INFO - ==================================================
2026-02-10 01:23:40,424 - INFO - config.yaml:
2026-02-10 01:23:40,424 - INFO - 
run:
  global_seed: 42
  cuda: true
  mode: train
  pth_best_name: best_model.pth
  evaluate_onnx: true
  only_inference: false
  show_log: true
  dataset:
    name: Sewer-TAPNEW
    type: image_folder
    train_split_ratio: 0.8
    paths:
      train_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train
      valid_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      test_img_dir: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid
      train_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv
      valid_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      test_csv: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv
      img_folder: /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16
training_main:
  epochs: 90
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: val_loss
training_baseline:
  epochs: 10
  batch_size: 16
  pre_trained: false
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: val_loss
finetuning_pruned:
  epochs: 80
  batch_size: 16
  optimizer: adamw
  lr: 0.001
  weight_decay: 0.0001
  loss_function: CrossEntropyLoss
  label_smoothing: 0.1
  scheduler: cosineannealinglr
  scheduler_params:
    T_max: 80
    eta_min: 0.0001
  best_model_criterion: val_loss
model:
  img_size: 224
  num_patches_per_side: 7
  num_decoder_patches: 1
  num_decoder_layers: 6
  num_heads: 2
  cnn_feature_extractor:
    name: custom24
  encoder_dim: 24
  emb_dim: 24
  adaptive_initial_query: true
  decoder_ff_ratio: 2
  dropout: 0.1
  positional_encoding: true
  res_attention: false
  drop_path_ratio: 0.2
  save_attention: false
  num_plot_attention: 600
baseline:
  model_name: mobile_vit_xxs
  use_l1_pruning: true
  pruning_flops_target: 0.1816

2026-02-10 01:23:40,424 - INFO - ==================================================
2026-02-10 01:23:40,573 - INFO - CUDA 사용 가능. GPU 사용을 시작합니다. (Device: NVIDIA GeForce RTX 5090)
2026-02-10 01:23:40,574 - INFO - 'Sewer-TAPNEW' 데이터 로드를 시작합니다 (Type: image_folder).
2026-02-10 01:23:40,574 - INFO - '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW' 경로의 데이터를 훈련/테스트셋으로 분할합니다.
2026-02-10 01:23:40,590 - INFO - 총 1692개 데이터를 훈련용 1353개, 테스트용 339개로 분할합니다 (split_seed=42).
2026-02-10 01:23:40,591 - INFO - 데이터셋 클래스: ['abnormal', 'normal'] (총 2개)
2026-02-10 01:23:40,591 - INFO - 훈련 데이터: 1353개, 검증 데이터: 339개, 테스트 데이터: 339개
2026-02-10 01:23:40,591 - INFO - Baseline 모델 'mobile_vit_xxs'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:23:40,684 - INFO - timm 모델(mobile_vit_xxs)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:23:40,689 - INFO - ==================================================
2026-02-10 01:23:40,689 - INFO - 모델 파라미터 수:
2026-02-10 01:23:40,689 - INFO -   - 총 파라미터: 951,666 개
2026-02-10 01:23:40,689 - INFO -   - 학습 가능한 파라미터: 951,666 개
2026-02-10 01:23:40,690 - INFO - ================================================================================
2026-02-10 01:23:40,690 - INFO - 단계 1/2: 사전 훈련(Pre-training)을 시작합니다.
2026-02-10 01:23:40,690 - INFO - ================================================================================
2026-02-10 01:23:40,690 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:23:40,690 - INFO - 스케줄러: CosineAnnealingLR (T_max=10, eta_min=0.0001)
2026-02-10 01:23:40,690 - INFO - ==================================================
2026-02-10 01:23:40,690 - INFO - train 모드를 시작합니다.
2026-02-10 01:23:40,690 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:23:40,690 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:23:40,690 - INFO - --------------------------------------------------
2026-02-10 01:23:40,691 - INFO - [LR]    [1/10] | Learning Rate: 0.001000
2026-02-10 01:23:42,758 - INFO - [Train] [1/10] | Loss: 0.5206 | Train Acc: 78.35%
2026-02-10 01:23:43,414 - INFO - [Valid] [1/10] | Loss: 0.5402 | Val Acc: 81.42%
2026-02-10 01:23:43,417 - INFO - [Metrics for 'abnormal'] | Precision: 0.8052 | Recall: 0.7898 | F1: 0.7974
2026-02-10 01:23:43,417 - INFO - [Metrics for 'normal'] | Precision: 0.8216 | Recall: 0.8352 | F1: 0.8283
2026-02-10 01:23:43,431 - INFO - [Best Model Saved] (val loss: 0.5402) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:23:43,431 - INFO - --------------------------------------------------
2026-02-10 01:23:43,431 - INFO - [LR]    [2/10] | Learning Rate: 0.000978
2026-02-10 01:23:45,083 - INFO - [Train] [2/10] | Loss: 0.4621 | Train Acc: 83.56%
2026-02-10 01:23:45,500 - INFO - [Valid] [2/10] | Loss: 0.5174 | Val Acc: 82.60%
2026-02-10 01:23:45,502 - INFO - [Metrics for 'abnormal'] | Precision: 0.8224 | Recall: 0.7962 | F1: 0.8091
2026-02-10 01:23:45,503 - INFO - [Metrics for 'normal'] | Precision: 0.8289 | Recall: 0.8516 | F1: 0.8401
2026-02-10 01:23:45,515 - INFO - [Best Model Saved] (val loss: 0.5174) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:23:45,515 - INFO - --------------------------------------------------
2026-02-10 01:23:45,515 - INFO - [LR]    [3/10] | Learning Rate: 0.000914
2026-02-10 01:23:47,201 - INFO - [Train] [3/10] | Loss: 0.4286 | Train Acc: 85.71%
2026-02-10 01:23:47,608 - INFO - [Valid] [3/10] | Loss: 0.5126 | Val Acc: 81.12%
2026-02-10 01:23:47,611 - INFO - [Metrics for 'abnormal'] | Precision: 0.7888 | Recall: 0.8089 | F1: 0.7987
2026-02-10 01:23:47,611 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8132 | F1: 0.8222
2026-02-10 01:23:47,623 - INFO - [Best Model Saved] (val loss: 0.5126) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:23:47,623 - INFO - --------------------------------------------------
2026-02-10 01:23:47,624 - INFO - [LR]    [4/10] | Learning Rate: 0.000815
2026-02-10 01:23:49,274 - INFO - [Train] [4/10] | Loss: 0.4004 | Train Acc: 87.65%
2026-02-10 01:23:49,679 - INFO - [Valid] [4/10] | Loss: 0.5266 | Val Acc: 82.30%
2026-02-10 01:23:49,681 - INFO - [Metrics for 'abnormal'] | Precision: 0.8089 | Recall: 0.8089 | F1: 0.8089
2026-02-10 01:23:49,681 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8352 | F1: 0.8352
2026-02-10 01:23:49,682 - INFO - --------------------------------------------------
2026-02-10 01:23:49,683 - INFO - [LR]    [5/10] | Learning Rate: 0.000689
2026-02-10 01:23:51,338 - INFO - [Train] [5/10] | Loss: 0.3823 | Train Acc: 88.69%
2026-02-10 01:23:51,744 - INFO - [Valid] [5/10] | Loss: 0.5275 | Val Acc: 82.60%
2026-02-10 01:23:51,747 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8280 | F1: 0.8150
2026-02-10 01:23:51,747 - INFO - [Metrics for 'normal'] | Precision: 0.8475 | Recall: 0.8242 | F1: 0.8357
2026-02-10 01:23:51,748 - INFO - --------------------------------------------------
2026-02-10 01:23:51,749 - INFO - [LR]    [6/10] | Learning Rate: 0.000550
2026-02-10 01:23:53,415 - INFO - [Train] [6/10] | Loss: 0.3533 | Train Acc: 91.07%
2026-02-10 01:23:53,819 - INFO - [Valid] [6/10] | Loss: 0.4919 | Val Acc: 82.89%
2026-02-10 01:23:53,822 - INFO - [Metrics for 'abnormal'] | Precision: 0.8153 | Recall: 0.8153 | F1: 0.8153
2026-02-10 01:23:53,822 - INFO - [Metrics for 'normal'] | Precision: 0.8407 | Recall: 0.8407 | F1: 0.8407
2026-02-10 01:23:53,834 - INFO - [Best Model Saved] (val loss: 0.4919) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:23:53,834 - INFO - --------------------------------------------------
2026-02-10 01:23:53,835 - INFO - [LR]    [7/10] | Learning Rate: 0.000411
2026-02-10 01:23:55,491 - INFO - [Train] [7/10] | Loss: 0.3085 | Train Acc: 93.90%
2026-02-10 01:23:55,882 - INFO - [Valid] [7/10] | Loss: 0.4899 | Val Acc: 82.30%
2026-02-10 01:23:55,885 - INFO - [Metrics for 'abnormal'] | Precision: 0.8050 | Recall: 0.8153 | F1: 0.8101
2026-02-10 01:23:55,885 - INFO - [Metrics for 'normal'] | Precision: 0.8389 | Recall: 0.8297 | F1: 0.8343
2026-02-10 01:23:55,897 - INFO - [Best Model Saved] (val loss: 0.4899) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:23:55,897 - INFO - --------------------------------------------------
2026-02-10 01:23:55,898 - INFO - [LR]    [8/10] | Learning Rate: 0.000285
2026-02-10 01:23:57,579 - INFO - [Train] [8/10] | Loss: 0.2885 | Train Acc: 96.21%
2026-02-10 01:23:57,987 - INFO - [Valid] [8/10] | Loss: 0.5091 | Val Acc: 81.71%
2026-02-10 01:23:57,990 - INFO - [Metrics for 'abnormal'] | Precision: 0.7746 | Recall: 0.8535 | F1: 0.8121
2026-02-10 01:23:57,990 - INFO - [Metrics for 'normal'] | Precision: 0.8614 | Recall: 0.7857 | F1: 0.8218
2026-02-10 01:23:57,990 - INFO - --------------------------------------------------
2026-02-10 01:23:57,991 - INFO - [LR]    [9/10] | Learning Rate: 0.000186
2026-02-10 01:23:59,640 - INFO - [Train] [9/10] | Loss: 0.2682 | Train Acc: 96.88%
2026-02-10 01:24:00,050 - INFO - [Valid] [9/10] | Loss: 0.4923 | Val Acc: 82.30%
2026-02-10 01:24:00,052 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8217 | F1: 0.8113
2026-02-10 01:24:00,052 - INFO - [Metrics for 'normal'] | Precision: 0.8427 | Recall: 0.8242 | F1: 0.8333
2026-02-10 01:24:00,053 - INFO - --------------------------------------------------
2026-02-10 01:24:00,054 - INFO - [LR]    [10/10] | Learning Rate: 0.000122
2026-02-10 01:24:01,712 - INFO - [Train] [10/10] | Loss: 0.2562 | Train Acc: 97.92%
2026-02-10 01:24:02,109 - INFO - [Valid] [10/10] | Loss: 0.5001 | Val Acc: 82.01%
2026-02-10 01:24:02,111 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-02-10 01:24:02,111 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-02-10 01:24:02,113 - INFO - ================================================================================
2026-02-10 01:24:02,113 - INFO - 단계 2/2: Pruning 및 미세 조정(Fine-tuning)을 시작합니다.
2026-02-10 01:24:02,113 - INFO - ================================================================================
2026-02-10 01:24:02,134 - INFO - 사전 훈련된 모델 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'을(를) 불러왔습니다.
2026-02-10 01:24:02,134 - INFO - ================================================================================
2026-02-10 01:24:02,134 - INFO - 목표 FLOPs (0.1816 GFLOPs)에 맞는 최적의 Pruning 희소도를 탐색합니다.
2026-02-10 01:24:02,197 - INFO - 원본 모델 FLOPs: 0.5384 GFLOPs
2026-02-10 01:24:02,228 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:02,228 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:02,229 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:02,379 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.495)에 맞춰 변경되었습니다.
2026-02-10 01:24:02,379 - INFO - ==================================================
2026-02-10 01:24:02,405 - INFO -   [탐색  1] 희소도: 0.4950 -> FLOPs: 0.1747 GFLOPs (감소율: 67.56%)
2026-02-10 01:24:02,421 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:02,421 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:02,421 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:02,694 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.2475)에 맞춰 변경되었습니다.
2026-02-10 01:24:02,694 - INFO - ==================================================
2026-02-10 01:24:02,719 - INFO -   [탐색  2] 희소도: 0.2475 -> FLOPs: 0.3329 GFLOPs (감소율: 38.17%)
2026-02-10 01:24:02,734 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:02,734 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:02,735 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:02,869 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.37124999999999997)에 맞춰 변경되었습니다.
2026-02-10 01:24:02,869 - INFO - ==================================================
2026-02-10 01:24:02,918 - INFO -   [탐색  3] 희소도: 0.3712 -> FLOPs: 0.2479 GFLOPs (감소율: 53.96%)
2026-02-10 01:24:02,934 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:02,934 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:02,935 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,069 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.433125)에 맞춰 변경되었습니다.
2026-02-10 01:24:03,069 - INFO - ==================================================
2026-02-10 01:24:03,092 - INFO -   [탐색  4] 희소도: 0.4331 -> FLOPs: 0.2093 GFLOPs (감소율: 61.13%)
2026-02-10 01:24:03,108 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:03,108 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,108 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,242 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4640625)에 맞춰 변경되었습니다.
2026-02-10 01:24:03,242 - INFO - ==================================================
2026-02-10 01:24:03,265 - INFO -   [탐색  5] 희소도: 0.4641 -> FLOPs: 0.1880 GFLOPs (감소율: 65.08%)
2026-02-10 01:24:03,281 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:03,281 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,281 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,558 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47953124999999996)에 맞춰 변경되었습니다.
2026-02-10 01:24:03,558 - INFO - ==================================================
2026-02-10 01:24:03,580 - INFO -   [탐색  6] 희소도: 0.4795 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:03,596 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:03,596 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,596 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,731 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.471796875)에 맞춰 변경되었습니다.
2026-02-10 01:24:03,731 - INFO - ==================================================
2026-02-10 01:24:03,751 - INFO -   [탐색  7] 희소도: 0.4718 -> FLOPs: 0.1838 GFLOPs (감소율: 65.85%)
2026-02-10 01:24:03,766 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:03,766 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,767 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,900 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4756640625)에 맞춰 변경되었습니다.
2026-02-10 01:24:03,901 - INFO - ==================================================
2026-02-10 01:24:03,918 - INFO -   [탐색  8] 희소도: 0.4757 -> FLOPs: 0.1826 GFLOPs (감소율: 66.08%)
2026-02-10 01:24:03,934 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:03,934 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:03,934 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,067 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47759765624999995)에 맞춰 변경되었습니다.
2026-02-10 01:24:04,067 - INFO - ==================================================
2026-02-10 01:24:04,084 - INFO -   [탐색  9] 희소도: 0.4776 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:04,099 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:04,099 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,099 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,232 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47856445312499996)에 맞춰 변경되었습니다.
2026-02-10 01:24:04,232 - INFO - ==================================================
2026-02-10 01:24:04,248 - INFO -   [탐색 10] 희소도: 0.4786 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:04,264 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:04,264 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,264 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,550 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47904785156249996)에 맞춰 변경되었습니다.
2026-02-10 01:24:04,550 - INFO - ==================================================
2026-02-10 01:24:04,568 - INFO -   [탐색 11] 희소도: 0.4790 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:04,583 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:04,583 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,584 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,718 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47928955078124996)에 맞춰 변경되었습니다.
2026-02-10 01:24:04,718 - INFO - ==================================================
2026-02-10 01:24:04,734 - INFO -   [탐색 12] 희소도: 0.4793 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:04,750 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:04,750 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,750 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,883 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916870117187493)에 맞춰 변경되었습니다.
2026-02-10 01:24:04,883 - INFO - ==================================================
2026-02-10 01:24:04,900 - INFO -   [탐색 13] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:04,915 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:04,915 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:04,915 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,048 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47910827636718745)에 맞춰 변경되었습니다.
2026-02-10 01:24:05,048 - INFO - ==================================================
2026-02-10 01:24:05,064 - INFO -   [탐색 14] 희소도: 0.4791 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:05,079 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:05,079 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,080 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,212 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791384887695312)에 맞춰 변경되었습니다.
2026-02-10 01:24:05,212 - INFO - ==================================================
2026-02-10 01:24:05,228 - INFO -   [탐색 15] 희소도: 0.4791 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:05,243 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:05,243 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,243 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,527 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791535949707031)에 맞춰 변경되었습니다.
2026-02-10 01:24:05,528 - INFO - ==================================================
2026-02-10 01:24:05,545 - INFO -   [탐색 16] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:05,561 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:05,561 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,561 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,695 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.479161148071289)에 맞춰 변경되었습니다.
2026-02-10 01:24:05,695 - INFO - ==================================================
2026-02-10 01:24:05,711 - INFO -   [탐색 17] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:05,727 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:05,727 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,727 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,860 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.479164924621582)에 맞춰 변경되었습니다.
2026-02-10 01:24:05,860 - INFO - ==================================================
2026-02-10 01:24:05,876 - INFO -   [탐색 18] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:05,892 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:05,892 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:05,892 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,025 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916681289672847)에 맞춰 변경되었습니다.
2026-02-10 01:24:06,025 - INFO - ==================================================
2026-02-10 01:24:06,042 - INFO -   [탐색 19] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:06,057 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:06,057 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,058 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,190 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916586875915523)에 맞춰 변경되었습니다.
2026-02-10 01:24:06,190 - INFO - ==================================================
2026-02-10 01:24:06,207 - INFO -   [탐색 20] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:06,374 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:06,374 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,374 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,509 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916634082794185)에 맞춰 변경되었습니다.
2026-02-10 01:24:06,509 - INFO - ==================================================
2026-02-10 01:24:06,526 - INFO -   [탐색 21] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:06,542 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:06,542 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,542 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,675 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916657686233516)에 맞춰 변경되었습니다.
2026-02-10 01:24:06,675 - INFO - ==================================================
2026-02-10 01:24:06,692 - INFO -   [탐색 22] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:06,707 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:06,707 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,707 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,840 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666948795318)에 맞춰 변경되었습니다.
2026-02-10 01:24:06,840 - INFO - ==================================================
2026-02-10 01:24:06,857 - INFO -   [탐색 23] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:06,872 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:06,872 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:06,873 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,005 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916663587093344)에 맞춰 변경되었습니다.
2026-02-10 01:24:07,005 - INFO - ==================================================
2026-02-10 01:24:07,021 - INFO -   [탐색 24] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:07,037 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:07,037 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,037 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,322 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666653752326)에 맞춰 변경되었습니다.
2026-02-10 01:24:07,322 - INFO - ==================================================
2026-02-10 01:24:07,339 - INFO -   [탐색 25] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:07,355 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:07,355 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,355 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,489 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916668012738217)에 맞춰 변경되었습니다.
2026-02-10 01:24:07,490 - INFO - ==================================================
2026-02-10 01:24:07,506 - INFO -   [탐색 26] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:07,522 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:07,522 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,522 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,655 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666727513074)에 맞춰 변경되었습니다.
2026-02-10 01:24:07,655 - INFO - ==================================================
2026-02-10 01:24:07,672 - INFO -   [탐색 27] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:07,687 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:07,687 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,687 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,820 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666906327)에 맞춰 변경되었습니다.
2026-02-10 01:24:07,820 - INFO - ==================================================
2026-02-10 01:24:07,836 - INFO -   [탐색 28] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:07,851 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:07,851 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,852 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:07,984 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666672192513)에 맞춰 변경되었습니다.
2026-02-10 01:24:07,984 - INFO - ==================================================
2026-02-10 01:24:08,000 - INFO -   [탐색 29] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:08,016 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:08,016 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,016 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,302 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666662972419)에 맞춰 변경되었습니다.
2026-02-10 01:24:08,302 - INFO - ==================================================
2026-02-10 01:24:08,320 - INFO -   [탐색 30] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:08,336 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:08,336 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,336 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,470 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666667582466)에 맞춰 변경되었습니다.
2026-02-10 01:24:08,470 - INFO - ==================================================
2026-02-10 01:24:08,487 - INFO -   [탐색 31] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:08,502 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:08,502 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,502 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,635 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666652774426)에 맞춰 변경되었습니다.
2026-02-10 01:24:08,635 - INFO - ==================================================
2026-02-10 01:24:08,652 - INFO -   [탐색 32] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:08,667 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:08,667 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,667 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,801 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666664299545)에 맞춰 변경되었습니다.
2026-02-10 01:24:08,801 - INFO - ==================================================
2026-02-10 01:24:08,817 - INFO -   [탐색 33] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:08,832 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:08,832 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,832 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,964 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.479166666700621)에 맞춰 변경되었습니다.
2026-02-10 01:24:08,964 - INFO - ==================================================
2026-02-10 01:24:08,980 - INFO -   [탐색 34] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:08,996 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:08,996 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:08,996 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,282 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666667180824)에 맞춰 변경되었습니다.
2026-02-10 01:24:09,282 - INFO - ==================================================
2026-02-10 01:24:09,299 - INFO -   [탐색 35] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:09,315 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:09,315 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,315 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,449 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666574018)에 맞춰 변경되었습니다.
2026-02-10 01:24:09,449 - INFO - ==================================================
2026-02-10 01:24:09,465 - INFO -   [탐색 36] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:09,480 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:09,480 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,481 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,614 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666460506)에 맞춰 변경되었습니다.
2026-02-10 01:24:09,614 - INFO - ==================================================
2026-02-10 01:24:09,630 - INFO -   [탐색 37] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:09,645 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:09,645 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,645 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,779 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666682066)에 맞춰 변경되었습니다.
2026-02-10 01:24:09,779 - INFO - ==================================================
2026-02-10 01:24:09,795 - INFO -   [탐색 38] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:09,810 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:09,810 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,810 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,943 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666640584)에 맞춰 변경되었습니다.
2026-02-10 01:24:09,943 - INFO - ==================================================
2026-02-10 01:24:09,959 - INFO -   [탐색 39] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:09,975 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:09,975 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:09,975 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:10,261 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666730623)에 맞춰 변경되었습니다.
2026-02-10 01:24:10,261 - INFO - ==================================================
2026-02-10 01:24:10,279 - INFO -   [탐색 40] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:10,295 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:10,295 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:10,295 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:10,429 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666685603)에 맞춰 변경되었습니다.
2026-02-10 01:24:10,429 - INFO - ==================================================
2026-02-10 01:24:10,446 - INFO -   [탐색 41] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:10,461 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:10,461 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:10,461 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:10,595 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666663094)에 맞춰 변경되었습니다.
2026-02-10 01:24:10,595 - INFO - ==================================================
2026-02-10 01:24:10,611 - INFO -   [탐색 42] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:10,627 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:10,627 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:10,627 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:10,760 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666674346)에 맞춰 변경되었습니다.
2026-02-10 01:24:10,760 - INFO - ==================================================
2026-02-10 01:24:10,777 - INFO -   [탐색 43] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:10,792 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:10,792 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:10,792 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:10,925 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666668717)에 맞춰 변경되었습니다.
2026-02-10 01:24:10,925 - INFO - ==================================================
2026-02-10 01:24:10,941 - INFO -   [탐색 44] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:11,113 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:11,113 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:11,113 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:11,248 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666591)에 맞춰 변경되었습니다.
2026-02-10 01:24:11,248 - INFO - ==================================================
2026-02-10 01:24:11,265 - INFO -   [탐색 45] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:11,281 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:11,281 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:11,281 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:11,415 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666731)에 맞춰 변경되었습니다.
2026-02-10 01:24:11,415 - INFO - ==================================================
2026-02-10 01:24:11,432 - INFO -   [탐색 46] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:11,447 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:11,447 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:11,447 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:11,580 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666661)에 맞춰 변경되었습니다.
2026-02-10 01:24:11,581 - INFO - ==================================================
2026-02-10 01:24:11,597 - INFO -   [탐색 47] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:11,612 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:11,612 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:11,612 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:11,745 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666696)에 맞춰 변경되었습니다.
2026-02-10 01:24:11,745 - INFO - ==================================================
2026-02-10 01:24:11,762 - INFO -   [탐색 48] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:11,777 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:11,777 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:11,777 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,065 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666785)에 맞춰 변경되었습니다.
2026-02-10 01:24:12,065 - INFO - ==================================================
2026-02-10 01:24:12,082 - INFO -   [탐색 49] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:12,098 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:12,098 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,098 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,232 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666696)에 맞춰 변경되었습니다.
2026-02-10 01:24:12,232 - INFO - ==================================================
2026-02-10 01:24:12,248 - INFO -   [탐색 50] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:12,263 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:12,263 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,264 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,397 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666665)에 맞춰 변경되었습니다.
2026-02-10 01:24:12,397 - INFO - ==================================================
2026-02-10 01:24:12,413 - INFO -   [탐색 51] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:12,429 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:12,429 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,429 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,562 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666674)에 맞춰 변경되었습니다.
2026-02-10 01:24:12,562 - INFO - ==================================================
2026-02-10 01:24:12,578 - INFO -   [탐색 52] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:12,594 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:12,594 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,594 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,726 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:12,726 - INFO - ==================================================
2026-02-10 01:24:12,743 - INFO -   [탐색 53] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:12,758 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:12,758 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:12,758 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,048 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.4791666666666667)에 맞춰 변경되었습니다.
2026-02-10 01:24:13,048 - INFO - ==================================================
2026-02-10 01:24:13,066 - INFO -   [탐색 54] 희소도: 0.4792 -> FLOPs: 0.1790 GFLOPs (감소율: 66.74%)
2026-02-10 01:24:13,081 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:13,081 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,082 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,216 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:13,216 - INFO - ==================================================
2026-02-10 01:24:13,232 - INFO -   [탐색 55] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:13,248 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:13,248 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,248 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,381 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:13,381 - INFO - ==================================================
2026-02-10 01:24:13,398 - INFO -   [탐색 56] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:13,413 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:13,414 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,414 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,547 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:13,547 - INFO - ==================================================
2026-02-10 01:24:13,563 - INFO -   [탐색 57] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:13,579 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:13,579 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,579 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,712 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:13,712 - INFO - ==================================================
2026-02-10 01:24:13,741 - INFO -   [탐색 58] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:13,757 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:13,758 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:13,758 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,044 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:14,044 - INFO - ==================================================
2026-02-10 01:24:14,062 - INFO -   [탐색 59] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:14,078 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:14,078 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,078 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,212 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:14,212 - INFO - ==================================================
2026-02-10 01:24:14,228 - INFO -   [탐색 60] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:14,244 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:14,244 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,244 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,377 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:14,377 - INFO - ==================================================
2026-02-10 01:24:14,394 - INFO -   [탐색 61] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:14,409 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:14,409 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,410 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,542 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:14,542 - INFO - ==================================================
2026-02-10 01:24:14,559 - INFO -   [탐색 62] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:14,574 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:14,574 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,575 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,707 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:14,707 - INFO - ==================================================
2026-02-10 01:24:14,724 - INFO -   [탐색 63] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:14,739 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:14,739 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:14,740 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,028 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:15,028 - INFO - ==================================================
2026-02-10 01:24:15,045 - INFO -   [탐색 64] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:15,061 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:15,061 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,062 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,196 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:15,196 - INFO - ==================================================
2026-02-10 01:24:15,213 - INFO -   [탐색 65] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:15,228 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:15,228 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,228 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,362 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:15,362 - INFO - ==================================================
2026-02-10 01:24:15,379 - INFO -   [탐색 66] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:15,394 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:15,394 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,395 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,528 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:15,528 - INFO - ==================================================
2026-02-10 01:24:15,544 - INFO -   [탐색 67] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:15,560 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:15,560 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,560 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,692 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:15,692 - INFO - ==================================================
2026-02-10 01:24:15,709 - INFO -   [탐색 68] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:15,724 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:15,724 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:15,724 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,013 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:16,013 - INFO - ==================================================
2026-02-10 01:24:16,031 - INFO -   [탐색 69] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:16,047 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:16,047 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,047 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,181 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:16,181 - INFO - ==================================================
2026-02-10 01:24:16,198 - INFO -   [탐색 70] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:16,213 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:16,213 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,213 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,347 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:16,347 - INFO - ==================================================
2026-02-10 01:24:16,363 - INFO -   [탐색 71] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:16,378 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:16,378 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,379 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,511 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:16,512 - INFO - ==================================================
2026-02-10 01:24:16,528 - INFO -   [탐색 72] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:16,544 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:16,544 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,544 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,676 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:16,676 - INFO - ==================================================
2026-02-10 01:24:16,844 - INFO -   [탐색 73] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:16,860 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:16,860 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,860 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:16,995 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:16,995 - INFO - ==================================================
2026-02-10 01:24:17,012 - INFO -   [탐색 74] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:17,027 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:17,027 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,028 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,162 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:17,162 - INFO - ==================================================
2026-02-10 01:24:17,178 - INFO -   [탐색 75] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:17,194 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:17,194 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,194 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,328 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:17,328 - INFO - ==================================================
2026-02-10 01:24:17,344 - INFO -   [탐색 76] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:17,360 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:17,360 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,360 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,492 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:17,492 - INFO - ==================================================
2026-02-10 01:24:17,509 - INFO -   [탐색 77] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:17,524 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:17,524 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,525 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,811 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:17,811 - INFO - ==================================================
2026-02-10 01:24:17,829 - INFO -   [탐색 78] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:17,844 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:17,845 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,845 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:17,979 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:17,979 - INFO - ==================================================
2026-02-10 01:24:17,996 - INFO -   [탐색 79] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:18,011 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:18,012 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,012 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,144 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:18,144 - INFO - ==================================================
2026-02-10 01:24:18,161 - INFO -   [탐색 80] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:18,176 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:18,177 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,177 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,310 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:18,310 - INFO - ==================================================
2026-02-10 01:24:18,326 - INFO -   [탐색 81] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:18,341 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:18,341 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,342 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,474 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:18,474 - INFO - ==================================================
2026-02-10 01:24:18,490 - INFO -   [탐색 82] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:18,506 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:18,506 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,506 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,793 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:18,794 - INFO - ==================================================
2026-02-10 01:24:18,811 - INFO -   [탐색 83] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:18,827 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:18,827 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,827 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,961 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:18,961 - INFO - ==================================================
2026-02-10 01:24:18,978 - INFO -   [탐색 84] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:18,993 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:18,993 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:18,993 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,127 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:19,127 - INFO - ==================================================
2026-02-10 01:24:19,144 - INFO -   [탐색 85] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:19,159 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:19,159 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,159 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,292 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:19,293 - INFO - ==================================================
2026-02-10 01:24:19,309 - INFO -   [탐색 86] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:19,324 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:19,324 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,325 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,457 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:19,458 - INFO - ==================================================
2026-02-10 01:24:19,474 - INFO -   [탐색 87] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:19,645 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:19,645 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,646 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,780 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:19,780 - INFO - ==================================================
2026-02-10 01:24:19,797 - INFO -   [탐색 88] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:19,813 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:19,813 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,814 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,948 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:19,948 - INFO - ==================================================
2026-02-10 01:24:19,964 - INFO -   [탐색 89] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:19,980 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:19,980 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:19,980 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,114 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:20,114 - INFO - ==================================================
2026-02-10 01:24:20,131 - INFO -   [탐색 90] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:20,146 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:20,146 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,147 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,280 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:20,280 - INFO - ==================================================
2026-02-10 01:24:20,296 - INFO -   [탐색 91] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:20,312 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:20,312 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,312 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,595 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:20,595 - INFO - ==================================================
2026-02-10 01:24:20,612 - INFO -   [탐색 92] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:20,629 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:20,629 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,629 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,763 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:20,763 - INFO - ==================================================
2026-02-10 01:24:20,780 - INFO -   [탐색 93] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:20,796 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:20,796 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,796 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,929 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:20,930 - INFO - ==================================================
2026-02-10 01:24:20,946 - INFO -   [탐색 94] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:20,961 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:20,961 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:20,962 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,095 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:21,095 - INFO - ==================================================
2026-02-10 01:24:21,112 - INFO -   [탐색 95] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:21,127 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:21,127 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,127 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,259 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:21,259 - INFO - ==================================================
2026-02-10 01:24:21,276 - INFO -   [탐색 96] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:21,291 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:21,291 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,291 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,581 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:21,581 - INFO - ==================================================
2026-02-10 01:24:21,599 - INFO -   [탐색 97] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:21,615 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:21,615 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,615 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,749 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:21,749 - INFO - ==================================================
2026-02-10 01:24:21,765 - INFO -   [탐색 98] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:21,781 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:21,781 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,781 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,915 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:21,915 - INFO - ==================================================
2026-02-10 01:24:21,932 - INFO -   [탐색 99] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:21,948 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:21,948 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:21,949 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:22,082 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47916666666666663)에 맞춰 변경되었습니다.
2026-02-10 01:24:22,082 - INFO - ==================================================
2026-02-10 01:24:22,099 - INFO -   [탐색 100] 희소도: 0.4792 -> FLOPs: 0.1824 GFLOPs (감소율: 66.12%)
2026-02-10 01:24:22,099 - INFO - 탐색 완료. 목표 FLOPs(0.1816)에 가장 근접한 최적 희소도는 0.4786 입니다.
2026-02-10 01:24:22,099 - INFO - ================================================================================
2026-02-10 01:24:22,100 - INFO - 계산된 Pruning 정보(희소도: 0.4786)를 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/pruning_info.yaml'에 저장했습니다.
2026-02-10 01:24:22,118 - INFO - Pruning 전 원본 모델의 FLOPs를 측정합니다.
2026-02-10 01:24:22,150 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:24:22,150 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:24:22,150 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:24:22,288 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47856445312499996)에 맞춰 변경되었습니다.
2026-02-10 01:24:22,288 - INFO - ==================================================
2026-02-10 01:24:22,289 - INFO - ==================================================
2026-02-10 01:24:22,289 - INFO - 모델 파라미터 수:
2026-02-10 01:24:22,289 - INFO -   - 총 파라미터: 320,501 개
2026-02-10 01:24:22,289 - INFO -   - 학습 가능한 파라미터: 320,501 개
2026-02-10 01:24:22,305 - INFO - Pruning 후 모델의 FLOPs를 측정합니다.
2026-02-10 01:24:22,337 - INFO - FLOPs가 0.5384 GFLOPs에서 0.1824 GFLOPs로 감소했습니다 (감소율: 66.12%).
2026-02-10 01:24:22,337 - INFO - 미세 조정을 위한 새로운 옵티마이저와 스케줄러를 생성합니다.
2026-02-10 01:24:22,337 - INFO - 옵티마이저: AdamW (lr=0.001, weight_decay=0.0001)
2026-02-10 01:24:22,337 - INFO - 스케줄러: CosineAnnealingLR (T_max=80, eta_min=0.0001)
2026-02-10 01:24:22,337 - INFO - ==================================================
2026-02-10 01:24:22,337 - INFO - train 모드를 시작합니다.
2026-02-10 01:24:22,337 - INFO - 손실 함수: CrossEntropyLoss (label_smoothing: 0.1)
2026-02-10 01:24:22,337 - INFO - Best metric을 초기값(inf)으로 설정합니다.
2026-02-10 01:24:22,337 - INFO - --------------------------------------------------
2026-02-10 01:24:22,338 - INFO - [LR]    [11/90] | Learning Rate: 0.001000
2026-02-10 01:24:24,042 - INFO - [Train] [11/90] | Loss: 0.5134 | Train Acc: 77.90%
2026-02-10 01:24:24,456 - INFO - [Valid] [11/90] | Loss: 0.5356 | Val Acc: 79.35%
2026-02-10 01:24:24,458 - INFO - [Metrics for 'abnormal'] | Precision: 0.7669 | Recall: 0.7962 | F1: 0.7812
2026-02-10 01:24:24,459 - INFO - [Metrics for 'normal'] | Precision: 0.8182 | Recall: 0.7912 | F1: 0.8045
2026-02-10 01:24:24,623 - INFO - [Best Model Saved] (val loss: 0.5356) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:24:24,623 - INFO - --------------------------------------------------
2026-02-10 01:24:24,624 - INFO - [LR]    [12/90] | Learning Rate: 0.001000
2026-02-10 01:24:26,296 - INFO - [Train] [12/90] | Loss: 0.4632 | Train Acc: 83.11%
2026-02-10 01:24:26,699 - INFO - [Valid] [12/90] | Loss: 0.5747 | Val Acc: 77.88%
2026-02-10 01:24:26,702 - INFO - [Metrics for 'abnormal'] | Precision: 0.7470 | Recall: 0.7898 | F1: 0.7678
2026-02-10 01:24:26,702 - INFO - [Metrics for 'normal'] | Precision: 0.8092 | Recall: 0.7692 | F1: 0.7887
2026-02-10 01:24:26,703 - INFO - --------------------------------------------------
2026-02-10 01:24:26,703 - INFO - [LR]    [13/90] | Learning Rate: 0.000999
2026-02-10 01:24:28,389 - INFO - [Train] [13/90] | Loss: 0.4296 | Train Acc: 85.19%
2026-02-10 01:24:28,795 - INFO - [Valid] [13/90] | Loss: 0.5772 | Val Acc: 78.17%
2026-02-10 01:24:28,798 - INFO - [Metrics for 'abnormal'] | Precision: 0.7399 | Recall: 0.8153 | F1: 0.7758
2026-02-10 01:24:28,798 - INFO - [Metrics for 'normal'] | Precision: 0.8253 | Recall: 0.7527 | F1: 0.7874
2026-02-10 01:24:28,798 - INFO - --------------------------------------------------
2026-02-10 01:24:28,799 - INFO - [LR]    [14/90] | Learning Rate: 0.000997
2026-02-10 01:24:30,476 - INFO - [Train] [14/90] | Loss: 0.4121 | Train Acc: 87.28%
2026-02-10 01:24:30,883 - INFO - [Valid] [14/90] | Loss: 0.5119 | Val Acc: 80.24%
2026-02-10 01:24:30,886 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.7643 | F1: 0.7818
2026-02-10 01:24:30,886 - INFO - [Metrics for 'normal'] | Precision: 0.8042 | Recall: 0.8352 | F1: 0.8194
2026-02-10 01:24:30,896 - INFO - [Best Model Saved] (val loss: 0.5119) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:24:30,896 - INFO - --------------------------------------------------
2026-02-10 01:24:30,897 - INFO - [LR]    [15/90] | Learning Rate: 0.000994
2026-02-10 01:24:32,580 - INFO - [Train] [15/90] | Loss: 0.3986 | Train Acc: 87.35%
2026-02-10 01:24:32,992 - INFO - [Valid] [15/90] | Loss: 0.5370 | Val Acc: 80.83%
2026-02-10 01:24:32,994 - INFO - [Metrics for 'abnormal'] | Precision: 0.8151 | Recall: 0.7580 | F1: 0.7855
2026-02-10 01:24:32,994 - INFO - [Metrics for 'normal'] | Precision: 0.8031 | Recall: 0.8516 | F1: 0.8267
2026-02-10 01:24:32,995 - INFO - --------------------------------------------------
2026-02-10 01:24:32,996 - INFO - [LR]    [16/90] | Learning Rate: 0.000991
2026-02-10 01:24:34,664 - INFO - [Train] [16/90] | Loss: 0.3724 | Train Acc: 89.43%
2026-02-10 01:24:35,068 - INFO - [Valid] [16/90] | Loss: 0.5260 | Val Acc: 82.01%
2026-02-10 01:24:35,071 - INFO - [Metrics for 'abnormal'] | Precision: 0.7927 | Recall: 0.8280 | F1: 0.8100
2026-02-10 01:24:35,071 - INFO - [Metrics for 'normal'] | Precision: 0.8457 | Recall: 0.8132 | F1: 0.8291
2026-02-10 01:24:35,072 - INFO - --------------------------------------------------
2026-02-10 01:24:35,072 - INFO - [LR]    [17/90] | Learning Rate: 0.000988
2026-02-10 01:24:36,733 - INFO - [Train] [17/90] | Loss: 0.3601 | Train Acc: 91.67%
2026-02-10 01:24:37,142 - INFO - [Valid] [17/90] | Loss: 0.5035 | Val Acc: 82.60%
2026-02-10 01:24:37,145 - INFO - [Metrics for 'abnormal'] | Precision: 0.8551 | Recall: 0.7516 | F1: 0.8000
2026-02-10 01:24:37,145 - INFO - [Metrics for 'normal'] | Precision: 0.8060 | Recall: 0.8901 | F1: 0.8460
2026-02-10 01:24:37,156 - INFO - [Best Model Saved] (val loss: 0.5035) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:24:37,156 - INFO - --------------------------------------------------
2026-02-10 01:24:37,156 - INFO - [LR]    [18/90] | Learning Rate: 0.000983
2026-02-10 01:24:38,831 - INFO - [Train] [18/90] | Loss: 0.3497 | Train Acc: 91.37%
2026-02-10 01:24:39,240 - INFO - [Valid] [18/90] | Loss: 0.5282 | Val Acc: 79.94%
2026-02-10 01:24:39,242 - INFO - [Metrics for 'abnormal'] | Precision: 0.7380 | Recall: 0.8790 | F1: 0.8023
2026-02-10 01:24:39,242 - INFO - [Metrics for 'normal'] | Precision: 0.8750 | Recall: 0.7308 | F1: 0.7964
2026-02-10 01:24:39,243 - INFO - --------------------------------------------------
2026-02-10 01:24:39,244 - INFO - [LR]    [19/90] | Learning Rate: 0.000978
2026-02-10 01:24:40,879 - INFO - [Train] [19/90] | Loss: 0.3255 | Train Acc: 92.93%
2026-02-10 01:24:41,287 - INFO - [Valid] [19/90] | Loss: 0.5016 | Val Acc: 80.83%
2026-02-10 01:24:41,289 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8217 | F1: 0.7988
2026-02-10 01:24:41,289 - INFO - [Metrics for 'normal'] | Precision: 0.8382 | Recall: 0.7967 | F1: 0.8169
2026-02-10 01:24:41,311 - INFO - [Best Model Saved] (val loss: 0.5016) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:24:41,311 - INFO - --------------------------------------------------
2026-02-10 01:24:41,311 - INFO - [LR]    [20/90] | Learning Rate: 0.000972
2026-02-10 01:24:43,000 - INFO - [Train] [20/90] | Loss: 0.3026 | Train Acc: 94.87%
2026-02-10 01:24:43,409 - INFO - [Valid] [20/90] | Loss: 0.5006 | Val Acc: 82.60%
2026-02-10 01:24:43,412 - INFO - [Metrics for 'abnormal'] | Precision: 0.7882 | Recall: 0.8535 | F1: 0.8196
2026-02-10 01:24:43,412 - INFO - [Metrics for 'normal'] | Precision: 0.8639 | Recall: 0.8022 | F1: 0.8319
2026-02-10 01:24:43,423 - INFO - [Best Model Saved] (val loss: 0.5006) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:24:43,423 - INFO - --------------------------------------------------
2026-02-10 01:24:43,424 - INFO - [LR]    [21/90] | Learning Rate: 0.000966
2026-02-10 01:24:45,123 - INFO - [Train] [21/90] | Loss: 0.3136 | Train Acc: 94.27%
2026-02-10 01:24:45,529 - INFO - [Valid] [21/90] | Loss: 0.5045 | Val Acc: 84.37%
2026-02-10 01:24:45,531 - INFO - [Metrics for 'abnormal'] | Precision: 0.8514 | Recall: 0.8025 | F1: 0.8262
2026-02-10 01:24:45,531 - INFO - [Metrics for 'normal'] | Precision: 0.8377 | Recall: 0.8791 | F1: 0.8579
2026-02-10 01:24:45,532 - INFO - --------------------------------------------------
2026-02-10 01:24:45,533 - INFO - [LR]    [22/90] | Learning Rate: 0.000959
2026-02-10 01:24:47,201 - INFO - [Train] [22/90] | Loss: 0.2849 | Train Acc: 95.83%
2026-02-10 01:24:47,613 - INFO - [Valid] [22/90] | Loss: 0.5687 | Val Acc: 80.24%
2026-02-10 01:24:47,615 - INFO - [Metrics for 'abnormal'] | Precision: 0.7616 | Recall: 0.8344 | F1: 0.7964
2026-02-10 01:24:47,615 - INFO - [Metrics for 'normal'] | Precision: 0.8443 | Recall: 0.7747 | F1: 0.8080
2026-02-10 01:24:47,616 - INFO - --------------------------------------------------
2026-02-10 01:24:47,617 - INFO - [LR]    [23/90] | Learning Rate: 0.000951
2026-02-10 01:24:49,315 - INFO - [Train] [23/90] | Loss: 0.2704 | Train Acc: 96.95%
2026-02-10 01:24:49,721 - INFO - [Valid] [23/90] | Loss: 0.5537 | Val Acc: 81.12%
2026-02-10 01:24:49,724 - INFO - [Metrics for 'abnormal'] | Precision: 0.8163 | Recall: 0.7643 | F1: 0.7895
2026-02-10 01:24:49,724 - INFO - [Metrics for 'normal'] | Precision: 0.8073 | Recall: 0.8516 | F1: 0.8289
2026-02-10 01:24:49,725 - INFO - --------------------------------------------------
2026-02-10 01:24:49,725 - INFO - [LR]    [24/90] | Learning Rate: 0.000943
2026-02-10 01:24:51,417 - INFO - [Train] [24/90] | Loss: 0.2595 | Train Acc: 97.40%
2026-02-10 01:24:51,825 - INFO - [Valid] [24/90] | Loss: 0.5641 | Val Acc: 81.12%
2026-02-10 01:24:51,828 - INFO - [Metrics for 'abnormal'] | Precision: 0.7853 | Recall: 0.8153 | F1: 0.8000
2026-02-10 01:24:51,828 - INFO - [Metrics for 'normal'] | Precision: 0.8352 | Recall: 0.8077 | F1: 0.8212
2026-02-10 01:24:51,829 - INFO - --------------------------------------------------
2026-02-10 01:24:51,829 - INFO - [LR]    [25/90] | Learning Rate: 0.000934
2026-02-10 01:24:53,560 - INFO - [Train] [25/90] | Loss: 0.2641 | Train Acc: 97.62%
2026-02-10 01:24:53,964 - INFO - [Valid] [25/90] | Loss: 0.5511 | Val Acc: 81.42%
2026-02-10 01:24:53,967 - INFO - [Metrics for 'abnormal'] | Precision: 0.8133 | Recall: 0.7771 | F1: 0.7948
2026-02-10 01:24:53,967 - INFO - [Metrics for 'normal'] | Precision: 0.8148 | Recall: 0.8462 | F1: 0.8302
2026-02-10 01:24:53,968 - INFO - --------------------------------------------------
2026-02-10 01:24:53,968 - INFO - [LR]    [26/90] | Learning Rate: 0.000924
2026-02-10 01:24:55,723 - INFO - [Train] [26/90] | Loss: 0.2756 | Train Acc: 96.35%
2026-02-10 01:24:56,135 - INFO - [Valid] [26/90] | Loss: 0.5874 | Val Acc: 78.17%
2026-02-10 01:24:56,137 - INFO - [Metrics for 'abnormal'] | Precision: 0.8374 | Recall: 0.6561 | F1: 0.7357
2026-02-10 01:24:56,137 - INFO - [Metrics for 'normal'] | Precision: 0.7500 | Recall: 0.8901 | F1: 0.8141
2026-02-10 01:24:56,138 - INFO - --------------------------------------------------
2026-02-10 01:24:56,139 - INFO - [LR]    [27/90] | Learning Rate: 0.000914
2026-02-10 01:24:58,373 - INFO - [Train] [27/90] | Loss: 0.2799 | Train Acc: 95.61%
2026-02-10 01:24:59,083 - INFO - [Valid] [27/90] | Loss: 0.5633 | Val Acc: 81.71%
2026-02-10 01:24:59,086 - INFO - [Metrics for 'abnormal'] | Precision: 0.8231 | Recall: 0.7707 | F1: 0.7961
2026-02-10 01:24:59,086 - INFO - [Metrics for 'normal'] | Precision: 0.8125 | Recall: 0.8571 | F1: 0.8342
2026-02-10 01:24:59,087 - INFO - --------------------------------------------------
2026-02-10 01:24:59,088 - INFO - [LR]    [28/90] | Learning Rate: 0.000903
2026-02-10 01:25:01,605 - INFO - [Train] [28/90] | Loss: 0.2684 | Train Acc: 96.58%
2026-02-10 01:25:02,281 - INFO - [Valid] [28/90] | Loss: 0.5033 | Val Acc: 81.42%
2026-02-10 01:25:02,285 - INFO - [Metrics for 'abnormal'] | Precision: 0.8219 | Recall: 0.7643 | F1: 0.7921
2026-02-10 01:25:02,285 - INFO - [Metrics for 'normal'] | Precision: 0.8083 | Recall: 0.8571 | F1: 0.8320
2026-02-10 01:25:02,286 - INFO - --------------------------------------------------
2026-02-10 01:25:02,287 - INFO - [LR]    [29/90] | Learning Rate: 0.000892
2026-02-10 01:25:05,406 - INFO - [Train] [29/90] | Loss: 0.2410 | Train Acc: 98.59%
2026-02-10 01:25:06,030 - INFO - [Valid] [29/90] | Loss: 0.5406 | Val Acc: 82.30%
2026-02-10 01:25:06,035 - INFO - [Metrics for 'abnormal'] | Precision: 0.8129 | Recall: 0.8025 | F1: 0.8077
2026-02-10 01:25:06,035 - INFO - [Metrics for 'normal'] | Precision: 0.8315 | Recall: 0.8407 | F1: 0.8361
2026-02-10 01:25:06,037 - INFO - --------------------------------------------------
2026-02-10 01:25:06,038 - INFO - [LR]    [30/90] | Learning Rate: 0.000880
2026-02-10 01:25:09,263 - INFO - [Train] [30/90] | Loss: 0.2538 | Train Acc: 97.84%
2026-02-10 01:25:10,357 - INFO - [Valid] [30/90] | Loss: 0.5662 | Val Acc: 80.83%
2026-02-10 01:25:10,362 - INFO - [Metrics for 'abnormal'] | Precision: 0.8026 | Recall: 0.7771 | F1: 0.7896
2026-02-10 01:25:10,362 - INFO - [Metrics for 'normal'] | Precision: 0.8128 | Recall: 0.8352 | F1: 0.8238
2026-02-10 01:25:10,363 - INFO - --------------------------------------------------
2026-02-10 01:25:10,365 - INFO - [LR]    [31/90] | Learning Rate: 0.000868
2026-02-10 01:25:14,376 - INFO - [Train] [31/90] | Loss: 0.2400 | Train Acc: 98.74%
2026-02-10 01:25:15,089 - INFO - [Valid] [31/90] | Loss: 0.5508 | Val Acc: 82.01%
2026-02-10 01:25:15,093 - INFO - [Metrics for 'abnormal'] | Precision: 0.8200 | Recall: 0.7834 | F1: 0.8013
2026-02-10 01:25:15,093 - INFO - [Metrics for 'normal'] | Precision: 0.8201 | Recall: 0.8516 | F1: 0.8356
2026-02-10 01:25:15,095 - INFO - --------------------------------------------------
2026-02-10 01:25:15,096 - INFO - [LR]    [32/90] | Learning Rate: 0.000855
2026-02-10 01:25:18,185 - INFO - [Train] [32/90] | Loss: 0.2665 | Train Acc: 96.73%
2026-02-10 01:25:18,777 - INFO - [Valid] [32/90] | Loss: 0.5386 | Val Acc: 80.53%
2026-02-10 01:25:18,781 - INFO - [Metrics for 'abnormal'] | Precision: 0.8273 | Recall: 0.7325 | F1: 0.7770
2026-02-10 01:25:18,782 - INFO - [Metrics for 'normal'] | Precision: 0.7900 | Recall: 0.8681 | F1: 0.8272
2026-02-10 01:25:18,783 - INFO - --------------------------------------------------
2026-02-10 01:25:18,785 - INFO - [LR]    [33/90] | Learning Rate: 0.000842
2026-02-10 01:25:21,936 - INFO - [Train] [33/90] | Loss: 0.2521 | Train Acc: 97.40%
2026-02-10 01:25:22,694 - INFO - [Valid] [33/90] | Loss: 0.5227 | Val Acc: 82.60%
2026-02-10 01:25:22,697 - INFO - [Metrics for 'abnormal'] | Precision: 0.8769 | Recall: 0.7261 | F1: 0.7944
2026-02-10 01:25:22,697 - INFO - [Metrics for 'normal'] | Precision: 0.7943 | Recall: 0.9121 | F1: 0.8491
2026-02-10 01:25:22,698 - INFO - --------------------------------------------------
2026-02-10 01:25:22,699 - INFO - [LR]    [34/90] | Learning Rate: 0.000829
2026-02-10 01:25:25,737 - INFO - [Train] [34/90] | Loss: 0.2606 | Train Acc: 96.88%
2026-02-10 01:25:26,259 - INFO - [Valid] [34/90] | Loss: 0.5233 | Val Acc: 82.01%
2026-02-10 01:25:26,262 - INFO - [Metrics for 'abnormal'] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-02-10 01:25:26,262 - INFO - [Metrics for 'normal'] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-02-10 01:25:26,263 - INFO - --------------------------------------------------
2026-02-10 01:25:26,264 - INFO - [LR]    [35/90] | Learning Rate: 0.000815
2026-02-10 01:25:28,027 - INFO - [Train] [35/90] | Loss: 0.2587 | Train Acc: 97.02%
2026-02-10 01:25:28,446 - INFO - [Valid] [35/90] | Loss: 0.5615 | Val Acc: 81.71%
2026-02-10 01:25:28,449 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8025 | F1: 0.8025
2026-02-10 01:25:28,449 - INFO - [Metrics for 'normal'] | Precision: 0.8297 | Recall: 0.8297 | F1: 0.8297
2026-02-10 01:25:28,450 - INFO - --------------------------------------------------
2026-02-10 01:25:28,450 - INFO - [LR]    [36/90] | Learning Rate: 0.000800
2026-02-10 01:25:30,203 - INFO - [Train] [36/90] | Loss: 0.2511 | Train Acc: 97.69%
2026-02-10 01:25:30,615 - INFO - [Valid] [36/90] | Loss: 0.5865 | Val Acc: 82.60%
2026-02-10 01:25:30,618 - INFO - [Metrics for 'abnormal'] | Precision: 0.8224 | Recall: 0.7962 | F1: 0.8091
2026-02-10 01:25:30,618 - INFO - [Metrics for 'normal'] | Precision: 0.8289 | Recall: 0.8516 | F1: 0.8401
2026-02-10 01:25:30,619 - INFO - --------------------------------------------------
2026-02-10 01:25:30,619 - INFO - [LR]    [37/90] | Learning Rate: 0.000785
2026-02-10 01:25:32,358 - INFO - [Train] [37/90] | Loss: 0.2386 | Train Acc: 98.14%
2026-02-10 01:25:32,770 - INFO - [Valid] [37/90] | Loss: 0.5209 | Val Acc: 80.83%
2026-02-10 01:25:32,772 - INFO - [Metrics for 'abnormal'] | Precision: 0.8194 | Recall: 0.7516 | F1: 0.7841
2026-02-10 01:25:32,772 - INFO - [Metrics for 'normal'] | Precision: 0.8000 | Recall: 0.8571 | F1: 0.8276
2026-02-10 01:25:32,773 - INFO - --------------------------------------------------
2026-02-10 01:25:32,774 - INFO - [LR]    [38/90] | Learning Rate: 0.000770
2026-02-10 01:25:34,518 - INFO - [Train] [38/90] | Loss: 0.2372 | Train Acc: 98.88%
2026-02-10 01:25:34,932 - INFO - [Valid] [38/90] | Loss: 0.5709 | Val Acc: 82.30%
2026-02-10 01:25:34,935 - INFO - [Metrics for 'abnormal'] | Precision: 0.7771 | Recall: 0.8662 | F1: 0.8193
2026-02-10 01:25:34,935 - INFO - [Metrics for 'normal'] | Precision: 0.8720 | Recall: 0.7857 | F1: 0.8266
2026-02-10 01:25:34,936 - INFO - --------------------------------------------------
2026-02-10 01:25:34,936 - INFO - [LR]    [39/90] | Learning Rate: 0.000754
2026-02-10 01:25:36,683 - INFO - [Train] [39/90] | Loss: 0.2406 | Train Acc: 98.29%
2026-02-10 01:25:37,098 - INFO - [Valid] [39/90] | Loss: 0.6276 | Val Acc: 81.42%
2026-02-10 01:25:37,100 - INFO - [Metrics for 'abnormal'] | Precision: 0.7866 | Recall: 0.8217 | F1: 0.8037
2026-02-10 01:25:37,101 - INFO - [Metrics for 'normal'] | Precision: 0.8400 | Recall: 0.8077 | F1: 0.8235
2026-02-10 01:25:37,101 - INFO - --------------------------------------------------
2026-02-10 01:25:37,102 - INFO - [LR]    [40/90] | Learning Rate: 0.000738
2026-02-10 01:25:38,869 - INFO - [Train] [40/90] | Loss: 0.2336 | Train Acc: 98.44%
2026-02-10 01:25:39,283 - INFO - [Valid] [40/90] | Loss: 0.6099 | Val Acc: 80.24%
2026-02-10 01:25:39,286 - INFO - [Metrics for 'abnormal'] | Precision: 0.7586 | Recall: 0.8408 | F1: 0.7976
2026-02-10 01:25:39,286 - INFO - [Metrics for 'normal'] | Precision: 0.8485 | Recall: 0.7692 | F1: 0.8069
2026-02-10 01:25:39,287 - INFO - --------------------------------------------------
2026-02-10 01:25:39,287 - INFO - [LR]    [41/90] | Learning Rate: 0.000722
2026-02-10 01:25:41,046 - INFO - [Train] [41/90] | Loss: 0.2308 | Train Acc: 98.96%
2026-02-10 01:25:41,457 - INFO - [Valid] [41/90] | Loss: 0.5505 | Val Acc: 80.24%
2026-02-10 01:25:41,459 - INFO - [Metrics for 'abnormal'] | Precision: 0.7711 | Recall: 0.8153 | F1: 0.7926
2026-02-10 01:25:41,459 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.7912 | F1: 0.8113
2026-02-10 01:25:41,460 - INFO - --------------------------------------------------
2026-02-10 01:25:41,461 - INFO - [LR]    [42/90] | Learning Rate: 0.000706
2026-02-10 01:25:43,965 - INFO - [Train] [42/90] | Loss: 0.2272 | Train Acc: 98.74%
2026-02-10 01:25:44,502 - INFO - [Valid] [42/90] | Loss: 0.6344 | Val Acc: 80.53%
2026-02-10 01:25:44,506 - INFO - [Metrics for 'abnormal'] | Precision: 0.7542 | Recall: 0.8599 | F1: 0.8036
2026-02-10 01:25:44,506 - INFO - [Metrics for 'normal'] | Precision: 0.8625 | Recall: 0.7582 | F1: 0.8070
2026-02-10 01:25:44,508 - INFO - --------------------------------------------------
2026-02-10 01:25:44,509 - INFO - [LR]    [43/90] | Learning Rate: 0.000689
2026-02-10 01:25:47,629 - INFO - [Train] [43/90] | Loss: 0.2358 | Train Acc: 98.51%
2026-02-10 01:25:48,234 - INFO - [Valid] [43/90] | Loss: 0.5097 | Val Acc: 81.42%
2026-02-10 01:25:48,238 - INFO - [Metrics for 'abnormal'] | Precision: 0.8133 | Recall: 0.7771 | F1: 0.7948
2026-02-10 01:25:48,239 - INFO - [Metrics for 'normal'] | Precision: 0.8148 | Recall: 0.8462 | F1: 0.8302
2026-02-10 01:25:48,240 - INFO - --------------------------------------------------
2026-02-10 01:25:48,241 - INFO - [LR]    [44/90] | Learning Rate: 0.000672
2026-02-10 01:25:52,426 - INFO - [Train] [44/90] | Loss: 0.2297 | Train Acc: 99.26%
2026-02-10 01:25:53,134 - INFO - [Valid] [44/90] | Loss: 0.5219 | Val Acc: 82.01%
2026-02-10 01:25:53,139 - INFO - [Metrics for 'abnormal'] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-02-10 01:25:53,139 - INFO - [Metrics for 'normal'] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-02-10 01:25:53,141 - INFO - --------------------------------------------------
2026-02-10 01:25:53,142 - INFO - [LR]    [45/90] | Learning Rate: 0.000655
2026-02-10 01:25:57,650 - INFO - [Train] [45/90] | Loss: 0.2255 | Train Acc: 99.26%
2026-02-10 01:25:58,259 - INFO - [Valid] [45/90] | Loss: 0.5577 | Val Acc: 83.19%
2026-02-10 01:25:58,264 - INFO - [Metrics for 'abnormal'] | Precision: 0.8205 | Recall: 0.8153 | F1: 0.8179
2026-02-10 01:25:58,265 - INFO - [Metrics for 'normal'] | Precision: 0.8415 | Recall: 0.8462 | F1: 0.8438
2026-02-10 01:25:58,266 - INFO - --------------------------------------------------
2026-02-10 01:25:58,268 - INFO - [LR]    [46/90] | Learning Rate: 0.000638
2026-02-10 01:26:02,987 - INFO - [Train] [46/90] | Loss: 0.2351 | Train Acc: 98.81%
2026-02-10 01:26:04,142 - INFO - [Valid] [46/90] | Loss: 0.5138 | Val Acc: 84.37%
2026-02-10 01:26:04,147 - INFO - [Metrics for 'abnormal'] | Precision: 0.8333 | Recall: 0.8280 | F1: 0.8307
2026-02-10 01:26:04,147 - INFO - [Metrics for 'normal'] | Precision: 0.8525 | Recall: 0.8571 | F1: 0.8548
2026-02-10 01:26:04,149 - INFO - --------------------------------------------------
2026-02-10 01:26:04,150 - INFO - [LR]    [47/90] | Learning Rate: 0.000620
2026-02-10 01:26:09,762 - INFO - [Train] [47/90] | Loss: 0.2191 | Train Acc: 99.70%
2026-02-10 01:26:11,039 - INFO - [Valid] [47/90] | Loss: 0.5482 | Val Acc: 83.78%
2026-02-10 01:26:11,044 - INFO - [Metrics for 'abnormal'] | Precision: 0.8110 | Recall: 0.8471 | F1: 0.8287
2026-02-10 01:26:11,044 - INFO - [Metrics for 'normal'] | Precision: 0.8629 | Recall: 0.8297 | F1: 0.8459
2026-02-10 01:26:11,045 - INFO - --------------------------------------------------
2026-02-10 01:26:11,047 - INFO - [LR]    [48/90] | Learning Rate: 0.000603
2026-02-10 01:26:17,087 - INFO - [Train] [48/90] | Loss: 0.2396 | Train Acc: 97.92%
2026-02-10 01:26:17,994 - INFO - [Valid] [48/90] | Loss: 0.5512 | Val Acc: 81.71%
2026-02-10 01:26:17,998 - INFO - [Metrics for 'abnormal'] | Precision: 0.8369 | Recall: 0.7516 | F1: 0.7919
2026-02-10 01:26:17,999 - INFO - [Metrics for 'normal'] | Precision: 0.8030 | Recall: 0.8736 | F1: 0.8368
2026-02-10 01:26:18,001 - INFO - --------------------------------------------------
2026-02-10 01:26:18,002 - INFO - [LR]    [49/90] | Learning Rate: 0.000585
2026-02-10 01:26:23,911 - INFO - [Train] [49/90] | Loss: 0.2228 | Train Acc: 98.88%
2026-02-10 01:26:25,028 - INFO - [Valid] [49/90] | Loss: 0.5704 | Val Acc: 83.78%
2026-02-10 01:26:25,034 - INFO - [Metrics for 'abnormal'] | Precision: 0.8148 | Recall: 0.8408 | F1: 0.8276
2026-02-10 01:26:25,035 - INFO - [Metrics for 'normal'] | Precision: 0.8588 | Recall: 0.8352 | F1: 0.8468
2026-02-10 01:26:25,036 - INFO - --------------------------------------------------
2026-02-10 01:26:25,038 - INFO - [LR]    [50/90] | Learning Rate: 0.000568
2026-02-10 01:26:32,877 - INFO - [Train] [50/90] | Loss: 0.2160 | Train Acc: 99.78%
2026-02-10 01:26:34,120 - INFO - [Valid] [50/90] | Loss: 0.5546 | Val Acc: 84.66%
2026-02-10 01:26:34,125 - INFO - [Metrics for 'abnormal'] | Precision: 0.8431 | Recall: 0.8217 | F1: 0.8323
2026-02-10 01:26:34,129 - INFO - [Metrics for 'normal'] | Precision: 0.8495 | Recall: 0.8681 | F1: 0.8587
2026-02-10 01:26:34,131 - INFO - --------------------------------------------------
2026-02-10 01:26:34,133 - INFO - [LR]    [51/90] | Learning Rate: 0.000550
2026-02-10 01:26:41,116 - INFO - [Train] [51/90] | Loss: 0.2267 | Train Acc: 98.96%
2026-02-10 01:26:42,419 - INFO - [Valid] [51/90] | Loss: 0.5252 | Val Acc: 82.60%
2026-02-10 01:26:42,424 - INFO - [Metrics for 'abnormal'] | Precision: 0.8101 | Recall: 0.8153 | F1: 0.8127
2026-02-10 01:26:42,424 - INFO - [Metrics for 'normal'] | Precision: 0.8398 | Recall: 0.8352 | F1: 0.8375
2026-02-10 01:26:42,425 - INFO - --------------------------------------------------
2026-02-10 01:26:42,427 - INFO - [LR]    [52/90] | Learning Rate: 0.000532
2026-02-10 01:26:48,685 - INFO - [Train] [52/90] | Loss: 0.2239 | Train Acc: 99.03%
2026-02-10 01:26:49,758 - INFO - [Valid] [52/90] | Loss: 0.5714 | Val Acc: 80.83%
2026-02-10 01:26:49,763 - INFO - [Metrics for 'abnormal'] | Precision: 0.7584 | Recall: 0.8599 | F1: 0.8060
2026-02-10 01:26:49,764 - INFO - [Metrics for 'normal'] | Precision: 0.8634 | Recall: 0.7637 | F1: 0.8105
2026-02-10 01:26:49,765 - INFO - --------------------------------------------------
2026-02-10 01:26:49,766 - INFO - [LR]    [53/90] | Learning Rate: 0.000515
2026-02-10 01:26:55,645 - INFO - [Train] [53/90] | Loss: 0.2215 | Train Acc: 99.33%
2026-02-10 01:26:56,850 - INFO - [Valid] [53/90] | Loss: 0.5523 | Val Acc: 84.07%
2026-02-10 01:26:56,860 - INFO - [Metrics for 'abnormal'] | Precision: 0.8121 | Recall: 0.8535 | F1: 0.8323
2026-02-10 01:26:56,860 - INFO - [Metrics for 'normal'] | Precision: 0.8678 | Recall: 0.8297 | F1: 0.8483
2026-02-10 01:26:56,861 - INFO - --------------------------------------------------
2026-02-10 01:26:56,863 - INFO - [LR]    [54/90] | Learning Rate: 0.000497
2026-02-10 01:27:02,289 - INFO - [Train] [54/90] | Loss: 0.2169 | Train Acc: 99.85%
2026-02-10 01:27:03,425 - INFO - [Valid] [54/90] | Loss: 0.5037 | Val Acc: 83.78%
2026-02-10 01:27:03,430 - INFO - [Metrics for 'abnormal'] | Precision: 0.8269 | Recall: 0.8217 | F1: 0.8243
2026-02-10 01:27:03,430 - INFO - [Metrics for 'normal'] | Precision: 0.8470 | Recall: 0.8516 | F1: 0.8493
2026-02-10 01:27:03,432 - INFO - --------------------------------------------------
2026-02-10 01:27:03,434 - INFO - [LR]    [55/90] | Learning Rate: 0.000480
2026-02-10 01:27:09,072 - INFO - [Train] [55/90] | Loss: 0.2188 | Train Acc: 99.70%
2026-02-10 01:27:10,209 - INFO - [Valid] [55/90] | Loss: 0.5737 | Val Acc: 84.07%
2026-02-10 01:27:10,213 - INFO - [Metrics for 'abnormal'] | Precision: 0.8323 | Recall: 0.8217 | F1: 0.8269
2026-02-10 01:27:10,214 - INFO - [Metrics for 'normal'] | Precision: 0.8478 | Recall: 0.8571 | F1: 0.8525
2026-02-10 01:27:10,215 - INFO - --------------------------------------------------
2026-02-10 01:27:10,216 - INFO - [LR]    [56/90] | Learning Rate: 0.000462
2026-02-10 01:27:14,954 - INFO - [Train] [56/90] | Loss: 0.2137 | Train Acc: 99.70%
2026-02-10 01:27:15,667 - INFO - [Valid] [56/90] | Loss: 0.5734 | Val Acc: 83.19%
2026-02-10 01:27:15,670 - INFO - [Metrics for 'abnormal'] | Precision: 0.8012 | Recall: 0.8471 | F1: 0.8235
2026-02-10 01:27:15,670 - INFO - [Metrics for 'normal'] | Precision: 0.8613 | Recall: 0.8187 | F1: 0.8394
2026-02-10 01:27:15,671 - INFO - --------------------------------------------------
2026-02-10 01:27:15,672 - INFO - [LR]    [57/90] | Learning Rate: 0.000445
2026-02-10 01:27:20,229 - INFO - [Train] [57/90] | Loss: 0.2133 | Train Acc: 99.70%
2026-02-10 01:27:20,949 - INFO - [Valid] [57/90] | Loss: 0.5005 | Val Acc: 83.48%
2026-02-10 01:27:20,954 - INFO - [Metrics for 'abnormal'] | Precision: 0.8217 | Recall: 0.8217 | F1: 0.8217
2026-02-10 01:27:20,954 - INFO - [Metrics for 'normal'] | Precision: 0.8462 | Recall: 0.8462 | F1: 0.8462
2026-02-10 01:27:20,989 - INFO - [Best Model Saved] (val loss: 0.5005) -> 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:27:20,989 - INFO - --------------------------------------------------
2026-02-10 01:27:20,991 - INFO - [LR]    [58/90] | Learning Rate: 0.000428
2026-02-10 01:27:25,508 - INFO - [Train] [58/90] | Loss: 0.2189 | Train Acc: 99.55%
2026-02-10 01:27:26,481 - INFO - [Valid] [58/90] | Loss: 0.5489 | Val Acc: 80.83%
2026-02-10 01:27:26,486 - INFO - [Metrics for 'abnormal'] | Precision: 0.8382 | Recall: 0.7261 | F1: 0.7782
2026-02-10 01:27:26,486 - INFO - [Metrics for 'normal'] | Precision: 0.7882 | Recall: 0.8791 | F1: 0.8312
2026-02-10 01:27:26,488 - INFO - --------------------------------------------------
2026-02-10 01:27:26,490 - INFO - [LR]    [59/90] | Learning Rate: 0.000411
2026-02-10 01:27:30,965 - INFO - [Train] [59/90] | Loss: 0.2163 | Train Acc: 99.63%
2026-02-10 01:27:31,831 - INFO - [Valid] [59/90] | Loss: 0.5816 | Val Acc: 80.53%
2026-02-10 01:27:31,834 - INFO - [Metrics for 'abnormal'] | Precision: 0.7542 | Recall: 0.8599 | F1: 0.8036
2026-02-10 01:27:31,834 - INFO - [Metrics for 'normal'] | Precision: 0.8625 | Recall: 0.7582 | F1: 0.8070
2026-02-10 01:27:31,835 - INFO - --------------------------------------------------
2026-02-10 01:27:31,835 - INFO - [LR]    [60/90] | Learning Rate: 0.000394
2026-02-10 01:27:35,494 - INFO - [Train] [60/90] | Loss: 0.2127 | Train Acc: 99.70%
2026-02-10 01:27:36,087 - INFO - [Valid] [60/90] | Loss: 0.5721 | Val Acc: 81.71%
2026-02-10 01:27:36,092 - INFO - [Metrics for 'abnormal'] | Precision: 0.7914 | Recall: 0.8217 | F1: 0.8063
2026-02-10 01:27:36,093 - INFO - [Metrics for 'normal'] | Precision: 0.8409 | Recall: 0.8132 | F1: 0.8268
2026-02-10 01:27:36,094 - INFO - --------------------------------------------------
2026-02-10 01:27:36,095 - INFO - [LR]    [61/90] | Learning Rate: 0.000378
2026-02-10 01:27:39,567 - INFO - [Train] [61/90] | Loss: 0.2101 | Train Acc: 99.78%
2026-02-10 01:27:40,395 - INFO - [Valid] [61/90] | Loss: 0.5670 | Val Acc: 81.12%
2026-02-10 01:27:40,399 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8280 | F1: 0.8025
2026-02-10 01:27:40,399 - INFO - [Metrics for 'normal'] | Precision: 0.8430 | Recall: 0.7967 | F1: 0.8192
2026-02-10 01:27:40,401 - INFO - --------------------------------------------------
2026-02-10 01:27:40,402 - INFO - [LR]    [62/90] | Learning Rate: 0.000362
2026-02-10 01:27:43,713 - INFO - [Train] [62/90] | Loss: 0.2086 | Train Acc: 100.00%
2026-02-10 01:27:44,459 - INFO - [Valid] [62/90] | Loss: 0.5697 | Val Acc: 82.89%
2026-02-10 01:27:44,464 - INFO - [Metrics for 'abnormal'] | Precision: 0.8153 | Recall: 0.8153 | F1: 0.8153
2026-02-10 01:27:44,464 - INFO - [Metrics for 'normal'] | Precision: 0.8407 | Recall: 0.8407 | F1: 0.8407
2026-02-10 01:27:44,466 - INFO - --------------------------------------------------
2026-02-10 01:27:44,467 - INFO - [LR]    [63/90] | Learning Rate: 0.000346
2026-02-10 01:27:48,986 - INFO - [Train] [63/90] | Loss: 0.2083 | Train Acc: 99.85%
2026-02-10 01:27:50,158 - INFO - [Valid] [63/90] | Loss: 0.5624 | Val Acc: 81.42%
2026-02-10 01:27:50,163 - INFO - [Metrics for 'abnormal'] | Precision: 0.7901 | Recall: 0.8153 | F1: 0.8025
2026-02-10 01:27:50,163 - INFO - [Metrics for 'normal'] | Precision: 0.8362 | Recall: 0.8132 | F1: 0.8245
2026-02-10 01:27:50,165 - INFO - --------------------------------------------------
2026-02-10 01:27:50,167 - INFO - [LR]    [64/90] | Learning Rate: 0.000330
2026-02-10 01:27:54,957 - INFO - [Train] [64/90] | Loss: 0.2129 | Train Acc: 99.55%
2026-02-10 01:27:55,891 - INFO - [Valid] [64/90] | Loss: 0.5543 | Val Acc: 82.89%
2026-02-10 01:27:55,896 - INFO - [Metrics for 'abnormal'] | Precision: 0.8194 | Recall: 0.8089 | F1: 0.8141
2026-02-10 01:27:55,896 - INFO - [Metrics for 'normal'] | Precision: 0.8370 | Recall: 0.8462 | F1: 0.8415
2026-02-10 01:27:55,897 - INFO - --------------------------------------------------
2026-02-10 01:27:55,899 - INFO - [LR]    [65/90] | Learning Rate: 0.000315
2026-02-10 01:28:01,266 - INFO - [Train] [65/90] | Loss: 0.2058 | Train Acc: 99.93%
2026-02-10 01:28:02,491 - INFO - [Valid] [65/90] | Loss: 0.5511 | Val Acc: 82.60%
2026-02-10 01:28:02,496 - INFO - [Metrics for 'abnormal'] | Precision: 0.8141 | Recall: 0.8089 | F1: 0.8115
2026-02-10 01:28:02,496 - INFO - [Metrics for 'normal'] | Precision: 0.8361 | Recall: 0.8407 | F1: 0.8384
2026-02-10 01:28:02,499 - INFO - --------------------------------------------------
2026-02-10 01:28:02,500 - INFO - [LR]    [66/90] | Learning Rate: 0.000300
2026-02-10 01:28:08,245 - INFO - [Train] [66/90] | Loss: 0.2077 | Train Acc: 99.78%
2026-02-10 01:28:09,275 - INFO - [Valid] [66/90] | Loss: 0.5440 | Val Acc: 80.83%
2026-02-10 01:28:09,279 - INFO - [Metrics for 'abnormal'] | Precision: 0.7840 | Recall: 0.8089 | F1: 0.7962
2026-02-10 01:28:09,280 - INFO - [Metrics for 'normal'] | Precision: 0.8305 | Recall: 0.8077 | F1: 0.8189
2026-02-10 01:28:09,281 - INFO - --------------------------------------------------
2026-02-10 01:28:09,282 - INFO - [LR]    [67/90] | Learning Rate: 0.000285
2026-02-10 01:28:14,977 - INFO - [Train] [67/90] | Loss: 0.2089 | Train Acc: 99.78%
2026-02-10 01:28:16,042 - INFO - [Valid] [67/90] | Loss: 0.5354 | Val Acc: 81.12%
2026-02-10 01:28:16,047 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8280 | F1: 0.8025
2026-02-10 01:28:16,048 - INFO - [Metrics for 'normal'] | Precision: 0.8430 | Recall: 0.7967 | F1: 0.8192
2026-02-10 01:28:16,049 - INFO - --------------------------------------------------
2026-02-10 01:28:16,051 - INFO - [LR]    [68/90] | Learning Rate: 0.000271
2026-02-10 01:28:21,851 - INFO - [Train] [68/90] | Loss: 0.2037 | Train Acc: 100.00%
2026-02-10 01:28:23,161 - INFO - [Valid] [68/90] | Loss: 0.5665 | Val Acc: 81.42%
2026-02-10 01:28:23,166 - INFO - [Metrics for 'abnormal'] | Precision: 0.7831 | Recall: 0.8280 | F1: 0.8050
2026-02-10 01:28:23,166 - INFO - [Metrics for 'normal'] | Precision: 0.8439 | Recall: 0.8022 | F1: 0.8225
2026-02-10 01:28:23,167 - INFO - --------------------------------------------------
2026-02-10 01:28:23,169 - INFO - [LR]    [69/90] | Learning Rate: 0.000258
2026-02-10 01:28:29,104 - INFO - [Train] [69/90] | Loss: 0.2082 | Train Acc: 99.78%
2026-02-10 01:28:30,488 - INFO - [Valid] [69/90] | Loss: 0.5641 | Val Acc: 81.42%
2026-02-10 01:28:30,493 - INFO - [Metrics for 'abnormal'] | Precision: 0.7611 | Recall: 0.8726 | F1: 0.8131
2026-02-10 01:28:30,494 - INFO - [Metrics for 'normal'] | Precision: 0.8742 | Recall: 0.7637 | F1: 0.8152
2026-02-10 01:28:30,496 - INFO - --------------------------------------------------
2026-02-10 01:28:30,498 - INFO - [LR]    [70/90] | Learning Rate: 0.000245
2026-02-10 01:28:35,920 - INFO - [Train] [70/90] | Loss: 0.2075 | Train Acc: 99.93%
2026-02-10 01:28:37,478 - INFO - [Valid] [70/90] | Loss: 0.5424 | Val Acc: 81.12%
2026-02-10 01:28:37,486 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8280 | F1: 0.8025
2026-02-10 01:28:37,486 - INFO - [Metrics for 'normal'] | Precision: 0.8430 | Recall: 0.7967 | F1: 0.8192
2026-02-10 01:28:37,488 - INFO - --------------------------------------------------
2026-02-10 01:28:37,490 - INFO - [LR]    [71/90] | Learning Rate: 0.000232
2026-02-10 01:28:44,336 - INFO - [Train] [71/90] | Loss: 0.2044 | Train Acc: 99.85%
2026-02-10 01:28:45,828 - INFO - [Valid] [71/90] | Loss: 0.5401 | Val Acc: 82.01%
2026-02-10 01:28:45,836 - INFO - [Metrics for 'abnormal'] | Precision: 0.8077 | Recall: 0.8025 | F1: 0.8051
2026-02-10 01:28:45,837 - INFO - [Metrics for 'normal'] | Precision: 0.8306 | Recall: 0.8352 | F1: 0.8329
2026-02-10 01:28:45,839 - INFO - --------------------------------------------------
2026-02-10 01:28:45,841 - INFO - [LR]    [72/90] | Learning Rate: 0.000220
2026-02-10 01:28:53,125 - INFO - [Train] [72/90] | Loss: 0.2065 | Train Acc: 99.85%
2026-02-10 01:28:54,694 - INFO - [Valid] [72/90] | Loss: 0.5566 | Val Acc: 81.12%
2026-02-10 01:28:54,699 - INFO - [Metrics for 'abnormal'] | Precision: 0.7719 | Recall: 0.8408 | F1: 0.8049
2026-02-10 01:28:54,699 - INFO - [Metrics for 'normal'] | Precision: 0.8512 | Recall: 0.7857 | F1: 0.8171
2026-02-10 01:28:54,701 - INFO - --------------------------------------------------
2026-02-10 01:28:54,702 - INFO - [LR]    [73/90] | Learning Rate: 0.000208
2026-02-10 01:29:01,708 - INFO - [Train] [73/90] | Loss: 0.2031 | Train Acc: 99.93%
2026-02-10 01:29:03,167 - INFO - [Valid] [73/90] | Loss: 0.5510 | Val Acc: 81.12%
2026-02-10 01:29:03,171 - INFO - [Metrics for 'abnormal'] | Precision: 0.7751 | Recall: 0.8344 | F1: 0.8037
2026-02-10 01:29:03,171 - INFO - [Metrics for 'normal'] | Precision: 0.8471 | Recall: 0.7912 | F1: 0.8182
2026-02-10 01:29:03,173 - INFO - --------------------------------------------------
2026-02-10 01:29:03,175 - INFO - [LR]    [74/90] | Learning Rate: 0.000197
2026-02-10 01:29:10,257 - INFO - [Train] [74/90] | Loss: 0.2038 | Train Acc: 99.85%
2026-02-10 01:29:11,805 - INFO - [Valid] [74/90] | Loss: 0.5683 | Val Acc: 82.60%
2026-02-10 01:29:11,811 - INFO - [Metrics for 'abnormal'] | Precision: 0.8224 | Recall: 0.7962 | F1: 0.8091
2026-02-10 01:29:11,811 - INFO - [Metrics for 'normal'] | Precision: 0.8289 | Recall: 0.8516 | F1: 0.8401
2026-02-10 01:29:11,813 - INFO - --------------------------------------------------
2026-02-10 01:29:11,814 - INFO - [LR]    [75/90] | Learning Rate: 0.000186
2026-02-10 01:29:18,918 - INFO - [Train] [75/90] | Loss: 0.2107 | Train Acc: 99.48%
2026-02-10 01:29:20,450 - INFO - [Valid] [75/90] | Loss: 0.5479 | Val Acc: 81.71%
2026-02-10 01:29:20,460 - INFO - [Metrics for 'abnormal'] | Precision: 0.7950 | Recall: 0.8153 | F1: 0.8050
2026-02-10 01:29:20,460 - INFO - [Metrics for 'normal'] | Precision: 0.8371 | Recall: 0.8187 | F1: 0.8278
2026-02-10 01:29:20,462 - INFO - --------------------------------------------------
2026-02-10 01:29:20,463 - INFO - [LR]    [76/90] | Learning Rate: 0.000176
2026-02-10 01:29:27,620 - INFO - [Train] [76/90] | Loss: 0.2046 | Train Acc: 100.00%
2026-02-10 01:29:29,205 - INFO - [Valid] [76/90] | Loss: 0.5634 | Val Acc: 82.01%
2026-02-10 01:29:29,212 - INFO - [Metrics for 'abnormal'] | Precision: 0.8077 | Recall: 0.8025 | F1: 0.8051
2026-02-10 01:29:29,212 - INFO - [Metrics for 'normal'] | Precision: 0.8306 | Recall: 0.8352 | F1: 0.8329
2026-02-10 01:29:29,214 - INFO - --------------------------------------------------
2026-02-10 01:29:29,216 - INFO - [LR]    [77/90] | Learning Rate: 0.000166
2026-02-10 01:29:36,148 - INFO - [Train] [77/90] | Loss: 0.2065 | Train Acc: 99.78%
2026-02-10 01:29:37,565 - INFO - [Valid] [77/90] | Loss: 0.5400 | Val Acc: 82.01%
2026-02-10 01:29:37,574 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-02-10 01:29:37,574 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-02-10 01:29:37,575 - INFO - --------------------------------------------------
2026-02-10 01:29:37,577 - INFO - [LR]    [78/90] | Learning Rate: 0.000157
2026-02-10 01:29:44,547 - INFO - [Train] [78/90] | Loss: 0.2043 | Train Acc: 99.93%
2026-02-10 01:29:45,965 - INFO - [Valid] [78/90] | Loss: 0.5593 | Val Acc: 82.01%
2026-02-10 01:29:45,970 - INFO - [Metrics for 'abnormal'] | Precision: 0.8000 | Recall: 0.8153 | F1: 0.8076
2026-02-10 01:29:45,970 - INFO - [Metrics for 'normal'] | Precision: 0.8380 | Recall: 0.8242 | F1: 0.8310
2026-02-10 01:29:45,972 - INFO - --------------------------------------------------
2026-02-10 01:29:45,974 - INFO - [LR]    [79/90] | Learning Rate: 0.000149
2026-02-10 01:29:53,474 - INFO - [Train] [79/90] | Loss: 0.2013 | Train Acc: 100.00%
2026-02-10 01:29:55,124 - INFO - [Valid] [79/90] | Loss: 0.5739 | Val Acc: 82.01%
2026-02-10 01:29:55,129 - INFO - [Metrics for 'abnormal'] | Precision: 0.7963 | Recall: 0.8217 | F1: 0.8088
2026-02-10 01:29:55,129 - INFO - [Metrics for 'normal'] | Precision: 0.8418 | Recall: 0.8187 | F1: 0.8301
2026-02-10 01:29:55,131 - INFO - --------------------------------------------------
2026-02-10 01:29:55,133 - INFO - [LR]    [80/90] | Learning Rate: 0.000141
2026-02-10 01:30:02,071 - INFO - [Train] [80/90] | Loss: 0.2024 | Train Acc: 100.00%
2026-02-10 01:30:03,689 - INFO - [Valid] [80/90] | Loss: 0.5606 | Val Acc: 81.12%
2026-02-10 01:30:03,694 - INFO - [Metrics for 'abnormal'] | Precision: 0.7784 | Recall: 0.8280 | F1: 0.8025
2026-02-10 01:30:03,694 - INFO - [Metrics for 'normal'] | Precision: 0.8430 | Recall: 0.7967 | F1: 0.8192
2026-02-10 01:30:03,696 - INFO - --------------------------------------------------
2026-02-10 01:30:03,698 - INFO - [LR]    [81/90] | Learning Rate: 0.000134
2026-02-10 01:30:10,460 - INFO - [Train] [81/90] | Loss: 0.2035 | Train Acc: 99.85%
2026-02-10 01:30:11,820 - INFO - [Valid] [81/90] | Loss: 0.5483 | Val Acc: 82.89%
2026-02-10 01:30:11,829 - INFO - [Metrics for 'abnormal'] | Precision: 0.8235 | Recall: 0.8025 | F1: 0.8129
2026-02-10 01:30:11,829 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8516 | F1: 0.8424
2026-02-10 01:30:11,831 - INFO - --------------------------------------------------
2026-02-10 01:30:11,833 - INFO - [LR]    [82/90] | Learning Rate: 0.000128
2026-02-10 01:30:18,759 - INFO - [Train] [82/90] | Loss: 0.2032 | Train Acc: 99.78%
2026-02-10 01:30:20,300 - INFO - [Valid] [82/90] | Loss: 0.5682 | Val Acc: 81.12%
2026-02-10 01:30:20,305 - INFO - [Metrics for 'abnormal'] | Precision: 0.7751 | Recall: 0.8344 | F1: 0.8037
2026-02-10 01:30:20,305 - INFO - [Metrics for 'normal'] | Precision: 0.8471 | Recall: 0.7912 | F1: 0.8182
2026-02-10 01:30:20,307 - INFO - --------------------------------------------------
2026-02-10 01:30:20,309 - INFO - [LR]    [83/90] | Learning Rate: 0.000122
2026-02-10 01:30:27,336 - INFO - [Train] [83/90] | Loss: 0.2036 | Train Acc: 99.93%
2026-02-10 01:30:29,056 - INFO - [Valid] [83/90] | Loss: 0.5464 | Val Acc: 81.71%
2026-02-10 01:30:29,064 - INFO - [Metrics for 'abnormal'] | Precision: 0.7950 | Recall: 0.8153 | F1: 0.8050
2026-02-10 01:30:29,064 - INFO - [Metrics for 'normal'] | Precision: 0.8371 | Recall: 0.8187 | F1: 0.8278
2026-02-10 01:30:29,066 - INFO - --------------------------------------------------
2026-02-10 01:30:29,068 - INFO - [LR]    [84/90] | Learning Rate: 0.000117
2026-02-10 01:30:35,167 - INFO - [Train] [84/90] | Loss: 0.2021 | Train Acc: 99.93%
2026-02-10 01:30:36,851 - INFO - [Valid] [84/90] | Loss: 0.5475 | Val Acc: 81.71%
2026-02-10 01:30:36,856 - INFO - [Metrics for 'abnormal'] | Precision: 0.7987 | Recall: 0.8089 | F1: 0.8038
2026-02-10 01:30:36,856 - INFO - [Metrics for 'normal'] | Precision: 0.8333 | Recall: 0.8242 | F1: 0.8287
2026-02-10 01:30:36,858 - INFO - --------------------------------------------------
2026-02-10 01:30:36,859 - INFO - [LR]    [85/90] | Learning Rate: 0.000112
2026-02-10 01:30:43,644 - INFO - [Train] [85/90] | Loss: 0.2016 | Train Acc: 100.00%
2026-02-10 01:30:45,145 - INFO - [Valid] [85/90] | Loss: 0.5639 | Val Acc: 81.71%
2026-02-10 01:30:45,150 - INFO - [Metrics for 'abnormal'] | Precision: 0.7844 | Recall: 0.8344 | F1: 0.8086
2026-02-10 01:30:45,151 - INFO - [Metrics for 'normal'] | Precision: 0.8488 | Recall: 0.8022 | F1: 0.8249
2026-02-10 01:30:45,152 - INFO - --------------------------------------------------
2026-02-10 01:30:45,154 - INFO - [LR]    [86/90] | Learning Rate: 0.000109
2026-02-10 01:30:52,051 - INFO - [Train] [86/90] | Loss: 0.2020 | Train Acc: 99.85%
2026-02-10 01:30:53,557 - INFO - [Valid] [86/90] | Loss: 0.5560 | Val Acc: 82.01%
2026-02-10 01:30:53,562 - INFO - [Metrics for 'abnormal'] | Precision: 0.7857 | Recall: 0.8408 | F1: 0.8123
2026-02-10 01:30:53,563 - INFO - [Metrics for 'normal'] | Precision: 0.8538 | Recall: 0.8022 | F1: 0.8272
2026-02-10 01:30:53,564 - INFO - --------------------------------------------------
2026-02-10 01:30:53,566 - INFO - [LR]    [87/90] | Learning Rate: 0.000106
2026-02-10 01:31:00,604 - INFO - [Train] [87/90] | Loss: 0.2009 | Train Acc: 99.93%
2026-02-10 01:31:01,925 - INFO - [Valid] [87/90] | Loss: 0.5564 | Val Acc: 81.71%
2026-02-10 01:31:01,930 - INFO - [Metrics for 'abnormal'] | Precision: 0.8025 | Recall: 0.8025 | F1: 0.8025
2026-02-10 01:31:01,930 - INFO - [Metrics for 'normal'] | Precision: 0.8297 | Recall: 0.8297 | F1: 0.8297
2026-02-10 01:31:01,932 - INFO - --------------------------------------------------
2026-02-10 01:31:01,934 - INFO - [LR]    [88/90] | Learning Rate: 0.000103
2026-02-10 01:31:09,323 - INFO - [Train] [88/90] | Loss: 0.2039 | Train Acc: 99.85%
2026-02-10 01:31:10,661 - INFO - [Valid] [88/90] | Loss: 0.5716 | Val Acc: 82.01%
2026-02-10 01:31:10,666 - INFO - [Metrics for 'abnormal'] | Precision: 0.8038 | Recall: 0.8089 | F1: 0.8063
2026-02-10 01:31:10,667 - INFO - [Metrics for 'normal'] | Precision: 0.8343 | Recall: 0.8297 | F1: 0.8320
2026-02-10 01:31:10,668 - INFO - --------------------------------------------------
2026-02-10 01:31:10,670 - INFO - [LR]    [89/90] | Learning Rate: 0.000101
2026-02-10 01:31:17,985 - INFO - [Train] [89/90] | Loss: 0.2028 | Train Acc: 99.85%
2026-02-10 01:31:19,498 - INFO - [Valid] [89/90] | Loss: 0.5663 | Val Acc: 81.42%
2026-02-10 01:31:19,504 - INFO - [Metrics for 'abnormal'] | Precision: 0.7937 | Recall: 0.8089 | F1: 0.8013
2026-02-10 01:31:19,505 - INFO - [Metrics for 'normal'] | Precision: 0.8324 | Recall: 0.8187 | F1: 0.8255
2026-02-10 01:31:19,506 - INFO - --------------------------------------------------
2026-02-10 01:31:19,508 - INFO - [LR]    [90/90] | Learning Rate: 0.000100
2026-02-10 01:31:26,439 - INFO - [Train] [90/90] | Loss: 0.2185 | Train Acc: 99.33%
2026-02-10 01:31:27,941 - INFO - [Valid] [90/90] | Loss: 0.5633 | Val Acc: 82.01%
2026-02-10 01:31:27,946 - INFO - [Metrics for 'abnormal'] | Precision: 0.7892 | Recall: 0.8344 | F1: 0.8111
2026-02-10 01:31:27,946 - INFO - [Metrics for 'normal'] | Precision: 0.8497 | Recall: 0.8077 | F1: 0.8282
2026-02-10 01:31:27,949 - INFO - ==================================================
2026-02-10 01:31:27,949 - INFO - 훈련 완료. 최고 성능 모델을 불러와 테스트 세트로 최종 평가합니다.
2026-02-10 01:31:27,949 - INFO - 최종 평가를 위해 새로운 모델 객체를 생성합니다.
2026-02-10 01:31:27,949 - INFO - Baseline 모델 'mobile_vit_xxs'을(를) 생성합니다 (사전 훈련 가중치: 미사용).
2026-02-10 01:31:27,980 - INFO - timm 모델(mobile_vit_xxs)의 Attention.forward를 Pruning 호환성을 위해 Monkey-patching 합니다.
2026-02-10 01:31:27,991 - INFO - 최종 평가 모델에 Pruning 구조를 재적용합니다.
2026-02-10 01:31:27,992 - INFO - torch-pruning을 사용한 L1 Norm Pruning을 시작합니다.
2026-02-10 01:31:27,992 - INFO - 분류 레이어 'ClassifierHead'을(를) Pruning 대상에서 제외합니다.
2026-02-10 01:31:27,993 - INFO - ViT 계열 모델의 모든 qkv 레이어를 Pruning 대상에서 제외합니다.
2026-02-10 01:31:28,408 - INFO - torch-pruning 완료. 모델 구조가 희소도(0.47856445312499996)에 맞춰 변경되었습니다.
2026-02-10 01:31:28,409 - INFO - ==================================================
2026-02-10 01:31:28,471 - INFO - 최고 성능 모델 가중치를 새로 생성된 모델 객체에 로드 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/best_model.pth'
2026-02-10 01:31:28,471 - INFO - ==================================================
2026-02-10 01:31:28,471 - INFO - Test 모드를 시작합니다.
2026-02-10 01:31:28,642 - INFO - 연산량 (MACs): 0.0912 GMACs per sample
2026-02-10 01:31:28,643 - INFO - 연산량 (FLOPs): 0.1824 GFLOPs per sample
2026-02-10 01:31:28,643 - INFO - ==================================================
2026-02-10 01:31:28,643 - INFO - GPU 캐시를 비우고 측정을 시작합니다.
2026-02-10 01:31:30,313 - INFO - 샘플 당 평균 Forward Pass 시간: 6.77ms (std: 0.88ms), FPS: 150.03 (std: 18.71) (1개 샘플 x 100회 반복)
2026-02-10 01:31:30,313 - INFO - 샘플 당 Forward Pass 시 최대 GPU 메모리 사용량: 44.36 MB
2026-02-10 01:31:30,313 - INFO - 테스트 데이터셋에 대한 추론을 시작합니다.
2026-02-10 01:31:32,877 - INFO - [Test] Loss: 0.4118 | Test Acc: 83.48%
2026-02-10 01:31:32,884 - INFO - [Metrics for 'abnormal'] | Precision: 0.8217 | Recall: 0.8217 | F1: 0.8217
2026-02-10 01:31:32,884 - INFO - [Metrics for 'normal'] | Precision: 0.8462 | Recall: 0.8462 | F1: 0.8462
2026-02-10 01:31:33,261 - INFO - ==================================================
2026-02-10 01:31:33,261 - INFO - 혼동 행렬 저장 완료. 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/confusion_matrix_20260210_012340.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/confusion_matrix_20260210_012340.pdf'
2026-02-10 01:31:33,261 - INFO - ==================================================
2026-02-10 01:31:33,261 - INFO - ONNX 변환 및 평가를 시작합니다.
2026-02-10 01:31:37,450 - INFO - 모델이 ONNX 형식으로 변환되어 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/model_fp32_20260210_012340.onnx'에 저장되었습니다. (크기: 1.40 MB)
2026-02-10 01:31:38,122 - INFO - [Model Load] ONNX 모델(FP32) 로드 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 11.81 MB
2026-02-10 01:31:38,123 - INFO - ONNX 런타임의 샘플 당 Forward Pass 시간 측정을 시작합니다...
2026-02-10 01:31:41,492 - INFO - 샘플 당 평균 Forward Pass 시간 (ONNX, CPU): 26.66ms (std: 12.55ms)
2026-02-10 01:31:41,492 - INFO - 샘플 당 평균 FPS (ONNX, CPU): 49.18 FPS (std: 31.00) (1개 샘플 x 100회 반복)
2026-02-10 01:31:41,492 - INFO - [Inference] 추론 중 피크 메모리 - 모델 로드 후 메모리 정리 직후 메모리: 12.00 MB
2026-02-10 01:31:41,492 - INFO - [Total] (FP32) 추론 중 피크 메모리 - 모델 로드 전 청소 직후 메모리: 22.86 MB
2026-02-10 01:31:57,878 - INFO - [Test (ONNX)] | Test Acc (ONNX): 83.48%
2026-02-10 01:31:57,895 - INFO - [Metrics for 'abnormal' (ONNX)] | Precision: 0.8217 | Recall: 0.8217 | F1: 0.8217
2026-02-10 01:31:57,896 - INFO - [Metrics for 'normal' (ONNX)] | Precision: 0.8462 | Recall: 0.8462 | F1: 0.8462
2026-02-10 01:31:58,194 - INFO - Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/val_acc.pdf'
2026-02-10 01:31:58,421 - INFO - Train/Val Acc 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/train_val_acc.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/train_val_acc.pdf'
2026-02-10 01:31:58,611 - INFO - F1 (normal) 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/F1_normal.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/F1_normal.pdf'
2026-02-10 01:31:58,837 - INFO - Validation Loss 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/val_loss.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/val_loss.pdf'
2026-02-10 01:31:59,037 - INFO - Learning Rate 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/learning_rate.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/learning_rate.pdf'
2026-02-10 01:32:01,544 - INFO - 종합 그래프 저장 완료: 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/compile.png' and 'log/Sewer-TAPNEW/baseline_mobile_vit_xxs_l1_20260210_012340/graph_20260210_012340/compile.pdf'
