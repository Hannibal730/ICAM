
%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
% \usepackage{amssymb}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{pifont}
\input{math_commands.tex}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}
\usepackage{wrapfig}  % 텍스트가 감싸는 표/그림을 만들기 위해 필수
\usepackage{graphicx} % resizebox를 위해 필요
\usepackage{subcaption} % for \subcaption command
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage{soul}
\usepackage{verbatim}
\usepackage[USenglish, nodayofweek]{datetime}
\usepackage{url}
\usepackage{graphics}
%\usepackage{subfig}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}
\usepackage{rotating}
\usepackage{geometry}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{placeins}
\usepackage[table]{xcolor}
\hypersetup{colorlinks = true, linkcolor = blue, anchorcolor =red, citecolor = blue, filecolor = red, urlcolor = red, pdfauthor=author}

\journal{Engineering Applications of Artificial Intelligence}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%            addressline={}, 
%%            city={},
%%            postcode={}, 
%%            state={},
%%            country={}}
%% \fntext[label3]{}

% \title{On-Device Friendly Sewer Defect Inspection via Query-conditioned Cross-Attention Model}
% \title{ICAM: Lightweight Input-conditioned Query Cross-Attention for On-device Sewer Defect Inspection}
% \title{On-device Friendly Sewer Defect Inspection via Query-conditioned Cross-Attention Model}
\title{On-device Friendly Sewer Defect Inspection via Instance-adaptive Cross-Attention Model}

%% use optional labels to link authors explicitly to addresses:

% \begin{highlights}
% %\item
% %\item 
% %\item 
% %\item 
% \end{highlights}
\author[b]{Deaseung Choi}
\author[a]{Mincheol Kim}
\author[a]{Jaeseong Kim}
\author[c]{Seungjin Ko}
\author[c]{Jaeeun Heo}
\author[a]{Hoki Kim\corref{cor1}}
\cortext[cor1]{corresponding author}
\ead{hokikim@cau.ac.kr}
\affiliation[a]{organization={Chung-Ang University},
            addressline={84 Heukseok-ro},
            city={Seoul},
            postcode={06974}, 
            country={Republic of Korea}}
            
\affiliation[b]{organization={Konkuk University},
            addressline={120-1 Neungdong-ro}, 
            city={Seoul},
            postcode={05030}, 
            country={Republic of Korea}}
\affiliation[c]{organization={TAP Electronics Co.,Ltd.},
            addressline={9 Gukasan-daero 30-gil}, 
            city={Daegu},
            postcode={43008}, 
            country={Republic of Korea}}

\begin{abstract}
%Accurate sewer defect inspection is critical for structural health monitoring, yet most high-performing vision models are too computationally heavy for deployment on resource-constrained inspection robots. We propose the \emph{Instance-adaptive Cross-Attention Model} (ICAM), an on-device friendly framework for real-time sewer defect classification. ICAM avoids quadratic-cost self-attention by using a small set of learnable queries that interact with patch tokens only through cross-attention, reducing attention complexity from $O(N^2)$ to $O(NN_q)$. To make shallow decoders effective under tight budgets, ICAM introduces an \emph{instance-adaptive query initialization} step that conditions the queries on each input via an initial cross-attention pass, providing an informative starting state before iterative refinement. On a real-world private sewer dataset (1,692 images), ICAM achieves 88.20\% accuracy and 87.42\% F1 on the abnormal class, outperforming lightweight CNN, ViT and domain-specific baselines. Despite strong accuracy, ICAM is extremely compact (31K parameters, 0.18 GFLOPs, 0.19\,MB ONNX) and runs in real time on a Raspberry Pi 5 (57.37 FPS), demonstrating a practical accuracy--efficiency trade-off for embedded sewer inspection.
\end{abstract}

\begin{keyword}

Sewer defect \sep Structural health monitoring (SHM) \sep On-device AI \sep Resource-constrained deployment \sep Cross-attention \sep Instance-adaptive queries

\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:intro}

Civil infrastructure comprises essential public utilities that support modern society and improve our quality of life \citep{fu2025optimizing}. Although these systems deliver long-term service, their performance inevitably degrades with age and continuous operation. This motivates the need for structural health monitoring (SHM) systems, such as detecting damage and ensuring operational reliability \citep{cha2024deep}.

Recent progress in deep learning has further accelerated the development of smart \citep{fu2025optimizing, malekloo2022machine, hsieh2020machine}. Numerous studies have demonstrated that deep learning substantially enhances the accuracy in SHM \citep{malekloo2022machine, qiu2023real}. Comprehensive reviews \citep{dong2021review, hsieh2020machine, panella2022semantic} have highlighted these advancements; for example, quantitative evaluation frameworks have shown that deep learning–based crack analysis outperforms traditional image-processing and classical machine-learning approaches.

Among various civil infrastructures, sewer systems are particularly critical because they support essential water and sanitation functions \citep{jo2022sewerage, haurum2021sewer}. Similar to other civil infrastructures, sewer networks require periodic inspection and maintenance since they can have defects such as cracks, joint displacements, root intrusions, and structural deformation over time, which leads to the requirement of SHM. Early detection of these defects is crucial for preventing failures and ensuring safe, continuous operation. The 2025 Infrastructure Report Card released by the American Society of Civil Engineers assigned a grade of D+ to the U.S. wastewater system. Specifically, failures per 100 miles of pipe remained stable at roughly 2 from 2017 onward but surged to 3.3 in 2021, while the costs associated with repairing or replacing aging pipelines continue to increase.
Therefore, sewer maintenance is a critical task for ensuring public safety, as undetected sewer defects can lead to urban failures such as sinkholes and groundwater or soil contamination, which in turn pose serious public health risks \citep{yang2026structural}. 

To diagnose the condition of sewer systems, remotely operated robots are used to record the interior of sewer pipes, and domain experts then inspect the recorded images \citep{nguyen2025sewer}. This manual inspection process is time-consuming and labor-intensive \citep{wang2021towards}. Moreover, \citet{dirksen2013consistency} reported that human inspectors failed to correctly classify approximately 25\% of sewer defects in datasets from Germany, the Netherlands, France, and Austria. To address these limitations, previous studies have proposed sewer defect detection systems based on conventional machine-learning techniques \citep{makar1999diagnostic}. More recently, \citet{kumar2018automated} proposed a deep learning based sewer defect detection system, and with the rapid progress of deep learning, such methods are now being actively investigated \citep{nashat2025hybrid}.
Despite recent progress, a critical gap remains between the computational demands of state-of-the-art inspection models and the physical constraints of deployment environments.
Deep learning–based inspection systems still demand substantial computational resources, memory, and power to process complex sewer-assessment tasks \citep{shuvo2022efficient}.
These requirements are particularly problematic in the sewer infrastructure, where inspection robots operate in harsh and confined environments with limited access to high-speed communication or cloud servers.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/intro.pdf}
    \caption{Challenges in deploying high-performance models on resource-constrained environments.}
    \label{fig:intro}
\end{figure}

Figure~\ref{fig:intro} illustrates these deployment challenges, highlighting the significant gap between the substantial computational demands of high-performance models and the strictly limited resources of confined inspection environments. Consequently, deploying accurate sewer defect detection models directly on edge or embedded devices remains a challenging task due to these hardware constraints \citep{ma2022training}.

In this paper, we propose an on-device friendly sewer defect inspection framework that enables efficient inference under limited computational budgets. Specifically, the proposed architecture, \textbf{Instance-adaptive Cross-Attention Model (ICAM)}, adopts (i) Instance-adaptive query initialization and (ii) Patch-wise cross-attention layers, allowing the model to selectively retrieve salient defect-related features without relying on computationally expensive self-attention. Extensive experiments on a real-world dataset show that ICAM is a highly efficient yet discriminative representation and well suited for real-time on-device sewer inspection applications.

This paper is organized as follows. Section~\ref{sec:related} reviews prior work on sewer defect inspection and architectures. Section~\ref{sec:method} motivates the need for efficient on-device inspection systems and introduces the proposed Instance-adaptive Cross-Attention Model (ICAM) in detail.
Section~\ref{sec:exp_settings} and ~\ref{sec:exp_results} presents the experimental settings and results. We evaluate ICAM using both public sewer inspection datasets and a newly gathered practical private dataset collected from real sewer pipelines to accurately reflect practical deployment scenarios. The proposed model consistently outperforms existing lightweight baselines while maintaining strict computational budgets suitable for edge devices.
Section~\ref{sec:exp_ablation}, we further analyze the model through ablation studies.

% Our main contributions are as follows:
% \begin{itemize}
% \item We identify the reliance on computationally heavy components in existing sewer defect inspection models and show that such complexity limits their practicality for deployment on edge platforms.

% \item We propose the Query-conditioned Cross-Attention Model (ICAM), a new architecture that integrates a memory-driven query mechanism with a shared visual encoder, enabling efficient yet highly discriminative feature extraction for sewer defect inspection.

% % \item We establish a new dataset collected from real operational sewer environments. Furthermore, we demonstrate that ICAM achieves competitive accuracy while substantially reducing FLOPs and latency, outperforming existing lightweight baselines on both public sewer inspection dataset and self-collected sewer dataset.

% \item We establish a new dataset collected from real operational sewer environments. Furthermore, we demonstrate that ICAM achieves competitive accuracy while substantially reducing FLOPs and latency, outperforming existing lightweight baselines on self-collected sewer dataset.

% \end{itemize}

\section{Related Work}
\label{sec:related}
\subsection{Sewer Defect Inspection in Civil Infrastructure}

As sewer systems age, deterioration of sewer pipes can occur, potentially leading to urban infrastructure failures such as sinkholes and flooding \citep{yang2026structural, yusuf2024deep}. Sewer defects can also cause serious public health problems, including contamination of drinking water and the spread of waterborne diseases \citep{nguyen2025sewer, myrans2019automated}. To address these issues, regular maintenance of sewer systems is essential to prevent hazardous and costly failures \citep{hassan2019underground, wang2022monitoring}.

Traditionally, human inspectors entered sewer systems and manually inspected the interior to detect defects; however, this approach is dangerous, time-consuming, and costly \citep{nguyen2025sewer}. Currently, closed-circuit television (CCTV) is commonly used to inspect the interior of sewers and to prevent structural issues \citep{wirahadikusumah1998assessment, wang2022monitoring}. In particular, with advances in robotics, manually controlled CCTV robots have been adopted to investigate sewer systems more safely \citep{seet2018design, jang2022review, kolvenbach2020towards}. On-site operators deploy the robot into the sewer pipes and control it to record the pipe interior, after which off-site inspectors review the recorded CCTV videos to detect defects \citep{yin2021automation, john2022pipe}. Nevertheless, even skilled inspectors often misclassify defect images due to human biases \citep{dirksen2013consistency, yang2024weakly, iyer2005robust}. Considering the effort and time required to review entire videos, automated sewer defect detection systems are highly desirable \citep{wang2021towards, guo2009automated}.

Recently, advances in deep learning for computer vision have led to active development of deep learning–based methods for sewer condition diagnosis \citep{nashat2025hybrid}. \citet{cheng2018automated} utilized Faster R-CNN to detect roots, cracks, infiltration, and deposits in sewer CCTV images. \citet{kumar2018automated} adopted a deep CNN to classify sewer defects such as root intrusion, deposits, and cracks. \citet{yin2020deep} used YOLOv3 as an object detector to detect sewer defects (i.e., broken pipes, holes, deposits, cracks, fractures, and roots). \citet{li2019sewer} introduced a hierarchical classification method based on ResNet-18 to address imbalanced datasets, where a high-level task detects defect images and a low-level task estimates the probabilities of each defect class. \citet{moradi2020automated} employed a support vector machine (SVM) together with maximally stable extremal regions (MSER) to detect sewer defects. \citep{xie2019automatic} proposed a two-level hierarchical deep CNN for both binary and multiclass classification, where the first level performs binary classification and the second level performs multiclass defect classification.

Research on developing sewer defect datasets has also been conducted. Sewer-ML is a benchmark sewer defect dataset consisting of 1.3 million sewer images. \citep{xie2019automatic} achieved the highest F1-score of 91.08\% on the validation set and 90.62\% on the test set, demonstrating the strong performance of the proposed binary classifier \citep{haurum2021sewer}. Although these approaches have improved the performance of defect classification, existing CNN architectures for sewer inspection typically contain a large number of parameters and therefore require substantial computational resources and inference time \citep{wang2025classification, yang2021attention}. Thus, there is a need to develop models for sewer diagnosis that achieve lower computational cost while further improving performance, making them suitable for automatic real-time diagnosis system of sewer pipelines \citep{lu2025real}.

Although these approaches have improved the performance of defect classification, existing CNN architectures for sewer inspection typically contain a large number of parameters and therefore require substantial computational resources and inference time \citep{wang2025classification, yang2021attention}. Thus, there is a need to develop models for sewer diagnosis that achieve lower computational cost while further improving performance, making them suitable for automatic real-time diagnosis system of sewer pipelines \citep{lu2025real}.

\subsection{Deep Learning and On-device Adaptation}

Regular condition assessment of civil infrastructure is essential for preventing catastrophic urban failures \citep{koch2015review}. With advances in deep learning, CNN-based methods have been widely adopted for defect inspection and monitoring, often outperforming classical image-processing pipelines such as Canny and Sobel edge detection \citep{cha2017deep}. Deep learning-based structural health monitoring (SHM) can reduce human labor and inspection cost while enabling automated systems that improve infrastructure safety \citep{zaurin2009integration, spencer2019advances}. However, deep learning inference typically demands substantial computation; offloading to the cloud can introduce prohibitive latency \citep{chen2019deep}. Consequently, edge intelligence—performing inference on edge devices immediately after data acquisition—has become increasingly important in vision-heavy domains \citep{zhou2019edge, deng2020edge}. In recent years, substantial effort has focused on deploying real-time deep learning vision modules on edge devices \citep{ananthanarayanan2017real, zhang2017live}. Yet, running deep neural networks (DNNs) on embedded platforms remains challenging due to model complexity, intensive computation, and high memory consumption \citep{han2015deep, jacob2018quantization, wang2020convergence}.

To reduce the parameter and computational footprint of DNNs, lightweight backbones tailored for mobile and edge deployment have emerged. MobileNet leverages depthwise separable convolutions to reduce computation \citep{howard2017mobilenets}. By factorizing a standard convolution into a depthwise convolution for spatial filtering and a pointwise convolution for channel mixing, MobileNet substantially reduces parameter count and floating-point operations (FLOPs). Such lightweight designs enable deployment on resource-constrained edge devices and help mitigate the computation and memory limitations of embedded platforms \citep{qiu2025trends}. For example, CR-YOLO enables real-time detection of hazardous bridge surface cracks on an NVIDIA Jetson Xavier NX and demonstrates strong speed and accuracy on both public and self-collected datasets \citep{zhang2022automated}. A YOLOv2-based model has also been deployed on an AMD Zynq-7000 SoC for rail fastener inspection, achieving 24 FPS and indic ating suitability for real-time edge deployment \citep{xiao2023real, redmon2017yolo9000}. Lite-V2, a lightweight CNN for crack detection and surface-type prediction, has been implemented on low-cost Raspberry Pi platforms and achieves an F1-score of 0.93 with only 0.28M parameters on an open-source concrete crack dataset \citep{zhang2023edge, raza2025efficient}. Collectively, these studies highlight the promise of deep learning-enabled SHM systems for automated, real-time inspection directly on edge devices, while underscoring the need to maintain reliability under evolving deployment conditions.


\subsection{Model Compression}

Beyond architectural design, model compression has been actively studied to further reduce inference cost. Unstructured pruning removes individual weights with small magnitudes, producing sparse weight matrices, but often fails to deliver practical speedups on standard GPUs due to irregular memory access patterns \citep{han2015deep}.

To mitigate this limitation, structured pruning removes entire filters—commonly based on L1-norm criteria—thereby preserving dense computation and enabling acceleration on general-purpose hardware without specialized implementations \citep{filters2016pruning}. However, norm-based criteria can be limited when small-norm filters still contribute meaningfully to performance. Filter Pruning via Geometric Median (FPGM) addresses this issue by identifying redundant filters based on geometric relationships rather than magnitude alone \citep{he2019filter}. More recently, Wanda (Pruning by Weights and Activations) introduces an activation-aware metric that estimates weight importance by combining weight magnitude with the L2-norm of input activations \citep{sun2023simple}. Although motivated by emergent activation outliers in large language models, Wanda has also shown strong performance on vision architectures such as ConvNeXt and DeiT (Data-efficient Image Transformer), outperforming standard magnitude pruning.

Complementing weight-reduction strategies, lowering numerical precision is another key pillar of model compression. Reduced-precision representations can substantially decrease memory footprint and improve throughput without changing network topology. Half-precision floating point (FP16) halves memory usage while maintaining numerical stability, and is now standard in modern high-performance inference \citep{micikevicius2017mixed}. Quantization can further reduce latency by mapping values to low-bit integers such as INT8, which is particularly beneficial for inference on edge devices \citep{jacob2018quantization}. Together, pruning and precision optimization can significantly reduce model size and inference cost, improving the feasibility of real-time on-device deployment in resource-constrained settings \citep{choudhary2020comprehensive, deng2020model}.

Despite these benefits, \citet{hooker2020compressed} show that compressed models can exhibit heightened sensitivity to common image corruptions and natural distribution shifts. On benchmarks such as ImageNet-C \citep{hendrycks2019benchmarking}, which contains algorithmically generated corruptions (e.g., noise and weather effects), both pruned and quantized networks become more brittle—an effect that can be obscured by stable aggregate accuracy. This suggests that capacity removed during compression may be important for robustness on challenging and atypical inputs. Consequently, compression can impair generalization to long-tail rare instances and noisy samples that frequently arise in real-world deployments.


\section{Methodology}
\label{sec:method}

\subsection{Revisiting Limitations of Vision Transformer}

In recent years, computer vision has advanced rapidly with the success of deep learning. Convolutional neural networks (CNNs), which learn hierarchical spatial representations \citep{lecun2002gradient}, achieved a major breakthrough in large-scale recognition with AlexNet \citep{krizhevsky2012imagenet}. More recently, the Vision Transformer (ViT) transferred the self-attention mechanism from natural language processing to vision by representing an image as a sequence of patch tokens, enabling the model to capture global relationships among patches \citep{vaswani2017attention, dosovitskiy2020image}. ViT and its variants have since demonstrated strong performance across a wide range of vision tasks \citep{han2022survey, khan2022transformers}.

Despite these successes, ViT-based models that rely on global self-attention often incur high computational cost because self-attention scales quadratically with the number of input patches: it computes attention scores for all pairs of tokens \citep{liu2021swin}.
This quadratic complexity hinders deployment in resource-constrained settings such as edge devices and embedded autonomous systems \citep{papa2024survey}. Moreover, because the cost grows with token length, ViTs are generally slower than lightweight CNNs, which can become a bottleneck for real-time inference on mobile devices \citep{li2022efficientformer}. As shown by \citet{mehta2021mobilevit}, even when scaled down, ViT models can underperform lightweight CNNs under typical mobile resource constraints, limiting their applicability in edge scenarios.





\subsection{ICAM: Instance-adaptive Cross-Attention Model}

\label{sec:icam}
To address the performance--efficiency trade-off of global self-attention in vision Transformers \citep{dosovitskiy2020image}, we propose the \textbf{Instance-adaptive Cross-Attention Model (ICAM)}, an on-device friendly architecture that eliminates encoder self-attention and interacts with the image only through a compact set of query tokens.
As illustrated in Figure~\ref{fig:algorithm} and summarized in Algorithm~\ref{alg:main}, ICAM consists of (i) a \emph{single-pass CNN patching encoder} and (ii) an \emph{instance-adaptive cross-attention decoder}.


\begin{algorithm}[t!]
\caption{ICAM inference pipeline.}
\label{alg:main}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\Require Image batch $I \in \mathbb{R}^{B \times C \times H \times W}$, learnable queries $Q_{\text{learn}} \in \mathbb{R}^{N_q \times D_{\text{enc}}}$
\Ensure Class logits $Y \in \mathbb{R}^{B \times N_{\text{cls}}}$

\Statex
\Statex \textbf{1. Patching encoder}
\State $F \leftarrow \text{CNN}(I) \in \mathbb{R}^{B \times D_{\text{enc}} \times H_f \times W_f}$ \Comment{full-frame feature map}
\State $G \leftarrow \text{AvgPool}_{H_p \times W_p}(F) \in \mathbb{R}^{B \times D_{\text{enc}} \times H_p \times W_p}$ \Comment{grid pooling}
\State $X_{\text{enc}} \leftarrow \text{LN}(\text{Flatten}(G)) \in \mathbb{R}^{B \times N \times D_{\text{enc}}}$ \Comment{$N = H_p W_p$}

\Statex
\Statex \textbf{2. Token embedding and positional encoding}
\State $X \leftarrow \text{Dropout}(X_{\text{enc}} W_{\text{emb}}) \in \mathbb{R}^{B \times N \times D}$ 
\State $P \leftarrow \text{2D-SinCosPE}() \in \mathbb{R}^{1 \times N \times D}$
 
\Statex
\Statex \textbf{3. Instance-adaptive query initialization}
\State $Q_{\text{latent}} \leftarrow Q_{\text{learn}} W_{\text{init}Q} \in \mathbb{R}^{N_q \times D}$ \Comment{$N_q$ is the number of learnable queries}
\State $K_0 \leftarrow X W_{\text{init}K} + P W_{\text{init}K} \in \mathbb{R}^{B \times N \times D}$ \Comment{keys receive 2D PE}
\State $V_0 \leftarrow X W_{\text{init}V} \in \mathbb{R}^{B \times N \times D}$ \Comment{values are content-only}
\State $A \leftarrow \text{Softmax}\left( \frac{Q_{\text{latent}} K_0^\top}{\sqrt{D}} \right) \in \mathbb{R}^{B \times N_q \times N}$
\State $Q^{(0)} \leftarrow A \cdot V_0 \in \mathbb{R}^{B \times N_q \times D}$

\Statex
\Statex \textbf{4. Cross-attention decoder (no self-attention)}
 \For{$l = 1, \dots, L$}
    \State $\tilde{Q} \leftarrow \text{LN}(Q^{(l-1)})$
    \State $Q' \leftarrow Q^{(l-1)} + \text{MHCA}(\tilde{Q}, X, X; P)$ \Comment{PE applied to keys only}
    \State $Q^{(l)} \leftarrow Q' + \text{FFN}(\text{LN}(Q'))$
 \EndFor

\Statex
\Statex \textbf{5. Classification head}
\State $z \leftarrow \text{MeanPool}(Q^{(L)}) \in \mathbb{R}^{B \times D}$
\State $h \leftarrow z W_{\text{proj}} \in \mathbb{R}^{B \times D_{\text{enc}}}$
\State $Y \leftarrow \text{MLP}_{\text{cls}}(h) \in \mathbb{R}^{B \times N_{\text{cls}}}$
\State \Return $Y$

\end{algorithmic}
\end{algorithm}


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/algorithm.pdf}
    \caption{Overview of ICAM. A lightweight CNN extracts a full-frame feature map once, and adaptive grid pooling produces $N$ patch tokens. Learnable queries are initialized via instance-adaptive cross-attention over the sequence of patch tokens, then refined by $L$ cross-attention layers (without self-attention). Finally, mean-pooled query features are fed to an MLP head for classification.}
    \label{fig:algorithm}
\end{figure}



Vision Transformers (ViT) represent an image as a sequence of patch embeddings processed by a Transformer encoder \citep{dosovitskiy2020image}. However, global self-attention over all tokens scales quadratically with sequence length, motivating architectures that interact with the input through a smaller set of latent or query tokens \citep{jaegle2021perceiver}.

Perceiver addresses this cost through asymmetric cross-attention that projects the input token array into a fixed-size latent bottleneck: queries are produced from a learned latent array, while keys and values are derived from the input tokens \citep{jaegle2021perceiver}. The resulting latent representations are subsequently refined via self-attention in latent space \citep{jaegle2021perceiver}. DETR similarly employs a fixed set of learned ``object queries'' that are iteratively updated in the decoder through decoder self-attention and encoder--decoder cross-attention to image features \citep{carion2020end}.

In both Perceiver and DETR, the latent or query embeddings are global learned parameters shared across samples and become instance-specific only through subsequent attention updates \citep{carion2020end,jaegle2021perceiver}. As a consequence, early decoder layers must simultaneously perform instance conditioning and representation refinement, placing a substantial burden on iterative decoding. Recent work on detection transformers has explicitly highlighted this inefficiency, demonstrating that initializing queries with dense priors \citep{yao2021efficient} or conditioning them on spatial context \citep{meng2021conditional} significantly accelerates convergence by narrowing the search space and reducing the need for prolonged refinement.
The burden of iterative refinement becomes particularly pronounced in resource-constrained scenarios where decoder depth and parameter budgets are severely restricted. Empirical studies have shown that reducing decoder depth while relying on static query initialization leads to a sharp performance degradation, as shallow decoders lack sufficient capacity to correct suboptimal initial query states through iterative refinement alone \citep{yao2021efficient}.
Consequently, explicitly conditioning queries on the input prior to decoding is not merely an optimization but a critical requirement for compact architectures, enabling effective instance-specific reasoning despite limited model depth.

Motivated by this observation, we introduce an instance-adaptive query initialization strategy.
Instead of relying on static embeddings, ICAM generates initial decoder queries $Q^{(0)}$ by conditioning a set of learnable seed queries on the input image features.
Specifically, we first extract a feature map via a single-pass CNN encoder and convert it into patch tokens using adaptive grid pooling.
The seed queries then attend to these patch tokens via cross-attention to produce input-conditioned states.
This is followed by fixed sine--cosine positional encodings \citep{carion2020end}.
Inspired by the attention score decomposition in Transformer-XL \citep{dai2019transformer}, we exploit linearity by adding the positional contribution in projection space: $W_K(X + P) = W_K(X) + W_K(P)$.
This allows caching $W_K(P)$ per layer during inference, avoiding repeated positional projections and preventing materialization of $(X{+}P)$ tensors.
This process ensures that the decoder begins with an informative, instance-aware representation rather than a generic prior, allowing for faster convergence.

Starting from the instance-adaptive query state, the decoder progressively refines task-relevant representations for classification via patch-wise cross-attention.
The decoder comprises $L$ identical layers, each consisting of a Multi-Head Cross-Attention (MHCA) block followed by a Feed-Forward Network (FFN), without any query self-attention.
Each sub-layer uses residual connections and Layer Normalization.
Unlike the initialization step, each MHCA layer has its own learnable projection matrices for queries, keys, and values. The FFN is implemented as an MLP with a ReLU6 activation.
The output of the final decoder layer is aggregated via mean pooling and projected, then fed to a lightweight MLP classifier to produce class logits.

Crucially, our architecture diverges from standard designs by eliminating self-attention layers entirely.
Unlike Perceiver’s latent-space self-attention or DETR’s decoder self-attention, all token interactions in ICAM are mediated exclusively by cross-attention between the small set of queries ($N_q \ll N$) and the encoder tokens.
This design reduces the computational complexity from quadratic $O(N^2)$ to linear $O(NN_q)$.

\begin{table}[b]
    \centering
    \caption{Statistics of the Sewer-ML and Private datasets used in this study.}
    \label{tab:dataset_stats}
    \resizebox{0.8\columnwidth}{!}{%
    \begin{tabular}{lrrrrrr}
        \toprule
        & \multicolumn{3}{c}{\textbf{Sewer-ML}} & \multicolumn{3}{c}{\textbf{Private Dataset}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        \textbf{Class} & \textbf{Training} & \textbf{Test} & \textbf{Total} & \textbf{Training} & \textbf{Test} & \textbf{Total} \\
        \midrule
        Normal   & 552,820 & 68,681 & 621,501 & 700 & 177 & 877 \\
        Abnormal & 487,309 & 61,365 & 548,674 & 653 & 162 & 815 \\
        \cmidrule(lr){1-7}
        \textbf{Total} & \textbf{1,040,129} & \textbf{130,046} & \textbf{1,170,175} & \textbf{1,353} & \textbf{339} & \textbf{1,692} \\
        \bottomrule
    \end{tabular}}
\end{table}

\section{Experimental Settings}
\label{sec:exp_settings}

We conducted experiments to validate the effectiveness and efficiency of the proposed method.
Section~\ref{sec:exp_datasets} introduces the public and private sewer datasets used in this study.
Section~\ref{sec:exp_baselines} introduces the baseline methods and
Section~\ref{sec:exp_hyperparams} describes the training hyperparameters and implementation details.
Section~\ref{sec:exp_metrics} presents the evaluation metrics and comparison strategy used to assess both performance and efficiency.

\subsection{Dataset}
\label{sec:exp_datasets}
We evaluate the proposed ICAM on both a large-scale public benchmark and a real-world private dataset, assessing performance against established baselines and robustness under operational conditions.

\paragraph{Public Dataset}
We use Sewer-ML \citep{haurum2021sewer} as the public benchmark. Sewer-ML contains approximately 1.3 million sewer images annotated with defect codes by certified sewer inspectors from three companies over nine years. The original video resolutions vary substantially (e.g., $352 \times 288$ and $720 \times 576$). Figure~\ref{fig:sewer-ml_samples} shows representative normal and abnormal examples. The dataset covers 18 defect types, and a single image may have multiple defect codes. For our experiments, we formulate a binary classification task: an image is labeled \textit{Abnormal} if it contains at least one defect code; otherwise, it is labeled \textit{Normal}. The resulting training and test splits after binarization are summarized in Table~\ref{tab:dataset_stats}.

\begin{figure}[t!]
    \centering
    \subfloat[Normal\label{fig:img1}]{
        \includegraphics[width=0.23\linewidth]{figures/00603609.pdf}
    }
    \hfill
    \subfloat[\centering Abnormal \\ (OB, FS, AF)\label{fig:img2}]{
        \includegraphics[width=0.23\linewidth]{figures/00507651.pdf}
    }
    \hfill
    \subfloat[Abnormal \\ (AF)\label{fig:img3}]{
        \includegraphics[width=0.23\linewidth]{figures/01039052.pdf}
    }
    \hfill
    \subfloat[Abnormal \\ (FS, BE)\label{fig:img4}]{
        \includegraphics[width=0.23\linewidth]{figures/00482857.pdf}
    }
    \caption{Samples from the Sewer-ML dataset.
    (a) A normal sewer pipe.
    (b)--(d) Abnormal sewer pipes containing defects with specific defect codes
    (e.g., OB: surface damage, FS: displaced joint, AF: settled deposits, BE: attached deposits).}
    \label{fig:sewer-ml_samples}
\end{figure}



\begin{figure}[t!]
    \centering
    % --- (a) Normal ---
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/12-1358.3-A.mp4_20190321144656.pdf}
        \caption{Normal}
        \label{fig:normal_private}
    \end{subfigure}
    \hfill
    % --- (b) Abnormal (DS) ---
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/62830.mp4_20250624093311_DS_.pdf}
        \caption{Abnormal (DS)}
        \label{fig:abnormal_ds}
    \end{subfigure}
    \hfill
    % --- (c) Abnormal (BK) ---
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        % BK 이미지 경로를 넣어주세요
        \includegraphics[width=\linewidth]{figures/12-08193-B.mp4_20190321190238_BK.pdf} 
        \caption{Abnormal (BK)}
        \label{fig:abnormal_bk}
    \end{subfigure}
    
    \caption{Samples from the private sewer dataset. (a) A normal sewer pipe. (b) An abnormal sewer image showing Deposits/Silty (DS), defined as an accumulation of soil, gravel, or silt within the pipe, often caused by the inflow of external fill material due to pipe breakage. (c) An abnormal sewer image showing Breakage (BK), defined as a structural defect where the pipe wall is fractured or pieces are missing, often caused by external pressure or ground movement.}
    \label{fig:private_dataset_samples}
\end{figure}

\paragraph{Private Dataset}
We also collected a private sewer inspection dataset from operational pipelines to evaluate ICAM under deployment-like conditions. The raw data consist of continuous CCTV inspection videos captured by a robotic crawler camera traversing sewer pipes. Data collection was conducted from August to September 2025 in collaboration with an industry partner. From 137 videos, multiple annotators extracted approximately 1{,}500 axial (front-view) still frames. For traceability, each extracted image is linked to its source video identifier at the capture-event level.
As shown in Figure~\ref{fig:private_dataset_samples}, the extracted frames predominantly present an axial viewpoint (i.e., the circular pipe cross-section is centered) and are annotated as \textit{Normal} or \textit{Abnormal}. We target roughly 900 normal and 600 abnormal frames to maintain a manageable class balance. To mitigate sampling bias and improve generalization, we do not restrict \textit{Normal} frames to visually pristine pipes: frames exhibiting typical aging patterns (e.g., discoloration or mild contamination) are labeled as normal as long as no reportable defects are present. The \textit{Abnormal} class follows a standardized defect taxonomy aligned with the Korean national standard for sewer conduit/manhole inspection and condition rating (Mar.\ 2025 revision provided to annotators), which defines 24 defect types (e.g., joint damage/displacement, adverse slope, collapse, and lining-related defects). When feasible, we aim to cover diverse defect types with an approximately balanced distribution across categories.
Annotators followed a consistent frame-extraction protocol that mirrors the inspection and reporting workflow used by practicing engineers. Specifically, they first identify visually normal segments or clear defect cases in full-length videos and then capture representative frames in which the target condition is centered and unambiguous, prioritizing morphologically informative appearances over fleeting or ambiguous scenes.
To reduce label noise, we apply quality-control procedures, including viewpoint auditing to remove non-axial frames and filename/metadata validation to ensure consistency. All images are stored at a resolution of $1280 \times 720$ to preserve visual details, and frames with substantial camera rotation that induce lateral/side views are excluded to reduce viewpoint-induced domain shift. We split the dataset into training and test sets using an 8:2 ratio; detailed statistics are provided in Table~\ref{tab:dataset_stats}.




\subsection{Baselines}
\label{sec:exp_baselines}
This section details the baselines used to evaluate ICAM.
To provide a comprehensive evaluation, we benchmark ICAM against a diverse set of baselines representing distinct architectural paradigms.
We select representative model from each category to assess performance against established standards across different design strategies.

\paragraph{Edge-efficient CNNs}
We employ MobileNetV4-S as a Edge-efficient CNN baseline, engineered for universal efficiency across mobile backends.
MNv4-S utilizes the Universal Inverted Bottleneck (UIB) block, which enhances architectural flexibility by integrating optional depthwise convolutions to balance spatial and channel mixing. Optimized through a hardware-aware Neural Architecture Search (NAS), it is specifically designed to alleviate memory and computational bottlenecks on modern mobile accelerators. As MNv4-S achieves a superior latency-accuracy Pareto front compared to other state-of-the-art efficient models, such as MobileOne and FastViT \citep{vasu2023mobileone, vasu2023fastvit, qin2024mobilenetv4}, we include it as a robust and representative CNN baseline for edge-based deployment.

\paragraph{Lightweight ViT}
To evaluate ICAM against purely attention-based architectures, we include DeiT-Tiny, a lightweight variant of the Vision Transformer specifically optimized for data efficiency \citep{touvron2021training}. While standard ViTs often depend on massive external datasets, DeiT-Tiny achieves high performance by training exclusively on ImageNet-1K through advanced augmentation and regularization. The architecture follows the vanilla ViT design but is significantly scaled down in embedding dimensions and attention heads to maintain a minimal parameter footprint suitable for resource-constrained environments. In our evaluation, we use the non-distilled version to assess ICAM against the fundamental capabilities of pure self-attention mechanisms.

\paragraph{Hybrid CNN--Transformer}
MobileViT is a lightweight, mobile-friendly architecture that conceptualizes transformers as a convolutional-like operation to bridge the gap between CNNs and ViTs. By introducing the MobileViT block, the model replaces local processing in standard convolutions with global processing via transformers, enabling it to learn non-local representations while retaining the spatial inductive biases inherent in CNNs \citep{mehta2021mobilevit}. We adopt MobileViT-XXS, the most compact configuration, to evaluate ICAM against a high-efficiency hybrid baseline that excels in encoding global context within a shallow and narrow network structure suitable for mobile devices.

\paragraph{Domain-specific model}
\citet{xie2019automatic} propose a hierarchical CNN framework specifically tailored for sewer defect classification. 
The architecture consists of a two-stage hierarchy: a binary classification network to detect the presence of any defect, followed by a multi-label classification network for specific defect identification. 
On the binary classification task of the Sewer-ML benchmark \citep{haurum2021sewer}, this method's first-stage network achieves state-of-the-art performance, significantly outperforming other domain-specific architectures such as those by \citet{chen2018intelligent}, \citet{hassan2019underground}, and \citet{myrans2019automated} in terms of F1-normal. 
Since ICAM is designed as a binary classifier for defect detection, we specifically utilize the binary classification component of \citet{xie2019automatic} as a domain-specialized baseline to assess practical deployment capability and diagnostic accuracy.




\subsection{Hyperparameters}
\label{sec:exp_hyperparams}
For model configuration, we use a compact cross-attention decoder with two layers and a single learnable query. Each input image is partitioned into non-overlapping patches using patch size $p=56$ and stride $s=56$. Each patch is then processed by a lightweight CNN feature extractor whose weights are shared across patches to reduce parameters. The extractor is derived from the early stages of EfficientNet-B0 \citep{tan2019efficientnet} and relies on Mobile Inverted Bottleneck Convolution (MBConv) blocks, which reduce computation while preserving representational capacity via depthwise separable convolutions and inverted residual connections \citep{sandler2018mobilenetv2}.

As shown in Table~\ref{tab:param_breakdown}, the model has only 47.59K trainable parameters.

\begin{table}[t]
    \centering
    \caption{Detailed breakdown of trainable parameters in our proposed architecture.}
    \label{tab:param_breakdown}

    \resizebox{0.6\columnwidth}{!}{%
        \begin{tabular}{lr}
            \toprule
            \textbf{Component} & \textbf{Parameters} \\
            \midrule
            \addlinespace[0.5em] 
            
            \textbf{Visual Encoder} & \textbf{15,840} \\
            \quad Single-pass Conv & 15,792 \\
            \quad LayerNorm & 48 \\
            \addlinespace 
            
            \textbf{Decoder (Cross-Attention-based)} & \textbf{31,392} \\
            \quad Embedding Layer ($W_{\text{emb}}$) & 600 \\
            \quad Init Query Proj ($W_{\text{init}Q}$) & 600 \\
            \quad Init Key Proj ($W_{\text{init}K}$) & 600 \\
            \quad Init Value Proj ($W_{\text{init}V}$) & 600 \\
            \quad Learnable Queries ($N_q=1$) & 24 \\
            \quad Decoder Layers (Cross-Attention) & 28,368 \\
            \quad Output Proj & 600 \\
            \addlinespace 
            
            \textbf{Classification Head} & \textbf{353} \\
            
            \addlinespace[0.5em] 
            \midrule 
            \textbf{Total Model Parameters} & \textbf{47,585} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}



For training, we follow the preprocessing protocol of Sewer-ML and train ICAM and all baselines from scratch for 90 epochs, consistent with the training schedule of Sewer-ML \citep{haurum2021sewer}. To account for differences in dataset scale, we use a batch size of 256 for Sewer-ML and 16 for the private dataset to stabilize optimization under limited data. We minimize the cross-entropy loss using AdamW with an initial learning rate of $1 \times 10^{-4}$ and weight decay of $1 \times 10^{-2}$, together with a cosine annealing learning-rate schedule. To mitigate overconfident predictions on ambiguous or visually subtle defects, we apply label smoothing \citep{szegedy2016rethinking, muller2019does}. Training is performed on a single NVIDIA RTX 5090 GPU.





\subsection{Metrics and Comparison Strategy}
\label{sec:exp_metrics}
This section describes the evaluation metrics and protocols used to assess the feasibility of deploying models in resource-constrained environments. We analyze both model complexity (FLOPs, parameters, model size) and runtime performance (latency, memory usage) against baseline models.

\paragraph{Experimental Setup}
To reflect a practical deployment pipeline, we convert all trained PyTorch checkpoints to ONNX format (opset version 17) and conduct measurements using ONNX Runtime (v1.23.2). All experiments are performed on a \textbf{Raspberry Pi 5} (4GB RAM), which features a Broadcom BCM2712 SoC with a quad-core ARM Cortex-A76 CPU and LPDDR4X memory. This platform provides a realistic testbed under computational and power constraints widely encountered in industrial edge computing and robotics, in contrast to proxy efficiency metrics collected on server-class GPUs \citep{mathe2024comprehensive}.

To ensure reproducibility on an embedded platform, we enforce strict system-level controls.
Specifically, the CPU frequency governor is fixed to \texttt{performance} to minimize run-to-run fluctuations caused by dynamic voltage and frequency scaling (DVFS).
The benchmark process is pinned to a fixed CPU set (cores 0--3) using CPU affinity to reduce scheduler-induced jitter and cache effects arising from core migration.
We additionally fix ONNX Runtime threading and execution behavior using \texttt{intra\_op\_num\_threads}=4, \texttt{inter\_op\_num\_threads}=1, and \texttt{ORT\_SEQUENTIAL} execution mode, thereby keeping the parallelism structure consistent across runs and limiting variability due to inter-operator parallel scheduling.
To prevent thread oversubscription from external math libraries that can introduce uncontrolled nested parallelism, we explicitly set \texttt{OMP\_NUM\_THREADS}=1 and \texttt{OPENBLAS\_NUM\_THREADS}=1 for all latency and memory experiments.
For robustness, each benchmark is repeated five times as independent executions using fresh processes.
These controls collectively reduce confounding factors from DVFS, OS scheduling, and nested threading, enabling fair and stable comparisons across models on Raspberry Pi 5.



\paragraph{Efficiency Metrics}
We measure inference latency and memory usage using a strictly defined protocol. The process consists of a warm-up phase (50 runs) followed by a measurement phase (1{,}000 runs) using randomly generated input with a single-sample batch size ($B=1$) and a fixed seed (42).
All values are reported as Mean $\pm$ Std across 5 independent runs (fresh processes).

For \textbf{latency}, we exclude the warm-up runs to eliminate initialization effects such as JIT compilation, cache warming, and dynamic frequency scaling, following the evaluation protocols established by the MLPerf Inference benchmark \citep{reddi2020mlperf}. We measure the elapsed time using \texttt{time.perf\_counter\_ns()}. To capture both average performance and runtime stability (tail latency), we report the mean, P50, P90, P95, and P99 percentiles.

For \textbf{memory usage}, we report the Peak Resident Set Size (RSS) to evaluate the actual physical memory footprint during inference. Captured via the standard GNU time utility (\texttt{/usr/bin/time -v}), this metric represents the maximum physical memory allocated by the process. This includes persistent memory for weights and the non-persistent arena for activation maps, consistent with memory management architectures for TinyML systems \citep{david2021tensorflow}. Additionally, we estimate the interval RSS by polling \texttt{proc.memory\_info().rss} at 1~ms intervals. This high-frequency monitoring isolates memory overheads during model loading from transient peaks during inference, which is essential for assessing system multitenancy and operational limits on edge devices \citep{david2021tensorflow}.

\paragraph{Performance Metrics}
We report Top-1 accuracy over all classes to ensure a comprehensive evaluation of global performance. However, our primary focus lies on the metrics for the \textbf{abnormal class}: precision, recall, and F1-score.
Following \citet{haurum2021sewer}, we prioritize identifying defects because false negatives incur significantly higher economic risks than false positives.
While this necessitates high recall, relying solely on recall is insufficient; frequent false alarms (low precision) can reduce system reliability and increase manual verification costs \citep{chandola2009anomaly}. Therefore, we report precision and F1-score alongside recall to provide a balanced assessment of anomaly detection capability.

We also assess robustness under varying \textbf{numerical precision}, a critical factor for deployment on NPU/DSP accelerators \citep{sze2017efficient}.
To this end, we evaluate the performance of ICAM and baselines across three standard inference precision levels: FP32, FP16, and INT8.
For FP16, we convert the trained FP32 ONNX graph to IEEE~754 half precision.
For static INT8, we adopt the integer-only inference formulation of \citet{jacob2018quantization}, quantizing weights to signed 8-bit integers (QInt8) and activations to unsigned 8-bit integers (QUInt8).
We export in Quantize--DeQuantize (QDQ) format with per-channel weight quantization, which has been shown to better preserve accuracy even after batch-normalization folding \citep{wu2020integer}.
Quantization parameters (scale and zero-point) are estimated using a calibration set of 512 randomly selected training images, consistent with the NVIDIA TensorRT developer guide, which suggests that approximately 500 samples are sufficient even for large-scale benchmarks like ImageNet.
We use percentile-based calibration at the 99.99th percentile, which sets clipping thresholds by discarding extreme activation values beyond this percentile, mitigating the impact of outliers on the quantization scale while preserving resolution for the bulk of the activation distribution \citep{li2019fully}.

\paragraph{Comparison Strategy}
We benchmark ICAM against the diverse baseline architectures introduced in Section~\ref{sec:exp_baselines}. Given that these models differ substantially in their native resource requirements, direct comparison can be inequitable. To ensure fair comparisons across architectures, we apply a unified pruning protocol to all baselines under two budget settings.

In the computational-budget setting, each baseline is pruned to match the computational cost of ICAM, measured in floating-point operations (FLOPs). FLOPs are a common proxy for efficiency in lightweight models and often correlate with inference latency and energy consumption on resource-constrained devices \citep{howard2017mobilenets}. As a result, benchmarking under a fixed computational budget is standard practice for evaluating efficient architectures \citep{sandler2018mobilenetv2,tan2019efficientnet,zhang2018shufflenet}. Following this convention, we constrain all baselines to the same FLOPs level as ICAM ($\approx$ 0.18\,GFLOPs) to compare representational efficiency under an identical arithmetic budget. We compute FLOPs as $2\times$ the number of multiply--accumulate operations (MACs) to adhere to standard conventions and avoid ambiguity regarding MAC counts.

However, FLOPs alone do not fully characterize deployability because they ignore static hardware constraints on edge devices, where parameter count can be the primary determinant of whether a model fits in on-chip memory \citep{iandola2016squeezenet}. Moreover, memory access can consume orders of magnitude more energy than arithmetic computation \citep{han2015deep}, and the bandwidth required to fetch parameters may dominate runtime and energy on battery-powered platforms \citep{chen2016eyeriss}.
Accordingly, we additionally evaluate models under a fixed parameter budget, scaling each baseline to match ICAM ($\approx$ 47.59K parameters). This approach follows prior comparative evaluations that control for parameter count to isolate gains from architectural design rather than increased capacity \citep{howard2017mobilenets}. Parameter matching enforces an equivalent static memory footprint and better reflects practical deployment constraints on resource-limited devices \citep{iandola2016squeezenet,howard2017mobilenets}.

To enforce these FLOPs and parameter constraints across diverse architectures, we employ a unified pruning procedure based on \texttt{torch-pruning}.
This library utilizes the Dependency Graph (DepGraph) algorithm \citep{fang2023depgraph} to maintain dimensional consistency across coupled layers, such as residual connections.
We evaluate three distinct pruning criteria, categorized into data-free and data-driven approaches. For the data-free category, we utilize L1-norm pruning and Filter Pruning via Geometric Median (FPGM). L1-norm pruning ranks filters based on the sum of absolute weight values, operating under the assumption that small-magnitude filters are less significant \citep{filters2016pruning}. In contrast, FPGM identifies redundancy by computing the geometric median of filters within a layer; filters closest to this median are considered replaceable and subsequently pruned \citep{he2019filter}.
To complement these heuristics, we also adopt Wanda \citep{sun2023simple}, a data-driven method that estimates importance by combining weight magnitudes with the L2-norm of input activations. Unlike the aforementioned methods, Wanda requires a calibration set to capture activation statistics.
For the large-scale Sewer-ML dataset (1{,}040{,}129 training images), we use 4{,}096 calibration samples randomly drawn from the training set, following \citet{sun2023simple}, who observed that this size was sufficient for the pruning metric to converge on ImageNet-1K.
For our smaller private dataset (1{,}353 training images), we utilize the full training set for calibration.
Since Wanda is unsupervised, this approach avoids label-driven overfitting while minimizing sampling bias and ensuring stable statistics on limited data.

Concretely, we first train each baseline for 10 epochs out of a 90-epoch schedule to estimate importance scores. We then perform a binary search up to 100 iterations over the global sparsity ratio to meet the target FLOPs or parameter budget, prune the model accordingly, and train the pruned model for the remaining 80 epochs.
This early-pruning protocol aligns with the Lottery Ticket Hypothesis \citep{frankle2018lottery} and leverages the finding of \citet{you2019drawing} that utilizing only 6.25\%--12.5\% of the training schedule is sufficient to identify high-quality subnetworks.
While DepGraph automatically groups coupled layers to preserve structural integrity, we explicitly exclude the final classification layer and, for transformer-based models, the query--key--value (QKV) projection layers to further ensure training stability.
Overall, this pruning protocol provides a standardized basis for comparing ICAM with all baselines under matched resource constraints.



\section{Experimental Results}
\label{sec:exp_results}
In this section, we validate the practical feasibility of ICAM by comparing it with baselines under standardized resource constraints.






\begin{table}[h!]
    \centering
    \caption{Performance comparison of unpruned models. FLOPs are reported assuming $2 \times$ FLOPs per MAC. Size denotes the physical storage requirements of the converted ONNX file.}
    \label{tab:original_spec}
    \resizebox{0.8\columnwidth}{!}{%
        \begin{tabular}{l c r r}
            \toprule
            \textbf{Model} & \textbf{FLOPs (G)} & \textbf{Params (\#)} & \textbf{Size (MB)} \\
            \midrule
            MobileNetV4-S \citep{qin2024mobilenetv4}    & 0.3853 & 2,495,586 & 9.49 \\
            Xie et al. \citep{xie2019automatic}         & 2.8696 & 9,160,194 & 34.95 \\
            DeiT-Tiny \citep{touvron2021training}       & 2.1493 & 5,524,802 & 21.18 \\
            MobileViT-XXS \citep{mehta2021mobilevit}    & 0.5384 & 951,666 & 3.80 \\
            ICAM (Ours) & \textbf{0.1816} & \textbf{47,585} & \textbf{0.30} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}



\begin{table*}[h!]
\centering
\caption{Performance comparison across different precisions (FP32, FP16, INT8). Models are evaluated on Sewer-ML and Private datasets. Note that Precision, Recall and F1-score are reported with respect to the \textbf{Abnormal} class.}
\label{tab:precision_based}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccccccc}
\toprule
\multirow{2}{*}[-2.5pt]{\textbf{Precision}} & \multirow{2}{*}[-2.5pt]{\textbf{Model}} & \multicolumn{4}{c}{\textbf{Sewer-ML}} & \multicolumn{4}{c}{\textbf{Private Dataset}} \\
\cmidrule(lr){3-6} \cmidrule(lr){7-10}
 & & \textbf{Acc. (\%)} & \textbf{Pre. (\%)} & \textbf{Rec. (\%)} & \textbf{F1 (\%)} & \textbf{Acc. (\%)} & \textbf{Pre. (\%)} & \textbf{Rec. (\%)} & \textbf{F1 (\%)} \\
\midrule
% --- FP32 Block ---
\multirow{5}{*}{FP32}
 & MobileNetV4-S \citep{qin2024mobilenetv4} & 90.53& 89.49& 90.56& 90.02& 84.66& 81.44& 86.62& 83.95\\
 & Xie et al. \citep{xie2019automatic} & 89.49& 88.54& 89.30& 88.92& 81.42& 77.65& 84.08& 80.73\\
 & DeiT-Tiny \citep{touvron2021training} & 89.08& 88.40& 88.47& 88.43& 80.83& 76.74& 84.08& 80.24\\
 & MobileViT-XXS \citep{mehta2021mobilevit} & 91.16& 90.72& 90.53& 90.62& 82.60& 81.41& 80.89& 81.15\\
 & Ours & 91.30& 89.79& 92.02& 90.89& 87.02& 84.66& 87.90& 86.25\\
\midrule
% --- FP16 Block ---
\multirow{5}{*}{FP16} 
 & MobileNetV4-S \citep{qin2024mobilenetv4} & 90.53& 89.50& 90.55& 90.02& 84.66& 81.44& 86.62& 83.95\\
 & Xie et al. \citep{xie2019automatic} & 89.50& 88.55& 89.29& 88.92& 81.42& 77.65& 84.08& 80.73\\
 & DeiT-Tiny \citep{touvron2021training} & 89.08& 88.40& 88.47& 88.44& 80.83& 76.74& 84.08& 80.24\\
 & MobileViT-XXS \citep{mehta2021mobilevit} & 91.16& 90.72& 90.54& 90.63& 82.01& 81.17& 79.62& 80.39\\
 & Ours & 91.30& 89.78& 92.03& 90.89& 87.32& 85.19& 87.90& 86.52\\
\midrule
% --- INT8 Block ---
\multirow{5}{*}{INT8} 
 & MobileNetV4-S \citep{qin2024mobilenetv4} & 83.20& 96.55& 66.79& 78.96& 84.96& 81.93& 86.62& 84.21\\
 & Xie et al. \citep{xie2019automatic} & 89.47& 88.92& 88.74& 88.83s& 81.42& 77.65& 84.08& 80.73\\
 & DeiT-Tiny \citep{touvron2021training} & 85.82& 81.52& 90.44& 85.75& 78.76& 72.73& 86.62& 79.09\\
 & MobileViT-XXS \citep{mehta2021mobilevit} & 90.41& 91.85& 87.42& 89.58& 82.89& 82.35& 80.25& 81.29\\
 & Ours & 91.24& 90.28& 91.27& 90.77& 87.02& 84.66& 87.90& 86.25\\
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[t!]
    \centering
    \caption{Inference latency and Peak Resident Set Size (RSS) performance analysis. 
    A warm-up phase was executed to stabilize the runtime state prior to measurement.
    Latency reports the average inference execution time of the \texttt{sess.run()} call, strictly excluding model loading and warm-up overheads.
    Peak RSS captures the maximum physical memory footprint of the entire process lifetime via the OS command \texttt{/usr/bin/time -v}, encompassing interpreter startup, model loading, warm-up, and inference.}
    \label{tab:original_efficiency}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l r@{ $\pm$ }l r@{ $\pm$ }l r@{ $\pm$ }l r@{ $\pm$ }l r@{ $\pm$ }l}
        \toprule
        \multirow{2}{*}[-2.5pt]{\textbf{Model}} & \multicolumn{8}{c}{\textbf{Latency (ms)}} & \multicolumn{2}{c}{\textbf{RSS (MB)}} \\
        \cmidrule(lr){2-9} \cmidrule(lr){10-11}
         & \multicolumn{2}{c}{\textbf{Mean}} & \multicolumn{2}{c}{\textbf{P50}} & \multicolumn{2}{c}{\textbf{P95}} & \multicolumn{2}{c}{\textbf{P99}} & \multicolumn{2}{c}{\textbf{Peak}} \\
        \midrule
        MobileNetV4-S \citep{qin2024mobilenetv4}   &  7.73& 0.03&  7.68& 0.01&  \textbf{7.92}& \textbf{0.11}& \textbf{9.05}& \textbf{0.40}& 74.79& 0.18\\
        Xie et al. \citep{xie2019automatic}        & 33.92 & 0.16 & 33.71 & 0.12 & 34.97 & 0.25 & 40.84 & 10.02 & 123.18 & 0.45 \\
        DeiT-Tiny \citep{touvron2021training}       & 33.12 & 0.17 & 32.93 & 0.20 & 34.41 & 0.15 & 35.71 & 0.16 & 91.24 & 2.40 \\
        MobileViT-XXS \citep{mehta2021mobilevit}   & 22.20 & 0.11 & 22.05 & 0.06 & 22.89 & 0.29 & 24.50 & 0.67 & 78.37 & 0.01 \\
        ICAM (Ours)  &  \textbf{7.68}& \textbf{0.02}&  \textbf{7.60}& \textbf{0.03}& 7.95& 0.09& 9.22& 0.55& \textbf{71.56}& \textbf{0.23}\\
        \bottomrule
    \end{tabular}
    }
\end{table*}



\begin{table*}[t!]
    \centering
    \caption{Comprehensive memory usage analysis comparing scenarios with and without warm-up. 
            Values represent RSS deltas ($\Delta$) in MB measured via \texttt{psutil} sampling at 1 ms intervals.
            Load $\Delta$ measures the memory increase during model initialization.
            Inference $\Delta$ is measured relative to the baseline before warm-up (post-load) for \textbf{w/ Warm-up}, and after warm-up (steady-state) for \textbf{w/o Warm-up}.
            Total $\Delta$ represents the peak total memory footprint increase from the pre-load state.}
    \label{tab:original_memory}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l r@{ $\pm$ }l r@{ $\pm$ }l r@{ $\pm$ }l r@{ $\pm$ }l r@{ $\pm$ }l r@{ $\pm$ }l}
        \toprule
        \multirow{2}{*}[-2.5pt]{\textbf{Model}} & \multicolumn{6}{c}{\textbf{w/ Warm-up ($\Delta$ MB)}} & \multicolumn{6}{c}{\textbf{w/o Warm-up ($\Delta$ MB)}} \\
        \cmidrule(lr){2-7} \cmidrule(lr){8-13}
         & \multicolumn{2}{c}{\textbf{Load}} & \multicolumn{2}{c}{\textbf{Infer.}} & \multicolumn{2}{c}{\textbf{Total}} & \multicolumn{2}{c}{\textbf{Load}} & \multicolumn{2}{c}{\textbf{Infer.}} & \multicolumn{2}{c}{\textbf{Total}} \\
        \midrule
        MobileNetV4-S \citep{qin2024mobilenetv4}   & 15.31 & 0.00 &  \textbf{0.42} & \textbf{0.01} & 15.73 & 0.01 & 15.31 & 0.00 & 0.02 & 0.00 & 15.73 & 0.00 \\
        Xie et al. \citep{xie2019automatic}        & 38.30 & 0.00 & 28.20 & 0.01 & 66.50 & 0.01 & 38.30 & 0.00 & 0.02 & 0.00 & 66.50 & 0.01 \\
        DeiT-Tiny \citep{touvron2021training}       & 24.84 & 1.13 &  2.28 & 0.01 & 27.12 & 1.13 & 26.67 & 1.46 & \textbf{0.01} & \textbf{0.01} & 29.12 & 1.69 \\
        MobileViT-XXS \citep{mehta2021mobilevit}   &  6.82 & 0.44 & 12.04 & 0.50 & 18.86 & 0.11 &  6.63 & 0.35 & 0.02 & 0.00 & 18.86 & 0.10 \\
        ICAM (Ours) &  \textbf{2.42}& \textbf{0.01}& 12.00& 0.01& \textbf{14.41}& \textbf{0.01}& \textbf{2.42}& \textbf{0.02}& 0.02 &0.01 & \textbf{14.41}& \textbf{0.01} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}




% Near-zero inference peak deltas indicate stable steady-state inference with memory reuse and no additional dynamic allocation.












% \paragraph{Performance of Unpruned Models}

% Table~\ref{tab:original_pi5} compares the resource utilization of ICAM with baselines on the Raspberry Pi 5.
% The most distinctive advantage of ICAM is its extremely small number of parameters.
% It operates with only 0.03M parameters and 0.18 GFLOPs, corresponding to an 83$\times$ reduction in parameter count compared to MobileNetV4-S.
% This results in an ultra-compact storage footprint of 0.19~MB in ONNX format, enabling ICAM to be deployed on edge devices with severely limited storage.









% Table~\ref{tab:sewer_ml_original} shows the performance comparison of unpruned models on the Sewer-ML dataset.

% % 여기 작성 필요

% Table~\ref{tab:private_original} presents the performance comparison of unpruned models on the Private dataset.
% Our proposed ICAM outperforms all baselines across all metrics, achieving an accuracy of 88.20\% and an F1-score of 87.42\%.
% Notably, compared to MobileNetV4-S, which serves as a highly competitive lightweight CNN, ICAM achieves a substantial improvement of 3.11 percentage points in precision while maintaining the highest recall rate of 88.54\%.
% This indicates that our model effectively minimizes false positives (i.e., cases predicted as abnormal but actually normal) without missing actual defects.
% Furthermore, while the transformer-based DeiT-Tiny demonstrates a high recall comparable to MobileNetV4-S, its performance is constrained by a significantly low precision of 77.25\%.
% ICAM also outperforms the domain-specific baseline Xie et al. \citep{xie2019automatic} by a margin of 7.42 percentage points in F1-score.











% We also assess practical deployability on hardware accelerators that rely on reduced-precision arithmetic.
% Tables~\ref{tab:sewer_ml_inference_precision} and \ref{tab:private_inference_precision} compare the robustness of ICAM and baselines when subjected to Post-Training Quantization (PTQ) for FP16 and INT8 inference formats.
% Table~\ref{tab:sewer_ml_inference_precision} shows the comparison of ICAM baselines on the Sewer-ML dataset, which conducted reduction of inference precision.
% Table~\ref{tab:private_inference_precision} shows the comparison of ICAM baselines on the private dataset, which conducted reduction of inference precision.


% To ensure a fair comparison under standardized resource constraints, we prune all baseline models to match the computational complexity (FLOPs) and model size (parameters) of ICAM.
% Table~\ref{tab:sparsity_ratios} details the specific sparsity ratios, determined via a 100-iteration binary search, applied to achieve these constraints.






% -----------------------------------------------------------------------------

% \FloatBarrier
% \paragraph{Comparison under Iso-FLOPs}

% We evaluate each model under the fixed computational budget.
% Table~\ref{tab:sewer_ml_iso_flops} shows the performance comparison of unpruned models on the Sewer-ML dataset.
% % 여기 작성 필요

% Table~\ref{tab:private_iso_flops} summarizes the performance of baselines on the private dataset, which pruned to match the computational complexity (FLOPs) of ICAM.
% Under this iso-FLOPs constraint ($\approx$ 0.18~G), most baselines maintain competitive performance, indicating that L1-norm, FPGM and Wanda pruning can effectively remove filters without severely degrading feature representations.


% \begin{table}[h!]
%     \centering
%     \caption{Comparison under the \textbf{\textit{iso-FLOPs}} constraint. 
%         The \textbf{Target Sparsity} denotes the pruning ratio required to match the ICAM's computational cost ($\approx$ 0.18~G), resulting in the final \textbf{Parameter} count.}
%     \label{tab:flops_specs}
%     \resizebox{0.85\columnwidth}{!}{%
%         \begin{tabular}{l c l c} 
%             \toprule
%             \textbf{Model} & \textbf{Target Sparsity (\%)} & \textbf{Method} & \textbf{Params (M)} \\
%             \midrule
%             \multirow{3}{*}{EfficientNet-B0 \citep{tan2019efficientnet}} & \multirow{3}{*}{57.38} 
%              & L1-norm & \multirow{3}{*}{4.01} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             \multirow{3}{*}{MobileNetV4-S \citep{qin2024mobilenetv4}} & \multirow{3}{*}{32.34}
%              & L1-norm & \multirow{3}{*}{2.50} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             \multirow{3}{*}{Xie et al. \citep{xie2019automatic}} & \multirow{3}{*}{92.14}
%              & L1-norm & \multirow{3}{*}{9.16} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             \multirow{3}{*}{DeiT-Tiny \citep{touvron2021training}} & \multirow{3}{*}{81.69}
%              & L1-norm & \multirow{3}{*}{5.52} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             \multirow{3}{*}{MobileViT-XXS \citep{mehta2021mobilevit}} & \multirow{3}{*}{47.57}
%              & L1-norm & \multirow{3}{*}{0.95} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             ICAM (Ours) & - & - & \textbf{0.03} \\
%             \bottomrule
%         \end{tabular}%
%     }
% \end{table}




% \FloatBarrier
% \paragraph{Comparison under Iso-Params}

% We further simulate strict edge storage constraints by limiting the model size.
% Tables~\ref{tab:swer_ml_iso_params} and \ref{tab:private_iso_params} present the comparison results where all baselines are heavily pruned to match the parameter count of ICAM ($\approx$ 31K parameters).




% \begin{table}[h!]
%     \centering
%     \caption{Comparison under the \textbf{\textit{iso-Params}} constraint. 
%         The \textbf{Target Sparsity} denotes the pruning ratio required to match the ICAM's parmeter count ($\approx$ 0.03~M), resulting in the final \textbf{FLOPs} cost.}
%     \label{tab:params_specs}
%     \resizebox{0.85\columnwidth}{!}{%
%         \begin{tabular}{l c l c} 
%             \toprule
%             \textbf{Model} & \textbf{Target Sparsity (\%)} & \textbf{Method} & \textbf{FLOPs (G)} \\
%             \midrule
%             \multirow{3}{*}{EfficientNet-B0 \citep{tan2019efficientnet}} & \multirow{3}{*}{57.38} 
%              & L1-norm & \multirow{3}{*}{4.01} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             \multirow{3}{*}{MobileNetV4-S \citep{qin2024mobilenetv4}} & \multirow{3}{*}{32.34}
%              & L1-norm & \multirow{3}{*}{2.50} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             \multirow{3}{*}{Xie et al. \citep{xie2019automatic}} & \multirow{3}{*}{92.14}
%              & L1-norm & \multirow{3}{*}{9.16} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             \multirow{3}{*}{DeiT-Tiny \citep{touvron2021training}} & \multirow{3}{*}{81.69}
%              & L1-norm & \multirow{3}{*}{5.52} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             \multirow{3}{*}{MobileViT-XXS \citep{mehta2021mobilevit}} & \multirow{3}{*}{47.57}
%              & L1-norm & \multirow{3}{*}{0.95} \\
%              & & FPGM    & \\
%              & & Wanda   & \\
%             \cmidrule{1-4}
%             ICAM (Ours) & - & - & \textbf{0.18} \\
%             \bottomrule
%         \end{tabular}%
%     }
% \end{table}



% Table~\ref{tab:swer_ml_iso_params} shows the performance of baselines on the Sewer-ML dataset, which pruned to match the parameter level of ICAM.


% Table~\ref{tab:private_iso_params} shows the performance of baselines on the private dataset, which pruned to match the parameter level of ICAM.
% It reveals a distinct pattern under the stricter parameter budget (iso-Params, $\approx$ 0.03~M).
% While CNN-based models such as MobileNetV4-S and Xie et al. \citep{xie2019automatic} exhibit resilience with only moderate performance degradation, the vision transformer-based distillation model DeiT-Tiny suffers a catastrophic performance collapse: its precision, recall, and F1-score drop to 0.00\% across both pruning methods.
% This observation aligns with previous findings by \citet{kuznedelev2023cap}, who experimentally demonstrated that DeiT models experience significantly larger accuracy drops than CNNs when subjected to magnitude-based pruning at high sparsity levels.
% Compared to these baselines, ICAM, which is inherently designed at this compact scale, achieves an F1-score of 87.42\%, outperforming even the best pruned CNN baseline (EfficientNet-B0, FPGM) by a margin of 6.51 percentage points.





\FloatBarrier
\section{Ablation Study}
\label{sec:exp_ablation}

% To validate the effectiveness of the proposed instance-adaptive query Initialization mechanism, we conduct an ablation study on the Private dataset.
% We compare our approach against a baseline using static learnable queries, which is a standard initialization strategy.
% In the static setting, queries are optimized as fixed parameters, independent of the input instance.
% As shown in Table~\ref{tab:ablation_adaptive_query}, the instance-adaptive query strategy yields consistently superior performance compared to the static baseline.
% This improvement suggests that, while static queries capture global dataset statistics, they lack the flexibility to adapt to the diverse visual characteristics of individual sewer defects.
% In contrast, our mechanism conditions the queries on the encoder output via an initial cross-attention step, generating instance-specific queries.
% This allows the lightweight decoder to focus on defect-relevant regions more effectively from the start, compensating for its limited depth.


% \begin{table}[t]
%     \centering
%     % Caption must be OUTSIDE the resizebox to avoid "Illegal unit of measure"
%     \caption{Ablation study on the effectiveness of the instance-adaptive query Initialization mechanism.}
%     \label{tab:ablation_adaptive_query}


%     \resizebox{0.7\columnwidth}{!}{
%         \begin{tabular}{c c c c c}
%             \toprule
%             \textbf{Instance-adaptive} & \multirow{2}{*}{\textbf{Acc. (\%)}} & \multirow{2}{*}{\textbf{Prec. (\%)}} & \multirow{2}{*}{\textbf{Rec. (\%)}} & \multirow{2}{*}{\textbf{F1 (\%)}} \\
%             \textbf{initial query} & & & & \\
%             \midrule
%             \ding{51} & 88.20 & 86.34 & 88.54 & 87.42 \\
%             \ding{55} & 85.84 & 84.28 & 85.35 & 84.81 \\ 
%             \bottomrule
%         \end{tabular}%
%     }

% \end{table}

% \begin{figure}[h!] 
%     \centering
%     % --- (a) Input Image ---
%     \begin{minipage}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/original/62910.mp4_20250416133851_LP__original.pdf}
%         \subcaption{Input image}
%         \label{fig:ablation_input_lp}
%     \end{minipage}
%     \hfill 
%     % --- (b) Ours (Input-Conditioned) ---
%     \begin{minipage}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/ICAM/62910.mp4_20250416133851_LP__avg.pdf}
%         \subcaption{Ours (Instance-adaptive)}
%         \label{fig:ablation_ours}
%     \end{minipage}
%     \hfill 
%     % --- (c) Baseline (Static) ---
%     \begin{minipage}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/ablation/62910.mp4_20250416133851_LP__avg.pdf}
%         \subcaption{Baseline (Static queries)}
%         \label{fig:ablation_static}
%     \end{minipage}

%     \vspace{-0.5em}
%     \caption{(a) Input sewer image showing a Lateral Protruding defect (LP), defined as an anomaly where a lateral connection pipe intrudes into the interior of the primary sewer line. (b) Our instance-adaptive strategy enables the model to focus on the protruded pipe. (c) The static baseline fails to focus on the defect.}
%     \label{fig:ablation_LP}
% \end{figure}



% \begin{figure}[h!] 
%     \centering
%     % --- (a) Input Image ---
%     \begin{minipage}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/original/12-08410-B.mp4_20190321182857_CL_original.pdf}
%         \subcaption{Input image}
%         \label{fig:ablation_input_cl}
%     \end{minipage}
%     \hfill 
%     % --- (b) Ours (Input-Conditioned) ---
%     \begin{minipage}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/ICAM/12-08410-B.mp4_20190321182857_CL_avg.pdf}
%         \subcaption{Ours (Instance-adaptive)}
%         \label{fig:ablation_ours}
%     \end{minipage}
%     \hfill 
%     % --- (c) Baseline (Static) ---
%     \begin{minipage}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/ablation/12-08410-B.mp4_20190321182857_CL_avg.pdf}
%         \subcaption{Baseline (Static queries)}
%         \label{fig:ablation_static}
%     \end{minipage}

%     \vspace{-0.5em}
%     \caption{(a) Input sewer image showing Crack. Longitudinal (CL), defined as a crack that runs parallel to the pipe's axis. (b) Our instance-adaptive strategy enables the model to focus on the areas where pipe cracks are present. (c) The static baseline does not detect all of these areas.}
%     \label{fig:ablation_CL}
% \end{figure}

% \begin{figure}[h!] 
%     \centering
%     % --- (a) Input Image ---
%     \begin{minipage}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/original/1753901.mp4_20230105085902_TO__original.pdf}
%         \subcaption{Input image}
%         \label{fig:ablation_input_to}
%     \end{minipage}
%     \hfill 
%     % --- (b) Ours (Input-Conditioned) ---
%     \begin{minipage}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/ICAM/1753901.mp4_20230105085902_TO__avg.pdf}
%         \subcaption{Ours (Instance-adaptive)}
%         \label{fig:ablation_ours}
%     \end{minipage}
%     \hfill 
%     % --- (c) Baseline (Static) ---
%     \begin{minipage}[b]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/ablation/1753901.mp4_20230105085902_TO__avg.pdf}
%         \subcaption{Baseline (Static queries)}
%         \label{fig:ablation_static}
%     \end{minipage}

%     \vspace{-0.5em}
%     \caption{(a) Input sewer image showing Temporary Obstructions (TO), defined as obstructions that are not permanently attached or sedimented on the pipe wall and can be removed. (b) Our instance-adaptive strategy enables the model to focuses on distinct parts of the obstruction. (c) The static baseline exhibits diffuse attention patterns, covering nearly half of the background area.}
%     \label{fig:ablation_TO}
% \end{figure}

% To complement these quantitative results, we visualize the cross-attention maps extracted from the final decoder layer in Figures~\ref{fig:ablation_LP}, \ref{fig:ablation_TO}, and \ref{fig:ablation_CL}.
% For a representative view, we average the attention weights across all heads, where warm colors indicate higher attention weights.
% The visualizations show that the model with input-conditioned initial queries assigns significantly higher attention weights to the regions corresponding to the defect codes compared to the static query baseline.
% This qualitative evidence confirms that our instance-specific query initialization enables the model to localize and focus on critical defect features more accurately, leading to the observed performance gains.


% \section{Conclusion}\label{sec:con}

% In this paper, we proposed the Instance-adaptive Cross-Attention Model (ICAM) to address the challenge of deploying accurate deep learning–based sewer inspection algorithms on resource-constrained platforms such as edge devices.
% ICAM leverages an Instance-adaptive query Initialization mechanism to efficiently extract defect-relevant features with minimal computational overhead.
% We compared ICAM against a variety of baseline models, including domain-tailored architectures as well as lightweight CNNs and ViTs designed for deployment on mobile devices.
% Furthermore, we evaluated not only performance metrics but also efficiency metrics—such as FLOPs, model size, latency, and peak memory usage on the Sewer-ML dataset and self-collected private sewer pipeline dataset deployed on a Raspberry Pi 5, in order to assess practical applicability in real-world environments.
% As a result, ICAM consistently outperformed the baselines, highlighting its effectiveness as a practical solution for automated sewer inspection and helping to bridge the gap between methods designed for high-performance hardware and deployment in resource-limited environments.
% Future work will explore multi-class inspection models that detect diverse defect types using lightweight deep learning architectures.
% -----------------------------------------------------------------------------




% \section*{Acknowledgement}

% This work was supported by the National Research
% Foundation of Korea (NRF) Grant funded by the Korean Government (Ministry of Science and Information \& Communications
% Technology, MSIT) (Nos. 2019R1A2C2002358, 2022R1F1A1074393, and RS-2023-00208412).

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-harv} 
\bibliography{ref}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% \begin{thebibliography}{ref}
%% \bibitem[Author(year)]{label}
%% Text of bibliographic item
% \bibitem[ ()]{}
% \end{thebibliography}

\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'